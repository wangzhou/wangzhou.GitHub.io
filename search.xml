<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>APUE学习笔记(第三章)</title>
    <url>/APUE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%89%E7%AB%A0/</url>
    <content><![CDATA[<ol>
<li><p>文件相关的几个数据结构：文件描述符，文件表，i node.<br>每个进程有一个文件描述符表(用户态中), 用来指代这个进程打开的文件，文件描述符<br>是0，1，2…这样的数字，对应的是一个个打开的文件。read(), write(), lseek()<br>中指代文件的入参用文件描述符表。</p>
<p>文件表是一个内核数据结构，每个进程打开的每个文件在内核里都有对应的一个结构<br>来描述，所有的这样的结构组成文件表。每个这样的结构中包括，文件状态标志，文件<br>偏移量，i node指针。</p>
<p>open(path, oflag, …); open函数的第二个入参，可以设置只读，只写等参数。这<br>些参数最终被写到上面的文件状态标志中。当再调用read, write时，文件状态标志<br>将起到判定的作用。</p>
<p>这三个数据结构的关系见APUE上的图。</p>
</li>
<li><p>缓冲概念：从硬盘上读数据，文件系统会做缓冲，这个在内核中。用户态的I/O也会坐<br>缓冲，这个存在用户态的buffer中。数据写入硬盘，会经过用户态缓冲，内核文件系统<br>缓冲，添加到写队列等步骤。</p>
</li>
<li><p>多个进程写一个文件：如果是都在文件末尾写入，则lseek到文件末尾，再write写入，<br>会出现竞争。用open(..,O_APPEND,..)可以避免。O_APPEND会设置文件状态标志，write<br>时会查看文件状态标志，如果有O_APPEND则在文件末尾写入。</p>
</li>
<li><p>./a.out 6&gt;test<br>在文件描述符6上打开文件test. 编译下面的代码，运行上面的命令。可以开到文件<br>test中写入“12345”。以此类推：ls 2&gt;&gt;log, 在文件描述符2上打开文件，文件描述<br>符2一般是错误错误输出，所以这样的命令把ls命令的错误输出写入log文件。<br>command 6&lt;&gt;log, 是运行command的时候，在文件描述符6上，以读写的方式打开文件log。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">#include &lt;errno.h&gt;</span><br><span class="line"></span><br><span class="line">char buffer[5] = &#123;&#x27;1&#x27;,&#x27;2&#x27;,&#x27;3&#x27;,&#x27;4&#x27;,&#x27;5&#x27;&#125;;</span><br><span class="line"></span><br><span class="line">int main(void)</span><br><span class="line">&#123;</span><br><span class="line">	int ret;</span><br><span class="line">	int file;</span><br><span class="line"></span><br><span class="line">	/* 把下面的5改成其他大于5的数，写入文件时会出错</span><br><span class="line">                * 貌似是要输出几个buffer中的数据，这里的count就是多少，</span><br><span class="line">                * 否则会出错</span><br><span class="line">                */		</span><br><span class="line">	ret = write(6, buffer, 5);</span><br><span class="line">	if (ret == -1) &#123;</span><br><span class="line">		perror(&quot;read read_test&quot;);</span><br><span class="line">		return -1;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>fcntl函数, 提供对文件描述符标志和文件状态标志的查询和修改。<br>不清楚文中所说的文件描述符是什么? 下面得到的结果均是0</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fd = fcntl(0, F_GETFD);	</span><br><span class="line">printf(&quot;fd: %d\n&quot;, fd);</span><br><span class="line">fd = fcntl(1, F_GETFD);	</span><br><span class="line">printf(&quot;fd: %d\n&quot;, fd);</span><br><span class="line">fd = fcntl(2, F_GETFD);	</span><br><span class="line">printf(&quot;fd: %d\n&quot;, fd);</span><br></pre></td></tr></table></figure>
<p>文件状态标志位和上面的描述一致。fcntl可以复制文件描述符：fcntl(fd, F_DUPFD, fd_min)，<br>大于或等于fd_min的文件描述符将指向fd所指向的文件表项，相当于把fd复制到了fd_min</p>
</li>
</ol>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title>APUE学习笔记(第九章)</title>
    <url>/APUE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B9%9D%E7%AB%A0/</url>
    <content><![CDATA[<p>最早的终端是电传打字机输入，纸带输出, 后来发展为键盘输入，显示器输出。终端在<br>linux下的文件是/dev/tty*, 虚拟终端/dev/tty1~6。之所以叫虚拟终端是因为/dev/tty1~6<br>公用一个物理的键盘和显示器。</p>
<p>终端无法输出系统启动的信息，所以需要用控制台输出系统启动信息用来调试系统。<br>控制台在linux下的文件是/dev/console, 一般控制台的物理接入是串口。</p>
<p>串口在linux下的文件是/dev/ttyS*</p>
<p>之前的终端一般由串口接入，由网络也可以接入终端，这个一般叫伪终端。像ubuntu上使用<br>terminal程序接入的系统的shell所对应的终端是伪终端。从这个伪终端上看，就好像是<br>通过一个终端接入系统的。在linux下伪终端一般表示为pts*</p>
<p>伪终端主设备和伪终端从设备是伪终端相关的两个重要概念。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                       (伪终端主设备)</span><br><span class="line">telnet client          telnet server-------------+</span><br><span class="line">      |                     |                    |</span><br><span class="line">networking driver    networking driver           |</span><br><span class="line">      |                     |                    |                  </span><br><span class="line">networking card      networking card             |    pts/0(伪终端从设备) pts/1(伪终端从设备) ...</span><br><span class="line">      |                     |</span><br><span class="line">      +-----internet -------+</span><br></pre></td></tr></table></figure>

<p>下面是所做的一些测试(ubuntu 14.04)</p>
<hr>
<p>who可以查看都现在系统中的登录情况</p>
<p>sherlock@T440:~/notes$ who<br>sherlock tty2         2015-12-14 14:57<br>sherlock tty1         2015-12-14 14:55<br>sherlock :0           2015-12-14 14:44 (:0)<br>sherlock pts/1        2015-12-14 14:48 (:0)<br>sherlock pts/13       2015-12-14 14:56 (:0)</p>
<hr>
<p>虚拟终端：tty1, tty2… tty6<br>echo “test” &gt; /dev/tty1<br>打开虚拟终端tty1（ctrl + alt + f1）, 可以看见在屏幕上输出了”test”</p>
<hr>
<p>在ubuntu terminal中打开的终端是伪终端，在/dev/pts/*是他们的设备文件<br>echo “test1” &gt; /dev/pts/0<br>在伪终端上有”test1”输出</p>
<p>进程组, 进程组ID, 作业控制，前后台进程都是和进程相关的概念, 用ps命令可以查看这些<br>信息<br>(ctrl + z 挂起作业中的前台进程组中的所有进程)</p>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">e.g. ps -alxf (desktop ubuntu 14.04)</span><br><span class="line"></span><br><span class="line">PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND</span><br><span class="line">...</span><br><span class="line">2113  6613  2296  2296 ?           -1 Sl    1000   0:17          \_ gnome-terminal</span><br><span class="line">6613  6620  2296  2296 ?           -1 S     1000   0:00          |   \_ gnome-pty-helper</span><br><span class="line">6613  6621  6621  6621 pts/0     7501 Ss    1000   0:00          |   \_ bash</span><br><span class="line">6621  7501  7501  6621 pts/0     7501 R+    1000   0:00          |   |   \_ ps -ajxf</span><br><span class="line">6613  6648  6648  6648 pts/18    7485 Ss    1000   0:00          |   \_ bash</span><br><span class="line">6648  7485  7485  6648 pts/18    7485 S+    1000   0:00          |   |   \_ vi APUE_note_5.md</span><br><span class="line">6613  7435  7435  7435 pts/20    7497 Ss    1000   0:00          |   \_ bash</span><br><span class="line">7435  7497  7497  7435 pts/20    7497 S+       0   0:00          |       \_ sudo grep -r wang</span><br><span class="line">7497  7498  7497  7435 pts/20    7497 D+       0   0:01          |           \_ grep -r wang</span><br></pre></td></tr></table></figure>
<p>如上图所示, 首先打开了Terminal的程序。Terminal下打开了三个bash, 分别对应三个<br>伪终端：pts/0, pts/18, pts/20. 每个bash和它的自进程都在一个session里(SID=session ID).<br>每个session里的情况又各有不同，比如，SID=6621 它由两个进程组组成(PGID=6621, 7501),<br>而SID=7435的session, 它由两个进程组组成，其中的第二个进程组又有两个进程组成</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>APUE</tag>
        <tag>Linux用户态编程</tag>
      </tags>
  </entry>
  <entry>
    <title>APUE学习笔记(第四章)</title>
    <url>/APUE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%9B%9B%E7%AB%A0/</url>
    <content><![CDATA[<p>这一章将文件系统的相关操作，比如文件，目录，软硬链接等等。要深入的了解这些需要<br>了解linux内核的相关知识。</p>
<ol>
<li><p>用户态和文件相关的基本数据结构是：struct stat<br>可以用int stat(const char *pathname, struct stat *buf); 读出stat的内容。<br>struct stat中的内容主要是对应文件的属性，一般从对应文件的inode节点得到这些<br>内容。inode节点是文件系统的内容，下面的部分会提到。</p>
</li>
<li><p>linux有多种文件类型。目录也是一种文件，其内容是目录中的文件名。另外符号链接，<br>设备文件，管道也是文件。</p>
</li>
<li><p>各种ID: 每个进程相关的ID</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+--------------+-----------+------------+</span><br><span class="line">|  实际用户ID   | 实际组ID   |           |</span><br><span class="line">+--------------+-----------+------------+</span><br><span class="line">|  有效用户ID   | 有效组ID  | 附加组ID   |</span><br><span class="line">+--------------+-----------+------------+</span><br><span class="line">|  ...         | ...       |  ...       |</span><br><span class="line">+--------------+-----------+------------+</span><br></pre></td></tr></table></figure>
<p> 一般决定文件访问权限的是第二行的有较<em><strong>，一般实际</strong></em> = 有效***</p>
<p> 除了和进程相关的ID, 每个文件有它自己的所有者和所有组。执行文件操作时，会<br> 比较进程的有效***和文件的所有者和所有组，来判定是否可以执行。</p>
<p> 这里有个特殊的文件属性位，当设置该位的时候，执行该文件的进程会自动把自己<br> 的有效用户ID设置成文件的所有这ID。文件的这个位一般用来提升执行权限，完成<br> 一些功能。比如/usr/bin/passwd</p>
<p> test@Latitude-D630:~$ ls -l <code>which passwd</code><br> -rwsr-xr-x 1 root root 45420  2月 17  2014 /usr/bin/passwd</p>
<p> 一般的用户可以用passwd去改自己的密码，但是修改密码要写/etc/shadow文件，</p>
<p> test@Latitude-D630:~$ ls -l /etc/shadow<br> -rw-r—– 1 root shadow 1513 12月 21 23:59 /etc/shadow</p>
<p> 可以看到/etc/shadow的所有者是root，且其他人不可以写。可以看到<br> /usr/bin/passwd的可执行位是’s’, 这表示文件/usr/bin/passwd的特殊文件属性位<br> 被设定，这样当普通用户执行passwd命令时当前进程把自己的有效用户ID提升成<br> root, 执行完后再恢复自己的ID。给文件加该属性要用chmod u+s file。</p>
</li>
<li><p>umask.<br>文件和目录创建时的默认属性一般是666和777。umask值可以去掉对应的读写执行<br>权限，比如现在的umask值是：022，那么同组用户和其他用户没有了写权限。<br>现在新建一个文件，得到的属性是644。使用umask命令可以查看当前shell的umask值，<br>使用umask *** 可以修改当前的umask值。</p>
</li>
<li><p>文件系统[1]</p>
</li>
</ol>
<p>  有关文件系统的进一步介绍，可以参考[1]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+----------------+------------------+--------------------+</span><br><span class="line">|      分区      |       分区       |      分区          |</span><br><span class="line">+----------------+------------------+--------------------+</span><br><span class="line"></span><br><span class="line">         boot block</span><br><span class="line">         +--------+----------+-----------+-------------+--------------+</span><br><span class="line">文件系统 | 自举块 |  超级块  |  柱面1    |    柱面2    |     ...      |</span><br><span class="line">         +--------+----------+-----------+-------------+--------------+</span><br><span class="line"></span><br><span class="line">+-------------+-------------+------------+-----------+---------+----------+</span><br><span class="line">| 超级块副本  |  配置信息   |  i节点图   |  块位图   |  i节点  |  数据块  |</span><br><span class="line">+-------------+-------------+------------+-----------+---------+----------+</span><br></pre></td></tr></table></figure>
<p>  上图根据APUE插图得到，一个磁盘可以划分几个分区，每个分区上可以部署不同的<br>  文件系统，文件系统的每个柱面中又有超级块副本，…，i节点，数据块等内容</p>
<p>  主要关注i节点和数据块，i节点存放文件的属性，数据块存放文件的内容。</p>
<p>  文件的根文件系统和磁盘上的根文件系统有说明区别？<br>  只是存储数据的介质不同，在挂载文件系统的时候加入的数据读写的回调函数不同。</p>
<p>  在ubuntu的根文件系统中有/boot/initrd.img-XXX, /boot/initrd.img-XXX是一个最小<br>  根文件系统, 它基本作用是在内核起来之时挂载的第一个文件系统，在这个文件系统中<br>  有必要的工具加载挂载磁盘文件系统需要的驱动。之后内核就可以挂载磁盘中真正的<br>  根文件系统了。这样内核里一开始就不用放相关的驱动了，initrd.img文件系统里，<br>  其实是使用的udev的机制，动态的识别系统外设，然后动态的加载所需要的内核驱动。<br>  之前内核驱动都已经以模块的形式编译。有关initrd.img的内容可以参考内核文档[2]</p>
<p>  制作一个文件上的文件系统, 相关的命令：<br>  dd mkfs.ext4 mount cp umount</p>
<ol start="6">
<li><p>符号链接和硬链接</p>
<p>创建一个B指向A的软连接, 可以建立指向文件和目录的软连接。软连接和硬连接的实质<br>区别是，硬链接只有一个inode节点，软链接的文件/目录本身的inode和软链接本身的<br>inode不同。</p>
<p>ln -s A B  /* A &lt;– B */</p>
</li>
<li><p>文件的时间有：文件最后访问时间，文件最后修改时间，文件inode节点最后修改时间。</p>
</li>
</ol>
<p>reference:<br>[1] Linux C编程一站式学习<br>[2] linux/Documentation/initrd.txt</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title>APUE学习笔记(第一章)</title>
    <url>/APUE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%80%E7%AB%A0/</url>
    <content><![CDATA[<ol>
<li><p>每个进程有一个工作目录，所有相对目录都从这个工作目录开始解释。可以用chdir改<br>变这个工作目录。</p>
</li>
<li><p>用户态程序出错处理：<br>GNU c库中会定义一个errno的全局变量(ubuntu下在/usr/include/errno.h中)。函数<br>发生错误的时候可以把错误值写入这个errno，用以指名是什么错误。比如read()错误<br>时返回-1，我们去查errno的值，知道发生了什么错误。</p>
<p>#include &lt;string.h&gt;<br>char *strerror(int errnum); 输入errno，输出和errno相关的字符串的指针。</p>
<p>#include &lt;stdio.h&gt;<br>void perror(const char *msg);<br>输入是一个自定义的字符串，输出输入的字符串 + “：” + errno对应的字符串。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title>ARM64 qemu native build</title>
    <url>/ARM64-qemu-native-build/</url>
    <content><![CDATA[<ol start="0">
<li><p>cd qemu; mkdir build; cd build;</p>
</li>
<li><p>install pkg-config</p>
</li>
<li><p>install zlib1g-dev zlib1g</p>
</li>
<li><p>ERROR: glib-2.22 gthread-2.0 is required to compile QEMU<br>install libglib2.0-dev</p>
</li>
<li><p>ERROR: pixman &gt;= 0.21.8 not present. Your options: …<br>install libpixman-1-dev</p>
</li>
<li><p>ERROR: DTC (libfdt) version &gt;= 1.4.0 not present. Your options: …<br>install libfdt-dev</p>
</li>
<li><p>./configure –target-list=aarch64-softmmu –enable-fdt –enable-kvm –disable-werror</p>
</li>
<li><p>make</p>
</li>
<li><p>make install # better uninstall previous qemu version if had one.</p>
</li>
<li><p>qemu-system-aarch64 –version</p>
<p> root@linaro-developer:~/qemu/build# qemu-system-aarch64 –version<br> QEMU emulator version 2.5.50, Copyright (c) 2003-2008 Fabrice Bellard</p>
</li>
</ol>
<p>In aarch64 ubuntu-16.04, there is doc: cnblogs.com/from-zero/p/14327440.html</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>ARM64内存屏障</title>
    <url>/ARM64%E5%86%85%E5%AD%98%E5%B1%8F%E9%9A%9C/</url>
    <content><![CDATA[<p> 要理解内存屏障，要知道memory的一些特性，这里的memory不是只指内存。而是从CPU角度<br> 看到的存储空间。如果CPU对一系列的指令执行是严格串行的，我们是不用外加上面内存<br> 屏障的。比如：</p>
<pre><code> str x1 [x2]   // A
str x2 [x3]   // B
</code></pre>
<p> A指令完全执行完，B指令才执行。这样根本不需要内存屏障指令的介入。</p>
<p> 但是现在处理器和内存系统之间的速度差距已经非常大，如果要上面的A执行完成才执行B<br> cpu要消耗大量的时间干等在那里。为了缓解CPU和内存系统间的速度差距，同时也为了不断<br> 提升CPU的效率，现代CPU存在很多指令执行上的技术，导致的结果是指令执行的结果和之前<br> 的逻辑已经不一样了。</p>
<p> CPU上存在着多种指令执行的技术，其中导致需要加上内存屏障执行的是CPU上的指令乱序<br> 执行。</p>
<p> 为了解释清楚CPU执行乱序执行需要引入两个基本的概念，一个是内存类型(memory type),<br> 另外一个是master/slave.</p>
<p> 内存类型定义的CPU和内存相互作用时的一些性质，内存类型只有normal和device两种,<br> normal基本上可以对应DDR，device基本上可以对应设备的MMIO. normal的内存可以配置成<br> cacheable和non-cacheable, 其中cacheable的内存属性又可以进一步配置shareability的属性。<br> device的内存的属性使用GRE表述，G是gather，R是re-order, E是write early aknowledge,<br> 每种属性可以是nX(e.g. nG), 就是不支持的意思。关于normal和device下面的诸多属性,<br> 可以查阅本文开始时提到的ARM编程手册13章中的内容。</p>
<p> master和slave是芯片里面的概念，master是一个动作的发起者，slave是一个动作的接收<br> 者。一般的一个芯片里，master有各个core，以及外设的DMA控制器。slave有内存和MMIO.</p>
<p> 基于以上的概念，我们看内存屏障为啥有存在的意义。master(CPU, 不确定DMA是不是这样)<br> 对normal内存的操作，存在很多提升效率的处理(多发射，乱序执行，执行预取…).<br> 我们回到乱序执行上来。ARMv8的CPU对指令乱序执行的几条规则是这样的:</p>
<ol>
<li><p>在单一的core上，指令完成的顺序是串行的。注意，master收到slave的(有时不一定是<br>slave, 比如，device内存的E属性)完成信息为一个执行完成。</p>
</li>
<li><p>在单一的core上下发的指令，slave上完成的顺序是无法保证串行的。</p>
</li>
<li><p>多个master上看到的操作是不保序的。</p>
</li>
</ol>
<p> 这样就会导致:</p>
<p> 在一个core上:</p>
<pre><code> 等待A条件出现
 A条件出现后执行B动作
</code></pre>
<p>  如果B动作先被执行，则可能出错。所以要在之间加上内存屏障：</p>
<pre><code> 等待A条件出现
 内存屏障
 A条件出现后执行B动作
</code></pre>
<p>  在多个core上:</p>
<pre><code> core 1           core 2

 DMA              check flag
 set flag         get date from DMA range
  
</code></pre>
<p>  core1的逻辑是先用DMA搬数据，搬完数据后设立一个标记位。core2不断的在检测标记<br>  位，当检测到标记位的时候，core2就可以使用DMA搬好的数据了。</p>
<p>  但是，core1的DMA和set flag两个操作可能是乱序执行的，可能在core2看来flags已经<br>  置位了，但是其实DMA的数据还没有搬完, 这时，如果core2去使用数据，就有可能出错。<br>  正确的做法是在DMA和set flag执行加上内存屏障，确保DMA完成了，再去set flag.<br>  注意，这里其实就是上面的第三点。另外，这里和cache一致性没有关系, 如果，我们<br>  单独看dma，或者单独看flag，两者各自都是硬件保证cache一致性的。</p>
<p>  具体来说ARMv8上的内存屏障执行有ISB，DMB，DSB。ISB和指令相关，后面两个和内存<br>  访问相关。具体的区别可以查文章开头提到的书。</p>
<p>  具体驱动代码, 可以参考下arm64下几组read/write的实现，代码在arch/arm64/include/asm/io.h.<br>  可以看到同一个write函数有一下三种类型:</p>
<pre><code>__raw_writeb
writeb_relaxed(v,c)
writeb(v,c)    
</code></pre>
<p>  第一个是str指令的封装，第二个用于I/O内存也是str的封装，第三个可以用于normal的<br>  内存读写，其实是真是write之前加了一个DSB的内存屏障指令。</p>
<p>  未完待续…</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>ARM64</tag>
        <tag>内存屏障</tag>
      </tags>
  </entry>
  <entry>
    <title>Build your own rootfs by buildroot</title>
    <url>/Build-your-own-rootfs-by-buildroot/</url>
    <content><![CDATA[<ol>
<li><p>git clone git://git.buildroot.net/buildroot</p>
</li>
<li><p>cd buildroot</p>
</li>
<li><p>make defconfig</p>
</li>
<li><p>make menuconfig to choose special tools, e.g. numactl</p>
</li>
<li><p>make menuconfig to choose BR2_TARGET_ROOTFS_CPIO=y and<br>Compression method to gzip.</p>
</li>
<li><p>make menuconfig to choose your arch, e.g. my arch is BR2_aarch64</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> Note: if you choose a wrong arch, after changing to right arch config,</span><br><span class="line">       you should make clean, than make, otherwise you may meet something</span><br><span class="line">wrong like: &quot;No working init found&quot;.</span><br></pre></td></tr></table></figure></li>
<li><p>wait and you will find built minirootfs in buildroot/output/images/<br>you can use rootfs.cpio.gz as rootfs here.</p>
</li>
</ol>
<p>Note: 编译glib(BR2_PACKAGE_LIBGLIB2), 依赖BR2_TOOLCHAIN_BUILDROOT_WCHAR, BR2_USE_WCHAR</p>
<p>如上是之前编译buildroot的一个笔记，其实buildroot在交叉编译构建文件系统的情况还是<br>非常好用的，因为buildroot里包含了很多基本库，如果你的app依赖了第三方的库，在交叉编译<br>的时候，一般你要先交叉编译依赖的库，然后再交叉编译app的时候链接之前交叉编译出来的<br>库，一两个这样的依赖库还好，要是有比较多的依赖库就会比较麻烦。如果使用buildroot，<br>只要在buildroot配置的时候打开依赖库的配置就好。</p>
<p>自己的app如何集成在buildroot编译生成的小系统里。一个好用的方法是使用buildroot的<br>override功能。它的基本逻辑是，现在buildroot的编译配置体系里加如你想编译的包的配置，<br>如下的patch中，我们加了一个叫devmmu的包的配置，这个包的配置基本是空的，然后我们<br>在buildroot的根目录下发放一个local.mk的文件，并在里指明devmmu这个包的源码目录，<br>相关的写法一定要按照<package_name>_OVERRIDE_SRCDIR = <package_path>的写法。<br>全部配置好后，在buildroot make menuconfig的时候把devmmu选上，make编译的时候就会<br>去指定的目录里找devmmu的代码，并编译安装到生成的小文件系统里。如果后面改了devmmu<br>的源码，使用make devmmu-rebuild all可以只便宜安装devmmu。</package_path></package_name></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff --git a/local.mk b/local.mk</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000000..2775c16f6b</span><br><span class="line">--- /dev/null</span><br><span class="line">+++ b/local.mk</span><br><span class="line">@@ -0,0 +1 @@</span><br><span class="line">+DEVMMU_OVERRIDE_SRCDIR = /home/wangzhou/devmmu-user/</span><br><span class="line">diff --git a/package/Config.in b/package/Config.in</span><br><span class="line">index 82b28d2835..bcb1649da7 100644</span><br><span class="line">--- a/package/Config.in</span><br><span class="line">+++ b/package/Config.in</span><br><span class="line">@@ -2524,4 +2524,6 @@ menu &quot;Text editors and viewers&quot;</span><br><span class="line"> 	source &quot;package/vim/Config.in&quot;</span><br><span class="line"> endmenu</span><br><span class="line"> </span><br><span class="line">+source &quot;package/devmmu/Config.in&quot;</span><br><span class="line">+</span><br><span class="line"> endmenu</span><br><span class="line">diff --git a/package/devmmu/Config.in b/package/devmmu/Config.in</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000000..4c7f9edcc4</span><br><span class="line">--- /dev/null</span><br><span class="line">+++ b/package/devmmu/Config.in</span><br><span class="line">@@ -0,0 +1,4 @@</span><br><span class="line">+config BR2_PACKAGE_DEVMMU</span><br><span class="line">+	bool &quot;devmmu&quot;</span><br><span class="line">+	help</span><br><span class="line">+		DevMMU testsuit</span><br><span class="line">diff --git a/package/devmmu/devmmu.mk b/package/devmmu/devmmu.mk</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000000..c02eedf5c3</span><br><span class="line">--- /dev/null</span><br><span class="line">+++ b/package/devmmu/devmmu.mk</span><br><span class="line">@@ -0,0 +1,12 @@</span><br><span class="line">+################################################################################</span><br><span class="line">+#</span><br><span class="line">+# devmmu</span><br><span class="line">+#</span><br><span class="line">+################################################################################</span><br><span class="line">+</span><br><span class="line">+DEVMMU_VERSION = 0.1</span><br><span class="line">+DEVMMU_SOURCE=</span><br><span class="line">+DEVMMU_INSTALL_STAGING=NO</span><br><span class="line">+DEVMMU_DEPENDENCIES=</span><br><span class="line">+</span><br><span class="line">+$(eval $(autotools-package))</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>rootfs</tag>
      </tags>
  </entry>
  <entry>
    <title>CPU微架构里的Flush概念</title>
    <url>/CPU%E5%BE%AE%E6%9E%B6%E6%9E%84%E9%87%8C%E7%9A%84Flush%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<p>现代CPU为了高性能会进行各种投机的指令执行，反正目前CPU里有空余的硬件资源，放着<br>也是放着，CPU就提前执行一些指令，如果这些指令就是后续应该指令的指令，那么到时候<br>这些指令就可以直接完成，如果这些指令不是后面要执行的指令，CPU就把它们抛弃，也就是<br>常说的把它们刷掉(flush)。</p>
<p>一条指令在CPU的各个部件里可能占据资源，所以flush一条指令，指的是清理掉这条指令在<br>CPU占据的所有资源。如下是一个CPU的内部示意图，flush掉一条就要flush掉一条指令在各<br>个部件里占据的资源。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                            +-------+   +----+</span><br><span class="line">                  +-----+--&gt;|issue q|--&gt;| EX |\</span><br><span class="line">                  |issue|   +-------+   +----+ \</span><br><span class="line">                  |logic|                       \</span><br><span class="line">+----+   +----+   |     |   +-------+   +----+   \+-----+    +----+</span><br><span class="line">| IF |--&gt;| ID |--&gt;|     |--&gt;|issue q|--&gt;| EX |---&gt;| MEM |---&gt;| WB |</span><br><span class="line">+----+   +----+   |     |   +-------+   +----+   /+-----+    +----+</span><br><span class="line">                  |     |                       /</span><br><span class="line">                  |     |   +-------+   +----+ /</span><br><span class="line">                  +-----+--&gt;|issue q|--&gt;| EX |/</span><br><span class="line">                            +-------+   +----+</span><br><span class="line">          +--+--+--+--+--+--+--+--+--+--+--+--+--+--+  old</span><br><span class="line">     ROB  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  </span><br><span class="line">          +--+--+--+--+--+--+--+--+--+--+--+--+--+--+</span><br><span class="line">                 ^        ^        ^  \-------------/</span><br><span class="line">              allocate  flush    commit  retired</span><br></pre></td></tr></table></figure>

<p>flush一般要刷掉一堆指令，我们从ROB的角度去看，如上图所示，在某个时刻，发现要从<br>flush位置开始flush，那么flush到allocate位置上的所有指令都要刷掉。这里allocate是<br>最新进入流水线的指令的位置。</p>
<p>CPU里触发指令flush的原因有很多，总结起来大概可以有：1. 分支预测错了，要刷掉错误<br>执行的指令；2. load/store冲突，提前指令的load已经load的后续指令要刷掉；3. 异常或<br>中断后要跳到异常处理程序，所以异常或者中断后之前取入流水线的指令也要都刷掉。</p>
<p>和软件不一样，CPU硬件各个部件是在时钟信号的触发下同步执行的，也就是说一拍中可能<br>会产生多个flush请求，那么硬件就要综合计算下要怎么flush，直观上看，应该是从最老的<br>flush点开始做flush，就是从下图最靠右的flush点开始做flush，而且这个flush已经包含<br>了左边的各个flush。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+  old</span><br><span class="line">ROB  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  </span><br><span class="line">     +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+</span><br><span class="line">            ^           ^        ^           ^        ^  \-------------/</span><br><span class="line">         allocate     flush    flush       flush    commit  retired</span><br></pre></td></tr></table></figure>

<p>当有flush请求时，说明检测到了硬件里错误投机的指令，flush这个动作应该马上进行，这<br>样被错误占据的硬件资源就可以被释放出来。但是，有时考虑到硬件实现的问题，也有先打<br>一个flush的标记，在后续合适的时间点再做具体的flush动作。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>Add CentOS yum repo in Redhat system</title>
    <url>/Add-CentOS-yum-repo-in-Redhat-system/</url>
    <content><![CDATA[<p>In Redhat system, you should buy it to use. However, CentOS is<br>free and almost same with related Redhat system. So after we<br>add the CentOS yum repo, then we can directly use Redhat system.<br>(If you already have a CentOS, please use it directly)</p>
<p>We can add the repo configuration file in /etc/yum.repos.d/ like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># CentOS-Base.repo</span><br><span class="line">#</span><br><span class="line"># The mirror system uses the connecting IP address of the client and the</span><br><span class="line"># update status of each mirror to pick mirrors that are updated to and</span><br><span class="line"># geographically close to the client.  You should use this for CentOS updates</span><br><span class="line"># unless you are manually picking other mirrors.</span><br><span class="line">#</span><br><span class="line"># If the mirrorlist= does not work for you, as a fall back you can try the </span><br><span class="line"># remarked out baseurl= line instead.</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">[base]</span><br><span class="line">name=CentOS-$releasever - Base</span><br><span class="line">baseurl=http://mirror.centos.org/altarch/$releasever/os/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</span><br><span class="line">       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7-aarch64</span><br><span class="line"></span><br><span class="line">#released updates </span><br><span class="line">[updates]</span><br><span class="line">name=CentOS-$releasever - Updates</span><br><span class="line">baseurl=http://mirror.centos.org/altarch/$releasever/updates/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</span><br><span class="line">       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7-aarch64</span><br><span class="line"></span><br><span class="line">#additional packages that may be useful</span><br><span class="line">[extras]</span><br><span class="line">name=CentOS-$releasever - Extras</span><br><span class="line">baseurl=http://mirror.centos.org/altarch/$releasever/extras/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</span><br><span class="line">       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7-aarch64</span><br></pre></td></tr></table></figure>
<p>but it also goes wrong.</p>
<p>here we should replace the variable “$releasever” and “$basearch” to the<br>specific one which we want to use. you can search in <span class="exturl" data-url="aHR0cDovL21pcnJvci5jZW50b3Mub3JnL2FsdGFyY2gv">http://mirror.centos.org/altarch/<i class="fa fa-external-link-alt"></i></span><br>to see which repo you will use.</p>
<p>In my case, I will use:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[base]</span><br><span class="line">name=CentOS-base</span><br><span class="line">baseurl=http://mirror.centos.org/altarch/7.4.1708/os/aarch64/</span><br></pre></td></tr></table></figure>
<p>and I also remove gpgcheck related lines.</p>
<p>It works fine with me :)</p>
]]></content>
      <tags>
        <tag>包管理</tag>
        <tag>RPM/YUM</tag>
      </tags>
  </entry>
  <entry>
    <title>Compile DPDK in ARMv8 machine</title>
    <url>/Compile-DPDK-in-ARMv8-machine/</url>
    <content><![CDATA[<p>Hardware: D05 board.<br>System: Linux 157 4.11.0-45.el7.aarch64 aarch64 GNU/Linux<br>        CentOS Linux release 7.4.1708 (AltArch) </p>
<ol start="0">
<li><p>download dpdk codes<br> git clone git://dpdk.org/dpdk</p>
</li>
<li><p>install kernel header files<br> yum install kernel-headers.aarch64</p>
</li>
<li><p>install libpcap<br> yum install libpcap-devel.aarch64</p>
</li>
<li><p>install numa header files<br> yum install numactl-devel.aarch64</p>
</li>
<li><p>config and make<br> make config T=arm64-armv8a-linuxapp-gcc<br> make</p>
<p> Note: above will put compiled files to dpdk/build</p>
<p> “make install T=arm64-armv8a-linuxapp-gcc” will create a directory<br> named as arm64-armv8a-linuxapp-gcc under dpdk and put compiled files<br> in it.</p>
</li>
<li><p>build the dpdk helloworld example:</p>
<p> export RTE_SDK=~/repos/dpdk<br> export RTE_TARGET=arm64-armv8a-linuxapp-gcc<br> cd examples/helloworld/<br> make</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">note: should use &quot;make install T=arm64-armv8a-linuxapp-gcc&quot; in step4</span><br><span class="line">      above to avoid compile error in step5.</span><br></pre></td></tr></table></figure></li>
<li><p>test helloworld in above build:</p>
<p> (1) enable hugepage</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo 2 &gt; /sys/kernel/mm/hugepages/hugepages-524288kB/nr_hugepages</span><br><span class="line">grep -i huge /proc/meminfo</span><br><span class="line">AnonHugePages:         0 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">HugePages_Total:       2</span><br><span class="line">HugePages_Free:        1</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:     524288 kB</span><br></pre></td></tr></table></figure>
<p> (2) run ./helloworld -c 3 -n 2 in examples/helloworld</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">EAL: Detected 64 lcore(s)</span><br><span class="line">EAL: Detected 4 NUMA nodes</span><br><span class="line">EAL: Multi-process socket /var/run/dpdk/rte/mp_socket</span><br><span class="line">EAL: No free hugepages reported in hugepages-524288kB</span><br><span class="line">EAL: No free hugepages reported in hugepages-524288kB</span><br><span class="line">EAL: 10 hugepages of size 2097152 reserved, but no mounted hugetlbfs found for that size</span><br><span class="line">EAL: Probing VFIO support...</span><br><span class="line">EAL: PCI device 0002:e9:00.0 on NUMA socket -1</span><br><span class="line">EAL:   Invalid NUMA socket, default to 0</span><br><span class="line">EAL:   probe driver: 8086:10fb net_ixgbe</span><br><span class="line">EAL: PCI device 0002:e9:00.1 on NUMA socket -1</span><br><span class="line">EAL:   Invalid NUMA socket, default to 0</span><br><span class="line">EAL:   probe driver: 8086:10fb net_ixgbe</span><br><span class="line">hello from core 1</span><br><span class="line">hello from core 0</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <tags>
        <tag>ARM64</tag>
        <tag>DPDK</tag>
      </tags>
  </entry>
  <entry>
    <title>CPU核中断设计基本逻辑</title>
    <url>/CPU%E6%A0%B8%E4%B8%AD%E6%96%AD%E8%AE%BE%E8%AE%A1%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>CPU根据一条一条指令执行程序，如果没有外界刺激CPU就这样周而复始的运行下去。可以看到<br>这样的程序CPU总是按照预定的程序同步的执行下去。</p>
<p>CPU在设计的时候会预留一些外部信号的输入接口，并从硬件上定义当这些接口有信号时CPU<br>的执行行为。一般的，当这些接口上有信号时，CPU把执行流跳到预定义的指令地址。</p>
<p>CPU执行被外部信号中断，进而跳到预定义地址执行指令，这个行为看似受到硬件或者外部<br>信号的配置，其实实际上所有控制的主体其实还是软件。软件决定当一个外部信号发生时CPU<br>的所有行为。</p>
<p>当我们仔细观察“软件”这个概念，就会发现CPU上可能分时运行着多个软件，比如CPU上可能<br>一段时间在运行BISO的代码，一段时间在运行Host内核的代码，一段时间在运行Guest内核的<br>代码，一段时间在运行用户态的代码。这些代码，有些在逻辑上都是相互隔离的软件。</p>
<p>我们从原始需求上去看CPU需要被中断这个诉求，实际上这些需求并不来自硬件，而是真实的<br>来自CPU上运行的各种软件。比如，BIOS在处理一些任务的时候需要一个精确的计时器，CPU<br>用指令轮询的办法没法做精确的计时，这样BIOS就需要设置一个计时器，当计时器时间到了时，<br>发中断中断当前CPU，需要注意的是，这个中断是发给BIOS这个软件的，CPU架构设计上要通过<br>各种软硬件设计保证如上的中断可以发给BIOS。</p>
<p>我们再看一个例子，比如CPU上运行很多虚拟机，每个虚拟机上都要收自己虚拟机上的外设的<br>中断。每个虚拟机运行的时候，都可以配置当外设中断到来时CPU的处理行为。但是，不论是<br>BIOS的例子还是虚拟机的例子，其中的一个关键问题是中断怎么投递到相应的软件上下文上。</p>
<p>我们再把这个问题打开看下，CPU在一个时间只能运行一个软件，而特定的中断要投递到特定<br>的软件上，这就要CPU核在收到中断时硬件内部有一套调度算法，把特定的中断调度到特定的<br>软件上下文:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">         +----------------------+</span><br><span class="line">         |                CPU   |</span><br><span class="line">         |+--------------+      |</span><br><span class="line">--------&gt;|| irq schedule |      |</span><br><span class="line">         |+--------------+      |</span><br><span class="line">         |                      |</span><br><span class="line">         |                      |</span><br><span class="line">         +----------------------+</span><br></pre></td></tr></table></figure>
<p>我们可以看下硬件的这个调度算法大概是怎么样的，把外界想要中断的CPU特权级叫做target_irq_level，<br>把CPU当前运行的特权级叫做current_cpu_run_level。一般的，不同的软件运行在不同的CPU<br>特权级，运行在相同特权级的相同软件的不同实例(比如，不同虚拟机)需要用新硬件变量标记。<br>下面我们用中断发给特权级或者发给特权级上的软件实例来描述问题。</p>
<p>当target_irq_level就是current_cpu_run_level时，这个表示外界想要中断的软件山下文<br>目前正在运行，硬件直接中断当前的上下文就好。</p>
<p>当target_irq_level不是current_cpu_run_level时，表示外界想要中断的上下文不在线，<br>如果target_irq_level比current_cpu_run_level高，CPU需要直接陷入到高特权级处理对应<br>的中断，如果target_irq_level比current_cpu_run_level低，那么中断可以被记录在CPU里，<br>等到CPU切换到中断目标特权级时再处理中断。</p>
<p>在如上的逻辑上，我们还要叠加上多实例的逻辑，这样才能和实际的软件使用场景相吻合。<br>以target_irq_level就是current_cpu_run_level为例，就是在中断目标特权级的基础上增加<br>了软件实例这个新变量，也就是说中断的发送目标是某个特权级上运行的一个软件实例，虽然<br>CPU可能正在对应的特权级上运行，但是，不一定运行的是目标软件实例，这样，硬件就需要<br>完成两个动作，一是有办法识别当前运行的软件实例是否是中断的目标实例，如果正好就是，<br>直接中断当前软件就好，二是如果需要中断的实例和当前实例不一样，这个中断怎么处理，<br>思路和特权级的处理基本一致，就是记录起来，但是软件实例的上下文完全可能是记录在<br>软件上的，而且软件实例一般是由比当前特权级高的特权级管理的，那么把这个中断报给<br>高特权级是一种合理的选择。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>中断</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言bit操作</title>
    <url>/C%E8%AF%AD%E8%A8%80bit%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">        /* test normal &lt;&lt; */</span><br><span class="line">        int i = 1;</span><br><span class="line">        int j = i &lt;&lt; 5;</span><br><span class="line">        long k;</span><br><span class="line">        printf(&quot;test normal &lt;&lt;\n&quot;);</span><br><span class="line">        printf(&quot;j = i &lt;&lt; 5 --&gt; i: %x, j: %x\n&quot;, i, j);</span><br><span class="line">        printf(&quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">        /* test normal &lt;&lt;, shift &gt; type size */</span><br><span class="line">        i = 1;</span><br><span class="line">        /* this is not right in gcc, j will be 1, not 0x100000000 */</span><br><span class="line">        j = i &lt;&lt; 32;</span><br><span class="line">        printf(&quot;test normal &lt;&lt;, shift &gt; type size\n&quot;);</span><br><span class="line">        printf(&quot;j = i &lt;&lt; 32 --&gt; i: %lx, j: %lx\n&quot;, i, j);</span><br><span class="line">        printf(&quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">         /* test normal &lt;&lt;, shift &gt; type size */</span><br><span class="line">        i = 1;</span><br><span class="line">        /* this is right in gcc */</span><br><span class="line">        k = (long)i &lt;&lt; 32;</span><br><span class="line">        printf(&quot;test normal &lt;&lt;, shift &gt; type size\n&quot;);</span><br><span class="line">        printf(&quot;size of long %d\n&quot;, sizeof(long));</span><br><span class="line">        printf(&quot;k = i &lt;&lt; 32 --&gt; i: %lx, k: %lx\n&quot;, i, k);</span><br><span class="line">        printf(&quot;\n&quot;);       </span><br><span class="line"></span><br><span class="line">        /* test normal &gt;&gt; */</span><br><span class="line">        i = 0xffe00000;</span><br><span class="line">        /* as i is a int, it will remove to right with bit 1 in top bit */</span><br><span class="line">        j = i &gt;&gt; 1;</span><br><span class="line">        printf(&quot;test normal &lt;&lt;\n&quot;);</span><br><span class="line">        printf(&quot;j = i &gt;&gt; 1 --&gt; i: %x, j: %x\n&quot;, i, j);</span><br><span class="line">        printf(&quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">         /* test |, type of two operands is different */</span><br><span class="line">        int l = 0x10000000;</span><br><span class="line">        long m = 0;</span><br><span class="line">        long g = l | m;</span><br><span class="line">        printf(&quot;test |, type of two operands is different\n&quot;);</span><br><span class="line">        printf(&quot;g = l | m --&gt; l: %lx, m: %llx, g: %llx\n&quot;, l, m, g);</span><br><span class="line">        printf(&quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">	printf(&quot;0x%08x\n&quot;, 2);</span><br><span class="line"></span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言volatile笔记</title>
    <url>/C%E8%AF%AD%E8%A8%80volatile%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>C语言里volatile用来修饰一个变量，告诉编译器怎么去编译一个变量。编译器编译一个变量，<br>看起来应该是很直白的：一个变量在内存里存放，读的话就读对应的内存，写这个变量的<br>话就向这个内存写值就可以了，为什么还要volatile这个词来修饰下？</p>
<p>在这之前我们要搞清楚两个问题.</p>
<p>第一，代码要编译成汇编指令才可以执行，编译成的汇编指令可能不是从内存里取这个值，<br>当然也可能写内存的时候不会立即生效。注意我这里只提到了内存，没有说cache，这里我<br>假设代码运行的机器是可以在硬件层面保证内存/cache的数据一致性的，就是说cache在软<br>件层面是透明的。其实现在大部分机器也确实是这样的。</p>
<p>第二，读写一个值的时候，不要认为它对应的物理存储一定是内存(DDR), 它还可以是设备<br>的一段IO寄存器(MMIO), 比如，我把一个物理设备的一段IO寄存器直接mmap出来，你在<br>用户态用mmap的到一个地址，你访问这个地址，其实访问的是这个设备的寄存器。IO寄存器<br>的属性和内存的完全不一样，你写下去一个值是为了触发一个操作，你再写下去一个相同的<br>值是为了再触发一次这个操作, 第二个操作是不能代替第一个操作的；硬件还可以通过IO<br>寄存器给你返回状态，也就是一个“内存”里的值，可能自己会改变 :)</p>
<p>还有一种情况和上面的情况相似，就是多个独立的执行流改变一个变量的情况。当一个写<br>操作已经改变了内存里的值的时候，如果读代码是去读一个缓存的值，这样就会出错。再次<br>强调这里不包括cache和内存不一致的问题，也就是说，我们假设如果写已经更新了内存，<br>读的时候，即使读的是对应的cache，硬件也会识别到cache里的数据和内存的数据是不一样<br>的，硬件会自动把内存里的新数据填到cache里，之后读是可以读到正确的值。那什么时候<br>会出问题呢: 当读操作把数据缓存在寄存器里的时候。</p>
<p>基于以上的认识，我们看一段代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* for test 1 */</span><br><span class="line">int test;</span><br><span class="line"></span><br><span class="line">/* for test 2 */</span><br><span class="line">volatile int test_v;</span><br><span class="line"></span><br><span class="line">/* for test 3 */</span><br><span class="line">int test_3, tmp1, tmp2, tmp3;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void volatile_test_1()</span><br><span class="line">&#123;</span><br><span class="line">	test = 111;	</span><br><span class="line">	test = 222;</span><br><span class="line">	test = 333;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void volatile_test_2()</span><br><span class="line">&#123;</span><br><span class="line">	test_v = 111;	</span><br><span class="line">	test_v = 222;</span><br><span class="line">	test_v = 333;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void volatile_test_3_read()</span><br><span class="line">&#123;</span><br><span class="line">	test = test_3;	</span><br><span class="line">	tmp1 = test_3;</span><br><span class="line">	tmp2 = test_3;</span><br><span class="line">	tmp3 = test_3;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void volatile_test_3_write()</span><br><span class="line">&#123;</span><br><span class="line">	test_3 = 111;	</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对这段代码普通的情况下，编译成的汇编代码是：(使用x86-arm64 gcc交叉编译器)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	.arch armv8-a</span><br><span class="line">	.file	&quot;test.c&quot;</span><br><span class="line">	.comm	test,4,4</span><br><span class="line">	.comm	test_v,4,4</span><br><span class="line">	.comm	test_3,4,4</span><br><span class="line">	.comm	tmp1,4,4</span><br><span class="line">	.comm	tmp2,4,4</span><br><span class="line">	.comm	tmp3,4,4</span><br><span class="line">	.text</span><br><span class="line">	.align	2</span><br><span class="line">	.global	volatile_test_1</span><br><span class="line">	.type	volatile_test_1, %function</span><br><span class="line">volatile_test_1:</span><br><span class="line">	adrp	x0, test</span><br><span class="line">	add	x0, x0, :lo12:test</span><br><span class="line">	mov	w1, 111</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test</span><br><span class="line">	add	x0, x0, :lo12:test</span><br><span class="line">	mov	w1, 222</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test</span><br><span class="line">	add	x0, x0, :lo12:test</span><br><span class="line">	mov	w1, 333</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	nop</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_1, .-volatile_test_1</span><br><span class="line">	.align	2</span><br><span class="line">	.global	volatile_test_2</span><br><span class="line">	.type	volatile_test_2, %function</span><br><span class="line">volatile_test_2:</span><br><span class="line">	adrp	x0, test_v</span><br><span class="line">	add	x0, x0, :lo12:test_v</span><br><span class="line">	mov	w1, 111</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test_v</span><br><span class="line">	add	x0, x0, :lo12:test_v</span><br><span class="line">	mov	w1, 222</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test_v</span><br><span class="line">	add	x0, x0, :lo12:test_v</span><br><span class="line">	mov	w1, 333</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	nop</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_2, .-volatile_test_2</span><br><span class="line">	.align	2</span><br><span class="line">	.global	volatile_test_3_read</span><br><span class="line">	.type	volatile_test_3_read, %function</span><br><span class="line">volatile_test_3_read:</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	add	x0, x0, :lo12:test_3</span><br><span class="line">	ldr	w1, [x0]</span><br><span class="line">	adrp	x0, test</span><br><span class="line">	add	x0, x0, :lo12:test</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	add	x0, x0, :lo12:test_3</span><br><span class="line">	ldr	w1, [x0]</span><br><span class="line">	adrp	x0, tmp1</span><br><span class="line">	add	x0, x0, :lo12:tmp1</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	add	x0, x0, :lo12:test_3</span><br><span class="line">	ldr	w1, [x0]</span><br><span class="line">	adrp	x0, tmp2</span><br><span class="line">	add	x0, x0, :lo12:tmp2</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	add	x0, x0, :lo12:test_3</span><br><span class="line">	ldr	w1, [x0]</span><br><span class="line">	adrp	x0, tmp3</span><br><span class="line">	add	x0, x0, :lo12:tmp3</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	nop</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_3_read, .-volatile_test_3_read</span><br><span class="line">	.align	2</span><br><span class="line">	.global	volatile_test_3_write</span><br><span class="line">	.type	volatile_test_3_write, %function</span><br><span class="line">volatile_test_3_write:</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	add	x0, x0, :lo12:test_3</span><br><span class="line">	mov	w1, 111</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	nop</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_3_write, .-volatile_test_3_write</span><br><span class="line">	.ident	&quot;GCC: (Linaro GCC 6.3-2017.05) 6.3.1 20170404&quot;</span><br><span class="line">	.section	.note.GNU-stack,&quot;&quot;,@progbits</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>但是加上-O3的选项，叫编译器帮忙给我优化下编译结果，最后的代码是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	.arch armv8-a</span><br><span class="line">	.file	&quot;test.c&quot;</span><br><span class="line">	.text</span><br><span class="line">	.align	2</span><br><span class="line">	.p2align 3,,7</span><br><span class="line">	.global	volatile_test_1</span><br><span class="line">	.type	volatile_test_1, %function</span><br><span class="line">volatile_test_1:</span><br><span class="line">	adrp	x0, test</span><br><span class="line">	mov	w1, 333</span><br><span class="line">	str	w1, [x0, #:lo12:test]</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_1, .-volatile_test_1</span><br><span class="line">	.align	2</span><br><span class="line">	.p2align 3,,7</span><br><span class="line">	.global	volatile_test_2</span><br><span class="line">	.type	volatile_test_2, %function</span><br><span class="line">volatile_test_2:</span><br><span class="line">	adrp	x0, test_v</span><br><span class="line">	mov	w3, 111</span><br><span class="line">	mov	w2, 222</span><br><span class="line">	mov	w1, 333</span><br><span class="line">	str	w3, [x0, #:lo12:test_v]</span><br><span class="line">	str	w2, [x0, #:lo12:test_v]</span><br><span class="line">	str	w1, [x0, #:lo12:test_v]</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_2, .-volatile_test_2</span><br><span class="line">	.align	2</span><br><span class="line">	.p2align 3,,7</span><br><span class="line">	.global	volatile_test_3_read</span><br><span class="line">	.type	volatile_test_3_read, %function</span><br><span class="line">volatile_test_3_read:</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	adrp	x4, test</span><br><span class="line">	adrp	x3, tmp1</span><br><span class="line">	adrp	x2, tmp2</span><br><span class="line">	adrp	x1, tmp3</span><br><span class="line">	ldr	w0, [x0, #:lo12:test_3]</span><br><span class="line">	str	w0, [x4, #:lo12:test]</span><br><span class="line">	str	w0, [x3, #:lo12:tmp1]</span><br><span class="line">	str	w0, [x2, #:lo12:tmp2]</span><br><span class="line">	str	w0, [x1, #:lo12:tmp3]</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_3_read, .-volatile_test_3_read</span><br><span class="line">	.align	2</span><br><span class="line">	.p2align 3,,7</span><br><span class="line">	.global	volatile_test_3_write</span><br><span class="line">	.type	volatile_test_3_write, %function</span><br><span class="line">volatile_test_3_write:</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	mov	w1, 111</span><br><span class="line">	str	w1, [x0, #:lo12:test_3]</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_3_write, .-volatile_test_3_write</span><br><span class="line">	.comm	tmp3,4,4</span><br><span class="line">	.comm	tmp2,4,4</span><br><span class="line">	.comm	tmp1,4,4</span><br><span class="line">	.comm	test_3,4,4</span><br><span class="line">	.comm	test_v,4,4</span><br><span class="line">	.comm	test,4,4</span><br><span class="line">	.ident	&quot;GCC: (Linaro GCC 6.3-2017.05) 6.3.1 20170404&quot;</span><br><span class="line">	.section	.note.GNU-stack,&quot;&quot;,@progbits</span><br></pre></td></tr></table></figure>

<p>可以看出普通编译的时候都是没有问题的。问题出现在编译器做优化的时候:</p>
<p>test_1多次写一个变量的时候, 编译器认为，只要最后一次写就可以了。如果，test1对应<br>的是一个IO寄存器，或者test_1的值，会被另一个执行流程识别，这里就会出问题。解决<br>办法就是给test_1加上volatile，告诉编译器每次写操作都不能被编译器优化掉。</p>
<p>test_3多次读一个变量的时候，编译器后两次的读都没有去正真读内存，而是使用了第一次<br>缓存在寄存器里的值。如果，test_3对应的是一个IO寄存器，或者如代码里一样test_3在<br>另一个执行流里被修改了(volatile_test_3_write), 但是read操作还是读的寄存器里的值，<br>这样就会出问题。要修改也是加volatile。</p>
<p>在上面的第一里提到，写内存的时候不是马上生效的, 这里也和cache没有啥关系，在硬件<br>保证cache一致性的系统里，对于单个操作，可以认为是写内存就马上生效的。这里说的<br>不生效是指CPU的多个写操作写下去的写完成的先后是不保证的。要想保证一个写操作的先后<br>顺序，就需要使用内存屏障指令。这个不是本问讨论的问题, 另外再说。这里只是说明，<br>volatile只是告诉编译器不做额外优化，它是不能保证写生效顺序的。</p>
]]></content>
      <tags>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言嵌入ARM64汇编</title>
    <url>/C%E8%AF%AD%E8%A8%80%E5%B5%8C%E5%85%A5ARM64%E6%B1%87%E7%BC%96/</url>
    <content><![CDATA[<ol start="0">
<li><p>使用场景</p>
<p>嵌入汇编一般用在c语言无法使用的地方，比如要操作系统寄存器了，比如要直接调用<br>汇编指令完成功能了(内存屏障，刷cache), 还有就是要优化性能。Linux内核里和硬件<br>直接接触的体系架构相关的代码有很多是直接用汇编写的。</p>
</li>
<li><p>嵌入汇编的语法</p>
<p>asm volatile(code : output operand list : input operand list: clobber list);</p>
<p>其中volatile告诉编译器不要去优化这里的汇编代码。防止你辛苦写的汇编代码一下被<br>编译器优化的面目全非。</p>
<p>有时你会看到这样的嵌入汇编代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">asm(&quot;mov %[result], %[value], ror #1&quot; : [result] &quot;=r&quot; (y) : [value] &quot;r&quot; (x) :)</span><br></pre></td></tr></table></figure>
<p>但是一般的，比如在linux内核里你看到的是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">asm(&quot;mov %0, %1, ror #1&quot; : &quot;=r&quot; (result) : &quot;r&quot; (value));</span><br></pre></td></tr></table></figure>
<p>唯一不同的是，code里和operand里参数的对应关系的写法。我们这里用第二种方法，<br>它的对应关系是，从output operand list到input operand list, 参数的命名从0开始，<br>依次是%0，%1…</p>
<p>operand list的一般格式是：”常量” (参数)。参数是和C语言里变量的接口，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static inline void __raw_writel(u32 val, volatile void __iomem *addr)</span><br><span class="line">&#123;</span><br><span class="line">	asm volatile(&quot;str %w0, [%1]&quot; : : &quot;rZ&quot; (val), &quot;r&quot; (addr));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的val和addr就是C代码里的变量。常量的意义可以参考:<br><span class="exturl" data-url="aHR0cHM6Ly9nY2MuZ251Lm9yZy9vbmxpbmVkb2NzL2djYy9Db25zdHJhaW50cy5odG1s">https://gcc.gnu.org/onlinedocs/gcc/Constraints.html<i class="fa fa-external-link-alt"></i></span>, 简单讲这个常量可以是r,<br>Q, m等，也可以再次被=，+， &amp;等修饰。r, m, Q讲的是这个变量可以被编译器编译在<br>寄存器里，内存里，或者是通过指针再间接寻址下，=标识这个变量只读，+表示这个<br>变量可读可写。这里code里的w表示这个变量的位宽，w表示是32bit的，x表示是64bit<br>的，这里再次注意，我们现在讨论的是aarch64下的汇编。</p>
<p>可以随便写一个程序，aarch64-linux-gnu-gcc -S xx.c编译下，看看生成的汇编代码<br>是怎么样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void test()</span><br><span class="line">&#123;</span><br><span class="line">        int y = 0;</span><br><span class="line">	int x = 1;</span><br><span class="line"></span><br><span class="line">	asm volatile(&quot;add %w0, %w1, 1&quot; : &quot;=r&quot; (y) : &quot;r&quot; (x) :);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面是编译成的汇编代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	.arch armv8-a</span><br><span class="line">	.file	&quot;embedded_asm.c&quot;</span><br><span class="line">	.text</span><br><span class="line">	.align	2</span><br><span class="line">	.global	test</span><br><span class="line">	.type	test, %function</span><br><span class="line">test:</span><br><span class="line">	sub	sp, sp, #16</span><br><span class="line">	str	wzr, [sp, 12]</span><br><span class="line">	mov	w0, 1</span><br><span class="line">	str	w0, [sp, 8]</span><br><span class="line">	ldr	w0, [sp, 8]</span><br><span class="line">#APP</span><br><span class="line">// 6 &quot;embedded_asm.c&quot; 1</span><br><span class="line">	add w0, w0, 1</span><br><span class="line">// 0 &quot;&quot; 2</span><br><span class="line">#NO_APP</span><br><span class="line">	str	w0, [sp, 12]</span><br><span class="line">	nop</span><br><span class="line">	add	sp, sp, 16</span><br><span class="line">	ret</span><br><span class="line">	.size	test, .-test</span><br><span class="line">	.ident	&quot;GCC: (Linaro GCC 6.3-2017.05) 6.3.1 20170404&quot;</span><br><span class="line">	.section	.note.GNU-stack,&quot;&quot;,@progbits</span><br></pre></td></tr></table></figure>
<p>把r改成m后是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test:</span><br><span class="line">	sub	sp, sp, #16</span><br><span class="line">	str	wzr, [sp, 12]</span><br><span class="line">	mov	w0, 1</span><br><span class="line">	str	w0, [sp, 8]</span><br><span class="line">#APP</span><br><span class="line">// 6 &quot;embedded_asm.c&quot; 1</span><br><span class="line">	add [sp, 12], [sp, 8], 1</span><br><span class="line">// 0 &quot;&quot; 2</span><br><span class="line">#NO_APP</span><br><span class="line">	nop</span><br><span class="line">	add	sp, sp, 16</span><br><span class="line">	ret</span><br></pre></td></tr></table></figure>
<p>把r改成Q后是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   test:</span><br><span class="line">	sub	sp, sp, #16</span><br><span class="line">	str	wzr, [sp, 12]</span><br><span class="line">	mov	w0, 1</span><br><span class="line">	str	w0, [sp, 8]</span><br><span class="line">	add	x0, sp, 12</span><br><span class="line">	add	x1, sp, 8</span><br><span class="line">#APP</span><br><span class="line">// 6 &quot;embedded_asm.c&quot; 1</span><br><span class="line">	add [x0], [x1], 1</span><br><span class="line">// 0 &quot;&quot; 2</span><br><span class="line">#NO_APP</span><br><span class="line">	nop</span><br><span class="line">	add	sp, sp, 16</span><br><span class="line">	ret</span><br></pre></td></tr></table></figure>

<p>clobber的解释gcc上说是：告知编译器嵌入的汇编代码除了影响output operand里的<br>变量外，还对哪些寄存器或者是对内存会产生影响。把他们可以列到clobber list里面<br>来。寄存器直接列名字，两个特殊的clobber参数是”memory”和”cc”, cc表示会影响到<br>flags register，memory就会影响到input operant参数指向的内存。</p>
</li>
<li><p>举例</p>
<p>…</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>ARM64</tag>
        <tag>汇编</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言温故而知新</title>
    <url>/C%E8%AF%AD%E8%A8%80%E6%B8%A9%E6%95%85%E8%80%8C%E7%9F%A5%E6%96%B0/</url>
    <content><![CDATA[<h2 id="宏"><a href="#宏" class="headerlink" title="宏"></a>宏</h2><p>  ## 用来拼接前后的符号<br>  # 用来把一个符号字符串化</p>
<p>  宏参数还是一个宏的展开规则是，如果宏是一个简单的宏就先展开里面的宏，再依次<br>  展开外面的宏。</p>
<p>  如果宏的实现是一个使用##或者#的宏，那么直接使用##或者#的规则。</p>
<p>  如果要使得比如下面test case 4的到“19”的输出，需要中间转下，类似case 5的写法。<br>  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">#define TEST(a, b) a##b</span><br><span class="line">#define STR(a) #a </span><br><span class="line">#define A 1000</span><br><span class="line">#define ADD(a, b) ((a) + (b))</span><br><span class="line">#define _TEST(a, b) TEST(a, b)</span><br><span class="line"></span><br><span class="line">#define _STR(a) STR(a)</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	printf(&quot;test 1: %d\n&quot;, TEST(1, 5));</span><br><span class="line">	printf(&quot;test 2: %s\n&quot;, STR(just_test));</span><br><span class="line">	printf(&quot;test 3: %d\n&quot;, ADD(A, A));</span><br><span class="line"></span><br><span class="line">	/* test case 4 */</span><br><span class="line">	printf(&quot;test 4: %s\n&quot;, STR(TEST(1, 9)));</span><br><span class="line"></span><br><span class="line">	/* test case 5 */</span><br><span class="line">	printf(&quot;test 5: %s\n&quot;, _STR(TEST(1, 9)));</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">xxx@kllp05:~/tests/c_note$ ./a.out </span><br><span class="line">test 1: 15</span><br><span class="line">test 2: just_test</span><br><span class="line">test 3: 2000</span><br><span class="line">test 4: TEST(1, 9)</span><br><span class="line">test 5: 19</span><br></pre></td></tr></table></figure><br>  <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vYWxhbnR1MjAxOC9wLzg0NjU5MTEuaHRtbA==">https://www.cnblogs.com/alantu2018/p/8465911.html<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="自动数据类型转换"><a href="#自动数据类型转换" class="headerlink" title="自动数据类型转换"></a>自动数据类型转换</h2><p>  各种截断数据</p>
<p>  float类型不能比较相等，不相等</p>
<p>  sizeof() 返回一个unsigned int</p>
<p>  有符号数和无符号数运算、赋值: <span class="exturl" data-url="aHR0cHM6Ly9ha2FlZHUuZ2l0aHViLmlvL2Jvb2svY2gxNXMwMy5odG1s">https://akaedu.github.io/book/ch15s03.html<i class="fa fa-external-link-alt"></i></span></p>
<p>  有符号右移，带符号位右移</p>
<h2 id="大小端和位域"><a href="#大小端和位域" class="headerlink" title="大小端和位域"></a>大小端和位域</h2><p>  <span class="exturl" data-url="aHR0cDovL21qZnJhemVyLm9yZy9tamZyYXplci9iaXRmaWVsZHMv">http://mjfrazer.org/mjfrazer/bitfields/<i class="fa fa-external-link-alt"></i></span></p>
<p>  这个实例说明为什么写代码的时候，特别是涉及底层硬件寄存器操作的驱动代码，尽量<br>  不要用位域。原因是在大小端不同的系统上，位域的表示方法是不同的。</p>
<h2 id="格式化打印"><a href="#格式化打印" class="headerlink" title="格式化打印"></a>格式化打印</h2><p>  printf %n: 已输出的字符数目(用户对齐下一行的输出比较有用 :))</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	int num;</span><br><span class="line"></span><br><span class="line">	printf(&quot;123456:%n\n&quot;, &amp;num);</span><br><span class="line">	printf(&quot;%*c %d\n&quot;, num, &#x27; &#x27;, 123);</span><br><span class="line">	printf(&quot;%*c %d\n&quot;, num, &#x27; &#x27;, 456);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">xxx@kllp05:~/tests/c_note$ ./a.out </span><br><span class="line">123456:</span><br><span class="line">        123</span><br><span class="line">        456</span><br></pre></td></tr></table></figure>

<h2 id="长跳转"><a href="#长跳转" class="headerlink" title="长跳转"></a>长跳转</h2><p>  setjmp, longjmp: <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vaGF6aXIvcC9jX3NldGptcF9sb25nam1wLmh0bWw=">https://www.cnblogs.com/hazir/p/c_setjmp_longjmp.html<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="可变参数函数"><a href="#可变参数函数" class="headerlink" title="可变参数函数"></a>可变参数函数</h2><p>  <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vY3BvaW50L3AvMzM2ODk5My5odG1s">https://www.cnblogs.com/cpoint/p/3368993.html<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="realloc"><a href="#realloc" class="headerlink" title="realloc"></a>realloc</h2><p>  <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vemhhb3lsL3AvMzk1NDIzMi5odG1s">https://www.cnblogs.com/zhaoyl/p/3954232.html<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="二维数组和指针"><a href="#二维数组和指针" class="headerlink" title="二维数组和指针"></a>二维数组和指针</h2><h2 id="const指针"><a href="#const指针" class="headerlink" title="const指针"></a>const指针</h2><p>  const char ** p; char * const * p; char ** const p;<br>  分别是**p, *p, p是常量。</p>
<h2 id="double类型的存储格式"><a href="#double类型的存储格式" class="headerlink" title="double类型的存储格式"></a>double类型的存储格式</h2><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>  c中简单结构体直接复制是ok的<br>  int fun(int a[10])的入参和指针一样</p>
]]></content>
      <tags>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>Dump PCIe设备BAR寄存器</title>
    <url>/Dump-PCIe%E8%AE%BE%E5%A4%87BAR%E5%AF%84%E5%AD%98%E5%99%A8/</url>
    <content><![CDATA[<p>下载后编译即可，可能需要安装readline和curses库。<br>gcc pci_debug.c -lreadline -lcurses -o pci_debug</p>
<p>这个小工具的用法很简单，看help就可以懂:</p>
<p>pci_debug -s <BDF><br>可以进入一个命令行交互界面。<br>(这个工具有一个bug，就是不支持设备有domain号, 对于有domain号的设备可以打上如<br> 下的补丁)</BDF></p>
<p>输入命令：d addr len 查看地址addr开始的len个单位的数值，这里的单位default数值是<br>          32bit。</p>
<pre><code>  q退出工具。

      e改变大小端。
  ...
</code></pre>
<p>这个工具需要sudo或者root权限。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff --git a/pci_debug.c b/pci_debug.c</span><br><span class="line">index f746840..30f4d45 100644</span><br><span class="line">--- a/pci_debug.c</span><br><span class="line">+++ b/pci_debug.c</span><br><span class="line">@@ -177,9 +177,9 @@ int main(int argc, char *argv[])</span><br><span class="line"> 	 */</span><br><span class="line"> </span><br><span class="line"> 	/* Extract the PCI parameters from the slot string */</span><br><span class="line">-	status = sscanf(slot, &quot;%2x:%2x.%1x&quot;,</span><br><span class="line">-			&amp;dev-&gt;bus, &amp;dev-&gt;slot, &amp;dev-&gt;function);</span><br><span class="line">-	if (status != 3) &#123;</span><br><span class="line">+	status = sscanf(slot, &quot;%4x:%2x:%2x.%1x&quot;,</span><br><span class="line">+			&amp;dev-&gt;domain, &amp;dev-&gt;bus, &amp;dev-&gt;slot, &amp;dev-&gt;function);</span><br><span class="line">+	if (status != 4) &#123;</span><br><span class="line"> 		printf(&quot;Error parsing slot information!\n&quot;);</span><br><span class="line"> 		show_usage();</span><br><span class="line"> 		return -1;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>GIC ITS学习笔记(一)</title>
    <url>/GIC-ITS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/</url>
    <content><![CDATA[<h2 id="current-GIC-ITS-code-in-v4-2-v4-3"><a href="#current-GIC-ITS-code-in-v4-2-v4-3" class="headerlink" title="current GIC ITS code in v4.2/v4.3"></a>current GIC ITS code in v4.2/v4.3</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* this is the arch of interrupt sub-system */</span><br><span class="line">struct irq_desc</span><br><span class="line">    --&gt; struct irq_data</span><br><span class="line">        --&gt; struct irq_chip</span><br><span class="line">        --&gt; struct irq_domain</span><br><span class="line">            --&gt; struct irq_domain_ops</span><br><span class="line"></span><br><span class="line">/* this is the arch of ITS sub-system, for v4.1, for v4.2, kernel changed a lot */</span><br><span class="line">struct its_node</span><br><span class="line">    --&gt; struct irq_domain</span><br><span class="line">        --&gt; struct irq_domain_ops (its_domain_ops)</span><br><span class="line">	        /* alloc put struct irq_chip (its_irq_chip) to related</span><br><span class="line">		 * struct irq_data</span><br><span class="line">		 *</span><br><span class="line">		 * Fix me for other work alloc does</span><br><span class="line">		 */</span><br><span class="line">	    --&gt; .alloc (its_irq_domain_alloc)</span><br><span class="line">	    --&gt; .activate (its_irq_domain_activate)</span><br><span class="line">	    --&gt; ...</span><br><span class="line">    --&gt; struct msi_controller</span><br><span class="line">            /* father irq_domain is irq_domain above */</span><br><span class="line">        --&gt; struct irq_domain</span><br><span class="line">	        /* msi_domain_ops defined in kernel/irq/msi.c */</span><br><span class="line">	    --&gt; struct irq_domain_ops (struct irq_domain_ops msi_domain_ops)</span><br><span class="line">	    --&gt; void *host_data (struct msi_domain_info its_pci_msi_domain_info)</span><br><span class="line">                --&gt; struct msi_domain_ops (its_pci_msi_ops)</span><br><span class="line">                    --&gt; .msi_prepare (its_msi_prepare)</span><br><span class="line">                    --&gt; ...</span><br><span class="line">                --&gt; struct irq_chip (its_msi_irq_chip)</span><br></pre></td></tr></table></figure>
<p>/* this is the arch of ITS sub-system for v4.2, ITS driver changed a lot in v4.2 */<br>there is no irq_domain in struct its_node. In drivers/irqchip/irq-gic-v3-its.c,<br>just build up below irq_domain.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gic irq_domain --&gt; irq_domain_ops(gic_irq_domain_ops)</span><br><span class="line">      ^                --&gt; .alloc(gic_irq_domain_alloc)</span><br><span class="line">      |</span><br><span class="line">its irq_domain --&gt; irq_domain_ops(its_domain_ops)</span><br><span class="line">      ^                --&gt; .alloc(its_irq_domain_alloc)</span><br><span class="line">      |                --&gt; ...</span><br><span class="line">      |        --&gt; host_data(struct msi_domain_info)</span><br><span class="line">      |            --&gt; msi_domain_ops(its_msi_domain_ops)</span><br><span class="line">      |                --&gt; .msi_prepare(its_msi_prepare)</span><br><span class="line">      |            --&gt; irq_chip, chip_data, handler...</span><br><span class="line">      |            --&gt; void *data(struct its_node)</span><br></pre></td></tr></table></figure>
<p>In drvers/irqchip/irq-gic-v3-its-pci-msi.c,<br>   drvers/irqchip/irq-gic-v3-its-platform-msi.c, it seems that we create two<br>other irq_domain:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_its irq_domain                      platform_its irq_domain</span><br><span class="line">        /* kernel/irq/msi.c */                  /* kernel/irq/msi.c */</span><br><span class="line">    --&gt; irq_domain_ops(msi_domain_ops)      --&gt; irq_domain_ops(msi_domain_op)</span><br><span class="line">        /* irq-gic-v3-its-pci-msi.c             /* irq-gic-v3-its-platform-msi.c</span><br><span class="line">	 * (its_pci_msi_domain_info)             * (its_pmsi_domain_info)</span><br><span class="line">	 */                                      */</span><br><span class="line">    --&gt; void *host_data                     --&gt; void *host_data</span><br><span class="line">        --&gt; .ops(its_pci_msi_ops)               --&gt; .ops(its_pmsi_ops)</span><br><span class="line">	        /* its_pci_msi_prepare */   	    /* its_pmsi_prepare */</span><br><span class="line">	    --&gt; .msi_prepare                    --&gt; .msi_prepare</span><br><span class="line">	--&gt; .chip(its_msi_irq_chip)             --&gt; .chip(its_pmsi_irq_chip)</span><br></pre></td></tr></table></figure>
<h2 id="msi-domain-struct"><a href="#msi-domain-struct" class="headerlink" title="msi domain struct"></a>msi domain struct</h2><p>basic struct:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct msi_domain_info</span><br><span class="line">    --&gt; struct msi_domain_ops</span><br><span class="line">        --&gt; .msi_prepare</span><br><span class="line">    --&gt; struct irq_chip</span><br><span class="line">        --&gt; .irq_write_msi_msg</span><br></pre></td></tr></table></figure>
<p>msi related struct should be part of interrupt sub-system as showed in part 1.<br>struct msi_domain_info will be stored in void *host_data of a irq_domain.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* will find below funtion in drvers/irqchip/irq-gic-v3-its-pci-msi.c */</span><br><span class="line">pci_msi_create_irq_domain(device_node, msi_domain_info, parent)</span><br><span class="line">        /* core function */</span><br><span class="line">    --&gt; msi_create_irq_domain(node, info, parent);</span><br><span class="line">            /* msi_domain_ops is irq_domain_ops in kernel/irq/msi.c</span><br><span class="line">             * info below will be stored in host_data of irq_domain</span><br><span class="line">             *</span><br><span class="line">             * both pci_its irq_domain and platform_its irq_domain use</span><br><span class="line">             * same msi_domain_ops, but different msi_domain_info.</span><br><span class="line">             * above msi_domain_ops is a struct irq_domain_ops.</span><br><span class="line">             */</span><br><span class="line">        --&gt; irq_domain_add_hierarchy(parent, 0, 0, node, &amp;msi_domain_ops, info)</span><br></pre></td></tr></table></figure>
<h2 id="pci-msi-struct"><a href="#pci-msi-struct" class="headerlink" title="pci msi struct "></a>pci msi struct </h2><p>in part 2, it creats related domain, then we will see how to use callbacks<br>in above domain. This part shows how to allocate irqs in a msi domain.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* this is the work flow of PCI MSI */</span><br><span class="line">/* kernel/drivers/pci/msi.c */</span><br><span class="line">pci_msi_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)</span><br><span class="line">        /* so this irq_domain is pci_its irq_domain ? */</span><br><span class="line">    --&gt; pci_msi_domain_alloc_irqs(domain, dev, nvec, type);</span><br><span class="line">        --&gt; msi_domain_alloc_irqs(domain, &amp;dev-&gt;dev, nvec);</span><br><span class="line">	        /* should be its_pci_msi_prepare ?</span><br><span class="line">                 * if below function, first get dev_id, then call parent domain</span><br><span class="line">                 * msi_prepare which is its domain msi_prepare and will build</span><br><span class="line">                 * up device table of ITS.</span><br><span class="line">                 */</span><br><span class="line">	    --&gt; ps-&gt;msi_prepare(domain, dev, nvec, &amp;arg);</span><br><span class="line">	       /* domain, virq, desc-&gt;nvec_used, dev_to_node(dev), &amp;arg, false */</span><br><span class="line">	    --&gt; __irq_domain_alloc_irqs()</span><br><span class="line"></span><br><span class="line">/* details of how ITS work, this prepare function just allocat device table and</span><br><span class="line"> * related structure in ITS</span><br><span class="line"> */</span><br><span class="line">/* in pci_msi irq_domain */</span><br><span class="line">its_pci_msi_prepare</span><br><span class="line">    --&gt; ... (get dev_ip)</span><br><span class="line">        /* in its irq_domain */</span><br><span class="line">    --&gt; its_msi_prepare</span><br><span class="line">        --&gt; its_create_device(its, dev_id, nvec);</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* there is an important point: how to configure MSI capability registers of one</span><br><span class="line"> * PCI device, this action takes place in irq_write_msi_msg mentioned in part 2&#x27;s</span><br><span class="line"> * &quot;basic struct&quot;. how to call this function ? obviouly, this function is stored</span><br><span class="line"> * in msi_domain_info, so we must use some msi layer function to access this</span><br><span class="line"> * function to interrup sub-system, in fact, it uses msi_domain_ops to do this.</span><br><span class="line"> */</span><br><span class="line">/* it stored irq_chip in msi_domain_info to interrupt sub-system in </span><br><span class="line"> * pci_msi_create_irq_domain, details as below:</span><br><span class="line"> */</span><br><span class="line">pci_msi_create_irq_domain</span><br><span class="line">    --&gt; msi_create_irq_domain</span><br><span class="line">        --&gt;  msi_domain_update_dom_ops</span><br><span class="line">                 /* (struct msi_domain_ops msi_domain_ops_default --&gt;</span><br><span class="line">                  * msi_domain_ops_init</span><br><span class="line">                  */</span><br><span class="line">             --&gt; ops-&gt;msi_init = msi_domain_ops_default.msi_init;</span><br><span class="line">                     /* para: domain, virq, hwirq, info-&gt;chip, info-&gt;chip_data */</span><br><span class="line">                 --&gt; irq_domain_set_hwirq_and_chip</span><br></pre></td></tr></table></figure>
<p>in function request_irq, it will call .active in irq_domain struct, here is will<br>call function msi_domain_activate in struct irq_domain_ops msi_domain_ops.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">request_irq</span><br><span class="line">    ...</span><br><span class="line">    --&gt; msi_domain_activate</span><br><span class="line">        --&gt; irq_chip_write_msi_msg</span><br><span class="line">                /* here is irq_write_msi_msg in struct msi_domain_info */</span><br><span class="line">            --&gt; data-&gt;chip-&gt;irq_write_msi_msg(data, msg);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>GIC</tag>
      </tags>
  </entry>
  <entry>
    <title>How to assign more than 31 VFs to one VM</title>
    <url>/How-to-assign-more-than-31-VFs-to-one-VM/</url>
    <content><![CDATA[<p>用QEMU模拟PCIe设备的时候，一般最多可以在系统中配置31个PCIe设备。比如，我们有这样<br>的QEMU启动参数配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 \</span><br><span class="line">-machine virt,gic-version=3 \</span><br><span class="line">-enable-kvm \</span><br><span class="line">-cpu host \</span><br><span class="line">-m 1024 \</span><br><span class="line">-kernel ./Image \</span><br><span class="line">-initrd ./minifs.cpio.gz \</span><br><span class="line">-nographic \</span><br><span class="line">-net none \</span><br><span class="line">-device vfio-pci,host=0002:81:10.0,id=net0 \</span><br><span class="line">-device vfio-pci,host=0002:81:20.0,id=net1 \</span><br></pre></td></tr></table></figure>
<p>上面的配置中，我们使用了host上的两个82599网卡的vf: 0002:81:10.0, 0002:81:20.0,<br>把它们直通到了guest上，可以看到这两个vf在guest上会直接连到pci bus 0上。按照同样<br>的方法，我们可以给一个虚拟机继续增加网口。但是这样的方式增加最多只能到31个vf[1]</p>
<p>这是因为PCIe中规定一条总线下最多只能接32个设备。</p>
<p>为了在一个虚拟机上接入更多的设备，我们可以接入一个PCIe switch，在switch的下游<br>端口上再接需要的PCIe设备。比如，我有可以如下启动一个QEMU虚拟机：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 \</span><br><span class="line">-machine virt,gic-version=3 \</span><br><span class="line">-enable-kvm \</span><br><span class="line">-cpu host \</span><br><span class="line">-m 1024 \</span><br><span class="line">-kernel ./Image \</span><br><span class="line">-initrd ./minifs.cpio.gz \</span><br><span class="line">-nographic \</span><br><span class="line">-net none \</span><br><span class="line">-device ioh3420,id=root_port1 \</span><br><span class="line">-device x3130-upstream,id=upstream_port1,bus=root_port1 \</span><br><span class="line">-device xio3130-downstream,id=downstream_port1,bus=upstream_port1,chassis=1,slot=1 \</span><br><span class="line">-device xio3130-downstream,id=downstream_port2,bus=upstream_port1,chassis=1,slot=2 \</span><br><span class="line">-device xio3130-downstream,id=downstream_port3,bus=upstream_port1,chassis=1,slot=3 \</span><br><span class="line">-device xio3130-downstream,id=downstream_port4,bus=upstream_port1,chassis=1,slot=4 \</span><br><span class="line">-device xio3130-downstream,id=downstream_port5,bus=upstream_port1,chassis=1,slot=5 \</span><br><span class="line">-device vfio-pci,host=0002:81:10.0,id=net0,bus=downstream_port1 \</span><br><span class="line">-append &#x27;console=ttyAMA0 root=/dev/vda2&#x27; \</span><br><span class="line">-nographic \</span><br></pre></td></tr></table></figure>
<p>上面的命令行参数可以搭建一个如下图所示的PCIe switch</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pcie.0 bus</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line">                             |</span><br><span class="line">                       -------------</span><br><span class="line">                       | Root Port1|</span><br><span class="line">                       -------------</span><br><span class="line">    -------------------------|-------------------------------------------</span><br><span class="line">    |                 -----------------------------------------         |</span><br><span class="line">    |    PCI Express  | Upstream Port1                        |         |</span><br><span class="line">    |      Switch     -----------------------------------------         |</span><br><span class="line">    |                  |            |                                   |</span><br><span class="line">    |    -------------------    -------------------                     |</span><br><span class="line">    |    | Downstream Port1|    | Downstream Port2|       ....          |</span><br><span class="line">    |    -------------------    -------------------                     |</span><br><span class="line">    -------------|-----------------------|-------------------------------</span><br><span class="line">           ------------                                                 </span><br><span class="line">           | PCIe Dev | vfio-pci device</span><br><span class="line">           ------------</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>[1] Fix me: 为什么是31个不是32个?<br>[2] qemu/docs/pcie.txt</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>Hixxxx PCIe + SMMU bad performance debug</title>
    <url>/Hixxxx-PCIe-SMMU-bad-performance-debug/</url>
    <content><![CDATA[<p>When we enabled SMMU in Dxx board, we found that the performance of 82599 pluged<br>in PCIe slot is very bad. LeiZhen and I spent some to debug this problem. This<br>document just shares the related results and information.</p>
<h2 id="test-scenarios-and-results"><a href="#test-scenarios-and-results" class="headerlink" title="test scenarios and results"></a>test scenarios and results</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+----------------+        +---------------+</span><br><span class="line">|   Dxx   82599  |&lt;------&gt;|  82599   Dxx  |</span><br><span class="line">+----------------+        +---------------+</span><br><span class="line"></span><br><span class="line">      +-----+         +-----+</span><br><span class="line">      | cpu |         | ddr |</span><br><span class="line">      +--+--+         +--+--+</span><br><span class="line">         |               |</span><br><span class="line"> --------+-----+---------+------- bus</span><br><span class="line">               |</span><br><span class="line">            +--+---+</span><br><span class="line">            | smmu |</span><br><span class="line">            +--+---+</span><br><span class="line">               |                   </span><br><span class="line">            +--+---+</span><br><span class="line">            |  rp  |</span><br><span class="line">            +--+---+</span><br><span class="line">               |</span><br><span class="line">            +--+---+</span><br><span class="line">            | 82599|</span><br><span class="line">            +------+</span><br></pre></td></tr></table></figure>
<p>Hardware topology as showed above. In order to use SMMU to translate data from<br>82599 to DDR, we need enable SMMU node in ACPI table.[1]</p>
<p>Then boot up two Dxx boards connected by two 82599 networking cards. When using<br>iperf to test the performance between two 82599 networking cards, it is very bad,<br>nearly 100Mbps.[2]</p>
<h2 id="analysis"><a href="#analysis" class="headerlink" title="analysis"></a>analysis</h2><p>The only difference is disable SMMU and enable SMMU. So the difference is we use<br>diffferent DMA callbacks.</p>
<p>We can see arch/arm64/mm/dma-mapping.c, when configuring CONFIG_IOMMU_DMA,<br>callbacks in struct dma_map_ops iommu_dma_ops will be used.</p>
<p>And we also know when 82599 sending/receiving packages, its driver will call<br>ixgbe_tx_map/ixgbe_alloc_mapped_page to allocate DMA memory function which will<br>finally call map_page in iommu_dma_ops when SMMU is enable. So we guess there<br>is something wrong with the map_page here.</p>
<p>So we should analyze related function to find the hot point. Here we firstly use<br>ftrace to confirm our idea, then use perf to locate the hot point explicitly.</p>
<ul>
<li><p>ftrace</p>
<p>We can use function profiling in ftrace to see durations of related function.<br>please refer[3] to know how to use ftrace function profiling.</p>
<p>Here we get:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">10)&lt;...&gt;-4313   |               |   ixgbe_xmit_frame_ring() &#123;</span><br><span class="line">10)&lt;...&gt;-4313   |               |     __iommu_map_page() &#123;</span><br><span class="line">10)&lt;...&gt;-4313   |   0.080 us    |       dma_direction_to_prot();</span><br><span class="line">10)&lt;...&gt;-4313   |               |       iommu_dma_map_page() &#123;</span><br><span class="line">10)&lt;...&gt;-4313   |               |         __iommu_dma_map() &#123;</span><br><span class="line">10)&lt;...&gt;-4313   |   0.480 us    |           iommu_get_domain_for_dev();</span><br><span class="line">10)&lt;...&gt;-4313   |               |           __alloc_iova() &#123;</span><br><span class="line">10)&lt;...&gt;-4313   |               |             alloc_iova() &#123;</span><br><span class="line">10)&lt;...&gt;-4313   |               |               kmem_cache_alloc() &#123;</span><br><span class="line">10)&lt;...&gt;-4313   |               |                 __slab_alloc.isra.21()</span><br><span class="line">10)&lt;...&gt;-4313   |   0.040 us    |                 memcg_kmem_put_cache();</span><br><span class="line">10)&lt;...&gt;-4313   | + 16.160 us   |               &#125;</span><br><span class="line">[...]</span><br><span class="line">10)&lt;...&gt;-4313   |   0.120 us    |               _raw_spin_lock_irqsave();</span><br><span class="line">10)&lt;...&gt;-4313   |               |               _raw_spin_unlock_irqrestore() &#123;</span><br><span class="line">10)&lt;...&gt;-4313   |   ==========&gt; |</span><br><span class="line">10)&lt;...&gt;-4313   |               |                 gic_handle_irq()</span><br><span class="line">10)&lt;...&gt;-4313   |   &lt;========== |</span><br><span class="line">10)&lt;...&gt;-4313   | + 88.620 us   |               &#125;</span><br><span class="line">10)&lt;...&gt;-4313   | ! 679.760 us  |             &#125;</span><br><span class="line">10)&lt;...&gt;-4313   | ! 680.480 us  |           &#125;</span><br></pre></td></tr></table></figure>
<p>Most time has been spent in alloc_iova.</p>
</li>
<li><p>perf</p>
<p>Sadly, there was no perf(PMU hardware event) support in ACPI in plinth<br>kernel :( So we directly set PMU related registers to get how many CPU cycles<br>each function has spent.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* Firstly call this init function to init PMU */</span><br><span class="line">static void pm_cycle_init(void)</span><br><span class="line">&#123;</span><br><span class="line">	u64 val;</span><br><span class="line"></span><br><span class="line">	asm volatile(&quot;mrs %0, pmccfiltr_el0&quot; : &quot;=r&quot; (val));</span><br><span class="line">	if (val &amp; ((u64)1 &lt;&lt; 31)) &#123;</span><br><span class="line">		val &amp;= ~((u64)1 &lt;&lt; 31);</span><br><span class="line">		asm volatile(&quot;msr pmccfiltr_el0, %0&quot; :: &quot;r&quot; (val));</span><br><span class="line">		dsb(sy);</span><br><span class="line">		asm volatile(&quot;mrs %0, pmccfiltr_el0&quot; : &quot;=r&quot; (val));</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	asm volatile(&quot;mrs %0, pmcntenset_el0&quot; : &quot;=r&quot; (val));</span><br><span class="line">	if (!(val &amp; ((u64)1 &lt;&lt; 31))) &#123;</span><br><span class="line">		val |= ((u64)1 &lt;&lt; 31);</span><br><span class="line">		asm volatile(&quot;msr pmcntenset_el0, %0&quot; :: &quot;r&quot; (val));</span><br><span class="line">		dsb(sy);</span><br><span class="line">		asm volatile(&quot;mrs %0, pmcntenset_el0&quot; : &quot;=r&quot; (val));</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	asm volatile(&quot;mrs %0, pmcr_el0&quot; : &quot;=r&quot; (val));</span><br><span class="line">	if (!(val &amp; ((u64)1 &lt;&lt; 6))) &#123;</span><br><span class="line">		val |= ((u64)1 &lt;&lt; 6) | 0x1;</span><br><span class="line">		asm volatile(&quot;msr pmcr_el0, %0&quot; :: &quot;r&quot; (val));</span><br><span class="line">		dsb(sy);</span><br><span class="line">		asm volatile(&quot;mrs %0, pmcr_el0&quot; : &quot;=r&quot; (val));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* Get the CPU cycles in PMU counter */</span><br><span class="line">u64 pm_cycle_get(void)</span><br><span class="line">&#123;</span><br><span class="line">	u64 val;</span><br><span class="line"></span><br><span class="line">	asm volatile(&quot;mrs %0, pmccntr_el0&quot; : &quot;=r&quot; (val));</span><br><span class="line"></span><br><span class="line">	return val;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(pm_cycle_get);</span><br></pre></td></tr></table></figure>
<p>Using above debug functions, we found almost 600000 CPU cycles will happen in<br>a while loop in function __alloc_and_insert_iova_range. If CPU frequency is<br>2G Hz, then 600000 CPU cycles is 300us! This is the hot point.</p>
<p>We found it will loop almost 10000 times in above while loop!</p>
</li>
</ul>
<ul>
<li><p>Code analysis </p>
<p>Firstly, this DMA software modules is like this:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">VA = kmalloc();</span><br><span class="line">IOVA = dma_map_function(PA = fun(VA)); </span><br></pre></td></tr></table></figure>
<p>Firstly, allocate memory for DMA memory and map a VA which can be used by CPU,<br>Then, build map between IOVA and PA in dma map function. In the case of SMMU<br>enable, .map_page(__iommu_map_page) in iommu_dma_ops will be call to build<br>above map.</p>
<p>Then common function iommu_dma_map_page in drivers/iommu/dma-iommu.c will be<br>called. There are two steps in above function: 1. allocate iova, this is a<br>common function; 2. build map between IOVA and PA, this is SMMU specific<br>function.   </p>
<p>The hot point is in the point 1 above, so we need understand the module of<br>how to allocate iova. A red black tree in iova_domain is used to store all iova<br>range in system, after allocating or freeing an iova range, an iova range<br>should be inserted or remove from above red black tree. Now we allocate the<br>iova range from the end of the iova domain, for 32 DMA mask, it is 0xffffffff,<br>for 64bit DMA mask it is 0xffffffff_ffffffff. There is a cache for 32bit DMA<br>mask to store the iova range in last time, but for 64bit DMA MASK, there is<br>no such cache. so for 64bit DMA, when we want to allocate a DMA range in<br>iova domain, we have to search from the 0xffffffff_ffffffff. If we already allocate<br>a lot iova range, then we have to search all iova range allocated before.</p>
</li>
</ul>
<h2 id="solution"><a href="#solution" class="headerlink" title="solution"></a>solution</h2><p>So we can fix this bug like this:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff --git a/drivers/iommu/iova.c b/drivers/iommu/iova.c</span><br><span class="line">index 080beca..1e582d8 100644</span><br><span class="line">--- a/drivers/iommu/iova.c</span><br><span class="line">+++ b/drivers/iommu/iova.c</span><br><span class="line">@@ -46,6 +46,7 @@ init_iova_domain(struct iova_domain *iovad, unsigned long granule,</span><br><span class="line"> 	spin_lock_init(&amp;iovad-&gt;iova_rbtree_lock);</span><br><span class="line"> 	iovad-&gt;rbroot = RB_ROOT;</span><br><span class="line"> 	iovad-&gt;cached32_node = NULL;</span><br><span class="line">+	iovad-&gt;cached64_node = NULL;</span><br><span class="line"> 	iovad-&gt;granule = granule;</span><br><span class="line"> 	iovad-&gt;start_pfn = start_pfn;</span><br><span class="line"> 	iovad-&gt;dma_32bit_pfn = pfn_32bit;</span><br><span class="line">@@ -56,13 +57,19 @@ EXPORT_SYMBOL_GPL(init_iova_domain);</span><br><span class="line"> static struct rb_node *</span><br><span class="line"> __get_cached_rbnode(struct iova_domain *iovad, unsigned long *limit_pfn)</span><br><span class="line"> &#123;</span><br><span class="line">-	if ((*limit_pfn &gt; iovad-&gt;dma_32bit_pfn) ||</span><br><span class="line">-		(iovad-&gt;cached32_node == NULL))</span><br><span class="line">+	struct rb_node *cached_node;</span><br><span class="line">+</span><br><span class="line">+	if (*limit_pfn &lt; iovad-&gt;dma_32bit_pfn)</span><br><span class="line">+		cached_node = iovad-&gt;cached32_node;</span><br><span class="line">+	else</span><br><span class="line">+		cached_node = iovad-&gt;cached64_node;</span><br><span class="line">+</span><br><span class="line">+	if (cached_node == NULL)</span><br><span class="line"> 		return rb_last(&amp;iovad-&gt;rbroot);</span><br><span class="line"> 	else &#123;</span><br><span class="line">-		struct rb_node *prev_node = rb_prev(iovad-&gt;cached32_node);</span><br><span class="line">+		struct rb_node *prev_node = rb_prev(cached_node);</span><br><span class="line"> 		struct iova *curr_iova =</span><br><span class="line">-			container_of(iovad-&gt;cached32_node, struct iova, node);</span><br><span class="line">+			container_of(cached_node, struct iova, node);</span><br><span class="line"> 		*limit_pfn = curr_iova-&gt;pfn_lo - 1;</span><br><span class="line"> 		return prev_node;</span><br><span class="line"> 	&#125;</span><br><span class="line">@@ -72,9 +79,10 @@ static void</span><br><span class="line"> __cached_rbnode_insert_update(struct iova_domain *iovad,</span><br><span class="line"> 	unsigned long limit_pfn, struct iova *new)</span><br><span class="line"> &#123;</span><br><span class="line">-	if (limit_pfn != iovad-&gt;dma_32bit_pfn)</span><br><span class="line">-		return;</span><br><span class="line">-	iovad-&gt;cached32_node = &amp;new-&gt;node;</span><br><span class="line">+	if (limit_pfn &lt;= iovad-&gt;dma_32bit_pfn)</span><br><span class="line">+		iovad-&gt;cached32_node = &amp;new-&gt;node;</span><br><span class="line">+	else</span><br><span class="line">+		iovad-&gt;cached64_node = &amp;new-&gt;node;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> static void</span><br><span class="line">@@ -82,21 +90,26 @@ __cached_rbnode_delete_update(struct iova_domain *iovad, struct iova *free)</span><br><span class="line"> &#123;</span><br><span class="line"> 	struct iova *cached_iova;</span><br><span class="line"> 	struct rb_node *curr;</span><br><span class="line">+	struct rb_node **cached_node;</span><br><span class="line">+</span><br><span class="line">+	if (free-&gt;pfn_hi &lt;= iovad-&gt;dma_32bit_pfn)</span><br><span class="line">+		cached_node = &amp;iovad-&gt;cached32_node;</span><br><span class="line">+	else</span><br><span class="line">+		cached_node = &amp;iovad-&gt;cached64_node;</span><br><span class="line"> </span><br><span class="line">-	if (!iovad-&gt;cached32_node)</span><br><span class="line">+	curr = *cached_node;</span><br><span class="line">+	if(!curr)</span><br><span class="line"> 		return;</span><br><span class="line">-	curr = iovad-&gt;cached32_node;</span><br><span class="line"> 	cached_iova = container_of(curr, struct iova, node);</span><br><span class="line"> </span><br><span class="line"> 	if (free-&gt;pfn_lo &gt;= cached_iova-&gt;pfn_lo) &#123;</span><br><span class="line"> 		struct rb_node *node = rb_next(&amp;free-&gt;node);</span><br><span class="line"> 		struct iova *iova = container_of(node, struct iova, node);</span><br><span class="line"> </span><br><span class="line">-		/* only cache if it&#x27;s below 32bit pfn */</span><br><span class="line">-		if (node &amp;&amp; iova-&gt;pfn_lo &lt; iovad-&gt;dma_32bit_pfn)</span><br><span class="line">-			iovad-&gt;cached32_node = node;</span><br><span class="line">+		if (node)</span><br><span class="line">+			*cached_node = node;</span><br><span class="line"> 		else</span><br><span class="line">-			iovad-&gt;cached32_node = NULL;</span><br><span class="line">+			*cached_node = NULL;</span><br><span class="line"> 	&#125;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line">diff --git a/include/linux/iova.h b/include/linux/iova.h</span><br><span class="line">index f27bb2c..d4670c1 100644</span><br><span class="line">--- a/include/linux/iova.h</span><br><span class="line">+++ b/include/linux/iova.h</span><br><span class="line">@@ -41,6 +41,7 @@ struct iova_domain &#123;</span><br><span class="line"> 	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */</span><br><span class="line"> 	struct rb_root	rbroot;		/* iova domain rbtree root */</span><br><span class="line"> 	struct rb_node	*cached32_node; /* Save last alloced node */</span><br><span class="line">+	struct rb_node	*cached64_node; /* Save last 64bit alloced node */</span><br><span class="line"> 	unsigned long	granule;	/* pfn granularity for this domain */</span><br><span class="line"> 	unsigned long	start_pfn; 	/* Lower limit for this domain */</span><br><span class="line"> 	unsigned long	dma_32bit_pfn;</span><br></pre></td></tr></table></figure>

<p>above solution just adds another cache for 64bit DMA Mask.</p>
<p>But now Linux kernel community just merged a PATCH:<br>        iommu/dma: Implement PCI allocation optimisation<br>into mainline kernel.</p>
<p>This patch just castes 64bit DMA mask to 32bit DMA mask, so we can still use<br>32bit DMA cache to improve the performance.</p>
<p>NOTE: but if this we can not allocate a 64bit iova to a PCIe device’s DMA target<br>      address. This is a problem :(</p>
<h2 id="problem"><a href="#problem" class="headerlink" title="problem"></a>problem</h2><ul>
<li><p>Performance</p>
<p>After SMMU enable and applying above patch, 82599 performance is 7.5Gbps,<br>only 80% performance comparing SMMU disable. We need check if this is correct<br>considering both hardware and software limitation.</p>
</li>
<li><p>NIC panic</p>
<p>After SMMU enable and applying above patch, xxx net will panic :( should fix this.<br>(p.s. already find where the problem is, xxx net dma map once, but unmap multiple<br> times)</p>
</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p>[1] xxx<br>[2] JIRA bug<br>[3] <span class="exturl" data-url="aHR0cHM6Ly9sd24ubmV0L0FydGljbGVzLzM3MDQyMy8=">https://lwn.net/Articles/370423/<i class="fa fa-external-link-alt"></i></span></p>
<pre><code>cd /sys/kernel/debug/tracing
echo ixgbe_* &gt; set_graph_function
echo function_graph &gt; current_tracer
cat trace &gt; ~/smmu_test
</code></pre>
]]></content>
      <tags>
        <tag>SMMU</tag>
        <tag>软件性能</tag>
      </tags>
  </entry>
  <entry>
    <title>How to do UT test in software</title>
    <url>/How-to-do-UT-test-in-software/</url>
    <content><![CDATA[<p>The idea of UT is to test the logic of your codes match with your plan, it is<br>in function level.</p>
<p>Let’s see an example function:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int tested_function(a, b)</span><br><span class="line">&#123;</span><br><span class="line">	c = function(a, b);</span><br><span class="line">	return c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Above example is very simple. We can see this tested_function inside is combined<br>logic. Every time you set inputs, e.g. a, b here, you get a output c. So the UT<br>test of this function is to find proper input combinations to test if return<br>value is expected.</p>
<p>However, we have another kind of function which likes timed-based logic. Even<br>“input” is same, but it can return different value. The reason of this is<br>because there is a memory inside of this kind of function. See blow example:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int tested_function(a, b)</span><br><span class="line">&#123;</span><br><span class="line">	void *c = system_api();</span><br><span class="line">	function1(global_d);</span><br><span class="line">	function2(global_e);</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Everytime you call system_api, its return value is different, as there are some<br>memory in this system_api. Everytime you use global_a, global_b, their values<br>are different, as the scope of these value is outside of this function, so from<br>the view of this tested_function, global_d/global_e is memory part.</p>
<p>So for this kind of time-based logic function. Its inputs are a, b, c, global_d,<br>global_e. So the UT test is to prepare proper inputs combinations and test if<br>return value are expected.</p>
<p>So we should control system_api to create expected c to test our logic in<br>tested_function. So we create a system_api() ourself to replace real system_api(),<br>it will be like below, which is called a stub function of system_api().</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void *system_api()</span><br><span class="line">&#123;</span><br><span class="line">	switch (control_system_api) &#123;</span><br><span class="line">	case CONTROL_1:</span><br><span class="line">		return c_1;</span><br><span class="line">	case CONTROL_2:</span><br><span class="line">		return c_2;</span><br><span class="line">	case CONTROL_3:</span><br><span class="line">		return c_3;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>An ut test for a C file, all outside functions should have related stub functions.<br>Stub functions are the preparations for after ut test.</p>
<p>We should write ut test case to do ut test: first setting inputs, remember if<br>your tested function is a time-base logic function, you should set “input points”<br>for all inputs and memory inputs, e.g. a, b, c, global_d, global_e; second, run<br>tested function; third, check return value comparing with expected ones. A test<br>case will be like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void prepare_input()</span><br><span class="line">&#123;</span><br><span class="line">	control_system_api = 1;</span><br><span class="line">	global_d = 123;</span><br><span class="line">	global_e = 456;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void check_result()</span><br><span class="line">&#123;</span><br><span class="line">	assert(c == 789);</span><br><span class="line">	log();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int test_case_1()</span><br><span class="line">&#123;</span><br><span class="line">	prepare_input();</span><br><span class="line">	c = tested_function(a, b);</span><br><span class="line">	check_result(c);</span><br><span class="line">	clear_input()；</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件测试</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use IO BAR in linux PCIe device driver</title>
    <url>/How-to-use-IO-BAR-in-linux-PCIe-device-driver/</url>
    <content><![CDATA[<h2 id="IO-window-parse-analysis"><a href="#IO-window-parse-analysis" class="headerlink" title="IO window parse analysis"></a>IO window parse analysis</h2><p>   In an ACPI based system, we parse the IO window configured in DSDT table, as<br>   showed in this <a href="https://wangzhou.github.io/PCI-parse-MEM-IO-range-in-CRS-in-ACPI-table/">link</a>.</p>
<p>   We can see in pci_acpi_root_prepare_resources:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_acpi_root_prepare_resources</span><br><span class="line">    --&gt; acpi_pci_probe_root_resources</span><br><span class="line">        --&gt; acpi_pci_root_remap_iospace</span><br></pre></td></tr></table></figure>
<p>   in acpi_pci_root_remap_iospace, CPU address of one PCIe IO window will be<br>   mapped to PCI_IOBASE based system IO space, like below picture:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  PCI_IOBASE            PCI_IOBASE + PCIE_IO_SIZE - 1</span><br><span class="line">        |&lt;-------------&gt;|</span><br><span class="line">--------+---------------+---------------------------   &lt;- CPU VA in kernel</span><br><span class="line">    |   |      |\      \</span><br><span class="line">    |   |      | \      \                  </span><br><span class="line">    v   |      |  \      \                  maps supported by MMU</span><br><span class="line">        |      |   \      \       </span><br><span class="line">        |      |    \      \</span><br><span class="line">        |&lt;----&gt;|     |&lt;----&gt;|   &lt;- CPU PA</span><br><span class="line">    |    \      \     \      \</span><br><span class="line">    |     \      \     \      \             maps supported by ATU</span><br><span class="line">    v      \      \     \      \</span><br><span class="line">            \      \     \      \</span><br><span class="line">             \      \     \      \</span><br><span class="line">              |&lt;----&gt;|     |&lt;----&gt;|  &lt;- PCI address</span><br><span class="line"></span><br><span class="line">             IO window 1   IO window 2</span><br><span class="line">            host bridge 1  host bridge 2</span><br></pre></td></tr></table></figure>

<p>   and offset between CPU VA and PCI_IOBASE will be stored in resource_entry,<br>   which will be passed to PCI enumeration. The offset in resource_entry will be<br>   offset(above) - PCI address. In the process of the enumeration, IO BAR will<br>   be allocated in related IO window.</p>
<p>   the base of IO BAR will be stored in (pci_dev -&gt; resource[IO BAR].start), which<br>   is the offset to PCI_IOBASE in CPU VA.</p>
<h2 id="How-to-use-in-PCIe-device-driver"><a href="#How-to-use-in-PCIe-device-driver" class="headerlink" title="How to use in PCIe device driver"></a>How to use in PCIe device driver</h2><p>   In one hardware arch, we use inb/outb, inw/outw … function to access IO<br>   space. In ARM64, these functions are defined in linux/include/asm-generic/io.h,<br>   like: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#ifndef outb</span><br><span class="line">#define outb outb</span><br><span class="line">static inline void outb(u8 value, unsigned long addr)</span><br><span class="line">&#123;</span><br><span class="line">        writeb(value, PCI_IOBASE + addr);</span><br><span class="line">&#125;</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure>
<p>   So when we want to read/write IO BAR in PCIe device driver, we should:</p>
<ol>
<li>get the base of one IO BAR by: addr = pci_resource_start(dev, bar)</li>
<li>use outb(value, addr) for an example to do port input by byte-width.</li>
</ol>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use Linux kernel crypto compression</title>
    <url>/How-to-use-Linux-kernel-crypto-compression/</url>
    <content><![CDATA[<p>Linux kernel crypto subsystem supports different algorithms including compression<br>algorithms. A hardware or software implementation can be added to crypto<br>subsystem to do specific work.</p>
<p>If you have a hardware module to do Zlib/Gzip compressions, you can write a<br>driver, which can be registered into Linux kernel crypto subsystem.</p>
<p>To use this hardware to do compression, you need not care about hardware detail,<br>only thing you need to know is crypto API, here, you should know crypto compress<br>related API.</p>
<p>Firstly, we should create a crypto compression context using:<br>We only consider synchronized API crypto_alloc_comp here.(struct crypto_comp)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_alloc_comp(const char *alg_name, u32 type, u32 mask)</span><br><span class="line">crypto_alloc_acomp(const char *alg_name, u32 type, u32 mask)</span><br></pre></td></tr></table></figure>
<p>alg_name here can be specific driver name or standard algorithm name. You can<br>find driver name in cra_driver_name item in struct crypto_alg, which may be<br>offered in hardware engine driver’s crypto_alg, we do not offer this; algorithm<br>name is offered in cra_name in struct crypto_alg, which also can be offered in<br>hardware engine driver, we do this, we offer “zlib-deflate” and “gzip”.</p>
<p>Crypto subsystem tries to find registered algorithms by driver name or standard<br>algorithm name. Driver name has the top priority, standard algorithm name and<br>priority item in struct crypto_alg also can be used to find proper algorithm.<br>In user space, you can use “cat /proc/crypto” to find registered crypto<br>algorithms, e.g.:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">name         : crct10dif</span><br><span class="line">driver       : crct10dif-pclmul</span><br><span class="line">module       : crct10dif_pclmul</span><br><span class="line">priority     : 200</span><br><span class="line">refcnt       : 1</span><br><span class="line">selftest     : passed</span><br><span class="line">internal     : no</span><br><span class="line">type         : shash</span><br><span class="line">blocksize    : 1</span><br><span class="line">digestsize   : 2</span><br><span class="line"></span><br><span class="line">name         : crc32</span><br><span class="line">driver       : crc32-pclmul</span><br><span class="line">module       : crc32_pclmul</span><br><span class="line">priority     : 200</span><br><span class="line">refcnt       : 1</span><br><span class="line">selftest     : passed</span><br><span class="line">internal     : no</span><br><span class="line">type         : shash</span><br><span class="line">blocksize    : 1</span><br><span class="line">digestsize   : 4</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>u32 type and u32 mask here, we use below, which are defined in[1]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define CRYPTO_ALG_TYPE_ACOMPRESS	0x0000000a</span><br><span class="line">#define CRYPTO_ALG_TYPE_COMPRESS	0x00000002</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">#define CRYPTO_ALG_TYPE_MASK		0x0000000f</span><br></pre></td></tr></table></figure>
<p>Here, we only support CRYPTO_ALG_TYPE_COMPRESS currently.</p>
<p>After creating compression context, we can use it to do compression/decompression by:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_comp_compress(struct crypto_comp *tfm, const u8 *src,</span><br><span class="line">		     unsigned int slen, u8 *dst, unsigned int *dlen)</span><br><span class="line">crypto_comp_decompress() (currrently we do not support)</span><br></pre></td></tr></table></figure>
<p>src, slen, dst, dlen are input/output buffer’s address and size.</p>
<p>After compression/decompress finished, we call below function to free related<br>context:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_free_comp()</span><br></pre></td></tr></table></figure>

<p>Currently, if you want to use HiSilicon’s Compression engine in D06, which<br>driver is under upstreaming.[2] You should open below kernel configures:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CONFIG_CRYPTO_DEV_HISILICON</span><br><span class="line">CONFIG_CRYPTO_DEV_HISI_QM</span><br><span class="line">CONFIG_CRYPTO_DEV_HISI_ZIP</span><br></pre></td></tr></table></figure>

<p>NOTE:</p>
<p> [1] linux/include/linux/crypto.h<br> [2] <span class="exturl" data-url="aHR0cHM6Ly9sa21sLm9yZy9sa21sLzIwMTgvOS8zLzY=">https://lkml.org/lkml/2018/9/3/6<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>crypto</tag>
      </tags>
  </entry>
  <entry>
    <title>Guest and host communication for QEMU</title>
    <url>/Guest-and-host-communication-for-QEMU/</url>
    <content><![CDATA[<p>We can use 9p fs to do this, qemu cmdline like  below:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 -machine virt -cpu cortex-a57 \</span><br><span class="line">-m 2G \</span><br><span class="line">-kernel ~/repos/kernel-dev/arch/arm64/boot/Image \</span><br><span class="line">-initrd ~/repos/buildroot/output/images/rootfs.cpio.gz \</span><br><span class="line">-append &quot;console=ttyAMA0 root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">-fsdev local,id=p9fs,path=p9root,security_model=mapped \</span><br><span class="line">-device virtio-9p-pci,fsdev=p9fs,mount_tag=p9 \</span><br><span class="line">-nographic \</span><br></pre></td></tr></table></figure>
<p>Here path=p9root is the directory which we can see in host and guest.<br>We can use path=/home/your_account/p9root for example also, but it should be a<br>full path.</p>
<p>Then we can start qemu, and in qemu do:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mount -t 9p p9 /mnt</span><br></pre></td></tr></table></figure>

<p>Then you can see the files in host p9root directory in guest /mnt.</p>
<p>Then we can also debug the kernel running in qemu by gdb. We should add<br>“-gdb tcp::1234” to start a gdb server in qemu and wait on local port 1234 of tcp.<br>Whole qemu cmdline is like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 -machine virt -cpu cortex-a57 \</span><br><span class="line">-m 2G \</span><br><span class="line">-kernel ~/repos/kernel-dev/arch/arm64/boot/Image \</span><br><span class="line">-initrd ~/repos/buildroot/output/images/rootfs.cpio.gz \</span><br><span class="line">-append &quot;console=ttyAMA0 root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">-fsdev local,id=p9fs,path=p9root,security_model=mapped \</span><br><span class="line">-device virtio-9p-pci,fsdev=p9fs,mount_tag=p9 \</span><br><span class="line">-nographic \</span><br><span class="line">-gdb tcp::1234</span><br></pre></td></tr></table></figure>
<p>After kernel in qemu boots up, you can start a gdb cline in host and connect to<br>the gdb server in qemu.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aarch64-linux-gnu-gdb</span><br><span class="line">(gdb) file ~/repos/kernel-dev/vmlinux</span><br><span class="line">(gdb) target remote :1234</span><br></pre></td></tr></table></figure>
<p>Here we use a arm64 based gdb as an example. After “target remote :1234”, we<br>can use gdb to debug kernel in qemu. You can just set break points, print value<br>of variables…</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>How to test PCIe SR-IOV in D0x</title>
    <url>/How-to-test-PCIe-SR-IOV-in-D0x/</url>
    <content><![CDATA[<h2 id="preparation"><a href="#preparation" class="headerlink" title="preparation"></a>preparation</h2><p>kernel: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hpc2lsaWNvbi9rZXJuZWwtZGV2LmdpdA==">https://github.com/hisilicon/kernel-dev.git<i class="fa fa-external-link-alt"></i></span> branch: private-topic-sriov-v3-4.10</p>
<p>UEFI: openlab1.0 101 server /home/wangzhou/repo/plinth_uefi/uefi</p>
<p>qemu: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2VhdWdlci9xZW11LmdpdA==">https://github.com/eauger/qemu.git<i class="fa fa-external-link-alt"></i></span> branch: v2.7.0-passthrough-rfc-v5</p>
<p>hardware topology:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------------+       +----------------------+</span><br><span class="line">|  D05 I        |       |         D05 II       |</span><br><span class="line">|               |       |                      |</span><br><span class="line">|   +-----------+       +------------------+   |</span><br><span class="line">|   |1P NA PCIe2|&lt;-----&gt;|any 10G networking|   |</span><br><span class="line">|   +-----------+       +------------------+   |</span><br><span class="line">+---------------+       +----------------------+</span><br></pre></td></tr></table></figure>
<h2 id="compile-kernel-and-UEFI"><a href="#compile-kernel-and-UEFI" class="headerlink" title="compile kernel and UEFI"></a>compile kernel and UEFI</h2><p>configure kernel: Add SMMU_V3=y, 82599 PF driver = m, 82599 VF driver = m,<br>                  VFIO PCI driver = m (p.s. ACPI boot)<br>compile kernel image and ko<br>compile UEFI using: ./uefi-tools/uefi-build.sh -c ./LinaroPkg/platforms.config d05</p>
<h2 id="basic-test"><a href="#basic-test" class="headerlink" title="basic test"></a>basic test</h2><pre><code>1. boot up host OS (firstly update UEFI above)[1]

2. copy modules to host OS:
   ixgbe.ko mdio.ko vfio_iommu_type1.ko vfio.ko vfio-pci.ko
   vfio_virqfd.ko irqbypass.ko

3. prepare host environment:

   mkdir /lib/modules/`uname -r`
   touch /lib/modules/`uname -r`/modules.order
   touch /lib/modules/`uname -r`/modules.builtin
   depmod ixgbe.ko  mdio.ko vfio_iommu_type1.ko vfio.ko vfio-pci.ko
          vfio_virqfd.ko irqbypass.ko

   /*
    * here we insert ixgbe as it will use function in ixgbe driver to triggre
    * VF. If we do no insert ixgbe here, we will not get VF&#39;s sysfs interface,
    * and lspci will not show information of VF.
    *
    * here we need not insert ixgbevf(VF driver), as we will try to bind vfio-pci
    * to VF device. However, if we want to use VF in host, we should insert
    * ixgbevf here. After inserting ixgbevf, if we want to use vfio-pci driver,
    * we should firstly unbind ixgbevf driver for VF device using:
    * echo 0002:81:10.0 &gt; /sys/bus/pci/drivers/ixgbevf/unbind, then we can
    * use  echo vfio-pci &gt; /sys/bus/pci/devices/0002:81:10.0/driver_override
    *      echo 0002:81:10.0 &gt; /sys/bus/pci/drivers_probe
    * to bind vfio-pci driver with VF device.
    */
   modprobe ixgbe

   /* trigger one VF, 0002:81:00.0 is the PF in which you want trigger a VF */
   echo 1 &gt; /sys/devices/pci0002:80/0002:80:00.0/0002:81:00.0/sriov_numvfs

   modprobe -v vfio-pci disable_idle_d3=1
   modprobe -r vfio_iommu_type1
   modprobe -v vfio_iommu_type1 allow_unsafe_interrupts=1

   /* set related PF up */
   ifconfig eth26 up

   /* 0002:81:10.0 is BDF of VF */
   echo vfio-pci &gt; /sys/bus/pci/devices/0002:81:10.0/driver_override
   echo 0002:81:10.0 &gt; /sys/bus/pci/drivers_probe

4. run qemu[2]

   qemu-system-aarch64 \
   -machine virt,gic-version=3 \
   -enable-kvm \
   -cpu host \
   -m 1024 \
   -kernel ./Image \
   -initrd ./minifs.cpio.gz \
   -nographic \
   -net none -device vfio-pci,host=0002:81:10.0,id=net0 \
   -D trace_log

5. set networking configurations in guest machine and remote machine
   
   run ping and iperf to test
</code></pre>
<h2 id="more-scenarios"><a href="#more-scenarios" class="headerlink" title="more scenarios"></a>more scenarios</h2><pre><code>1. enable multiple VFs, assigned to one VM
2. enable multiple VFs, assigned to different VMs
3. use VF directly
4. VF and PF communicate
</code></pre>
<h2 id="performance"><a href="#performance" class="headerlink" title="performance"></a>performance</h2><pre><code>1. should at least reach the performance of PF[3]
</code></pre>
<p>reference:<br>[1] should add pcie_acs_override=downstream in kernel command line:<br>    As our PCIe RP does not support ACS capbility, we should add this command line<br>    and related patch(<span class="exturl" data-url="aHR0cHM6Ly9sa21sLm9yZy9sa21sLzIwMTMvNS8zMC81MTM=">https://lkml.org/lkml/2013/5/30/513<i class="fa fa-external-link-alt"></i></span>) which will never be<br>    merged into mainline to tell kernel our PCIe RP indeed did something to<br>    provide address isolation just like ACS does.<br>    we will upstream in ACS quirk patch to fix this.<br>[2] how to compile qemu locally in D05:<br>    <a href="https://wangzhou.github.io/ARM64-qemu-native-build/">https://wangzhou.github.io/ARM64-qemu-native-build/</a><br>[3] should add 82599 patch to test, firstly make sure performance of 82599 PF is<br>    good.</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>软件测试</tag>
      </tags>
  </entry>
  <entry>
    <title>LDD3 study note 1</title>
    <url>/LDD3-study-note-1/</url>
    <content><![CDATA[<h2 id="这是一个什么设备"><a href="#这是一个什么设备" class="headerlink" title="这是一个什么设备"></a>这是一个什么设备</h2><p>这是一个内存模拟出来的设备，简单说就是一些内存区域。在用户态，可以对相应的设备<br>文件read/write，相应的结果是读写这些内存区域中的数据。</p>
<p>这些内存的区域按照下面的数据结构组成：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-------+         +-------+          +-------+         +-------+   </span><br><span class="line">| list  +--------&gt;+       +---------&gt;+       +--------&gt;+       |   ....</span><br><span class="line">|       |         |       |          |       |         |       |</span><br><span class="line">+---+---+         +-------+          +-------+         +-------+</span><br><span class="line">    |               ...                                            </span><br><span class="line">    +--------+      +-------------------------+                 </span><br><span class="line">    | pointer+-----&gt;+ quantum(e.g. 4000) Byte |</span><br><span class="line">    +--------+      +-------------------------+</span><br><span class="line">    |        +-----&gt; ...                                            </span><br><span class="line">    +--------+                                                  </span><br><span class="line">    |        |                                                  </span><br><span class="line">    +--------+                                                  </span><br><span class="line">    ...                                                 </span><br><span class="line">    +--------+                                                  </span><br><span class="line">    |        |                                                  </span><br><span class="line">    +--------+                                                  </span><br><span class="line">    |        |                                                  </span><br><span class="line">    +--------+                                                  </span><br></pre></td></tr></table></figure>
<p>可以看到数据只是存在quantum的单元里的，其他的数据结构都是为了索引到这个结构。<br>这个数据结构的特点是，链表的长度是不固定的(quantum, 指针数组的大小是固定的), 这样<br>只要一直往这个设备里写数据，链表就不断的加长，同时不断的创建新的quantum, 把数据<br>存在里面。这个过程可以吃光系统里所有的内存。还有一个特点就是，设备一开始不会分配<br>quantum, 只有有数据需要写入，又没有quantum时才开始分配。所有分配的quantum在模块<br>卸载时释放掉。同时，设备的读写函数设计成如果一次读写的数据超过一个quantum的界限，<br>那么只读写当前一个quantum中的数据。</p>
<h2 id="open-release-read-write的实现"><a href="#open-release-read-write的实现" class="headerlink" title="open/release/read/write的实现"></a>open/release/read/write的实现</h2><p>open: 把scull_dev挂成struce file的数据，方便以后其他的操作找到设备<br>read/write: …<br>release: 关闭文件的时候要调用到, 对scull_dev不做处理。模块退出的时候, 释放设备对应的<br>         内存, 这个操作应该是在exit函数里，不放到这里。</p>
<h2 id="调试宏"><a href="#调试宏" class="headerlink" title="调试宏"></a>调试宏</h2><p>在代码中插入了一些断言的宏，方便调试。</p>
<h2 id="自动创建设备节点"><a href="#自动创建设备节点" class="headerlink" title="自动创建设备节点"></a>自动创建设备节点</h2><p>LDD3源代码中是加了一个脚本创建设备节点，这里创建一个class, 然后在class里创建这个<br>字符设备对应的设备文件。模块加载时会自动在/dev下创建设备文件。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>LDD3</tag>
      </tags>
  </entry>
  <entry>
    <title>How to dump acpi tables in CentOS</title>
    <url>/How-to-dump-acpi-tables-in-CentOS/</url>
    <content><![CDATA[<p>My system is ARM64 CentOS7.4</p>
<p>1.yum install acpica-tools.aarch64</p>
<p>2.acpidump &gt; acpidump.out </p>
<p>3.acpixtract -a acpidump.out</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@157 acpi_test]# acpixtract -a acpidump.out</span><br><span class="line"></span><br><span class="line">Intel ACPI Component Architecture</span><br><span class="line">ACPI Binary Table Extraction Utility version 20160527-64</span><br><span class="line">Copyright (c) 2000 - 2016 Intel Corporation</span><br><span class="line"></span><br><span class="line">Acpi table [SPCR] -      80 bytes written to spcr.dat</span><br><span class="line">Acpi table [MCFG] -     172 bytes written to mcfg.dat</span><br><span class="line">Acpi table [GTDT] -     124 bytes written to gtdt.dat</span><br><span class="line">Acpi table [APIC] -    5348 bytes written to apic.dat</span><br><span class="line">Acpi table [IORT] -    1300 bytes written to iort.dat</span><br><span class="line">Acpi table [SLIT] -      60 bytes written to slit.dat</span><br><span class="line">Acpi table [DSDT] -   26427 bytes written to dsdt.dat</span><br><span class="line">Acpi table [SRAT] -    1400 bytes written to srat.dat</span><br><span class="line">Acpi table [DBG2] -      90 bytes written to dbg2.dat</span><br><span class="line">Acpi table [FACP] -     276 bytes written to facp.dat</span><br><span class="line"></span><br><span class="line">10 binary ACPI tables extracted</span><br><span class="line">[root@157 acpi_test]# ls</span><br><span class="line">acpidump.out  apic.dat  dbg2.dat  dsdt.dat  facp.dat  gtdt.dat  iort.dat  mcfg.dat  slit.dat  spcr.dat  srat.dat</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>   After this command, you can see related ACPI tables.</p>
<p>4.iasl -d *.dat</p>
<p>e.g. un-compile mcfg.dat:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@157 acpi_test]# iasl -d mcfg.dat</span><br><span class="line"></span><br><span class="line">Intel ACPI Component Architecture</span><br><span class="line">ASL+ Optimizing Compiler version 20160527-64</span><br><span class="line">Copyright (c) 2000 - 2016 Intel Corporation</span><br><span class="line"></span><br><span class="line">Input file mcfg.dat, Length 0xAC (172) bytes</span><br><span class="line">ACPI: MCFG 0x0000000000000000 0000AC (v01 HISI   HIP07    00000000 INTL 20151124)</span><br><span class="line">Acpi Data Table [MCFG] decoded</span><br><span class="line">Formatted output:  mcfg.dsl - 3955 bytes</span><br><span class="line">[root@157 acpi_test]# ls mcfg.dsl</span><br><span class="line">mcfg.dsl</span><br></pre></td></tr></table></figure>

<p>5.Then you can view it: vim mcfg.dsl</p>
]]></content>
      <tags>
        <tag>UEFI</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>LDD3 study note 2</title>
    <url>/LDD3-study-note-2/</url>
    <content><![CDATA[<h2 id="ioctl"><a href="#ioctl" class="headerlink" title="ioctl"></a>ioctl</h2><p>驱动可以通过ioctl函数定义一组和用户态程序交互的接口.<br>ioctl的用户态接口是：int ioctl(int d, int request, …). 一般我们可以认为是：<br>int ioctl(int fd, int cmd, int arg)。其中fd是要操作的文件，cmd是下发的命令字,<br>和驱动里ioctl实现的命令字是一一对应的。arg是传入内核的参数，可以看到一般的情况<br>下arg这个参数是一个指针变量。</p>
<h2 id="命令码"><a href="#命令码" class="headerlink" title="命令码"></a>命令码</h2><p>cmd不是可以随便定义的，具体可以参考linux/Documentation/ioctl/ioctl-number.txt<br>这个文档。简单来讲，一个命令字是四段组成的。每段具体是什么内容可以查看LDD3或者<br>上面的文档。生成命令字需要用内核提供的一组宏，这组宏的定义在：<br>linux/include/uapi/asm-generic/ioctl.h</p>
<p>一般定义命令字用下面这组宏:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define _IO(type,nr)		_IOC(_IOC_NONE,(type),(nr),0)</span><br><span class="line">#define _IOR(type,nr,size)	_IOC(_IOC_READ,(type),(nr),(_IOC_TYPECHECK(size)))</span><br><span class="line">#define _IOW(type,nr,size)	_IOC(_IOC_WRITE,(type),(nr),(_IOC_TYPECHECK(size)))</span><br><span class="line">#define _IOWR(type,nr,size)	_IOC(_IOC_READ|_IOC_WRITE,(type),(nr),(_IOC_TYPECHECK(size)))</span><br></pre></td></tr></table></figure>

<p>type是在ioctl-number.txt里讲过的魔术字，作为命令字最基本的区分。nr是ioctl接口<br>的第几个命令，一般是0,1,2…, size LDD3上解释的不是很清楚，暂时用int(Fix me)。<br>命令的类型会合成命令字中的一段。这样，以上四段内容会拼成一个ioctl的命令字。</p>
<p>可以看到一个ioctl支持的接口命令中type，size一样的，命令的类型和序号有可能有区别。</p>
<h2 id="用户态头文件"><a href="#用户态头文件" class="headerlink" title="用户态头文件"></a>用户态头文件</h2><p>ioctl是一种用户态和内核交流信息的方式。用户态调用ioctl的时候，发起的命令、传入<br>内核的数据结构, 在用户态都要有定义。目前的本人的做法是，在用户态的头文件中拷贝<br>内核中命令码生成的相关宏定义(Fix me)，对于交流信息的数据结构，也在用户态头文件中<br>再定义一次。</p>
<h2 id="传变量和传指针"><a href="#传变量和传指针" class="headerlink" title="传变量和传指针"></a>传变量和传指针</h2><p>受到ioctl接口的限制, 用户态可以使用传变量和传指针的方式向内核发送信息, 传变量<br>也只能是一个unsigned long类型。内核向用户态传信息，就只能向用户态传进来的指针里<br>写数据了。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>LDD3</tag>
      </tags>
  </entry>
  <entry>
    <title>Intel QAT ZIP初步分析</title>
    <url>/Intel-QAT-ZIP%E5%88%9D%E6%AD%A5%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol>
<li>基本信息 </li>
</ol>
<hr>
<p> QAT的官网: <span class="exturl" data-url="aHR0cHM6Ly93d3cuaW50ZWwuY24vY29udGVudC93d3cvY24vemgvYXJjaGl0ZWN0dXJlLWFuZC10ZWNobm9sb2d5L2ludGVsLXF1aWNrLWFzc2lzdC10ZWNobm9sb2d5LW92ZXJ2aWV3Lmh0bWw=">https://www.intel.cn/content/www/cn/zh/architecture-and-technology/intel-quick-assist-technology-overview.html<i class="fa fa-external-link-alt"></i></span></p>
<p> QAT相关的代码在01.org网站: <span class="exturl" data-url="aHR0cHM6Ly8wMS5vcmcvemgvaW50ZWwtcXVpY2thc3Npc3QtdGVjaG5vbG9neT9sYW5ncmVkaXJlY3Q9MQ==">https://01.org/zh/intel-quickassist-technology?langredirect=1<i class="fa fa-external-link-alt"></i></span></p>
<p> 基本QAT相关的用户手册，代码都可以从上面的URL获得。</p>
<ol start="2">
<li>用户APP、QATzip、libqat库的关系</li>
</ol>
<hr>
<p> 我们这里看看QAT里支持的压缩解压缩是怎么最终叫用户APP使用到的。</p>
<p> QAT整个压缩解压缩的软件栈由: 内核驱动，libqat用户态基础库，QATzip用户态库组成。<br> 我们这里以ceph作为APP，一起看下，ceph里的压缩解压缩可以直接调用QATzip库提供的<br> 接口使用QAT的硬件压缩解压缩引擎。</p>
<p> 以上软件的位置在：1. QAT的内核态驱动和libqat用户态基础库合在一起放在上面的01.org<br> 的这个包里：Intel® QuickAssist Technology Driver for Linux* - HW version 1.7.<br> 2. QATzip的代码在：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ludGVsL1FBVHppcA==">https://github.com/intel/QATzip<i class="fa fa-external-link-alt"></i></span>. 3. ceph的代码：github.com/ceph.</p>
<p> 整个调用链的逻辑是：</p>
<ol>
<li><p>内核态QAT crypto驱动除了向crypto子系统上注册外，也会向内核UIO子系统注册，<br>通过UIO把QAT的硬件资源暴露给用户态。由于UIO存在安全上的问题，可以看到主线<br>内核里注册到UIO的驱动很少，这也是QAT内核驱动中注册UIO这部分无法上传到主线<br>的原因。</p>
</li>
<li><p>libqat用户态基础库封装UIO用户态接口，向上提供一组基础的API。这个在01.org<br>网站的接口说明文档中有介绍。</p>
</li>
<li><p>QATzip这个库调用libqat API对外提供QAT的压缩解压缩基本接口。提供的接口在<br>QATzip/include/qatzip.h这个头文件中。</p>
</li>
<li><p>Ceph代码里压缩解压缩的部分ceph/src/compressor/有QatAccel.cc, 这部分代码<br>调用QATzip的接口封装ceph里的压缩解压缩接口供同目录下的zlib/zlibCompressor.cc<br>使用。(目前竟然是HAVE_QATZIP这个宏隔开的 :( )</p>
</li>
<li><p>接口分析</p>
</li>
</ol>
<hr>
<ol>
<li><p>libqat有一个qat_service的服务，这个服务可能和/dev/qat_adf_ctl的这个字符设备<br>配合提供一些管理工作。</p>
</li>
<li><p>qat卡可以load不同的固件，从而改变qat卡自身的功能。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>ZIP</tag>
        <tag>QAT</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux IO DMA地址映射流程分析</title>
    <url>/Linux-IO-DMA%E5%9C%B0%E5%9D%80%E6%98%A0%E5%B0%84%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol>
<li>SMMU页表以及相关配置的初始化流程</li>
</ol>
<hr>
<p> iommu_ops里的attach_dev回调用来在SMMU一侧为master设备建立各种数据结构。如下是<br> arm_smmu_attach_dev的流程:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">arm_smmu_attach_dev</span><br><span class="line">  +-&gt; arm_smmu_domain_finalise</span><br><span class="line">        /*</span><br><span class="line">         * 目的是创建smmu_domain里的pgtbl_ops， 这个结构的原型是struct io_pgtable_ops</span><br><span class="line">         * struct io_pgtable_ops</span><br><span class="line">         *   +-&gt; map   </span><br><span class="line">         *   +-&gt; unmap</span><br><span class="line">         *   +-&gt; iova_to_phys</span><br><span class="line">         */</span><br><span class="line">    +-&gt; alloc_io_pgtable_ops</span><br><span class="line">        /*</span><br><span class="line">         * 以64bit s1为例子， 如下函数初始化页表的pgd， 并且初始化页表map/unmap的</span><br><span class="line">         * 操作函数</span><br><span class="line">         */</span><br><span class="line">      +-&gt; arm_64_lpae_alloc_pgtable_s1</span><br><span class="line"></span><br><span class="line">            /* 主要创建页表操作函数 */</span><br><span class="line">        +-&gt; arm_lpae_alloc_pgtable</span><br><span class="line">          +-&gt; map = arm_lpae_map</span><br><span class="line">          +-&gt; unmap = arm_lpae_unmap</span><br><span class="line">          +-&gt; iova_to_phys = arm_lpae_iova_to_phys</span><br><span class="line"></span><br><span class="line">            /* 创建pgd */</span><br><span class="line">        +-&gt; __arm_lpae_alloc_pages</span><br><span class="line"></span><br><span class="line">            /* 得到页表基地址 */</span><br><span class="line">        +-&gt; cfg-&gt;arm_lpae_s1_cfg.ttbr = virt_to_phys(data-&gt;pgd);</span><br><span class="line"></span><br><span class="line">        /* 收尾的配置再搞下，目前是用来配置CD表 */</span><br><span class="line">    +-&gt; finalise_stage_fn</span><br><span class="line">        /* 得到的io_pgtable_ops存放到smmu_domain中 */</span><br><span class="line">    +-&gt; smmu_domain-&gt;pgtbl_ops</span><br><span class="line"></span><br><span class="line">  +-&gt; arm_smmu_install_ste_for_dev</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>map流程分析</li>
</ol>
<hr>
<p> 我们从内核DMA API接口向下跟踪，观察dma内存的申请和map的流程。以dma_alloc_coherent<br> 为例分析，这个接口按照用户的请求申请内存，返回CPU虚拟地址和iova。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dma_alloc_coherent</span><br><span class="line">  +-&gt; dma_alloc_attrs /* kernel/dma/mapping.c */</span><br><span class="line">    +-&gt; iommu_dma_alloc /* drivers/iommu/dma-iommu.c */</span><br><span class="line">          /*</span><br><span class="line">           * 如下是内存分配和map的主体逻辑，大概可以分成两块。第一块是iomm_dma_alloc_remap，</span><br><span class="line">           * 这个内存分配和map在这个函数里一起完成，第二块是其余的逻辑，这部分逻辑把分配</span><br><span class="line">           * 内存和map分开了。第二部分里又有从dma pool里分内存和直接分内存，我们不分析</span><br><span class="line">           * dma pool里的case。</span><br><span class="line">           *</span><br><span class="line">           * 如上的case1和case3的核心区别是有没有开DMA_REMAP的内核配置，对应到具体的实现</span><br><span class="line">           * 是，REMAP的情况可以申请不连续的物理页面，调用remap函数得到连续的CPU虚拟地址。</span><br><span class="line">           * 可以看到REMAP的情况才真正支持大范围的dma地址。如果REMAP没有开，也就是case3，</span><br><span class="line">           * iommu_dma_alloc_pages中实际是调用伙伴系统的接口(不考虑CMA的情况)，受MAX_ORDER</span><br><span class="line">           * 的影响，一次可分配的连续物理内存是有限制的。</span><br><span class="line">           */</span><br><span class="line">      +-&gt; iommu_dma_alloc_remap</span><br><span class="line">            /*</span><br><span class="line">             * 根据size分配物理页面，多次调用伙伴系统接口分配不连续的物理页面块。同时</span><br><span class="line">             * 这个函数还做了iommu的map。我们仔细看下这个函数的细节。</span><br><span class="line">             */</span><br><span class="line">        +-&gt; __iommu_dma_alloc_noncontiguous</span><br><span class="line">              /* 这个风骚的bit运算取到的是最小一级的数值， 一般最小一级就是系统页大小 */</span><br><span class="line">          +-&gt; min_size = alloc_sizes &amp; -alloc_sizes;</span><br><span class="line">              /*</span><br><span class="line">               * 分配的算法在如下的函数里，count是需要分配页面的个数，这里的页是指系统</span><br><span class="line">               * 页大小。order_mask是页表里各级block的大小的mask，显然拿到这个信息是为了</span><br><span class="line">               * 分配的时候尽量从block分配，这个信息从iommu_domain的pgsize_bitmap中得到，</span><br><span class="line">               * pgsize_bitmap和具体的页表实现有关，在具体的iommu驱动里赋值，比如ARM的</span><br><span class="line">               * SMMUv3在4K页大小下，他的各级block大小是4K、2M和1G，所以，pgsize_bitmap</span><br><span class="line">               * 是 SZ_4K | SZ_2M | SZ_1G。</span><br><span class="line">               */</span><br><span class="line">          +-&gt; __iommu_dma_alloc_pages(...， count， order_mask， ...)</span><br><span class="line">                /*</span><br><span class="line">                 * 这段while循环是分配的主逻辑，通过位运算计算每次分配内存的大小。</span><br><span class="line">                 * (2U &lt;&lt; __fls(count) - 1)得到count的mask，比如count是0b1000，</span><br><span class="line">                 * mask就是0b1111， mask和order_mask相与，取出最高bit，就是针对</span><br><span class="line">                 * 当前count可以分配内存的最大block size，然后调用伙伴系统的接口</span><br><span class="line">                 * 去分配连续的物理内存。然后，跳出循环，更新下次需要分配的count，</span><br><span class="line">                 * 把本次分配的物理内存一页一页的放到输出pages数组里。虽然分配</span><br><span class="line">                 * 出的可以是物理地址连续的一个block，但是输出还是已page保存的。</span><br><span class="line">                 */</span><br><span class="line">            +-&gt; while (count) &#123;...&#125;</span><br><span class="line">              /* 分配iova */</span><br><span class="line">          +-&gt; iommu_dam_alloc_iova</span><br><span class="line">              /*</span><br><span class="line">               * 把如上分配的物理页用一个sgl的数据组合起来，注意连续的物理也会</span><br><span class="line">               * 又合并到一个sgl节点里。后面的iommu_map就可以把一个block映射到</span><br><span class="line">               * 一个页表的block里。不过具体的map逻辑还要在具体iommu驱动的map</span><br><span class="line">               * 回调函数中实现。从这里的分析可以看出，iommu驱动map回调函数输入</span><br><span class="line">               * 的size值并不是一定是page size大小。</span><br><span class="line">               */</span><br><span class="line">          +-&gt; sg_alloc_table_from_pages</span><br><span class="line">              /* 把iova到物理页面的map建立好 */</span><br><span class="line">          +-&gt; iommu_map_sg_atomic</span><br><span class="line">       </span><br><span class="line">            /* 把离散的物理页面remap到连续的CPU虚拟地址上 */</span><br><span class="line">        +-&gt; dma_common_pages_remap </span><br><span class="line"></span><br><span class="line">      +-&gt; iommu_dma_alloc_pages</span><br><span class="line"></span><br><span class="line">      +-&gt; __iommu_dma_map</span><br><span class="line">        [...]</span><br><span class="line">            /* 可以看到这个函数的while循环里也是如上从最大block分配的类似算法 */</span><br><span class="line">        +-&gt; __iommu_map</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/iommu/io-pgtbl-arm.c */</span><br><span class="line">/*</span><br><span class="line"> * 这个函数就是具体做页表映射的函数，函数的输入iova，paddr， size，iomm_prot已经</span><br><span class="line"> * 表示了要映射地址的va， pa， size和属性。这里iova和paddr的具体分配在上层的dma</span><br><span class="line"> * 框架里已经搞定。lvl是和ARM SMMUv3页表level相关的一个参数，不同页大小、VA位数</span><br><span class="line"> * stage对应的页表level以及起始level有不一样的情况。比如，如下的48bit、4K page</span><br><span class="line"> * size的情况，就是有level0/1/2/3四级页表。__arm_lpae_map具体要做的把一个给定</span><br><span class="line"> * map参数的翻译添加到页表里。</span><br><span class="line"> */</span><br><span class="line">arm_smmu_map</span><br><span class="line">  +-&gt; arm_lpae_map</span><br><span class="line">    +-&gt; __arm_lpae_map(...， iova， paddr， size， prot， lvl， ptep， ...)</span><br></pre></td></tr></table></figure>
<p> __arm_lpae_map的实现比较直白，就是递归的创建页表。完全按照上层给的page或者block<br> 的map来做页表映射。</p>
<ol start="3">
<li>页表相关的细节</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    level              0       1        3        3</span><br><span class="line">    block size                 1G       2M       4K</span><br><span class="line">+--------+--------+--------+--------+--------+--------+--------+</span><br><span class="line">|63    56|55    48|47    39|38    30|29    21|20    12|11     0|</span><br><span class="line">+--------+--------+--------+--------+--------+--------+--------+</span><br><span class="line"> |                 |         |         |         |         |</span><br><span class="line"> |                 |         |         |         |         v</span><br><span class="line"> |                 |         |         |         |   [11:0]  in-page offset</span><br><span class="line"> |                 |         |         |         +-&gt; [20:12] L3 index</span><br><span class="line"> |                 |         |         +-----------&gt; [29:21] L2 index</span><br><span class="line"> |                 |         +---------------------&gt; [38:30] L1 index</span><br><span class="line"> |                 +-------------------------------&gt; [47:39] L0 index</span><br><span class="line"> +-------------------------------------------------&gt; [63] TTBR0/1</span><br></pre></td></tr></table></figure>
<p> 如上是一个ARM64(SMMUv3)48bit、4K page size的VA用来做每级页表索引的划分, 这个划分<br> 比较常见riscv sv39也是这样的划分，只不过是少了最高的一级。在这样的划分下，每一级<br> 页表都有512个entry，如果一个页表项是64bit，每一级页表的每个table就正好占用4KB内存。</p>
]]></content>
      <tags>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
        <tag>DMA</tag>
        <tag>iommu</tag>
        <tag>页表</tag>
      </tags>
  </entry>
  <entry>
    <title>KAE笔记</title>
    <url>/KAE%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>梳理KAE实现Openssl engine的基本逻辑。本文基于的KAE代码在这个位置:<br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1bnBlbmdjb21wdXRlL0tBRQ==">https://github.com/kunpengcompute/KAE<i class="fa fa-external-link-alt"></i></span></p>
<ol>
<li>注册为openssl engine的位置在: engine_kae.c</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IMPLEMENT_DYNAMIC_BIND_FN(bind_kae)                                             </span><br><span class="line">	+-&gt; kae_engine_setup() // 配置引擎的参数</span><br><span class="line">		+-&gt; cipher_module_init() // 初始化sec引擎</span><br><span class="line">			初始化队列池，但是没有申请队列</span><br><span class="line">			+-&gt; wd_ciphers_init_qnode_pool()</span><br><span class="line"></span><br><span class="line">			+-&gt; sec_create_ciphers()</span><br><span class="line"></span><br><span class="line">				+—&gt; sec_ciphers_set_cipher_method(cipher_info_t cipherinfo)      </span><br><span class="line"></span><br><span class="line">					注册一个实例的回调函数。每个实例的创建、做任务、销毁分别是：init, do_cipher, cleanup。</span><br><span class="line">					EVP_CIPHER *cipher = EVP_CIPHER_meth_new(cipherinfo.nid, cipherinfo.blocksize, cipherinfo.keylen);</span><br><span class="line">					EVP_CIPHER_meth_set_iv_length(cipher, cipherinfo.ivlen);             </span><br><span class="line">					EVP_CIPHER_meth_set_flags(cipher, cipherinfo.flags);                 </span><br><span class="line">					EVP_CIPHER_meth_set_init(cipher, sec_ciphers_init);                  </span><br><span class="line">					EVP_CIPHER_meth_set_do_cipher(cipher, sec_ciphers_do_cipher);        </span><br><span class="line">					EVP_CIPHER_meth_set_set_asn1_params(cipher, EVP_CIPHER_set_asn1_iv); </span><br><span class="line">					EVP_CIPHER_meth_set_get_asn1_params(cipher, EVP_CIPHER_get_asn1_iv); </span><br><span class="line">					EVP_CIPHER_meth_set_cleanup(cipher, sec_ciphers_cleanup);            </span><br><span class="line">					EVP_CIPHER_meth_set_impl_ctx_size(cipher, sizeof(cipher_priv_ctx_t));</span><br><span class="line"></span><br><span class="line">			注册sec的回调处理函数</span><br><span class="line">			+-&gt; async_register_poll_fn(ASYNC_TASK_CIPHER, sec_cipher_engine_ctx_poll)</span><br><span class="line">		...</span><br><span class="line"></span><br><span class="line">		+-&gt; async_module_init() //起异步线程, 整个引擎里只有一个异步的轮训线程</span><br><span class="line">			+-&gt; async_polling_thread_init()</span><br><span class="line"></span><br><span class="line">	+-&gt; ENGINE_set_ciphers(e, sec_engine_ciphers) // 向系统里设置sec引擎</span><br></pre></td></tr></table></figure>
<p>所以, 一个openssl engine使用wd队列的模型是：每次上层用户请求创建一个实例，每个实例<br>里申请一个wd队列，可以反复向这一个队列上发送任务，发送任务的时候在这个队列上创建<br>一个ctx，反复发送任务时, 每次都复用这个队列和ctx。</p>
<p>发送异步任务和以上同步任务情况是一样的。不一样的是，每一个异步任务发送后，都把队列<br>的信息加入到一个自定义的软件队列里，然后向异步轮寻线程发送通知，异步轮寻线程接受<br>到通知后轮寻对应的硬件队列:</p>
<p>sec_ciphers_do_cipher(EVP_CIPHER_CTX *ctx, unsigned char *out, const unsigned char *in, size_t inl)<br>    +-&gt; sec_ciphers_async_do_crypto(e_cipher_ctx, &amp;op_done)<br>        +-&gt; async_add_poll_task(e_cipher_ctx, op_done, type)</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h2><ol>
<li><p>KAE openssl并没有整理规划wd队列的使用，如果是一个新请求就创建一个队列<br>的话，最大并发请求个数会是2048</p>
</li>
<li><p>KAE在支持存储的时候，spark，ceph都会出现大量同步并发请求的情况。<br>目前需要把ctx-&gt;q, 多对1的情况支持起来。</p>
</li>
<li><p>异步的时候，使用少量线程就可以跑满硬件性能。所以，异步并发请求不大。<br>同步会有大的并发请求</p>
</li>
<li><p>一个进程里的请求大概是相同大小的。</p>
</li>
<li><p>KAE一个engine一个异步轮寻队列，目前可以满足他的性能需求。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>openssl engine</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux PCIe DPC analysis</title>
    <url>/Linux-PCIe-DPC-analysis/</url>
    <content><![CDATA[<p>linux implements PCIe dpc feature as a service, which is registed in PCIe port<br>driver subsystem in drivers/pci/pcie/pcie-dpc.c</p>
<p>to check: if dpc_probe will be called for any port type, as in<br>          struct pcie_port_service_driver dpcdriver, port_type = PCIE_ANY_PORT.</p>
<p>let’s assume dpc_probe will be called for any port type, then the flow of probe:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; INIT_WORK(&amp;dpc-&gt;work, interrupt_event_handler);</span><br><span class="line">--&gt; devm_request_irq(&amp;dev-&gt;device, dev-&gt;irq, dpc_irq, IRQF_SHARED, &quot;pcie-dpc&quot;, dpc);</span><br><span class="line"></span><br><span class="line">    /* set PCI_EXP_DPC_CTL_EN_NONFATAL, 0x6 offset in dpc cap: 0x02</span><br><span class="line">     * PCI_EXP_DPC_CTL_INT_EN</span><br><span class="line">     */</span><br><span class="line">--&gt; pci_write_config_word(pdev, dpc-&gt;cap_pos + PCI_EXP_DPC_CTL, ctl);</span><br></pre></td></tr></table></figure>
<p>so it is quiet easy above, just enable the non-fatal and top interrupt.</p>
<p>In the irq handler:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    /* just read some data from dpc cap, then print info to user, we can</span><br><span class="line">     * see which kind of error happened: unconrrectable, non-fatal, fatal.</span><br><span class="line">     */</span><br><span class="line">--&gt; pci_read_config_word(pdev, dpc-&gt;cap_pos + PCI_EXP_DPC_STATUS, &amp;status);</span><br><span class="line">--&gt; pci_read_config_word(pdev, dpc-&gt;cap_pos + PCI_EXP_DPC_SOURCE_ID, &amp;source);</span><br><span class="line"></span><br><span class="line">    /* run work queue register above, to check: how long to wait */</span><br><span class="line">--&gt; schedule_work(&amp;dpc-&gt;work);</span><br><span class="line"></span><br><span class="line">        /* This handler will try to stop and remove all devices under this port.</span><br><span class="line">         * Then wait link inactive, and set dpc cap&#x27;s status register.</span><br><span class="line">         *</span><br><span class="line">         * If your rp implement dpc, and there is just one ep connnected to</span><br><span class="line">         * rp directly, then this ep will be stopped and removed. when stopping</span><br><span class="line">         * it, pme/aspm functions will be called.</span><br><span class="line">         *</span><br><span class="line">         * If your rp connects a switch, and there are some devices connected</span><br><span class="line">         * to dp of this switch, when this rp </span><br><span class="line">         * under this rp will be stopped and removed.</span><br><span class="line">         */</span><br><span class="line">    --&gt; interrupt_event_handler</span><br><span class="line">            /* If your rp implements dpc, then the parent below should be</span><br><span class="line">             * the subordinate bus. Then for devices under this bus, it will</span><br><span class="line">             * call pci_stop_and_remove_bus_device(dev) to them one by one</span><br><span class="line">             */</span><br><span class="line">        --&gt; struct pci_bus *parent = pdev-&gt;subordinate;</span><br><span class="line">        --&gt; pci_stop_and_remove_bus_device(dev);</span><br><span class="line">                /* below function will be called in a recursive way, so the</span><br><span class="line">                 * first device to be stopped and removed will be the deepest one.</span><br><span class="line">                 * Then one by one, all devices will be stopped and removed.</span><br><span class="line">                 */</span><br><span class="line">            --&gt; pci_stop_bus_device(dev);</span><br><span class="line">                    /* to check: it will call pme/aspm functions in below function */ </span><br><span class="line">                --&gt; pci_stop_dev(dev);</span><br><span class="line">                /* in a same recursive way in below function */</span><br><span class="line">            --&gt; pci_remove_bus_device(dev);</span><br><span class="line">                    /* to check: will call pm functions */</span><br><span class="line">                --&gt; pci_bridge_d3_device_removed(dev);</span><br><span class="line">                    /* all software structures destroy */</span><br><span class="line">                --&gt; pci_destroy_dev(dev);</span><br><span class="line"></span><br><span class="line">            /* here stop maximum 1s to wait Data Link Layer Link Active bit</span><br><span class="line">             * in Link status register to become 0.</span><br><span class="line">             * If this bit is 1, it indicates the DL_Active state ?</span><br><span class="line">             */</span><br><span class="line">        --&gt; dpc_wait_link_inactive(pdev);</span><br><span class="line">            /* set dpc trigger status bit and dpc interrupt status bit in</span><br><span class="line">             * dpc status register.</span><br><span class="line">             */</span><br><span class="line">        --&gt; pci_write_config_word(pdev, dpc-&gt;cap_pos + PCI_EXP_DPC_STATUS,</span><br><span class="line">                    PCI_EXP_DPC_STATUS_TRIGGER | PCI_EXP_DPC_STATUS_INTERRUPT);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>LDD3 study note 3</title>
    <url>/LDD3-study-note-3/</url>
    <content><![CDATA[<h2 id="mmap"><a href="#mmap" class="headerlink" title="mmap"></a>mmap</h2><p> 在linux用户态调用mmap函数可以把文件内容直接映射到内存，这样用户态程序可以像访问<br> 内存一样访问文件。同样，使用mmap也可以把设备的一段IO空间映射到用户态，用户态程序<br> 可以直接访问这个设备的寄存器。当然，要在程序驱动里添加mmap的对应支持。</p>
<h2 id="驱动实现"><a href="#驱动实现" class="headerlink" title="驱动实现"></a>驱动实现</h2><p>为了支持把设备的IO空间映射到用户态，驱动里要实现.mmap的回调，struct vm_area_struct<br>会把用户态想要映射到的用户态虚拟地址传到内核。在.mmap回调里需要把这些参数，连同<br>想要映射的实际物理地址传递给remap_pfn_range函数，这个函数帮助建立虚拟地址到物理<br>地址的映射。</p>
<p>下面的例子是在scull驱动里，申请了一段内核态的内存，我们可以通过下面的操作把它映射<br>到用户态。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int scull_mmap(struct file *file, struct vm_area_struct *vma)</span><br><span class="line">&#123;</span><br><span class="line">        unsigned long page = virt_to_phys(scull_device-&gt;mmap_memory);</span><br><span class="line">        unsigned long start = (unsigned long)vma-&gt;vm_start;</span><br><span class="line">        unsigned long size = (unsigned long)(vma-&gt;vm_end - vma-&gt;vm_start);</span><br><span class="line">        vma-&gt;vm_flags |= (VM_IO | VM_LOCKED | VM_DONTEXPAND | VM_DONTDUMP);</span><br><span class="line"></span><br><span class="line">        if (remap_pfn_range(vma, start, page &gt;&gt; PAGE_SHIFT, size, PAGE_SHARED)) &#123;</span><br><span class="line">                printk(KERN_ALERT &quot;remap_pfn_range failed!\n&quot;);</span><br><span class="line">                return -1;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>LDD3上讲，remap_pfn_range只能映射IO空间和系统保留内存，但是上面例子的mmap_memory<br>实际在就是用get_free_page分配的(把get_free_page换成kmalloc也是可以做映射的), 测试<br>的结果是用get_free_page分配的内存也是可以被映射到用户态的。所以，实际上<br>remap_pfn_range也可以把系统内存映射到用户态。</p>
<p>内核用struct vm_area_struct管理进程空间的各个虚拟地址区域。cat /proc/pid/maps<br>可以看到一个进程所有的虚拟地址区域。在下面的例子中，我们可以看到测试程序(read.c)<br>的进程的各个地址区域。可以看到调用mmap创建起来的一个地址区域/dev/scull0.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">estuary:/$ cat /proc/1251/maps</span><br><span class="line">00400000-00401000 r-xp 00000000 00:01 882            /a.out</span><br><span class="line">00410000-00411000 rw-p 00000000 00:01 882            /a.out</span><br><span class="line">ffffae624000-ffffae634000 rw-p 00000000 00:00 0 </span><br><span class="line">ffffae634000-ffffae764000 r-xp 00000000 00:01 629    /lib/aarch64-linux-gnu/libc-2.21.so</span><br><span class="line">ffffae764000-ffffae773000 ---p 00130000 00:01 629    /lib/aarch64-linux-gnu/libc-2.21.so</span><br><span class="line">ffffae773000-ffffae777000 r--p 0012f000 00:01 629    /lib/aarch64-linux-gnu/libc-2.21.so</span><br><span class="line">ffffae777000-ffffae779000 rw-p 00133000 00:01 629    /lib/aarch64-linux-gnu/libc-2.21.so</span><br><span class="line">ffffae779000-ffffae77d000 rw-p 00000000 00:00 0 </span><br><span class="line">ffffae77d000-ffffae799000 r-xp 00000000 00:01 575    /lib/aarch64-linux-gnu/ld-2.21.so</span><br><span class="line">ffffae7a1000-ffffae7a2000 rw-s 00000000 00:01 1385   /dev/scull0</span><br><span class="line">ffffae7a2000-ffffae7a7000 rw-p 00000000 00:00 0 </span><br><span class="line">ffffae7a7000-ffffae7a8000 r--p 00000000 00:00 0      [vvar]</span><br><span class="line">ffffae7a8000-ffffae7a9000 r-xp 00000000 00:00 0      [vdso]</span><br><span class="line">ffffae7a9000-ffffae7aa000 r--p 0001c000 00:01 575    /lib/aarch64-linux-gnu/ld-2.21.so</span><br><span class="line">ffffae7aa000-ffffae7ac000 rw-p 0001d000 00:01 575    /lib/aarch64-linux-gnu/ld-2.21.so</span><br><span class="line">fffff5f89000-fffff5faa000 rw-p 00000000 00:00 0      [stack]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>LDD3</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux PCIe hotplug arch analysis</title>
    <url>/Linux-PCIe-hotplug-arch-analysis/</url>
    <content><![CDATA[<p>This analysis is based on v4.8-rc5.</p>
<p>There are two kinds of PCIe hotplug: one is PCIe native hotplug which is implemented<br>just used the codes in linux kernel, second is PCIe hotplug based on ACPI. Here<br>we just analyze the native hotplug.</p>
<p>PCIe hotplug is implemented as a pcie port service, it is registered in pcie port<br>driver in drivers/pci/hotplug/pciehp_core.c:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; pcied_init(void)</span><br><span class="line">    --&gt; pcie_port_service_register(&amp;hpdriver_portdrv)</span><br></pre></td></tr></table></figure>
<p>so system will finally call pciehp_probe in struct pcie_port_service_driver hpdriver_portdrv,<br>main flow of this probe:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pciehp_probe(struct pcie_device *dev)</span><br><span class="line">        /* create struct controller, struct slot, get slot capability and slot status info */</span><br><span class="line">    --&gt; pcie_init(dev);</span><br><span class="line">            /* will register a delay_work in slot:</span><br><span class="line">             * INIT_DELAYED_WORK(&amp;slot-&gt;work, pciehp_queue_pushbutton_work)</span><br><span class="line">             *</span><br><span class="line">             * This will be called, e.g. in hot-insertion, after inserting card</span><br><span class="line">             * and press button and wait 5s.</span><br><span class="line">             */</span><br><span class="line">        --&gt; pcie_init_slot</span><br><span class="line">        /* create struct hotplug_slot, struct hotplug_slot_info, struct hotplug_slot_ops,</span><br><span class="line">         * fill functions in hotplug_slot_ops</span><br><span class="line">         *</span><br><span class="line">         *   controller</span><br><span class="line">         *        +------&gt; slot</span><br><span class="line">         *                   +----&gt; hotplug_slot</span><br><span class="line">         *                               +-------&gt; pci_slot</span><br><span class="line">         *                                         hotplug_slot_ops</span><br><span class="line">         *                                         hotplug_slot_info</span><br><span class="line">         *                   +----&gt; delayed_work</span><br><span class="line">         *                   +----&gt; workqueue_struct</span><br><span class="line">         *</span><br><span class="line">         * then the whole structures will like above.</span><br><span class="line">         *</span><br><span class="line">         * After above structure been built, it will call pci_hp_register</span><br><span class="line">         */</span><br><span class="line">    --&gt; init_slot(ctrl);</span><br><span class="line">            /* add hotplug_slot into pci_hotplug_slot_list */</span><br><span class="line">        --&gt; pci_hp_register(hotplug, ctrl-&gt;pcie-&gt;port-&gt;subordinate, 0, name);</span><br><span class="line">                /* to check: where to init pci_slot */</span><br><span class="line">            --&gt; pci_create_slot</span><br><span class="line">            --&gt; list_add(&amp;slot-&gt;slot_list, &amp;pci_hotplug_slot_list);</span><br><span class="line">                /* expose pci_slot related file in sys-fs */</span><br><span class="line">            --&gt; fs_add_slot(pci_slot);</span><br><span class="line">        /* register irq */</span><br><span class="line">    --&gt; pcie_init_notification(ctrl);</span><br><span class="line">            /* when request_irq, will let struct controller to be its private data */</span><br><span class="line">        --&gt; pciehp_request_irq(ctrl)</span><br><span class="line">            /* hardware set here ?? enable some bits in slot control reg,</span><br><span class="line">             *</span><br><span class="line">             * here, in pcie_write_cmd function, we need wait former cmd finished,</span><br><span class="line">             * if needed, we should check the details of this wait function.</span><br><span class="line">             */</span><br><span class="line">        --&gt; pcie_enable_notification(ctrl);</span><br><span class="line">        /* if there is a device on the slot */</span><br><span class="line">    --&gt; pciehp_enable_slot(slot);</span><br><span class="line">            /* after check the status of device, if needed, will enable the device</span><br><span class="line">             *</span><br><span class="line">             * As we already implements linkup in UEFI, if we enable PCIe controller</span><br><span class="line">             * driver, it will not call board_added. If we do not enable PCIe controller,</span><br><span class="line">             * it will call board_added here.</span><br><span class="line">             */</span><br><span class="line">        --&gt; board_added(struct slot *p_slot)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pcie_isr</span><br><span class="line">        /* to check: where to define the handle of below queue */</span><br><span class="line">    --&gt; wake_up(&amp;ctrl-&gt;queue);</span><br><span class="line">        /* create related work_struct, then put work_struct to the work_queue in struct slot,</span><br><span class="line">         * Here, we may have: Attention Button Pressed, Presence Detect Changed</span><br><span class="line">         * Power Fault Detected, Link up/down check.</span><br><span class="line">         *</span><br><span class="line">         * same handle will be added to work_struct: interrupt_event_handler, </span><br><span class="line">         * in above function, it handle all cases.</span><br><span class="line">         *</span><br><span class="line">         * We can not analyze all cases, the key point to understand the standard</span><br><span class="line">         * hot-insertion and hot-removal is to consider the whole flow together</span><br><span class="line">         * with INIT_DELAYED_WORK(&amp;slot-&gt;work, pciehp_queue_pushbutton_work) in</span><br><span class="line">         * pcie_init mentioned above.</span><br><span class="line">         *</span><br><span class="line">         * when you analyze the different cases, please pay great attention on</span><br><span class="line">         * struct slot -&gt; state, which indicates the current status of hot-plug</span><br><span class="line">         * state machine. And we should get clear the map between these state</span><br><span class="line">         * and real physical state.</span><br><span class="line">         *</span><br><span class="line">         * STATIC_STATE: at this state, pcie ep already runs fine or ep already</span><br><span class="line">         * had been remove the system.</span><br><span class="line">         *</span><br><span class="line">         * BLINKINGON_STATE: in hot-insertion, after step1, card insertion; step2,</span><br><span class="line">         * press button, power indicator led will blinking 5s. This state just </span><br><span class="line">         * indicates this.</span><br><span class="line">         *</span><br><span class="line">         * BLINKINGOFF_STATE: same as above, but in hot-removal.</span><br><span class="line">         *</span><br><span class="line">         * POWERON_STATE: in hot-insertion, in the 5s of power indicator led blinking,</span><br><span class="line">         * if we do not press button again, it will try to bring the ep power on,</span><br><span class="line">         * linkup, then add ep to system. This state just shows this.</span><br><span class="line">         *</span><br><span class="line">         * POWEROFF_STATE: same as above, but in hot-removal.</span><br><span class="line">         */</span><br><span class="line">    --&gt; pciehp_queue_interrupt_event(slot, INT_BUTTON_PRESS);</span><br></pre></td></tr></table></figure>

<p>If we only see the flow of “interrupt_event_handler: INT_PRESENCE_ON”, it can not<br>match the stardard hot-inertion flow, which firstly inserting a PCIe card and then<br>pressing the attention button.</p>
<p>If in a system, when a pcie card been plugged in, a hotplug interrupt will be<br>triggered and presence detect change bit set 1 in slot status register.<br>This will trigger the operation in flow 1 below. you can see that now the whole<br>hot-insertion process has no relationship with attention button press.</p>
<p>pcie device hot insertion, trigger Presence Detect interrupt:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; interrupt_event_handler: INT_PRESENCE_ON</span><br><span class="line">    --&gt; handle_surprise_event</span><br><span class="line">            /* if presence detect bit in slot status set 1 */</span><br><span class="line">        --&gt; pciehp_queue_power_work(..., ENABLE_REQ)</span><br><span class="line">                /* will create work_queue, and add it to struct slot&#x27;s wq,</span><br><span class="line">                 * the handler function is pciehp_power_thread</span><br><span class="line">                 *</span><br><span class="line">                 * above handler will be called by work queue in slot.</span><br><span class="line">                 */</span><br><span class="line">            --&gt; pciehp_power_thread: case ENABLE_REQ</span><br><span class="line">                --&gt; pciehp_enable_slot(p_slot)</span><br><span class="line">                        /* before doing this, we will check slot status */</span><br><span class="line">                    --&gt; board_added</span><br><span class="line">                        --&gt; pciehp_green_led_blink</span><br><span class="line">                                /* send green led blink */</span><br><span class="line">                            --&gt; pcie_write_cmd_nowait</span><br><span class="line">                            /* check linkup, here maximum wait 1s */</span><br><span class="line">                        --&gt; pciehp_check_link_status</span><br><span class="line">                            /* to check resource assign if has problem */</span><br><span class="line">                        --&gt; pciehp_configure_device</span><br><span class="line">                        --&gt; pciehp_green_led_on</span><br><span class="line">                --&gt; p_slot-&gt;state = STATIC_STATE;</span><br></pre></td></tr></table></figure>

<p>Let’s see standard pcie hot-removal:<br>button pressed, trigger Attention Button Pressed interrupt</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; interrupt_event_handler: INT_BUTTON_PRESS</span><br><span class="line">        /* In this function, it depends on hot-plug state machine to decide</span><br><span class="line">         * what to do next. Here assuming, power on, we operate hot-removal.</span><br><span class="line">         */</span><br><span class="line">    --&gt; handle_button_press_event(p_slot)</span><br><span class="line">            /* if pcie card is running fine, we are in STATIC_STATE,</span><br><span class="line">             * and at this time, we check power, it is power on, so we know</span><br><span class="line">             * this operation will be a hot-removal, print:</span><br><span class="line">             * &quot;powering off due to button press&quot;</span><br><span class="line">             *</span><br><span class="line">             * Then we change state to BLINKINGOFF_STATE, which means</span><br><span class="line">             * PCIe controller/card will wait 5s to turn off power.</span><br><span class="line">             */</span><br><span class="line">            /* blink power indicator to show power unstable */</span><br><span class="line">        --&gt; pciehp_green_led_blink(p_slot)</span><br><span class="line">            /* turn off attention indicator to show everything fine */</span><br><span class="line">        --&gt; pciehp_set_attention_status(p_slot, 0)</span><br><span class="line">            /* delay work queue 5s to wait if we will cancel this operation */</span><br><span class="line">        --&gt; queue_delayed_work(p_slot-&gt;wq, &amp;p_slot-&gt;work, 5*HZ)</span><br></pre></td></tr></table></figure>
<p>As [1] page 394, step 4, it said, hotplug driver should let the driver of PCIe card<br>stop, and card driver should handle stop the data/interrupt. We should check<br>pci_stop_and_remove_bus_device below to get the details.</p>
<p>If in 5s, we do not press button, it will eventually call pciehp_queue_pushbutton_work<br>which initialized in pcie_init.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pciehp_queue_pushbutton_work</span><br><span class="line">        /* when hot-removal, state is BLINKINGOFF_STATE */</span><br><span class="line">    --&gt; pciehp_queue_power_work(p_slot, DISABLE_REQ)</span><br><span class="line">            /* POWEROFF_STATE */</span><br><span class="line">        --&gt; pciehp_power_thread</span><br><span class="line">            --&gt; pciehp_disable_slot</span><br><span class="line">                --&gt; remove_board(p_slot)</span><br><span class="line">                    --&gt; pciehp_unconfigure_device(p_slot)</span><br><span class="line">                            /* the point here is how to removal the ep specific</span><br><span class="line">                             * work. If needed, we should check the details here.</span><br><span class="line">                             */</span><br><span class="line">                        --&gt; pci_stop_and_remove_bus_device</span><br></pre></td></tr></table></figure>
<p>above is the code map of standard hot-removal.</p>
<p>If at any time, the pcie card has been surprise hot-removed, linux kernel can<br>handle it by:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; interrupt_event_handler: INT_PRESENCE_OFF</span><br><span class="line">    --&gt; handle_surprise_event</span><br><span class="line">        --&gt; pciehp_queue_power_work(p_slot, DISABLE_REQ)</span><br><span class="line">                /* add this to slot&#x27;s work queue */</span><br><span class="line">            --&gt; pciehp_power_threa</span><br><span class="line">                --&gt; pciehp_disable_slot(p_slot)</span><br><span class="line">                    --&gt; remove_board(p_slot)</span><br><span class="line">                --&gt; p_slot-&gt;state = STATIC_STATE;</span><br></pre></td></tr></table></figure>
<p>but the point here is how to handle ep specific work, need more investigation here.</p>
<p>Let’s see standard pcie hot-insertion:<br>If we consider pciehp_queue_pushbutton_work in the flow of standard hot-insertion,<br>it is easy to get the work flow analysis like standard hot-removal above.</p>
<p>The point here is that only if the power of slot is off before inserting ep, can<br>we use the button to trigger standard hot-insertion flow. If power of slot is on,<br>once a ep has been inserted in slot, it will trigger presence ditect change interrupt,<br>then code in kernel can handle following part of hot-insertion, we indeed do not<br>need to press a button. We just show this in above section.</p>
<p>Resource assignment problem in linux hotplug:<br>kernel use below code to add a ep to system.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; pciehp_enable_slot</span><br><span class="line">    --&gt; board_added(p_slot)</span><br><span class="line">        --&gt; pciehp_configure_device</span><br><span class="line">            ...</span><br><span class="line">            --&gt; pci_hp_add_bridge(dev)</span><br></pre></td></tr></table></figure>
<p>But how does linux assign bus number ahead for a hotplug capable slot ?<br>we should pay attention to a value: max in pci enumeration code.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     --&gt; pci_scan_child_bus</span><br><span class="line">  --&gt; if (bus-&gt;self &amp;&amp; bus-&gt;self-&gt;is_hotplug_bridge &amp;&amp; pci_hotplug_bus_size)</span><br><span class="line">if (max - bus-&gt;busn_res.start &lt; pci_hotplug_bus_size - 1)</span><br><span class="line">	max = bus-&gt;busn_res.start + pci_hotplug_bus_size - 1;</span><br></pre></td></tr></table></figure>
<p>It will check if this is a hotplug bridge and assign more pci buses for this bridge<br>according to pci_hotplug_bus_size which can be set in command line of kernel.</p>
<h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><p>struct slot -&gt; state:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(to finish)</span><br><span class="line">                         +-------------------+</span><br><span class="line">                         | BLINKINGOFF_STATE |</span><br><span class="line">                         +-------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">+--------------+ slot enable +------------+  slot disable +--------------+</span><br><span class="line">|POWERON_STATE | ----------&gt; |STATIC_STATE|  &lt;----------- |POWEROFF_STATE|</span><br><span class="line">+--------------+             +------------+               +--------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                          +----------------+</span><br><span class="line">                          |BLINKINGON_STATE|</span><br><span class="line">                          +----------------+</span><br></pre></td></tr></table></figure>

<p>from PCIe spec, power indicator should be a green led:<br>off means power off, so we can insertion/removal<br>on means power on, so we can not insertion/removal<br>blinking means, it is powering up/down or a feedback from attention button pressed<br>                or hot-plug operation is initiated through software.</p>
<p>attention indicator should be a yellow or amber led:<br>on means, should be pay attention to<br>off means, everything fine</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>PCI.EXPRESS系统体系结构标准教材].(美)Pavi.Budruk,Don.Anderson,Tom.Shanley.扫描版.pdf</li>
</ol>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux dma_map_sg API</title>
    <url>/Linux-dma-map-sg-API/</url>
    <content><![CDATA[<p>如linux/Documentation/DMA-API-HOW.txt里提到的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">With scatterlists, you map a region gathered from several regions by::</span><br><span class="line"></span><br><span class="line">	int i, count = dma_map_sg(dev, sglist, nents, direction);</span><br><span class="line">	struct scatterlist *sg;</span><br><span class="line"></span><br><span class="line">	for_each_sg(sglist, sg, count, i) &#123;</span><br><span class="line">		hw_address[i] = sg_dma_address(sg);</span><br><span class="line">		hw_len[i] = sg_dma_len(sg);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">where nents is the number of entries in the sglist.</span><br><span class="line"></span><br><span class="line">The implementation is free to merge several consecutive sglist entries</span><br><span class="line">into one (e.g. if DMA mapping is done with PAGE_SIZE granularity, any</span><br><span class="line">consecutive sglist entries can be merged into one provided the first one</span><br><span class="line">ends and the second one starts on a page boundary - in fact this is a huge</span><br><span class="line">advantage for cards which either cannot do scatter-gather or have very</span><br><span class="line">limited number of scatter-gather entries) and returns the actual number</span><br><span class="line">of sg entries it mapped them to. On failure 0 is returned.</span><br><span class="line"></span><br><span class="line">Then you should loop count times (note: this can be less than nents times)</span><br><span class="line">and use sg_dma_address() and sg_dma_len() macros where you previously</span><br><span class="line">accessed sg-&gt;address and sg-&gt;length as shown above.</span><br><span class="line"></span><br><span class="line">To unmap a scatterlist, just call::</span><br><span class="line"></span><br><span class="line">	dma_unmap_sg(dev, sglist, nents, direction);</span><br><span class="line"></span><br><span class="line">Again, make sure DMA activity has already finished.</span><br><span class="line"></span><br><span class="line">.. note::</span><br><span class="line"></span><br><span class="line">	The &#x27;nents&#x27; argument to the dma_unmap_sg call must be</span><br><span class="line">	the _same_ one you passed into the dma_map_sg call,</span><br><span class="line">	it should _NOT_ be the &#x27;count&#x27; value _returned_ from the</span><br><span class="line">	dma_map_sg call.</span><br></pre></td></tr></table></figure>
<p>dma_map_sg对一个sgl做map的时候，返回的map的sge的个数可能是小于输入sgl的sge的个数<br>的，这是因为连续页做了合并操作，在有IOMMU的情况下，也可能是因为IOMMU的存在把一段<br>连续的IOVA映射成了一些离散的物理地址块，而这些离散的物理地址块正是前面的sgl输入,<br>这个连续的IOVA正式dma_map_sg的输出。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>DMA</tag>
        <tag>iommu</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux/UEFI memory block note</title>
    <url>/Linux-UEFI-memory-block-note/</url>
    <content><![CDATA[<p>D05 CPU and memory hardware topology</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|||| +--------+  |||| ||||  +---------+  ||||</span><br><span class="line">|||| |        |  |||| ||||  |         |  ||||</span><br><span class="line">|||| |  CPU   |  |||| ||||  |   CPU   |  ||||</span><br><span class="line">|||| |        |  |||| ||||  |         |  ||||</span><br><span class="line">|||| +--------+  |||| ||||  +---------+  ||||</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> Dim A        Dim B</span><br><span class="line">   |     | |    |   | |</span><br><span class="line">   +---&gt; | | &lt;--+   | |</span><br><span class="line">         | |        | |</span><br><span class="line">         | |        | |</span><br><span class="line"> +-------+-+--------+-+---------+</span><br><span class="line"> |      ++-+-+     ++-+-+       | &lt;-- CPU</span><br><span class="line"> |      |ddrc|     |ddrc|       |</span><br><span class="line"> |     ++----+-+  ++----+-+     |</span><br><span class="line"> |     |       |  |       |     |</span><br><span class="line"> |     |       +--+       |     |</span><br><span class="line"> |     | C Die |  | C Die |     |</span><br><span class="line"> |     |       +--+       |     |</span><br><span class="line"> |     |       |  |       |     |</span><br><span class="line"> |     ++----+-+  ++----+-+ ... |</span><br><span class="line"> |      |ddrc|     |ddrc|       |</span><br><span class="line"> |      ++-+-+     ++-+-+       |</span><br><span class="line"> +-------+-+--------+-+---------+</span><br><span class="line">         | |        | |</span><br><span class="line">         | |        | |</span><br><span class="line">         | |        | |</span><br><span class="line">         | |        | |</span><br></pre></td></tr></table></figure>
<p>ACPI SRAT table will discribe this topology. In SRAT table CPU will be discribed<br>statically, but memory block will be discribed drynamically. UEFI will detect<br>the DDR in the Dims and report memory blocks in SRAT table to OS.</p>
<p>The logic of how UEFI defines a memmroy block is based on specific UEFI.<br>Now D05 UEFI will assigned memory range for each ddrc, e.g. 0 ~ 32G. ddr plugging<br>in specific dim will get the cpu address from its ddrc.</p>
<p>D05 UEFI can connect two memory rangs of different ddrc to one memory block and<br>report this to OS, if these two memory ranges’ ddr is contiguous:</p>
<p>  contiguous:      ddrc 0 ~ 32G, plugging 2 16G ddr.</p>
<p>  not contiguous:  ddrc 0 ~ 32G, plugging 1 16G ddr.</p>
<p>In D05, run dmesg | grep SRAT:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[    0.000000] ACPI: SRAT 0x00000000397F0000 000578 (v03 HISI   HIP07    00000000 INTL 20151124)</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10000 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10001 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10002 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10003 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10100 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10101 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10102 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10103 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10200 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10201 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10202 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10203 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10300 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10301 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10302 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10303 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30000 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30001 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30002 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30003 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30100 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30101 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30102 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30103 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30200 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30201 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30202 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30203 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30300 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30301 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30302 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30303 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50000 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50001 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50002 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50003 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50100 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50101 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50102 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50103 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50200 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50201 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50202 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50203 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50300 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50301 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50302 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50303 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70000 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70001 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70002 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70003 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70100 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70101 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70102 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70103 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70200 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70201 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70202 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70203 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70300 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70301 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70302 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70303 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: SRAT: Node 0 PXM 0 [mem 0x00000000-0x3fffffff]</span><br><span class="line">[    0.000000] ACPI: SRAT: Node 1 PXM 1 [mem 0x1400000000-0x17ffffffff]</span><br><span class="line">[    0.000000] ACPI: SRAT: Node 0 PXM 0 [mem 0x1000000000-0x13ffffffff]</span><br><span class="line">[    0.000000] ACPI: SRAT: Node 3 PXM 3 [mem 0x8800000000-0x8bffffffff]</span><br><span class="line">[    0.000000] ACPI: SRAT: Node 2 PXM 2 [mem 0x8400000000-0x87ffffffff]</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux kernel PCI is_physfn的逻辑</title>
    <url>/Linux-kernel-PCI-is-physfn%E7%9A%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>Linux内核里struct pci_dev里有一个叫is_physfn的域段, 从名字上来看，这个域段可以<br>用来表示一个pci设备是不是PF。对应的有一个is_virtfn的域段。</p>
<p>显然在驱动里如果有的操作需要区分PF和VF, 我们可以用这个域段作为判断依据。但是，<br>实际上如果你去看pci代码的实现，他的语义要以SRIOV cap存在为前提的。</p>
<p>他的调用关系是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_iov_init</span><br><span class="line">  +-&gt; pci_find_ext_capability(dev, PCI_EXT_CAP_ID_SRIOV)</span><br><span class="line">    +-&gt; sriov_init</span><br><span class="line">      +-&gt; dev-&gt;is_physfn = 1</span><br></pre></td></tr></table></figure>

<p>他的语义是pci设备只有在SRIOV cap存在的时候，才有区分PF和VF的必要, 才会去设定<br>is_physfn。</p>
<p>所以，如果我们用一套驱动同时支持PF和VF, 需要区分PF和VF的操作的时候，如果这时<br>系统里有SRIOV cap时，依然可以用is_phyfn这个标记。当系统里没有SRIOV cap时，is_physfn<br>和is_virtfn都不会设置，这时判断就会错。这样的场景发生在PF和VF公用一套驱动，而且<br>VF直通到guest里工作之时。</p>
<p>这种情况下，我们一般要在设备驱动里增加新的标记位，明确的表示这是一个PF还是一个VF。<br>这个标记位只和设备相关，和SRIOV cap无关。</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux initramfs分析</title>
    <url>/Linux-initramfs%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h1 id="代码逻辑"><a href="#代码逻辑" class="headerlink" title="代码逻辑"></a>代码逻辑</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* 从内核的c代码入口看起 */</span><br><span class="line">start_kernel</span><br><span class="line">      /* 这个函数已经接近start_kernel末尾，但是内容却无比丰富 */</span><br><span class="line">  +-&gt; arch_call_rest_init</span><br><span class="line">        /* 在这个函数里拉起1号进程和2号线程 */</span><br><span class="line">    +-&gt; rest_init</span><br><span class="line">          /*</span><br><span class="line">           * kernel_init是1号进程对应的函数，这个函数有点特殊，这个函数除了最终</span><br><span class="line">           * 拉起第一个用户态进程外，还做了一堆初始化操作。</span><br><span class="line">           */</span><br><span class="line">      |</span><br><span class="line">    --+-&gt; user_mode_thread(kernel_init, NULL, CLONE_FS)</span><br><span class="line">      |</span><br><span class="line">        +-&gt; kernel_init</span><br><span class="line">              /* 进来啥也不干，先等2号线程起来 */</span><br><span class="line">              wait_for_complete(...)</span><br><span class="line">              /*</span><br><span class="line">               * 继续初始化一堆东西: smp, smp schedule, work queue, 各种驱动的初始化,</span><br><span class="line">               * 各种__init函数的调用，跑内核自带的各种测试，等待initramfs解压完成，</span><br><span class="line">               * 其中最后一个和本文的主题相关，在这之前的do_basic_setup()里会开始</span><br><span class="line">               * 做initramfs/initrd的解压。</span><br><span class="line">               */</span><br><span class="line">          +-&gt; kernel_init_freeable</span><br><span class="line">            +-&gt; do_basic_setup</span><br><span class="line">                  /*</span><br><span class="line">                   * 这是一个rootfs_initcall修饰过的函数，在do_basic_setup依次调用</span><br><span class="line">                   * 各种类型的__init函数的时候，会调用到。这个函数的本质是把do_populate_rootfs</span><br><span class="line">                   * 放到一个workqueue里去执行。</span><br><span class="line">                   *</span><br><span class="line">                   * 对于rootfs.cpio.gz的压缩过的内存文件系统，这个函数里会做相关</span><br><span class="line">                   * 的解压缩操作。</span><br><span class="line">                   */</span><br><span class="line">              +-&gt; populate_rootfs</span><br><span class="line">                +-&gt; async_schedule_domain(do_populate_rootfs, ...)</span><br><span class="line">            +-&gt; wait_for_initramfs</span><br><span class="line"></span><br><span class="line">      |   /* 创建threadd这个内核线程，这个线程是所有内核线程的父线程，他就是2号线程 */</span><br><span class="line">    --+-&gt; kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES)</span><br><span class="line">      |</span><br><span class="line"></span><br><span class="line">      +-&gt; cpu_startup_entry()</span><br></pre></td></tr></table></figure>
<p> 总结下如上的线程模型，内核执行到这里会拉起1号和2好进程，这个时候会出现3个进程并发<br> 的场景，这三个进程分别是0/1/2号进程。kernel_init会等2好线程启动后才会继续跑。可见<br> initramfs的解压缩是被放到内核线程里去跑的，1号线程会异步的等initramfs解压缩完后<br> 再继续跑。</p>
<p> 1号线程等解压完成在wait_for_initramfs里，这个函数用了wait_event，这个函数wait<br> 在D状态(TASK_UNINTERRUPTIBLE)，这个状态的线程只能被特定事件唤醒，内核里会定时检查<br> D状态的线程，如果超过一定事件没有被唤醒(一般是120s)，就会报异常。所以，如果解压<br> 过程超过120s，会导致等待时间超过120s，进而报异常(linux/kernel/hung_task.c)。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux irq domain</title>
    <url>/Linux-irq-domain/</url>
    <content><![CDATA[<p> linux中断管理中有irq domain的概念。顾名思义irq domain是针对有中断控制功能的IP模<br> 块中断管理域，是一个软件概念。以linux kernel中的GPIO举个例子。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pins  +-----+     +-----+     +-----+</span><br><span class="line">    ——|     |     |     |     |     |</span><br><span class="line">    ——|     |----&gt;|     |----&gt;|     |</span><br><span class="line">    ——|     |     |     |     |     |</span><br><span class="line">    ——|     |     |     |     |     |</span><br><span class="line">      +-----+     +-----+     +-----+</span><br><span class="line">       GPIO         GIC       CPU core</span><br><span class="line">     controller    </span><br></pre></td></tr></table></figure>
<p> GPIO，GIC，CPU core的硬件示意图如上。 一个GPIO控制器有多个输入输出管脚, 可以输入<br> 输出高低电平信号，也可以接收外部信号作为中断输入。GPIO控制器把多个外部输入的中断<br> 信号转成一个中断信号，通过一条中断线报给GIC。CPU core接收到GIC上报的中断后, 通过<br> 查询GIC的相关寄存器得到中断来自GIC的哪个输入管脚，再通过查询GPIO控制器的相关寄存<br> 器得到中断真正来自GPIO的哪个输入管脚。然后调用相应的中断处理程序。</p>
<p> linux内核中断管理的最核心数据结构是struct irq_desc, 为了使用irq号申请中断，中断<br> 管理系统要为每个中断建立相应的struct irq_desc结构(之后驱动可以使用request_irq()在<br> 相应的struct irq_desc中注册中断处理程序)。</p>
<p> 上面的硬件框图中，GPIO controller到GIC的中断信号线是在制作Soc的时候就连死的，Soc<br> 手册中会有该中断线的中断号。在驱动程序中可以直接使用request_irq()直接注册中断处理<br> 函数。当然在该中断处理函数中也可以直接去读GPIO controller中的中断相关的寄存器，查<br> 看实际的中断来自哪个GPIO输入管脚，然后调用相应的函数去处理。这样也就没有irq domain<br> 什么事情了。</p>
<p> 但是如果像上面这样做，会在GPIO controller的驱动代码中加入GPIO所接下级设备的中断<br> 处理函数。破坏了程序的结构。合理的处理方式是GPIO controller只需要暴露给下级设备输入<br> 管脚号，下级设备在其自身的驱动程序中，通过管脚号调用GPIO驱动提供的接口，获得相应的<br> irq号，然后通过request_irq()注册自己的中断处理函数。基于这样的逻辑，需要在GPIO驱动<br> 中加入一个struct irq_domain的结构，该结构的作用是：1. 为GPIO的各个输入管脚，在中断<br> 管理系统中建立相应的struct irq_desc。2. 建立一个输入管脚号到irq号的映射表。</p>
<p> 下面分析一段具体的代码，以内核代码linux/drivers/gpio/gpio-mvebu.c为例：<br> 在struct mvebu_gpio_chip中包含struct irq_domain *domain用来管理GPIO controller的<br> irq domain。在probe函数中<br>    mvchip-&gt;irqbase = irq_alloc_descs(-1, 0, ngpios, -1);<br> 分配了ngpios个struct irq_desc结构体，返回第一个结构对应的irq号。</p>
<pre><code>mvchip-&gt;domain = irq_domain_add_simple();
</code></pre>
<p> 建立并初始化irq_domain结构, 并建立从管脚号到中断号的映射。</p>
<pre><code> mvebu_gpio_to_irq();
</code></pre>
<p> 通过管脚号找到中断号。</p>
<p> 本文以一个GPIO控制器的例子介绍了linux内核中irq domain的基本逻辑。只要是相似的地方都<br> 可以引入irq domain的处理方式。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux kernel crypto scomp/acomp arch analysis</title>
    <url>/Linux-kernel-crypto-scomp-acomp-arch-analysis/</url>
    <content><![CDATA[<h2 id="General-introduce-of-linux-kernel-crypto-compression-API"><a href="#General-introduce-of-linux-kernel-crypto-compression-API" class="headerlink" title="General introduce of linux kernel crypto compression API"></a>General introduce of linux kernel crypto compression API</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_register_alg()</span><br><span class="line">crypto_alloc_comp()</span><br><span class="line"></span><br><span class="line">crypto_register_scomp() </span><br><span class="line">crypto_alloc_acomp()</span><br><span class="line"></span><br><span class="line">crypto_register_acomp() </span><br><span class="line">crypto_alloc_acomp()</span><br></pre></td></tr></table></figure>
<p>As showed in above, we can only use crypto_register_alg() to register a general<br>crypto compress alg, and use crypto_alloc_comp() to get the crypto_comp to do<br>compression/decompression. We can not register a new compress alg by this old<br>general API in mainline kernel now and in future.</p>
<p>Crypto system also supports to register a sync compress alg using<br>crypto_register_scomp, and use crypto_register_acomp to register an async<br>compress alg. Now we use only crypto_alloc_acomp() to get crypto_scomp/crypto_acomp.<br>There is no interface to get crypto_scomp, but crypto_alloc_acomp().</p>
<p>For the detail usage of scomp/acomp, we can have an example in kenrel/crypto/testmgr.c<br>alg_test_comp().</p>
<p>Currently all compression/decompression algorithms are registered to kernel<br>crypto system by crypto_register_alg() or crypto_register_scomp(). And all users<br>in kernel use crypto_alloc_comp() to get a compression/decompression context.</p>
<p>In future, as mentioned in above, all compression/decompression alg should<br>register to crypto by scomp/acomp.</p>
<h2 id="comp"><a href="#comp" class="headerlink" title="comp"></a>comp</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_alloc_comp()</span><br><span class="line">crypto_comp_compress()/crypto_comp_decompress()</span><br><span class="line">crypto_free_comp()</span><br></pre></td></tr></table></figure>
<p>We can use above APIs to do compression/decompress. Crypto subsystem has done<br>little about them. So When we add a hardware/software implementation for these<br>APIs, there is little limitation about them.</p>
<h2 id="Scomp"><a href="#Scomp" class="headerlink" title="Scomp"></a>Scomp</h2><p>We should use acomp interface to use scomp, they are as below:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_alloc_acomp()</span><br><span class="line">acomp_request_alloc()</span><br><span class="line">acomp_request_set_params()</span><br><span class="line">acomp_request_set_callback()</span><br><span class="line">crypto_init_wait()</span><br><span class="line">crypto_wait_req()</span><br><span class="line">crypto_acomp_compress()/crypto_acomp_decompress()</span><br><span class="line">acomp_request_free();</span><br><span class="line">crypto_free_acomp()</span><br></pre></td></tr></table></figure>
<p>For the view of user, acomp APIs offer scatterlists to store input/output data.</p>
<p>crypto scomp has internal buffers: when creating a scomp context, it allocates<br>a input buffer and a output buffer in every cpu core, each buffer is 128KB.</p>
<p>scomp arch first copies data in scatterlist into above internal input buffer,<br>then call compression/decompression inplementations to do real work. As one<br>cpu core use the its own input/output buffer, so before doing real compression/<br>decompression work, scomp arch will call get_cpu to disable scheduling, otherwise<br>another scomp job may run in the same cpu core, which will break the data in<br>input/output buffer. This means struct scomp_alg’s compress/decompress can not<br>do scheduling in themselves.</p>
<h2 id="Acomp"><a href="#Acomp" class="headerlink" title="Acomp"></a>Acomp</h2><p>We also use acomp interface to use acomp, they are as above.<br>As it is an async interface, the difference from scomp is user can offer a<br>callback to software/hardware implementation, which can call the callback after<br>compression/decompression is done.</p>
<p>The callback is set by acomp_request_set_callback(), and it will be passed to<br>compression/decompression process by struct acomp_req.</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>crypto</tag>
        <tag>ZIP</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux kernel kthread</title>
    <url>/Linux-kernel-kthread/</url>
    <content><![CDATA[<p>Linux kernel has kthread_* APIs to create/stop thread in kernel.<br>You can use kthread_run to create a thread and put it into running.<br>Normally, after thread function running over, created thread will stop. However,<br>what is the logic of the stop of thread. We can write a small driver to test it.</p>
<p>In test case 1 beblow, we create a kthread which is alway running. We will find<br>that kthread_stop can not stop it.</p>
<p>In test case 2 below, we use kthread_stop to stop a thread which is created<br>righ now. You can find that the “created thread” will never put into running,<br>and kthread_stop will return -4.</p>
<p>In test case 3 below, created thread will use thread_should_stop to check<br>if it should stop. If it finds it should stop, it will go to return by itself.<br>This test case can work well.</p>
<p>So the logic of thread stop should be:</p>
<ol>
<li><p>If thread function is finished, related thread will stop.</p>
</li>
<li><p>If thread function is not finished, there should be another code who tells<br>the thread that others want it to be stopped. Other code uses kthread_stop()<br>to set related flag for the thread. This is not an async message, which<br>means thread should check this flag to find if others want it to stop. If<br>thread does not to check this flag, it will be running alway. So it seems<br>even kernel schedule will not stop a thread whose “should stop flag” is<br>already set by “other code”.</p>
<p>So normally you can see the usage in test case 3. Or we can do thread<br>function, and at the end of function wait “other code” to stop it like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kthread_function ()</span><br><span class="line">&#123;</span><br><span class="line">	ret = do_thread_job();		</span><br><span class="line"></span><br><span class="line">	while (!kthread_should_stop())</span><br><span class="line">		schedule();</span><br><span class="line"></span><br><span class="line">	return ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>In this way, “other code” can get the return value of thread.</p>
</li>
<li><p>We can not use kthread_stop right after the creation of a thread, it seems<br>kernel thinks that maybe we do not want the thread to be put into running.</p>
<p>(to do: when can we use kthread_stop after kthread_run?)</p>
</li>
<li><p>Last but not least, as always mentioned in other blogs, kthread_stop should<br>be called when thread is running.</p>
</li>
</ol>
<p>Test driver:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;linux/kernel.h&gt;</span><br><span class="line">#include &lt;linux/kthread.h&gt;</span><br><span class="line">#include &lt;linux/module.h&gt;</span><br><span class="line">#include &lt;linux/delay.h&gt;</span><br><span class="line"></span><br><span class="line">MODULE_LICENSE(&quot;Dual BSD/GPL&quot;);</span><br><span class="line">static int case_num = 1;</span><br><span class="line">module_param(case_num, int, S_IRUGO);</span><br><span class="line"></span><br><span class="line">static int thread_fun(void *date)</span><br><span class="line">&#123;</span><br><span class="line">	unsigned long i;</span><br><span class="line"></span><br><span class="line">	while (1) &#123;</span><br><span class="line">		i++;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* test if kthread_stop can stop a thread right now */</span><br><span class="line">static int test_kthread_stop_a_thread(void)</span><br><span class="line">&#123;</span><br><span class="line">	struct task_struct *thread;</span><br><span class="line"></span><br><span class="line">	thread = kthread_run(thread_fun, NULL, &quot;kthread_test&quot;);</span><br><span class="line">	if (IS_ERR(thread)) &#123;</span><br><span class="line">		pr_info(&quot;test: fail to create a kthread\n&quot;);</span><br><span class="line">		return -EPERM;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	msleep(400);</span><br><span class="line"></span><br><span class="line">	kthread_stop(thread);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static int thread_hello(void *date)</span><br><span class="line">&#123;</span><br><span class="line">	pr_info(&quot;hello world!\n&quot;);</span><br><span class="line">	</span><br><span class="line">	while (!kthread_should_stop())</span><br><span class="line">		schedule();</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* test to find when to call thread_stop */</span><br><span class="line">static int test_kthread_stop_righ_after_run(void)</span><br><span class="line">&#123;</span><br><span class="line">	struct task_struct *thread;</span><br><span class="line"></span><br><span class="line">	thread = kthread_run(thread_hello, NULL, &quot;kthread_hello&quot;);</span><br><span class="line">	if (IS_ERR(thread)) &#123;</span><br><span class="line">		pr_info(&quot;test: fail to create a kthread\n&quot;);</span><br><span class="line">		return -EPERM;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	kthread_stop(thread);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static int thread_loop_hello(void *date)</span><br><span class="line">&#123;</span><br><span class="line">	int i = 0;</span><br><span class="line"></span><br><span class="line">	while (!kthread_should_stop()) &#123;</span><br><span class="line">		pr_info(&quot;hello world: %d!\n&quot;, i++);</span><br><span class="line">		msleep(1000);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	pr_info(&quot;bye :)\n&quot;);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* normal */</span><br><span class="line">static int test_kthread_stop_normal(void)</span><br><span class="line">&#123;</span><br><span class="line">	struct task_struct *thread;</span><br><span class="line"></span><br><span class="line">	thread = kthread_run(thread_loop_hello, NULL, &quot;kthread_loop_hello&quot;);</span><br><span class="line">	if (IS_ERR(thread)) &#123;</span><br><span class="line">		pr_info(&quot;test: fail to create a kthread\n&quot;);</span><br><span class="line">		return -EPERM;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	msleep(4000);</span><br><span class="line"></span><br><span class="line">	kthread_stop(thread);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static int __init kthread_init(void)</span><br><span class="line">&#123;</span><br><span class="line">	switch (case_num) &#123;</span><br><span class="line">	case 1:</span><br><span class="line">		test_kthread_stop_a_thread();</span><br><span class="line">		break;</span><br><span class="line">	case 2:</span><br><span class="line">		test_kthread_stop_righ_after_run();</span><br><span class="line">		break;</span><br><span class="line">	case 3:</span><br><span class="line">		test_kthread_stop_normal();</span><br><span class="line">		break;</span><br><span class="line">	default:		</span><br><span class="line">		pr_err(&quot;no test case found!\n&quot;);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static void __exit kthread_exit(void)</span><br><span class="line">&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module_init(kthread_init);</span><br><span class="line">module_exit(kthread_exit);</span><br><span class="line"></span><br><span class="line">MODULE_AUTHOR(&quot;Sherlock&quot;);</span><br><span class="line">MODULE_DESCRIPTION(&quot;The driver for kthread study&quot;);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux kernel poll</title>
    <url>/Linux-kernel-poll/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* fs/select.c */</span><br><span class="line">SYSCALL_DEFINE3(poll, struct pollfd __user *, ufds, unsigned int, nfds,</span><br><span class="line">		int, timeout_msecs)</span><br><span class="line">  +-&gt; do_sys_poll</span><br><span class="line">    +-&gt; do_poll(head, &amp;table, end_time)</span><br><span class="line"></span><br><span class="line">    /* poll的主流程在do_poll这个函数里。可以看到这个函数是一个大for循环，基本逻辑是：*/</span><br><span class="line"></span><br><span class="line">    for () &#123;</span><br><span class="line">	/*</span><br><span class="line">	 * poll系统调用可能是poll一组fd，所以这里循环调用这组fd对应的底层poll</span><br><span class="line">	 * 回调函。以uacce驱动为例, 驱动的poll回调函数里，一般先调用poll_wait</span><br><span class="line">	 * 把自己挂到一个等待队列上，注意这个操作就是在等待队列的链表上加上一个</span><br><span class="line">	 * entry, 函数会马上返回，这之后会检测一下是否已经poll到数据。注意，真正</span><br><span class="line">	 * 把当前进程睡眠的点在下面的poll_schedule_timeout函数处，所以如果第一次</span><br><span class="line">	 * poll的时候已经有任务完成，程序走到下面if count的地方会直接返回，不会</span><br><span class="line">	 * 睡眠。</span><br><span class="line">	 */</span><br><span class="line">	for () &#123;</span><br><span class="line">		do_pollfd();</span><br><span class="line">		  +-&gt; vfs_poll();</span><br><span class="line">		    +-&gt; file-&gt;f_op-&gt;poll(file, pt);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	/* poll到有event发生或者超时就退出大循环 */</span><br><span class="line">	if (count || timed_out)</span><br><span class="line">		break;</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * 睡眠等待event发生，以uacce驱动为例，当硬件有任务完成的时候，会在中断</span><br><span class="line">	 * 处理中唤醒等待队列上的进程。进程被唤醒后，从这里继续执行，在上面的</span><br><span class="line">	 * 内部for循环里依次调用各个fd的poll函数查找有event发生的fd。</span><br><span class="line">	 */</span><br><span class="line">	poll_schedule_timeout();</span><br><span class="line"></span><br><span class="line">    &#125;;</span><br></pre></td></tr></table></figure>

<p>uacce驱动里的poll回调函数:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static __poll_t uacce_fops_poll(struct file *file, poll_table *wait)</span><br><span class="line">&#123;</span><br><span class="line">	struct uacce_queue *q = file-&gt;private_data;</span><br><span class="line">	struct uacce_device *uacce = q-&gt;uacce;</span><br><span class="line"></span><br><span class="line">	poll_wait(file, &amp;q-&gt;wait, wait);</span><br><span class="line">	if (uacce-&gt;ops-&gt;is_q_updated &amp;&amp; uacce-&gt;ops-&gt;is_q_updated(q))</span><br><span class="line">		return EPOLLIN | EPOLLRDNORM;</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux rsync备份数据</title>
    <url>/Linux-rsync%E5%A4%87%E4%BB%BD%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<p> 需要备份的数据在服务器上，这里叫做server。把数据备份到client上，这里也叫<br> backup server。下面是具体的步骤：</p>
<p>步骤：<br>0. run “sudo apt-get install rsync”, if do not have one</p>
<ol>
<li>configure /etc/rsyncd.conf in server, need to touch a file yourself.</li>
<li>configure /home/***/security/rsync.pass in server indicating client’s<br>user_name and key.</li>
<li>configure /etc/default/rsync: RSYNC_ENABLE=true in server</li>
<li>run “sudo /etc/init.d/rsync start” in server</li>
<li>run “rsync -vzrtopg –progress user_name@server_ip::test /home/test” in client<br>(backup server) to backup directory indicating in [test] in /etc/rsyncd.conf<br>in server to /home/test in client, using user user_name</li>
</ol>
<p>说明：</p>
<ol>
<li>遇到这样的错误”auth failed on module xxx”, 可以查看：<br><span class="exturl" data-url="aHR0cDovL2Jsb2cuc2luYS5jb20uY24vcy9ibG9nXzRkYTA1MWE2MDEwMWg4YW0uaHRtbA==">http://blog.sina.com.cn/s/blog_4da051a60101h8am.html<i class="fa fa-external-link-alt"></i></span></li>
<li>配置中遇到错误，可以在/var/log/rsyncd.log查看log记录</li>
<li>/etc/rsyncd.conf中的配置([test]), 可以参考：<br><span class="exturl" data-url="aHR0cDovL3d3dy5pdGV5ZS5jb20vdG9waWMvNjA0NDM2">http://www.iteye.com/topic/604436<i class="fa fa-external-link-alt"></i></span></li>
<li>在ubuntu 14.04上，/etc/rsyncd.conf中的pid file为：/var/run/rsyncd.pid</li>
<li>步骤1～4在服务器上完成，其中user_name是client执行备份命令时的用户名，key是密码</li>
<li>步骤5说明在client上应该用什么样的命令备份服务器上的目录，user_name是步骤2中<br>的user_name, server_ip是服务器的ip, test是在/etc/rsyncd.conf中的[test],<br>/home/test表示把[test]中指示的目录备份到client的/home/test下</li>
<li>具体可以根据需求把步骤5中的命令在每天的某个时候定时执行，这样每天的数据都得到了备份</li>
</ol>
]]></content>
      <tags>
        <tag>运维</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux syscon and regmap</title>
    <url>/Linux-syscon-and-regmap/</url>
    <content><![CDATA[<h2 id="What-is-regmap-and-syscon"><a href="#What-is-regmap-and-syscon" class="headerlink" title="What is regmap and syscon"></a>What is regmap and syscon</h2><p>regmap was introduced by <span class="exturl" data-url="aHR0cHM6Ly9sd24ubmV0L0FydGljbGVzLzQ1MTc4OS8=">https://lwn.net/Articles/451789/<i class="fa fa-external-link-alt"></i></span><br>From my understanding, it provided us a set API to read/write non memory-map I/O<br>(e.g. I2C and SPI read/write) at first. Then after introduced regmap-mmio.c,<br>we can use regmap to access memory-map I/O.</p>
<p>code path: drivers/base/regmap/*</p>
<p>syscon was introduced by <span class="exturl" data-url="aHR0cHM6Ly9sa21sLm9yZy9sa21sLzIwMTIvOS80LzU2OA==">https://lkml.org/lkml/2012/9/4/568<i class="fa fa-external-link-alt"></i></span><br>It provides a set API to access a misc device(e.g. pci-sas subsystem registers<br>in P660) based on regmap, explicitly based on regmap-mmio.c I guess.</p>
<p>code path: drivers/mfd/*</p>
<h2 id="arch-of-regmap-and-syscon"><a href="#arch-of-regmap-and-syscon" class="headerlink" title="arch of regmap and syscon"></a>arch of regmap and syscon</h2><p>basic structure of regmap:</p>
<p>struct regmap:                   per base address per regmap<br>struct regmap_bus:               include read/write callback, different “bus”<br>                                 (e.g. I2C, SPI, mmio) have different regmap_bus<br>struct regmap_mmio_context:      don’t know…<br>struct regmap_config:            confiuration info.</p>
<p>regmap-mmio call flow:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/base/regmap/regmap-mmio.c */</span><br><span class="line">__devm_regmap_init_mmio_clk</span><br><span class="line">    --&gt; __devm_regmap_init</span><br><span class="line">        /* regmap_bus(regmap_mmio), config as input, create regmap */</span><br><span class="line">        --&gt; __regmap_init</span><br><span class="line">	    /* if don&#x27;t have bus-&gt;read or bus-&gt;write */</span><br><span class="line">	    --&gt; map-&gt;reg_read = _regmap_bus_reg_read;</span><br><span class="line">	    --&gt; map-&gt;reg_write = _regmap_bus_reg_write;</span><br><span class="line">	    ...</span><br><span class="line">	    /* if have bus-&gt;read */</span><br><span class="line">            --&gt; map-&gt;reg_read  = _regmap_bus_read;</span><br><span class="line">	        --&gt; map-&gt;bus-&gt;read</span><br><span class="line"></span><br><span class="line">/* drivers/base/regmap/regmap.c */</span><br><span class="line">regmap_read(struct regmap *map, unsigned int reg, unsigned int *val)</span><br><span class="line">    --&gt; _regmap_read</span><br><span class="line">        /* _regmap_bus_reg_read */</span><br><span class="line">        --&gt; map-&gt;reg_read(context, reg, val);</span><br><span class="line">	    --&gt; map-&gt;bus-&gt;reg_read(map-&gt;bus_context, reg, val)</span><br><span class="line"></span><br><span class="line">/* drivers/base/regmap/regmap.c */</span><br><span class="line">regmap_update_bits</span><br><span class="line">    --&gt; _regmap_update_bits</span><br><span class="line">        --&gt; _regmap_read</span><br><span class="line">	--&gt; _regmap_write</span><br><span class="line">	    --&gt; map-&gt;reg_write</span><br></pre></td></tr></table></figure>
<p>basic structure of syscon:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct syscon:                 include a strutct regmap; an element in list below</span><br><span class="line">static LIST_HEAD(syscon_list)</span><br><span class="line"></span><br><span class="line">syscon driver init a regmap:</span><br><span class="line">syscon-&gt;regmap = devm_regmap_init_mmio(dev, base, &amp;syscon_regmap_config);</span><br></pre></td></tr></table></figure>
<h2 id="why-we-need-a-syscon-to-describe-a-misc-device"><a href="#why-we-need-a-syscon-to-describe-a-misc-device" class="headerlink" title="why we need a syscon to describe a misc device"></a>why we need a syscon to describe a misc device</h2><p>To understand this, we shoudl search related discussion in community:<br><span class="exturl" data-url="aHR0cHM6Ly9saXN0cy5vemxhYnMub3JnL3BpcGVybWFpbC9kZXZpY2V0cmVlLWRpc2N1c3MvMjAxMi1BdWd1c3QvMDE4NzA0Lmh0bWw=">https://lists.ozlabs.org/pipermail/devicetree-discuss/2012-August/018704.html<i class="fa fa-external-link-alt"></i></span></p>
<p>From my understanding, syscon firstly registers a syscon dts node to syscon_list,<br>we could find this node when we try to access related registers.</p>
<h2 id="how-to-use-syscon-to-access-a-misc-device"><a href="#how-to-use-syscon-to-access-a-misc-device" class="headerlink" title="how to use syscon to access a misc device"></a>how to use syscon to access a misc device</h2><p>e.g.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">need dts node:</span><br><span class="line">	pcie_sas: pcie_sas@0xb0000000 &#123;</span><br><span class="line">		compatible = &quot;hisilicon,pcie-sas-subctrl&quot;, &quot;syscon&quot;;</span><br><span class="line">		reg = &lt;0xb0000000 0x10000&gt;;</span><br><span class="line">	&#125;;</span><br></pre></td></tr></table></figure>
<p>use below function read/write:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">regmap_read(hisi_pcie-&gt;subctrl, PCIE_SUBCTRL_SYS_STATE4_REG +</span><br><span class="line">	    0x100 * hisi_pcie-&gt;port_id, &amp;val);</span><br><span class="line"></span><br><span class="line">regmap_update_bits(pcie-&gt;subctrl, reg, bit_mask, mode &lt;&lt; bit_shift);</span><br></pre></td></tr></table></figure>
<p>use below function create struct regmap:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hisi_pcie-&gt;subctrl =</span><br><span class="line">	syscon_regmap_lookup_by_compatible(&quot;hisilicon,pcie-sas-subctrl&quot;);</span><br></pre></td></tr></table></figure>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference:"></a>reference:</h2><ol>
<li>Documentation/devicetree/bindings/regmap/regmap.txt</li>
<li>../mfd/mfd.txt</li>
<li>./syscon.txt</li>
</ol>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux pin page测试</title>
    <url>/Linux-pin-page%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<p>我们写一个pin page的代码观察pin page系统的具体行为。这个测试代码分为两部分，一部分<br>是一个内核驱动，一部分是一个用户态的测试代码。</p>
<p>内核驱动的代码在：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL3RyZWUvbWFzdGVyL3BhZ2VfcGlu">https://github.com/wangzhou/tests/tree/master/page_pin<i class="fa fa-external-link-alt"></i></span><br>page_pin_test.c, 用户态代码在相同目录的u_page_pin.c里。</p>
<p>内核驱动暴露一个字符设备到用户态，用户态测试代码可以通过这个字符设备的ioctl接口<br>做pin page的操作。这个操作把用户态申请的VA对应的物理地址pin住，使得对应的物理<br>地址常驻内存，而且不会在不同numa节点之间迁移。</p>
<p>对应的用户态测试代码先用mmap申请一段匿名页，然后调用上述字符设备的ioctl接口把这<br>段匿名页pin住。</p>
<p>pin page的过程会触发缺页流程然后拿到具体的物理页。可以在pin ioctl的前后分别加上<br>getrusage()统计系统缺页的次数, 我们用struct rusage里的ru_minflt统计建立物理页时<br>的缺页流程。使用如上目录中的u_uacce_pin_page.c可以看到pin ioctl前后的的ru_minflt<br>差值正好就是mmap申请内存页的数目。</p>
<p>我们使用虚拟机观察内存迁移的情况。具体的虚拟机启动参数如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./qemu/aarch64-softmmu/qemu-system-aarch64 -machine virt,gic-version=3,iommu=smmuv3 \</span><br><span class="line">-bios ./QEMU_EFI.fd \</span><br><span class="line">-enable-kvm -cpu host -m 4G \</span><br><span class="line">-smp 8 \</span><br><span class="line">-numa node,nodeid=0,mem=2G,cpus=0-3 \</span><br><span class="line">-numa node,nodeid=1,mem=2G,cpus=4-7 \</span><br><span class="line">-kernel ./linux-kernel-warpdrive/arch/arm64/boot/Image \</span><br><span class="line">-initrd ~/repos/buildroot/output/images/rootfs.cpio.gz -nographic -append \</span><br><span class="line">&quot;rdinit=init console=ttyAMA0 earlycon=pl011,0x9000000 acpi=force&quot; \</span><br><span class="line">-device virtio-9p-pci,fsdev=p9fs,mount_tag=p9 \</span><br><span class="line">-fsdev local,id=p9fs,path=p9root,security_model=mapped</span><br></pre></td></tr></table></figure>
<p>如上，我们给虚拟机配置了两个numa节点，一个numa节点配置了2G内存。在u_uacce_pin_page.c<br>里再启动一个线程，并把第二个线程绑定到cpu4，就是numa node1上跑。主线程先分配内存<br>然后写内存，过10s后，第二个线程写同样的地址。分别测试pin page和没有pin page的情况，<br>并用numastat的跟踪查看：(下面a.out是u_uacce_pin_page.c编译出来的可执行程序)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">./a.out &amp;</span><br><span class="line">PID=`pidof a.out`</span><br><span class="line"></span><br><span class="line">for i in `seq 10`</span><br><span class="line">do</span><br><span class="line">	numastat -p $PID</span><br><span class="line">	sleep 2</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">kill -9 $PID</span><br></pre></td></tr></table></figure>
<p>可以看出，在有pin page的时候，内存始终在一个numa节点上。在没有pin page的时候，<br>内存可能是一开始在numa node0上，但是10s后被迁移到numa node1上。这个是因为在没有<br>pin page时，内核会根据实际使用内存的cpu，动态的调整实际物理内存的分布。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Per-node process memory usage (in MBs) for PID 417 (a.out)</span><br><span class="line">                           Node 0          Node 1           Total</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         0.01            0.00            0.01</span><br><span class="line">Stack                        0.00            0.00            0.01</span><br><span class="line">Private                      0.57            0.00            0.57</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                        0.58            0.00            0.59</span><br><span class="line"></span><br><span class="line">Per-node process memory usage (in MBs) for PID 417 (a.out)</span><br><span class="line">                           Node 0          Node 1           Total</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         0.01            0.00            0.01</span><br><span class="line">Stack                        0.00            0.00            0.01</span><br><span class="line">Private                      0.57            0.00            0.57</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                        0.58            0.01            0.59</span><br><span class="line"></span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">Per-node process memory usage (in MBs) for PID 417 (a.out)</span><br><span class="line">                           Node 0          Node 1           Total</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         0.01            0.00            0.01</span><br><span class="line">Stack                        0.00            0.00            0.01</span><br><span class="line">Private                      0.17            0.40            0.57</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                        0.18            0.41            0.59</span><br><span class="line"></span><br><span class="line">Per-node process memory usage (in MBs) for PID 417 (a.out)</span><br><span class="line">                           Node 0          Node 1           Total</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         0.01            0.00            0.01</span><br><span class="line">Stack                        0.00            0.00            0.01</span><br><span class="line">Private                      0.17            0.40            0.57</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                        0.18            0.41            0.59</span><br><span class="line"></span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux zswap构架分析</title>
    <url>/Linux-zswap%E6%9E%84%E6%9E%B6%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本模型"><a href="#基本模型" class="headerlink" title="基本模型"></a>基本模型</h2><ul>
<li><p>zswap初始化流程</p>
<ol>
<li><p>为系统里的每一个cpu core分配per cpu dst内存。</p>
</li>
<li><p>为每个cpu core分配内核压缩解压缩的上下文。</p>
</li>
<li><p>创建zpool, 并根据用户态模块参数选择zpool后端使用的内存分配器。创建<br>zpool的时候需要给zpool注册一个evict的回调函数，这个函数用于在zpool的<br>后端内存分配器没有内存的时候, 把zpool里的压缩内存向swap设备写入。可以<br>看到现在的evict在向swap设备写数据的时候还要先把压缩的数据解压缩，如果写<br>入swap设备的数据将来被使用，重新加载回内存的代码路径是标准的缺页流程，<br>和zswap没有关系。这个步骤整体上把创建出来的各种基础数据结构封装在一个<br>struct zswap_pool的结构中, 上面步骤里的per cpu压缩解压缩上下文也放在了<br>zswap_pool。</p>
</li>
<li><p>注册frontswap的回调函数。根据内核Documentation/vm/frontswap.rst，store<br>用于把swap页存入zpool, load用于从zpool重新加载swap的页，invalidate_*<br>用于把zpool里存放的压缩页面释放。</p>
</li>
</ol>
</li>
<li><p>核心数据结构</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   struct zswap_pool</span><br><span class="line"></span><br><span class="line">   swap page ---&gt; frontswap</span><br><span class="line">                     .store</span><br><span class="line">.load</span><br><span class="line">.invalidate_page</span><br><span class="line">.invalidate_area</span><br><span class="line">.init</span><br></pre></td></tr></table></figure>
<p>zswap初始化的时候注册frontswap的一个实例，frontswap的各个回调中使用zpool接口<br>存储压缩的内存，zpool接口的后端可以是不同的专用于存储压缩内存的内存分配器。</p>
</li>
<li><p>对用户态的接口</p>
<ol>
<li><p>zswap用多个模块参数用来配置zswap的参数: zpool的后端内存分配器是可以选的<br>(默认是zbud); 压缩解压算法(默认是LZO)，测试硬件offload的时候，这里要选<br>则相应的算法; zpool占内存的大小; 对相同页的优化处理。</p>
<p>这些参数在/sys/module/zswap/parameters/下也可以配置。</p>
</li>
<li><p>在/sys/kernel/debug/zswap/下有zswap的相关统计项。</p>
</li>
</ol>
</li>
<li><p>对内核crypto comp的接口</p>
<ol>
<li>当前代码使用的是内核crypto comp压缩解压缩接口，没有使用crypto acomp接口。</li>
</ol>
</li>
</ul>
<h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>  不清楚这里的逻辑，为什么要一个zswap pool的链表。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">__zswap_pool_create_fallback(void)</span><br><span class="line">  +-&gt; zswap_pool_create(char *type, char *compressor)</span><br><span class="line">    +-&gt; cpuhp_state_add_instance(CPUHP_MM_ZSWP_POOL_PREPARE, &amp;pool-&gt;node)</span><br><span class="line"></span><br><span class="line">list_add(&amp;pool-&gt;list, &amp;zswap_pools);</span><br></pre></td></tr></table></figure>
<p>  这里选一个zswap_frontswap_store的实现分析:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> zswap_frontswap_store</span><br><span class="line">   +-&gt; entry = zswap_entry_cache_alloc(GFP_KERNEL)</span><br><span class="line">   	分配一个zswap_entry, 用来描述要压缩存储的一个页。</span><br><span class="line">   +-&gt; crypto_comp_compress</span><br><span class="line">       调用crypto comp API做压缩。</span><br><span class="line">   +-&gt; zpool_malloc(entry-&gt;pool-&gt;zpool, hlen + dlen, gfp, &amp;handle)</span><br><span class="line">       从zpool里分配一段内存, 用来存压缩的页。</span><br><span class="line">   +-&gt; zpool_map_handle(entry-&gt;pool-&gt;zpool, handle, ZPOOL_MM_RW)</span><br><span class="line">   +-&gt; memcpy(buf, &amp;zhdr, hlen)</span><br><span class="line">   +-&gt; memcpy(buf + hlen, dst, dlen)</span><br><span class="line">       把压缩后的swap页存入zpool。</span><br><span class="line">   +-&gt; zswap_rb_insert(&amp;tree-&gt;rbroot, entry, &amp;dupentry)</span><br><span class="line">       把对应的swap页的entry插入一个红黑树，以后load，invalidate等可以从这个</span><br><span class="line">红黑树查找对应的swap页。</span><br></pre></td></tr></table></figure>
<pre><code>从上面store函数的分析中可以看到，因为整个设计都是基于per cpu的，所以做
crypto_comp_compress的时候都是关闭抢占的(是否要关闭调度), 这和acomp的基本
使用方式是不兼容的，在crypto testmgr.c里acomp的test case是wait等待任务完成
的。还有一点，压缩完的数据需要copy到zpool的内存里。
</code></pre>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>zswap</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux thermal子系统和lm_sensors用户态工具</title>
    <url>/Linux-thermal%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%92%8Clm-sensors%E7%94%A8%E6%88%B7%E6%80%81%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<ol>
<li>Linux thermal驱动</li>
</ol>
<hr>
<p>   Linux thermal是一个内核和温度检测控制有关的驱动子系统，他的位置在drivers/thermal/*.<br>   相关的内核头文件在include/linux/thermal.h。具体的设备驱动需要向thermal框架注册<br>   thermal_zone_device, thermal框架会在thermal_zone_device里封装一个device向系统<br>   注册，通过这个device向用户态暴露一组sysfs属性文件。用户态可以通过这组文件设置相关<br>   参数、获取相关信息。</p>
<p>   在我的笔记本上，这组属性文件大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wangzhou@kllp05:/sys/class$ tree thermal/thermal_zone0</span><br><span class="line">thermal/thermal_zone0</span><br><span class="line">├── available_policies</span><br><span class="line">├── device -&gt; ../../../LNXSYSTM:00/LNXSYBUS:01/LNXTHERM:00</span><br><span class="line">├── emul_temp</span><br><span class="line">├── integral_cutoff</span><br><span class="line">├── k_d</span><br><span class="line">├── k_i</span><br><span class="line">├── k_po</span><br><span class="line">├── k_pu</span><br><span class="line">├── mode</span><br><span class="line">├── offset</span><br><span class="line">├── passive</span><br><span class="line">├── policy</span><br><span class="line">├── power</span><br><span class="line">│   ├── async</span><br><span class="line">│   ├── autosuspend_delay_ms</span><br><span class="line">│   ├── control</span><br><span class="line">│   ├── runtime_active_kids</span><br><span class="line">│   ├── runtime_active_time</span><br><span class="line">│   ├── runtime_enabled</span><br><span class="line">│   ├── runtime_status</span><br><span class="line">│   ├── runtime_suspended_time</span><br><span class="line">│   └── runtime_usage</span><br><span class="line">├── slope</span><br><span class="line">├── subsystem -&gt; ../../../../class/thermal</span><br><span class="line">├── sustainable_power</span><br><span class="line">├── temp</span><br><span class="line">├── trip_point_0_temp</span><br><span class="line">├── trip_point_0_type</span><br><span class="line">├── type</span><br><span class="line">└── uevent</span><br></pre></td></tr></table></figure>
<p>   这里只是展示了thermal_zone0, 当然一个系统里可以多个这样的设备。</p>
<p>   实际上一个具体的驱动和thermal子系统的关系可以通过三个对象去描述，一个就是<br>   这里的thermal_zone_thermal，它表示测量温度的sensor；还可以注册这个sensor所在<br>   温度管理域的降温设备；有了降温设备，还可以注册相应的温度调节策略。（to do: …）</p>
<p>   实际上，你要是只想读温度出来，只注册一个thermal_zone_thermal并提供其中一个<br>   获取温度的回调函数的实现就可以了：thermal_zone_device_ops-&gt;.get_temp。这个函数<br>   会被/sys/class/thermal/<dev>/temp的show函数调用，从而显示测量的温度。</dev></p>
<p>   thermal子系统还会根据注册thermal_zone_device时的参数，把设备的信息通过<br>   /sys/class/hwmon子系统暴露出来。如果你不带注册参数，thermal子系统默认会通过<br>   /sys/class/hwmon暴露信息, 比如这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wangzhou@kllp05:/sys/class$ tree hwmon/hwmon0</span><br><span class="line">hwmon/hwmon0</span><br><span class="line">├── name</span><br><span class="line">├── power</span><br><span class="line">│   ├── async</span><br><span class="line">│   ├── autosuspend_delay_ms</span><br><span class="line">│   ├── control</span><br><span class="line">│   ├── runtime_active_kids</span><br><span class="line">│   ├── runtime_active_time</span><br><span class="line">│   ├── runtime_enabled</span><br><span class="line">│   ├── runtime_status</span><br><span class="line">│   ├── runtime_suspended_time</span><br><span class="line">│   └── runtime_usage</span><br><span class="line">├── subsystem -&gt; ../../../../class/hwmon</span><br><span class="line">├── temp1_crit</span><br><span class="line">├── temp1_input</span><br><span class="line">└── uevent</span><br></pre></td></tr></table></figure>
<p>   temp1_input的show函数会最终调用到驱动里的.get_temp。</p>
<ol start="2">
<li>lm_sensors用户态工具的使用</li>
</ol>
<hr>
<p>  lm_sensors可以读取系统上sensor的信息。它的源代码可以在这里下载到：<br>  <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xtLXNlbnNvcnMvbG0tc2Vuc29ycy5naXQ=">https://github.com/lm-sensors/lm-sensors.git<i class="fa fa-external-link-alt"></i></span></p>
<p>  粗略从代码上看，它使用的是hwmon接口提供的信息。</p>
<p>  在ubuntu系统上你可以使用如下命令简单尝试下lm_sensors:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install lm-sensors</span><br><span class="line"></span><br><span class="line">/* this is a Perl script in /usr/sbin/ */</span><br><span class="line">sudo sensors-detect</span><br><span class="line">输入这个命令后，一路YES。</span><br><span class="line"></span><br><span class="line">wangzhou@kllp05:~/notes$ sensors</span><br><span class="line">iwlwifi-virtual-0</span><br><span class="line">Adapter: Virtual device</span><br><span class="line">temp1:        +37.0°C  </span><br><span class="line"></span><br><span class="line">thinkpad-isa-0000</span><br><span class="line">Adapter: ISA adapter</span><br><span class="line">fan1:           0 RPM</span><br><span class="line"></span><br><span class="line">acpitz-virtual-0</span><br><span class="line">Adapter: Virtual device</span><br><span class="line">temp1:        +30.0°C  (crit = +128.0°C)</span><br><span class="line"></span><br><span class="line">coretemp-isa-0000</span><br><span class="line">Adapter: ISA adapter</span><br><span class="line">Package id 0:  +33.0°C  (high = +100.0°C, crit = +100.0°C)</span><br><span class="line">Core 0:        +31.0°C  (high = +100.0°C, crit = +100.0°C)</span><br><span class="line">Core 1:        +33.0°C  (high = +100.0°C, crit = +100.0°C)</span><br><span class="line"></span><br><span class="line">pch_skylake-virtual-0</span><br><span class="line">Adapter: Virtual device</span><br><span class="line">temp1:        +30.0°C  </span><br></pre></td></tr></table></figure>

<ol start="3">
<li>lm_sensors分析</li>
</ol>
<hr>
<p>  (to do: sensors-detect, sensors, sensord, config…)</p>
<ol start="4">
<li>Linux thermal和lm_sensors的关系</li>
</ol>
<hr>
<p>  如上，现在的lm_sensors使用的是hwmon接口获取信息。<br>  (to do: …)</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux vfio driver arch analysis</title>
    <url>/Linux-vfio-driver-arch-analysis/</url>
    <content><![CDATA[<p>The whole vfio subsystem should support 3 sub-features:</p>
<ol>
<li>cfg/mem/io support: user space can access cfg/mem/io of vf.</li>
<li>dma support: data in vf can be translated to user space dma memory range.</li>
<li>interrupt from vf can be routed to VM OS.</li>
</ol>
<p>From view of code, we can see whole vfio driver as:</p>
<ol>
<li>init base vfio arch in drivers/vfio/vfio.c</li>
<li>init pci/platform vfio device driver in drivers/vfio/pci/vfio_pci.c, drivers/vfio/platform/vfio_platform.c</li>
<li>init vfio iommu driver and register to vfio system in drivers/vfio/vfio_iommu*</li>
</ol>
<p>This vfio system will create /dev/vfio/vfio as a vfio container, which indicates<br>an address space share by multiple devices. It will also create /dev/vfio/<group_number><br>as a vfio group, which indicates a group shared by multiple devices using a iommu<br>or smmu unit. when we open a /dev/vfio/<group_number>, we will get a fd, which<br>indicates a device handled by vfio system. Device can be controlled by this fd.</group_number></group_number></p>
<p>vfio system does not create new bus, however, we should unbind original device<br>driver, and bind device with vfio device driver. So for a PCI device, we need<br>vfio pci driver to handle this device. This vfio pci driver becomes the agent of<br>this device and export all its resource to user space.</p>
<p>The interfaces for userspace:</p>
<h2 id="vfio-init-in-vfio-c"><a href="#vfio-init-in-vfio-c" class="headerlink" title="vfio init in vfio.c"></a>vfio init in vfio.c</h2><p>vfio registers a misc device in /dev/vfio/vfio.</p>
<p>initialize items in vfio: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">register vfio_dev(miscdevice) in misc sub-system, file: /dev/vfio/vfio</span><br><span class="line">              |</span><br><span class="line">              |--&gt; fops(vfio_fops(file_operations))</span><br><span class="line">                             |</span><br><span class="line">                             |--&gt; open: create vfio_container and</span><br><span class="line">                                        set this as file&#x27;s private data</span><br><span class="line">                                  unlocked_ioctl:</span><br><span class="line">                                          /* bind vfio_container and</span><br><span class="line">                                           * vfio_iommu_driver which had been</span><br><span class="line">                                           * registered in vfio.iommu_driver_list</span><br><span class="line">                                           * in specific iommu file, like:</span><br><span class="line">                                           * vfio_iommu_type1.c</span><br><span class="line">                                           */</span><br><span class="line">                                      --&gt; vfio_ioctl_set_iommu</span><br><span class="line">                                   read/write/mmap: will call functions in vfio_iommu_driver</span><br></pre></td></tr></table></figure>

<p>vfio.c creat a vfio class, this will work together with device_create in<br>vfio_create_group. vfio creates a vfio group indeed is creating a device in this<br>vfio class, vfio group file will be /dev/vfio/<group_number>.</group_number></p>
<p>vfio_create_group is called in vfio_pci_probe and vfio_platform_probe. In the probe,<br>we get the devices which we want to handle by vfio system, then find which iommu group<br>these devices belong to, then create the related vfio_group to help to store related<br>iommu group. Here just use device_creat to create a file under /dev/vfio/ to refer to<br>the vfio_group. At last, we creat vfio_pci/vfio_platform_device for the devices<br>which we want vfio system to take care of. For details, please refer to part2.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio.class = class_create(THIS_MODULE, &quot;vfio&quot;)</span><br></pre></td></tr></table></figure>
<p>when we operate /dev/vfio/<group_number>, indeed we will call<br>functions in vfio_group_fops.</group_number></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">register chr device: vfio.group_cdev(struct cdev)</span><br><span class="line">                              |</span><br><span class="line">                              |--&gt; file_operations(vfio_group_fops)</span><br><span class="line">                                               |</span><br><span class="line">                                               |--&gt; open</span><br><span class="line">                                                    unlocked_ioctl</span><br><span class="line">                                                            /* register ops of</span><br><span class="line">                                                             * vfio_device</span><br><span class="line">                                                             */</span><br><span class="line">                                                        --&gt; vfio_group_get_device_fd</span><br></pre></td></tr></table></figure>
<p>so what happen if we call above callback:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">open: find vfio_group --&gt; share vfio_group to private_data of related struct file.</span><br><span class="line">      above vfio_group was created and added in vfio_pci_probe.</span><br><span class="line">unlocked_ioctl: </span><br><span class="line">    VFIO_GROUP_GET_DEVICE_FD:</span><br><span class="line">	--&gt;vfio_group_get_device_fd</span><br><span class="line">    VFIO_GROUP_SET_CONTAINER: </span><br><span class="line">        --&gt; vfio_group_set_container</span><br></pre></td></tr></table></figure>
<p>An ioctl of vfio_group can get a fd for the device.</p>
<p>We already get the iommu_group of a device, why do we use vfio_group_set_container<br>to add this vfio_group to a vfio container?</p>
<p>The concept of vfio container is to build an address space shared by multiple<br>devices.</p>
<pre><code>  vfio container


          ------+--------------+--------------+-------
                |              |              |
              +-+--+         +-+--+         +-+--+
              |smmu|         |smmu|         |smmu|
              +-+--+         +-+--+         +-+--+
                |              |              |
              +-+--+         +-+--+         +-+--+
              |dev |         |dev |         |dev |
              +----+         +----+         +----+
</code></pre>
<p>When vfio_group is added to vfio container, mappings in this vfio_group will be<br>added to other smmus physically. So all smmus above have same mapping if vfio_groups<br>have been added into same vfio container. All mappings are maintained in vfio<br>container. </p>
<p>how to add vfio_group to vfio_container:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio_ioctl_set_iommu</span><br><span class="line">    --&gt; __vfio_container_attach_groups(container, driver, data)</span><br><span class="line">            --&gt; driver-&gt;ops-attach_group(vfio_iommu, group-&gt;iommu_group)</span><br></pre></td></tr></table></figure>

<h2 id="probe-of-vfio-pci-c-vfio-platform-c"><a href="#probe-of-vfio-pci-c-vfio-platform-c" class="headerlink" title="probe of vfio_pci.c/vfio_platform.c"></a>probe of vfio_pci.c/vfio_platform.c</h2><p>All working in vfio system will help build below vfio struct:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">global: vfio</span><br><span class="line">            --&gt; group_list(vfio_group)</span><br><span class="line">                               |</span><br><span class="line">                               |--&gt; iommu_group</span><br><span class="line">                                    device_list(vfio_device)</span><br><span class="line">                                                     |--&gt;group(vfio_group)</span><br><span class="line">                                                         ops(vfio_device_ops)</span><br><span class="line">                                                                   |--&gt; open</span><br><span class="line">                                                                        ...</span><br><span class="line">            --&gt; device_list(vfio_device)</span><br><span class="line">                               |</span><br><span class="line">                               |--&gt; ops(vfio_device_ops)</span><br><span class="line">                                    group(vfio_group)</span><br><span class="line">         </span><br><span class="line">            --&gt; iommu_drivers_list(vfio_iommu_driver)</span><br><span class="line">                                          |</span><br><span class="line">                                          |--&gt; ops(vfio_iommu_driver_ops)</span><br></pre></td></tr></table></figure>
<p>Here we analyze the flows in vfio_pci.</p>
<p>in vfio_pci_init, use pci_register_driver(&amp;vfio_pci_driver) to probe the PCIe<br>devices in the whole PCIe domain, which devices we had already build up in<br>standard PCIe enumeration process.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio_pci_probe</span><br><span class="line">        /* get iommu_group from device, iommu_group had been added to device&#x27;s</span><br><span class="line">         * iommu_group during the init of iommu(smmu)</span><br><span class="line">         */</span><br><span class="line">    --&gt; vfio_iommu_group_get</span><br><span class="line">    --&gt; allocate memory for vfio_pci_device, which in vfio system refers to above device</span><br><span class="line">        /* this will be called in pci or platform device file to create vfio_group,</span><br><span class="line">         * vfio_device. ops for specific kind of vfio_device is different.</span><br><span class="line">         */</span><br><span class="line">    --&gt; vfio_add_group_dev</span><br><span class="line">            /* create vfio_group, and add it to vfio.group_idr and vfio.group_list.</span><br><span class="line">             * and add iommu_group to this vfio_group.</span><br><span class="line">             */</span><br><span class="line">        --&gt; vfio_create_group</span><br><span class="line">                /* create vfio_group file as /dev/vfio/&lt;group number&gt; */</span><br><span class="line">            --&gt; device_create</span><br><span class="line">            /* create vfio_device and add it to vfio_group.device_list, ops will be</span><br><span class="line">             * called at:</span><br><span class="line">             *</span><br><span class="line">             * vfio_group_fops_unl_ioctl can get a fd which refers to related</span><br><span class="line">             * devcie&#x27;s fd. the operations to this fd will route to the operations</span><br><span class="line">             * in vfio_device_fops(struct file_operations) which will call the</span><br><span class="line">             * callbacks in vfio_device.</span><br><span class="line">             *</span><br><span class="line">             * vfio_pci_device is vfio_device&#x27;s private data.</span><br><span class="line">             */</span><br><span class="line">        --&gt; vfio_group_create_device</span><br></pre></td></tr></table></figure>

<h2 id="vfio-register-iommu-driver-in-specific-iommu-file"><a href="#vfio-register-iommu-driver-in-specific-iommu-file" class="headerlink" title="vfio_register_iommu_driver in specific iommu file"></a>vfio_register_iommu_driver in specific iommu file</h2><p>Physically we can use different iommu implementation, e.g. SMMU in ARM, IOMMU for<br>Intel. This vfio iommu driver is used to control this.</p>
<p>register vfio_iommu_driver to vfio:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio</span><br><span class="line">    --&gt; iommu_drivers_list(vfio_iommu_driver)</span><br><span class="line">                                   |</span><br><span class="line">                                   |--&gt; ops(vfio_iommu_driver_ops)</span><br><span class="line">                                                   |--&gt; open: create vfio_iommu.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>This ops is called by vfio_container-&gt;vfio_smmu_driver-&gt;ops, we bind<br>vfio_container and vfio_smmu_driver together in unlocked_ioctl(using VFIO_SET_IOMMU)<br>of /dev/vfio/vfio. Here we can register specific iommu driver to vfio, now there are<br>vfio iommu driver from X86(vfio_iommu_type1.c) and POWER(vfio_iommu_spapr_tce.c).</p>
<p>how to bind vfio_container and vfio_smmu_driver:<br>for a /dev/vfio/vfio container fd, its ioctl VFIO_SET_IOMMU will set specific<br>IOMMU for the container:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio_fops_unl_ioctl</span><br><span class="line">    --&gt; VFIO_SET_IOMMU</span><br><span class="line">        --&gt; vfio_ioctl_set_iommu(container, arg)</span><br></pre></td></tr></table></figure>

<p>how to call the ops in vfio_smmu_driver:<br>vfio_container-&gt;ops will call the ops in vfio_smmu_driver.</p>
<h2 id="how-to-access-cfg-mem-io-of-VFs"><a href="#how-to-access-cfg-mem-io-of-VFs" class="headerlink" title="how to access cfg/mem/io of VFs"></a>how to access cfg/mem/io of VFs</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* ioctl of vfio_group to get a fd of device */</span><br><span class="line">vfio_group_get_device_fd</span><br><span class="line"></span><br><span class="line">/* open in vfio_pci_device */</span><br><span class="line">--&gt; device-&gt;ops-&gt;open</span><br><span class="line">    --&gt; anon_inode_getfile(&quot;[vfio-device]&quot;, &amp;vfio_device_fops, device, O_RDWR);</span><br><span class="line"></span><br><span class="line">/* to do: So it seems we can use both ways to access vf cfg/mem/io range */</span><br><span class="line">read/write: use fd&#x27;s read/write to access vf cfg/mem/io.</span><br><span class="line">mmap: map cfg/mem/io range in pci_dev to user space.</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWJtLmNvbS9kZXZlbG9wZXJ3b3Jrcy9jb21tdW5pdHkvYmxvZ3MvNTE0NDkwNGQtNWQ3NS00NWVkLTlkMmItY2YxNzU0ZWU5MzZhL2VudHJ5LzIwMTYwNjA1P2xhbmc9ZW4=">https://www.ibm.com/developerworks/community/blogs/5144904d-5d75-45ed-9d2b-cf1754ee936a/entry/20160605?lang=en<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXExMjMzODY5MjYvYXJ0aWNsZS9kZXRhaWxzLzQ3NzU3MDg5">http://blog.csdn.net/qq123386926/article/details/47757089<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNTQ4OTAzNQ==">https://zhuanlan.zhihu.com/p/35489035<i class="fa fa-external-link-alt"></i></span></li>
</ul>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>vfio</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux workqueue分析</title>
    <url>/Linux-workqueue%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol>
<li>workqueue基本使用方法</li>
</ol>
<hr>
<p> workqueue机制可以异步执行内核其他模块的任务。内核其他模块有任务需要异步执行的<br> 时候，可以调用workqueue提供的接口，把任务丢给一个对应的内核线程去执行。这里<br> 提到的任务定义是一个函数。</p>
<p> 这里我们提到的workqueue是内核最新的版本，它的正规名字叫做Concurrency Managed<br> Workqueue。代码路劲在：linux/kernel/workqueue.c, linux/include/linux/workqueue.h</p>
<p> workqueue相关的基本概念有: work, workqueue, pool workqueue, worker pool, worker,<br> per-cpu worker pool, unbound worker pool, per-cpu workqueue, unbound workqueue.</p>
<p> work, workqueue是workqueue对其他模块的接口。使用者要先创建需要执行的任务, 即work。<br> 然后调用workqueue API把这个任务放入相关的workqueue里执行。</p>
<p> 整个workqueue又分per-cpu workqueue和unbound workqueue。per-cpu workqueue是系统<br> 启动的时候就分配好的(to do: 需要分析)。unbound workqueue需要用户自己创建。</p>
<p> 如果用户使用per-cpu workqueue，只需要调用schedule_work(struct work_struct *work)<br> 即可。这个API会把work放到当前cpu的normal kworker pool中的一个worker上跑。<br> 使用queue_work_on(cpu, system_highpri_wq, work)可以把一个work放到指定cpu 高优先<br> 级pool上跑，使用queue_work_on(cpu, system_wq, work)可以把work放到指定cpu normal<br> kworker pool上跑。</p>
<p> 如果用户使用unbound workqueue, 需要先使用<br> alloc_workqueue(const char *fmt, unsigned int flags, int max_active, …)申请<br> workqueue, 其中flags中要使用WQ_UNBOUND。随后使用<br> queue_work(struct workqueue_struct *wq, struct work_strct *work)把work放到wq<br> 里执行，也可以用queue_work_on(int cpu, struct workqueue_struct *wq, struct work_strct *work)，<br> 但是，上面的这个函数执行的时候并不能精确的把work放在cpu上执行。这两个函数的<br> 效果都是，把work放到当前cpu对应的numa node上的一个cpu跑。</p>
<p> 可以看到，虽然workqueue的用户接口是work定义和work queue，但是最新workqueue的设计<br> 把前端的用户接口和后端的执行线程解耦开来，而且进一步加入了work pool即线程池的<br> 概念，把具体的执行线程即worker的创建和销毁变成了一个动态的过程。所以，我们的讨论<br> 要先基于线程池来。对于per-cpu workqueue, 在workqueue初始化的过程中，为每一个cpu<br> 都创建一个normal work pool和高优先级的work pool。对于unbound workqueue使用的<br> unbound work pool，系统建立一个全局的hash表保存所有不同种类的unbound work pool,<br> 所谓的种类由nice、cpumask、no numa三个参数决定，如果发现需要的unbound work pool<br> 在系统里已经有了，那么直接使用已有的unbound work pool, 一个work可以显示的放在<br> 一个unbound workqueue上跑，但是真正调度到哪个unbound work pool、还是新建立一个<br> unbound work pool, 这个需要运行时才能决定。</p>
<p> 如上，现在workqueue的设计分了前端和后端, 前端是workqueue，后端是kworker pool，<br> worker。中间靠一个pool workqueue的东西连接。这里kworker pool就是一个线程池，<br> worker就是一个个线程，pool workqueue的pool是一个动作，这个动作把前端的workqueue<br> 和后端的一个线程池建立起联系。</p>
<p> 至此，我们还需要知道线程池里线程创建和销毁的机制。首先创建线程池的时候会默认<br> 创建一个线程，注意，这里不是在创建队列的时候。也就是说，对于一个unbound work<br> queue, 在复用系统已经的unbound线程池的时候(绝大多数是复用已有的, 从调试结果看,<br> 系统一开始就会给每个numa node上创建一个normal unbound work pool和一个高优先级<br> unbound work pool), 是完全可能不新建线程的。当线程池里的线程没有被使用的时候，<br> 会自动进入idle状态，idle装态的线程在一定时间后会被销毁。线程池里至少要保持有一个<br> idle线程在，即使超时也不会被销毁。所以，当一个线程池里只有一个idle线程，这时我们<br> 又queue work到这个线程池时，workqueue的代码会首先wake up这个idle线程，这个idle<br> 线程起来后首先要做的就是看看有没有idle线程，如果没有，就要创建一个线程出来，可以<br> 看到idle线程被wake up后，这个线程池里已经没有idle线程，所以这里就会在线程池里<br> 创建新线程。当线程池里的线程被阻塞，这时又有新的work被调度的到这个线程池里时，<br> 也会创建新的线程(to do: 分析code)。处于运行状态的线程会把work queue里缓存的任务<br> 都执行一遍。当一个任务需要执行的时候，如果work pool里有运行状态的线程时，workqueue<br> 代码不会wake up idle线程。</p>
<p> 当频繁把work放到一个unbound work pool上时，会有新worker创建被创建出来。但是<br> 当频繁的把work放到per-cpu work pool上的时候，任务只会在一个相同的worker上执行。<br> (to do: 分析code)</p>
<p> 以上的数据结构大致是这样的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+----------+</span><br><span class="line">|unbound wq+-+- kworker pool(node0)-+- worker0 (u&lt;work pool id&gt;:&lt;worker num&gt;)</span><br><span class="line">+----------+ |                      +- worker1</span><br><span class="line">             |                      `- ...</span><br><span class="line">             |</span><br><span class="line">             `- kworker pool(node1)-+- worker0</span><br><span class="line">                                    +- worker1</span><br><span class="line">                                    `- ...</span><br><span class="line">+------------+</span><br><span class="line">|numa node 0 |</span><br><span class="line">|            |  .-- kworker pool    -+- worker0 (&lt;cpu num&gt;:&lt;worker num&gt;)</span><br><span class="line">|   cpu 0 ---+--+-- kworker pool[H]  +- worker1 </span><br><span class="line">|            |  .-- kworker pool     `- ...</span><br><span class="line">|   cpu 1 ---+--+-- kworker pool[H] -+- worker0 (&lt;cpu num&gt;:&lt;worker num&gt;H)</span><br><span class="line">+------------+                       +- worker1</span><br><span class="line">                                     `- ...</span><br><span class="line">+------------+</span><br><span class="line">|numa node 1 |</span><br><span class="line">|            |</span><br><span class="line">|   cpu 2    |      ...</span><br><span class="line">|            |</span><br><span class="line">|   cpu 3    |</span><br><span class="line">+------------+</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>workqueue代码的基本结构</li>
</ol>
<hr>
<p> 详见简介中的连接, 这四篇文章已经讲的很好。</p>
<ol start="3">
<li>workqeueu API再挖掘</li>
</ol>
<hr>
<p> 如上，workqueue的work不会直接和worker绑定，而是在queue_work的时候选择一个<br> kworker pool。</p>
<p> 对于unbound workqueue, alloc_workqueue中会判断系统里是否有相同的kworker pool,<br> 如果有，就用已有的kworker pool。判断相同的依据是nice，cpumask，no numa。对于<br> per-cpu workqueue，直接选择当前cpu上的kworker pool。</p>
<p> 创建一个新的kworker pool肯定至少要创建一个内核线程。所以，如果alloc_workqueue<br> 复用旧的kworker pool可能会用之前kworker pool里的线程，如果alloc_workqueue里创建<br> 新的kworker pool，那么这个步骤就会新建一个内核线程。</p>
<p> kworker pool的worker是动态调整的，pool里的线程都处于阻塞状态的时候，pool就会<br> 新起一个线程执行work。下面的测试中，我们给一个unbound wq里发work，每个work里<br> sleep一段时间，可以看到，这个unbound wq是复用的系统已经有的kworker pool，在用完<br> 该kworker pool里本来已有的线程后，该worker pool会起新的线程执行work, 过一会<br> 可以发现，该worker pool里的线程有一部分被回收。(case 3)</p>
<p> 注意, 在申请workqueue的时候加上WQ_SYSFS参数，可以把该workqueue的信息通过sysfs<br> 暴露到用户态：e.g. /sys/bus/workqueue/devices/writeback</p>
<ol start="4">
<li>测试</li>
</ol>
<hr>
<p> 如下附录中的测试代码, 以下是测试得到的结论:</p>
<p> case 1: queue_work_on, 对unbound work queue操作时只能把work放在对应numa node上<br>         的cpu，无法具体到cpu。</p>
<p> case 2: schedule_work可以把work放到当前的cpu上。</p>
<p> case 3: 当kwork pool里的线程都处于block状态的时候，如果有work需要执行，对应<br>         的pool就会新分配内核线程。(这个对于per-cpu和unbound work queue都是一样的)</p>
<p> case 4: schedule_work会把work放到当前cpu上跑，block的时候，pool会新分配线程。</p>
<p> case 5: schedule_work_on会只把work放到指定cpu上，block的时候，pool会新分配线程。</p>
<p> case 6: 使用unboud pool, 频繁queue_work_on, 即使没有block，pool里也会创建新<br>         线程。</p>
<p> case 7: 使用queue_work_on(cpu, system_highpri_wq, work)可以把一个work放到指定<br>         cpu上的高优先级pool上跑。如果有block, pool里会分配新线程，如果没有block，<br>     即使频繁调用queue_work_on pool也不会创建新线程。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p> wq_test.c测试代码：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL2t3cS93cV90ZXN0LmM=">https://github.com/wangzhou/tests/blob/master/kwq/wq_test.c<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux中断学习笔记1</title>
    <url>/Linux%E4%B8%AD%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<h2 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h2><p>  cpu在正常执行一条条指令的时候，可以由于某些原因跳到一些异常处理程序，执行一些<br>  代码，然后再返回原来的程序运行。按照触发异常的源头，异常包括：</p>
<ol>
<li><p>中断，异步的由外部器件发起，cpu响应后，保存现场，然后跳到特定位置执行，然后返回<br>中断处继续执行下面的程序</p>
</li>
<li><p>同步中断，由系统中一些错误的指令引起（比如除零，非法指令等），是和cpu同步的。<br>软中断（实现系统调用的地方）就是人为的通过指令来产生一个异常，然后cpu切换工作模式，<br>对应的进入内核空间开始执行代码</p>
</li>
</ol>
<p>  中断系统的整个硬件包括：cpu,中断控制器，外设。在arm体系中这里的cpu指的是cpu核，<br>  中断控制器一般是标准的gic,通常gic已经被Soc厂商集成在Soc中了, gic的输出接cpu核<br>  上的irq和frq引脚, gic的输入接Soc内各ip模块的中断产生引脚, ip模块对应的中断线<br>  （gic上的输入引脚）在Soc内已经定死。不清楚gic的输入引脚可否直接连Soc的io管脚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Soc pin |  in Soc:</span><br><span class="line">   \--&gt;      -|--ip模块中断控制----&gt;gic中断控制----&gt;cpu中断控制  </span><br><span class="line">              | </span><br></pre></td></tr></table></figure>
<p>  一个中断从发起到cpu接收到的流程可以用上面的图来表示,需要配置各个中断控制中的<br>  相关寄存器，使得中断在物理上被cpu接收到</p>
<h2 id="中断实现"><a href="#中断实现" class="headerlink" title="中断实现 "></a>中断实现 </h2><p>  linux内核和中断相关的核心数据结构有：struct irq_desc (linux/irqdesc.h), 每个<br>  中断号对应着一个这样的结构，所有的irq_desc以数组或是树的形式存在</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  struct irq_desc</span><br><span class="line">      |--&gt; struct irq_data irq_data -----&gt; |-&gt; mask,irq,hwirq,state_use_accessors</span><br><span class="line">      |                                                           |-&gt; struct irq_chip *chip</span><br><span class="line">      |                                                           |-&gt; struct irq_domain *domain </span><br><span class="line">      |                                                           |-&gt; void *handler_data, *chip_data...</span><br><span class="line">      |--&gt; irq_flow_handler_t handle_irq   </span><br><span class="line">      |--&gt; struct irqaction *action ----&gt;|-&gt; irq_handler_t handler, thread_fn</span><br><span class="line">      |--&gt; raw_spinlock_t lock           |-&gt; void *dev_id, int irq, flags</span><br><span class="line">      |--&gt; struct proc_dir_entry *dir    |-&gt; unsigned long thread_flags, thread_mask</span><br><span class="line">      |--&gt; const char *name...           |-&gt; const char *name</span><br><span class="line">      |-&gt; struct proc_dir_entry *dir...</span><br></pre></td></tr></table></figure>
<p>  对着上面的数据结构分析中断流程:(arm体系)</p>
<p>  中断被cpu接收到之后，汇编程序处理后调用的第一个程序是：<br>  asm_do_IRQ(unsigned int irq, struct pt_regs *regs)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        --&gt;handle_IRQ(irq, regs);</span><br><span class="line">             --&gt;struct pt_regs *old_regs = set_irq_regs(regs);</span><br><span class="line">             irq_enter();</span><br><span class="line">             generic_handle_irq(irq);</span><br><span class="line">                  --&gt;generic_handle_irq_desc(irq, desc);</span><br><span class="line">                             --&gt;desc-&gt;handle_irq(irq, desc);</span><br><span class="line">     irq_exit();</span><br><span class="line">     set_irq_regs(old_regs);</span><br></pre></td></tr></table></figure>

<p>  最后调用的 handle_irq() 是注册在对应irq_desc中的的中断流函数, 处理中断嵌套等问题<br>  一般电平中断用标准的: handle_level_irq() (kernel/irq/chip.c)<br>      边沿中断用标准的: handle_edge_irq(), 上面的chip.c还有另外几种中断流函数,接着<br>  中断流函数向下分析:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  handle_edge_irq(unsigned int irq, struct irq_desc *desc)</span><br><span class="line">      --&gt;raw_spin_lock(&amp;desc-&gt;lock);</span><br><span class="line">      ...</span><br><span class="line">     desc-&gt;irq_data.chip-&gt;irq_ack(&amp;desc-&gt;irq_data); </span><br><span class="line">     ...</span><br><span class="line">    handle_irq_event(desc);</span><br><span class="line">     --&gt;struct irqaction *action = desc-&gt;action;</span><br><span class="line">     raw_spin_unlock(&amp;desc-&gt;lock);</span><br><span class="line">     handle_irq_event_percpu(desc, action);</span><br><span class="line">     --&gt;...</span><br><span class="line">            action-&gt;handler(irq, action-&gt;dev_id);</span><br><span class="line">          ...</span><br><span class="line">     raw_spin_lock(&amp;desc-&gt;lock);</span><br><span class="line"> raw_spin_unlock(&amp;desc-&gt;lock);</span><br></pre></td></tr></table></figure>
<p>  <br>  最后调用的 handle()就是request_irq中注册的中断处理函数, 对应irq_desc中的action <br>  中的handler。上面的中断流函数中调用了irq_ack(), irq_ack()是注册在irq_chip中的<br>  函数, 最上面的数据结构显示irq_chip在irq_desc中的irq_data中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  struct irq_chip &#123;</span><br><span class="line">    ...</span><br><span class="line">void (*irq_enable)(struct irq_data *data);</span><br><span class="line">void (*irq_ack)(struct irq_data *data);</span><br><span class="line">void (*irq_mask)(struct irq_data *data);</span><br><span class="line">void (*irq_unmask)(struct irq_data *data);</span><br><span class="line">void (*irq_eoi)(struct irq_data *data);</span><br><span class="line">int (*irq_set_type)(struct irq_data *data, unsigned int flow_type);</span><br><span class="line">    ...</span><br><span class="line">unsigned long flags;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>  irq_chip包括和硬件相关的一组回调函数，他们直接操作中断相关的寄存器, 比如irq_enble<br>  使能中断线，irq_mask屏蔽中断线</p>
<h2 id="中断使用"><a href="#中断使用" class="headerlink" title="中断使用"></a>中断使用</h2><p>  在驱动程序中只需要调用request_irq()注册中断处理程序即可使用中断，上面的中断实现<br>  为我们做了很多工作。注册中断和释放中断：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,</span><br><span class="line">const char *name, void *dev)</span><br><span class="line">    request_irq有可能会睡眠，里面会调用kmalloc()分配内存</span><br><span class="line">    free_irq(unsigned int irq, void *dev_id)</span><br></pre></td></tr></table></figure>

<p>  中断处理函数: irqreturn_t handler(int irq, void *dev_id) 无需要可重入，一个中<br>  断在执行的时候，相同的中断在其他的所有的处理器上的中断线都会被屏蔽?</p>
<p>  request_irq()中使用中断号irq注册中断处理函数, 一个中断号也可以注册多个处理函数。<br>  中断发生了，cpu就跳去执行中断处理函数，一个外设也可以发出几个不同的中断, 这时<br>  可以在外设的驱动程序中注册多个中断处理函数</p>
<p>  当外设的中断引脚和gic的输入直接相连时,直接注册中断处理函数就可以使用中断。这是<br>  因为gic的驱动程序(drivers/irqchip/irq-gic.c)已经实现了开头的数据结构和回调函数,<br>  其中包括：gic对应的irq_chip中的回调函数(这些回调函数操作gic的输入，可以mask,<br>  unmask, enable等等?), gic对应的中断流函数(gic_handle_irq()?)</p>
<p>  若是Soc内部的ip存在中断控制，比如gpio, 它一端接N个输入管脚，每个输入管脚可以接<br>  收中断，它的中断输出接一个gic的输入，也就是说gpio的N个管脚上的中断输入，都反应<br>  在gic的一个输入上。gpio中也有一组寄存器可以控制中断(enable, mask, unmask gpio<br>  输入管脚上的中断)。这时我们要自己为这些中断建立开头的那写数据结构和回调函数</p>
<h2 id="中断流函数"><a href="#中断流函数" class="headerlink" title="中断流函数"></a>中断流函数</h2><p>  以一个gpio控制器的驱动程序为例说明，linux内核中各个厂商的gpio驱动在/drivers/gpio<br>  内, 以gpio-mvebu.c来分析</p>
<p>  irq = platform_get_irq(pdev, i) 取出gpio对应的中断号，这个中断号是系统一开是就<br>  定好的，是gic给gpio分配的中断号，这一个中断号可以对应gpio的多个输入。当gpio<br>  引脚配置成中断引脚时，任何一个引脚上产生的中断都通过这个中断号上报给gic</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">irq_set_handler_data(irq, mvchip)</span><br><span class="line">   --&gt;desc-&gt;irq_data.handler_data = data;</span><br><span class="line"></span><br><span class="line">irq_set_chained_handler(irq, mvebu_gpio_irq_handler)</span><br><span class="line">   --&gt;__irq_set_handler(irq, handle, 1, NULL); // 1 表示：is_chained</span><br><span class="line">   desc-&gt;handled_irq = handle;</span><br><span class="line">         </span><br><span class="line">mvchip-&gt;irqbase = irq_alloc_descs(-1, 0, ngpios, -1)</span><br><span class="line">__irq_alloc_descs(irq, from, cnt, node, THIS_MODULE)</span><br><span class="line">   --&gt;start=bitmap_find_next_zero_area(allocated_irqs,IRQ_BITMAP_BITS,from,cnt,0);</span><br><span class="line">            bitmap_set(allocated_irqs, start, cnt);</span><br><span class="line">   alloc_descs(start, cnt, node, owner);</span><br><span class="line">    以上分配了ngpios个连续的中断号，最后返回的是第一个中断号</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gc = irq_alloc_generic_chip(&quot;mvebu_gpio_irq&quot;, 2, mvchip-&gt;irqbase,</span><br><span class="line">   mvchip-&gt;membase, handle_level_irq); </span><br><span class="line">   --&gt;struct irq_chip_generic *gc;</span><br><span class="line">   unsigned long sz = sizeof(*gc) + num_ct * sizeof(struct irq_chip_type);</span><br><span class="line">   gc = kzalloc(sz, GFP_KERNEL);</span><br><span class="line">            irq_init_generic_chip(gc, name, num_ct, irq_base, reg_base, handler);</span><br><span class="line">        --&gt; raw_spin_lock_init(&amp;gc-&gt;lock);</span><br><span class="line">                gc-&gt;num_ct = num_ct;</span><br><span class="line">                gc-&gt;irq_base = irq_base;</span><br><span class="line"></span><br><span class="line">                gc-&gt;reg_base = reg_base;</span><br><span class="line"></span><br><span class="line">                gc-&gt;chip_types-&gt;chip.name = name;</span><br><span class="line"></span><br><span class="line">                gc-&gt;chip_types-&gt;handler = handler;</span><br></pre></td></tr></table></figure>
<p>  以上动态分配了一个irq_chip_generic结构, 然后用之前分配的连续中断号中的第一个<br>  中断号填充irq_base, 并且填充中断流函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    stuct irq_chip_generic</span><br><span class="line">    |--&gt; irq_base</span><br><span class="line">    |--&gt; u32 mask_cache, tpye_cache</span><br><span class="line">    |--&gt; void *private</span><br><span class="line">    |...</span><br><span class="line">    |--&gt; struct irq_chip_type chip_types[0]---&gt;|--&gt; struct irq_chip</span><br><span class="line">    |                                                                         |--&gt; irq_flow_handler_t handler</span><br><span class="line">                                                                              |--&gt; type</span><br><span class="line">                                                                              |...</span><br><span class="line">gc-&gt;private = mvchip; // gc: *irq_chip_generic </span><br><span class="line">ct = &amp;gc-&gt;chip_types[0]; // ct: *irq_chip_type</span><br><span class="line">ct-&gt;type = IRQ_TYPE_LEVEL_HIGH | IRQ_TYPE_LEVEL_LOW;</span><br><span class="line">ct-&gt;chip.irq_mask = mvebu_gpio_level_irq_mask;</span><br><span class="line">ct-&gt;chip.irq_unmask = mvebu_gpio_level_irq_unmask;</span><br><span class="line">ct-&gt;chip.irq_set_type = mvebu_gpio_irq_set_type;</span><br><span class="line">ct-&gt;chip.name = mvchip-&gt;chip.label;</span><br></pre></td></tr></table></figure>
<p>  以上向irq_chip中注册了相应的回调函数，这些回调函数处理gpio中断相关，如mask,<br>  ack, unmask gpio中断线。现在irq_desc结构也分配好了, irq_chip中的回调函数也注册<br>  好了, 需要做的是向irq_desc中注册相应的irq_chip</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">irq_setup_generic_chip(gc, IRQ_MSK(ngpios), 0,</span><br><span class="line">      IRQ_NOREQUEST, IRQ_LEVEL | IRQ_NOPROBE);</span><br><span class="line">        --&gt;list_add_tail(&amp;gc-&gt;list, &amp;gc_list);</span><br><span class="line">irq_set_chip_and_handler(i, chip, ct-&gt;handler); // for loop to do this</span><br><span class="line">irq_set_chip_data(i, gc);</span><br></pre></td></tr></table></figure>
<p>  以上先把上面动态生成的irq_chip_generic结构加入gc_list链表，然后向之前申请的<br>  irq_desc注册各自的irq_chip和中断流函数</p>
<p>  mvchip-&gt;domain = irq_domain_add_simple(); <br>  添加一个irq_domain, 作用暂时不清楚</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux信号笔记</title>
    <url>/Linux%E4%BF%A1%E5%8F%B7%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>Linux里有进程，线程，基于此有进程组，线程组的概念。Linux内核把进程，线程做一样<br>的实现，都用一个task_struct表示, 这里我们统一用linux内核的概念来看待以上的概念。<br>内核include/linux/pid.h中对PID的种类有如下的定义:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">enum pid_type</span><br><span class="line">&#123;</span><br><span class="line">	PIDTYPE_PID,</span><br><span class="line">	PIDTYPE_TGID,</span><br><span class="line">	PIDTYPE_PGID,</span><br><span class="line">	PIDTYPE_SID,</span><br><span class="line">	PIDTYPE_MAX,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这里PID指的是一个内核线程ID，对应到用户态可以是一个进程或是一个线程。TGID指线程组<br>ID一个用户态进程和他创建的所有线程组成一个线程组。PGID是指进程组ID，这个要调用linux<br>系统函数创建，把多个用户态进程组成一个进程组。SID是指会话ID。</p>
<p>信号可以从一个用户态进程发到一个用户态进程，也可以从一个用户态线程发到一个用户态<br>线程。当然信号还可以从内核发到用户态，这里就涉及到上面PID的类型，通过指定不同的<br>PID类型, 内核可以把信号发到单个线程(进程)、线程组、进程组等。</p>
<p>信号是一种进程的系统资源, 而且传递时携带的信息很少。这样的性质决定，使用信号最好<br>由整个系统的顶层设计规划，不然如果底层设计中使用了信号, 很容易和其他的软件部件<br>相互冲突。因为信号传递的信息很少，必然要再加入其他的逻辑才可以完成整个业务逻辑，<br>这就会带来系统的复杂度。信号执行是打断原有进程(线程)执行流的，编写信号处理函数<br>要使用可重入函数，而且为了防止死锁，信号处理函数里不能使用锁，这些限制都使得信号<br>处理函数的编写很容易出错。</p>
<p>可以把信号处理从使用信号处理函数转变到使用线程，在线程中等待信号到来，然后处理。<br>这样可以把之前异步的处理放在线程里处理，避免上面说的信号处理函数里不能加锁的限制，<br>(fix me: 是否可以使用可重入函数)。像这里介绍的那样:<br><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWJtLmNvbS9kZXZlbG9wZXJ3b3Jrcy9jbi9saW51eC9sLWNuLXNpZ25hbHNlYy9pbmRleC5odG1s">https://www.ibm.com/developerworks/cn/linux/l-cn-signalsec/index.html<i class="fa fa-external-link-alt"></i></span>.<br>但是这样处理在架构上也是有代价的，他要求你的业务线程和信号处理的线程在一个线程<br>组里，这样才可以在信号处理线程里即使的做处理。还有一个要求是，需要在这个线程组<br>主线程里就设置屏蔽信号处理线程要处理的信号，这样在随后的新线程里才可以继承这个<br>信号屏蔽，然后单独在信号处理线程里不去屏蔽这个信号。可以看到这种方案适用于自己<br>构建的方案，每个部分是自己可以控制的。但是，当你的方案要嵌入到更高一层的方案里<br>时, 用一个线程单独处理信号会带来非常多的麻烦。</p>
<p>内核驱动可以使用fasync发信号给一个fd绑定的进程。具体可以参考<a href="https://wangzhou.github.io/Linux%E5%BC%82%E6%AD%A5%E9%80%9A%E7%9F%A5/">这里</a>。<br>以上内核向用户态进程发信号需要用户态先执行fd和进程绑定的fcntl操作。实际上内核<br>可以直接类似send_sig_info的函数给一个PID发信号。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>信号</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内存相关的测试工具</title>
    <url>/Linux%E5%86%85%E5%AD%98%E7%9B%B8%E5%85%B3%E7%9A%84%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<ol>
<li>smem</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  PID User     Command                         Swap      USS      PSS      RSS </span><br><span class="line">40200 test     bash                               0   524.0K   687.0K     3.1M </span><br><span class="line">39757 test     bash                               0   524.0K   692.0K     3.2M </span><br><span class="line">40203 test     /usr/bin/python /usr/bin/sm        0     6.1M     6.2M     7.7M </span><br></pre></td></tr></table></figure>
<p>运行smem -k命令可以看到系统里进程占用内存的情况。如上各个SS的含义是:</p>
<p> USS: unique set size. 表示的是进程独占的物理内存，不包含动态库占的内存。<br> PSS: proportional set size. 进程自己的物理内存 + 动态库折算到进程里的内存。<br> RSS: resident set size. 进程实际使用的物理内存，包括共享库内存。<br> VSS: virtual set size. 进程使用的所有虚拟内存，包括共享库虚拟内存。</p>
<ol start="2">
<li>/proc/buddyinfo</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Node 1, zone      DMA     10     12     11     11     13     11     10     10      6      6    226 </span><br><span class="line">Node 1, zone    DMA32      1      0      1      1      4      2      4      3      4      4    176 </span><br><span class="line">Node 1, zone   Normal  15807  52091  12723    877    258    703    579    194     35     34   5546 </span><br><span class="line">Node 3, zone   Normal  54266  41801  23819  10038   3476    641     65     29     13     19   6714 </span><br></pre></td></tr></table></figure>
<p>表示系统里伙伴系统中，2^1, 2^2…连续页有多少个，这个可以看到伙伴系统内存碎片的<br>情况。</p>
<ol start="3">
<li>/sys/kernel/mm/*</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── hugepages</span><br><span class="line">├── ksm</span><br><span class="line">├── swap</span><br><span class="line">└── transparent_hugepage</span><br></pre></td></tr></table></figure>
<p>可以看到如上各个特性的一些统计值。</p>
<ol start="4">
<li>pidof + pmap + /proc/pid/maps /proc/pid/smaps /proc/pid/stack</li>
</ol>
<hr>
<p>./a.out &amp;<br>pmap -x <code>pidof a.out</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">41677:   ./a.out</span><br><span class="line">Address           Kbytes     RSS   Dirty Mode  Mapping</span><br><span class="line">0000000000400000       4       4       0 r-x-- a.out</span><br><span class="line">0000000000400000       0       0       0 r-x-- a.out</span><br><span class="line">0000000000410000       4       4       4 r---- a.out</span><br><span class="line">0000000000410000       0       0       0 r---- a.out</span><br><span class="line">0000000000411000       4       4       4 rw--- a.out</span><br><span class="line">0000000000411000       0       0       0 rw--- a.out</span><br><span class="line">0000ffff8dac8000    1204     480       0 r-x-- libc-2.23.so</span><br><span class="line">0000ffff8dac8000       0       0       0 r-x-- libc-2.23.so</span><br><span class="line">0000ffff8dbf5000      60       0       0 ----- libc-2.23.so</span><br><span class="line">0000ffff8dbf5000       0       0       0 ----- libc-2.23.so</span><br><span class="line">0000ffff8dc04000      16      16      16 r---- libc-2.23.so</span><br><span class="line">0000ffff8dc04000       0       0       0 r---- libc-2.23.so</span><br><span class="line">0000ffff8dc08000       8       8       8 rw--- libc-2.23.so</span><br><span class="line">0000ffff8dc08000       0       0       0 rw--- libc-2.23.so</span><br><span class="line">0000ffff8dc0a000      16       8       8 rw---   [ anon ]</span><br><span class="line">0000ffff8dc0a000       0       0       0 rw---   [ anon ]</span><br><span class="line">0000ffff8dc0e000     112     112       0 r-x-- ld-2.23.so</span><br><span class="line">0000ffff8dc0e000       0       0       0 r-x-- ld-2.23.so</span><br><span class="line">0000ffff8dc2b000       8       8       8 rw---   [ anon ]</span><br><span class="line">0000ffff8dc2b000       0       0       0 rw---   [ anon ]</span><br><span class="line">0000ffff8dc37000       8       0       0 r----   [ anon ]</span><br><span class="line">0000ffff8dc37000       0       0       0 r----   [ anon ]</span><br><span class="line">0000ffff8dc39000       4       4       0 r-x--   [ anon ]</span><br><span class="line">0000ffff8dc39000       0       0       0 r-x--   [ anon ]</span><br><span class="line">0000ffff8dc3a000       4       4       4 r---- ld-2.23.so</span><br><span class="line">0000ffff8dc3a000       0       0       0 r---- ld-2.23.so</span><br><span class="line">0000ffff8dc3b000       8       8       8 rw--- ld-2.23.so</span><br><span class="line">0000ffff8dc3b000       0       0       0 rw--- ld-2.23.so</span><br><span class="line">0000ffffc48f9000     132      12      12 rw---   [ stack ]</span><br><span class="line">0000ffffc48f9000       0       0       0 rw---   [ stack ]</span><br><span class="line">---------------- ------- ------- ------- </span><br><span class="line">total kB            1592     672      72</span><br></pre></td></tr></table></figure>

<p>/proc/pid/smaps反应一个进程里的各个vma里的信息。提供的信息比/proc/pid/maps多</p>
<ol start="5">
<li>numastat -p <pid></pid></li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Per-node process memory usage (in MBs) for PID 417 (a.out)</span><br><span class="line">                           Node 0          Node 1           Total</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         0.01            0.00            0.01</span><br><span class="line">Stack                        0.00            0.00            0.01</span><br><span class="line">Private                      0.17            0.40            0.57</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                        0.18            0.41            0.59</span><br></pre></td></tr></table></figure>
<p>可以看到一个进程在各个numa节点上的内存使用情况，可以用这个命令观察一段时间进程<br>的内存在各个numa之间使用的情况。</p>
<ol start="6">
<li>mallopt</li>
</ol>
<hr>
<p>C库中设定内存分配配置的接口, 通过mallopt这个函数可以改变malloc/free的行为。</p>
<ol start="7">
<li>mbind/madvise</li>
</ol>
<hr>
<p>mbind是对应系统调用的封装，可以把虚拟内存对应的物理内存和对应的numa节点绑定。<br>madvise也是对应系统调用的封装，可以从用户态传递一些内存相关的属性给内核。</p>
<ol start="8">
<li>getrusage</li>
</ol>
<hr>
<p>getrusage是一个库函数，可以得到一个进程的相关资源的使用情况。其中包括内存使用<br>的统计。比如其中，ru_minflt就是系统里初次分配内存这种缺页的次数，而ru_majflt<br>是从swap分区里换入内存这样缺页的数目。</p>
<ol start="9">
<li>proc下的关于内存的信息</li>
</ol>
<hr>
<p>/proc/slabinfo<br>/proc/iomem<br>/proc/meminfo<br>/proc/vmstat<br>/proc/zoneinfo<br>/proc/vmallocinfo<br>/proc/sys/vm/*</p>
]]></content>
      <tags>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核crypto子系统学习笔记</title>
    <url>/Linux%E5%86%85%E6%A0%B8crypto%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>Analysis will start from crypto test cases in crypto/testmgr.c, e.g. deflate.<br>上面的路径上是内核里这对crypto子系统的一个测试程序。通过分析这个程序可以大概<br>看出crypto子系统向外提供的API. 整个系统的情况大概是这样的：</p>
<p>crypto API &lt;—&gt; crypto core &lt;—&gt; crypto_register_alg</p>
<p>设备驱动通过crypto_register_alg把一个设备支持的算法注册到crypto系统里。<br>注册的时候会通过struct crypto_alg把相关的信息传递给crypto core.</p>
<p>struct crypto_alg的结构是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct crypto_alg &#123;</span><br><span class="line">	struct list_head cra_list;</span><br><span class="line">	struct list_head cra_users;</span><br><span class="line"></span><br><span class="line">	u32 cra_flags;</span><br><span class="line">	unsigned int cra_blocksize;</span><br><span class="line">	unsigned int cra_ctxsize;</span><br><span class="line">	unsigned int cra_alignmask;</span><br><span class="line"></span><br><span class="line">	int cra_priority;</span><br><span class="line">	atomic_t cra_refcnt;</span><br><span class="line"></span><br><span class="line">	char cra_name[CRYPTO_MAX_ALG_NAME];</span><br><span class="line">	char cra_driver_name[CRYPTO_MAX_ALG_NAME];</span><br><span class="line"></span><br><span class="line">	const struct crypto_type *cra_type;</span><br><span class="line"></span><br><span class="line">	union &#123;</span><br><span class="line">		struct ablkcipher_alg ablkcipher;</span><br><span class="line">		struct blkcipher_alg blkcipher;</span><br><span class="line">		struct cipher_alg cipher;</span><br><span class="line">		struct compress_alg compress;</span><br><span class="line">	&#125; cra_u;</span><br><span class="line"></span><br><span class="line">	int (*cra_init)(struct crypto_tfm *tfm);</span><br><span class="line">	void (*cra_exit)(struct crypto_tfm *tfm);</span><br><span class="line">	void (*cra_destroy)(struct crypto_alg *alg);</span><br><span class="line">	</span><br><span class="line">	struct module *cra_module;</span><br><span class="line">&#125; CRYPTO_MINALIGN_ATTR;</span><br></pre></td></tr></table></figure>
<p>这个结构的几个关键的信息是: cra_ctxsize, cra_u(下面以compress_alg说明), cra_init,<br>                cra_exit.<br>这个结构表述的是算法相关的系统，但是在执行一个请求的时候，还有维护一组上下文的信息，<br>这些信息记录在结构体: struct crypto_tfm.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct crypto_tfm &#123;</span><br><span class="line"></span><br><span class="line">	u32 crt_flags;</span><br><span class="line">	</span><br><span class="line">	union &#123;</span><br><span class="line">		struct ablkcipher_tfm ablkcipher;</span><br><span class="line">		struct blkcipher_tfm blkcipher;</span><br><span class="line">		struct cipher_tfm cipher;</span><br><span class="line">		struct compress_tfm compress;</span><br><span class="line">                    --&gt; cot_compress</span><br><span class="line">                    --&gt; cot_decompress</span><br><span class="line">	&#125; crt_u;</span><br><span class="line"></span><br><span class="line">	void (*exit)(struct crypto_tfm *tfm);</span><br><span class="line">	</span><br><span class="line">	struct crypto_alg *__crt_alg;</span><br><span class="line"></span><br><span class="line">	void *__crt_ctx[] CRYPTO_MINALIGN_ATTR;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>其中最后一个__crt_ctx是这个上下文的私有数据。上面的cra_ctxsize就是这个私有数据的<br>size. cra_init是准备上下文的函数，比如，你用一个硬件设备压缩数据，实际的物理操作<br>发生在这个硬件的一个队列上，那么就需要准备这个队列，准备必要的缓存等等。cra_exit<br>是退出上下文。cra_u里是具体执行算法的函数，比如可以压缩和解压缩的函数。</p>
<p>从设备驱动的角度讲, 设备驱动只是看到了crypto_alg这个结构。这个结构里的crypt_tfm<br>即一个操作执行的上下问是从哪里知道的呢？毕竟crypto_alg这个结构里的.cra_init,<br>.cra_exit, .cra_u里的.coa_compress和.coa_decompress都需要这个执行上下文。<br>我们在下面具体看一下。</p>
<p>知道这些内部的数据结构对我们理解外部的API有帮助。现在假设crypto的设备驱动已经有了，<br>那么，其他的内核模块怎么用呢？ 其实一开头我们已经讲到crypto/testmgr.c测试程序。</p>
<p>测试的代码里有异步的测试和同步的测试流程，我们这里先看同步的测试:</p>
<p>主要的逻辑就三个函数, 第一先需要分配一个压缩的上下文(本文用压缩的例子), 其实它<br>就是crypto_tfm的包装，和cryto_tfm是一样的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct crypto_comp &#123;</span><br><span class="line">	struct crypto_tfm base;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>struct crypto_comp = crypto_alloc_comp(driver, type, mask), 这个过程中会调用到<br>cra_init函数，这个函数是设备驱动实现的，完成硬件相关的配置，上面已经提到过。<br>调用关系如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* 分配一个压缩解压缩的上下文, 可以看到这里的压缩解压缩的上下文完全就是crypto_tfm */</span><br><span class="line">struct crypto_comp = crypto_alloc_comp(driver, type, mask);</span><br><span class="line">    --&gt; crypto_alloc_base(alg_name, type, mask)</span><br><span class="line">            /* find algrithm: use alg_name, driver name */</span><br><span class="line">        --&gt; alg = crypto_alg_mod_lookup(alg_name, type, mask);</span><br><span class="line">            /* 上下文是依据具体的算法去分配的 */</span><br><span class="line">        --&gt; tfm = __crypto_alloc_tfm(alg, type, mask);</span><br><span class="line">	        /* 上下文中指定相关的算法 */</span><br><span class="line">            --&gt; tfm-&gt;__crt_alg = alg;</span><br><span class="line">            --&gt; crypto_init_ops</span><br><span class="line">	            /* 把相应的算法中的压缩解压缩函数传递给上下文 */</span><br><span class="line">                --&gt; crypto_init_compress_ops(tfm)</span><br><span class="line">                        /* ops is struct compress_tfm */</span><br><span class="line">	            --&gt; ops-&gt;cot_compress = crypto_compress;</span><br><span class="line">                            /* tfm-&gt;__crt_alg-&gt;cra_u.compress.coa_compress */ </span><br><span class="line">                            /*</span><br><span class="line">                             * e.g. drivers/crypto/cavium/zip/zip_main.c</span><br><span class="line">                             *      struct crypto_alg zip_comp_deflate.</span><br><span class="line">                             * will finally call zip_comp_compress!</span><br><span class="line">                             */</span><br><span class="line">                        --&gt; tfm-&gt;__crt_alg-&gt;cra_compress.coa_compress</span><br><span class="line"></span><br><span class="line">	            --&gt; ops-&gt;cot_decompress = crypto_decompress;</span><br><span class="line">		/*</span><br><span class="line">                 * 在创建上下文的最后调用下，算法里的初始化函数，如果是和一个硬件</span><br><span class="line">		 * 的驱动适配，那么这里就可以执行相应硬件初始化的内容。</span><br><span class="line">		 */</span><br><span class="line">            --&gt; if (!tfm-&gt;exit &amp;&amp; alg-&gt;cra_init &amp;&amp; (err = alg-&gt;cra_init(tfm)))</span><br></pre></td></tr></table></figure>

<p>第二，就是执行压缩的操作:<br>crypto_comp_compress(tfm, input, ilen, result, &amp;dlen)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_comp_compress(crypto_comp, src, slen, dst, dlen)</span><br><span class="line">        /* so hardware can do compress here! */</span><br><span class="line">    --&gt; compress_tfm-&gt;cot_compress;</span><br></pre></td></tr></table></figure>

<p>第三，就是释放这个压缩的上下文<br>crypto_free_comp(comp)</p>
<p>内核虽然现在提供了压缩的异步接口，但是貌似还没有驱动会用到。异步接口的使用要比同步<br>接口复杂一点。下面具体看看。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">In alg_test_comp, async branch:</span><br><span class="line">/* 和同步一样，这里也创建一个异步的上下文 */</span><br><span class="line">acomp = crypto_alloc_acomp(driver, type, mask);</span><br><span class="line">/*</span><br><span class="line"> * 不过和同步接口不一样的是，这里又创建一个acomp_req的上下文, 后续的操作都围绕</span><br><span class="line"> * 着这个req结构展开。可以看到req里面包含了异步接口需要的回调函数。</span><br><span class="line"> */</span><br><span class="line">req = acomp_request_alloc(tfm);</span><br><span class="line">acomp_request_set_params(req, &amp;src, &amp;dst, ilen, dlen);</span><br><span class="line">acomp_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG,</span><br><span class="line">			   crypto_req_done, &amp;wait);</span><br><span class="line">crypto_acomp_compress(req)</span><br></pre></td></tr></table></figure>
<p>这里需要说明的是，testmsg.c里的这个acomp的测试程序里加了wait/complete的相关<br>内容。这里应该是为了测试方便而加的，一般的异步接口里, 当硬件完成操作的时候，在<br>中断函数里直接调用异步接口的回调函数就可以了。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>crypto</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核DMA子系统分析</title>
    <url>/Linux%E5%86%85%E6%A0%B8DMA%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="DMA-engine使用"><a href="#DMA-engine使用" class="headerlink" title="DMA engine使用"></a>DMA engine使用</h2><p> DMA子系统下有一个帮助测试的测试驱动(drivers/dma/dmatest.c), 从这个测试驱动入手<br> 我们了解到内核里的其他部分怎么使用DMA engine。配置内核，选则CONFIG_DMATEST可以<br> 把这个模块选中，编译会生成dmatest.ko。可以参考这个文档来快速了解怎么使用dmatest.ko:<br> <span class="exturl" data-url="aHR0cHM6Ly93d3cua2VybmVsLm9yZy9kb2MvaHRtbC92NC4xNS9kcml2ZXItYXBpL2RtYWVuZ2luZS9kbWF0ZXN0Lmh0bWw=">https://www.kernel.org/doc/html/v4.15/driver-api/dmaengine/dmatest.html<i class="fa fa-external-link-alt"></i></span>.</p>
<p> 具体上来讲，内核的其他模块使用dma engine的步骤是:</p>
<ul>
<li><p>使用dma_request_channel先申请一个dma channel，之后的dma请求都基于这个申请<br>的dma channel。</p>
</li>
<li><p>调用dma_dev-&gt;device_prep_dma_memcpy(chan, dst, src, len, flag)把dma操作的参<br>数传给dma子系统。同时返回一个从chan申请的异步传输描述符: struct dma_async_tx_descriptor.</p>
<p>可以把用户的回调函数设置在上面的描述符里。通常这里的回调函数里是一个complete<br>函数，用来在传输完成后通知用户业务流程里的wait等待。</p>
</li>
<li><p>tx-&gt;tx_submit(tx) 把请求提交。</p>
</li>
<li><p>dma_submit_error 判断提交的请求是否合法。</p>
</li>
<li><p>dma_async_issue_pending 触发请求真正执行。</p>
<p>如上，在发送请求之后，一般可以在这里wait等待，通过上面注册的回调函数在dma<br>执行完成后通知这里的wait。</p>
</li>
<li><p>dma_async_is_tx_complete 查看请求的状态。</p>
</li>
<li><p>做完dma操作之后使用dma_release_channel释放申请的dma channel。</p>
</li>
</ul>
<h2 id="DMA子系统分析"><a href="#DMA子系统分析" class="headerlink" title="DMA子系统分析"></a>DMA子系统分析</h2><p> 分析一个现有的dmaengine驱动可以看到，dmaengine驱动需要使用dmaenginem_async_device_register<br> 向dma子系统注册驱动自己的struct dma_device结构。在注册之前，设备驱动要先填充<br> 这个结构里的一些域段。cap_mask是设备驱动支持的特性，还有dma子系统需要的各种<br> 回调函数。</p>
<p> DMA子系统用一个全局链表记录系统里的dma engine设备。对于dma engine设备上的各个<br> channel，DMA子系统为每个channel创建一个struct device设备，这个设备的class是dma_dev<br> class, DMA子系统把创建的device用device_register向系统注册，这样在用户态sysfs<br> 的/sys/class/dma下面就会出现dma<xx>chan<xx>的dma channel描述文件。每个dma<xx>chan<xx><br> 下有对应的属性描述文件。</xx></xx></xx></xx></p>
<p> DMA子系统还对外提供一套第一节中所描述的API。</p>
<h2 id="DMA-engine驱动分析"><a href="#DMA-engine驱动分析" class="headerlink" title="DMA engine驱动分析"></a>DMA engine驱动分析</h2><p> 可以看到，DMA系统在dma engine注册的时候需要设备驱动提供的一套回调函数来支持<br> 第一小节里的各个API，这些回调函数操作具体硬件，完成相关硬件的配置。我们这里可以<br> 以MEMCPY要提供的回调函数示例说明回调函数的意义。</p>
<ul>
<li><p>device_alloc_chan_resources</p>
<p>分配chan的硬件资源</p>
</li>
<li><p>device_free_chan_resources</p>
<p>释放chan的硬件资源</p>
</li>
<li><p>device_prep_dma_memcpy</p>
<p>接收用户传入的请求，分配驱动层面的用户请求</p>
</li>
<li><p>device_issue_pending</p>
<p>操作硬件发起具体的dma请求</p>
</li>
</ul>
<p> 分析现有的dma驱动，可以看到里面用了virt-dma.[ch]里提供的接口。这里也简单看下<br> virt-dma的使用方法。virt-dma的核心数据结构是一组链表，这组链表记录处于不同阶段<br> 的dma请求。当用 e.g. device_prep_dma_memcpy创建一个请求后，这个请求应该挂入<br> desc_allocated链表，当用tx-&gt;tx_submit提交这个请求后，应该把请求挂入desc_submitted<br> 链表，当用dma_async_issue_pending执行请求后，应该把请求挂入desc_issued链表，<br> 当最后请求执行完成后，应该挂入desc_completed链表。virt-dma在原来的dma_chan上<br> 封装了virt_dma_chan，在virt_dma_chan创建的时候, vchan_init为每一个vchan创建<br> 一个tasklet，设备驱动可以在中断处理里调用 e.g. vchan_cookie_complete-&gt;tasklet_schedule<br> 执行tasklet函数vchan_complete, 这个函数里会执行dma请求中用户设置的回调函数。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>DMA</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核模块编译</title>
    <url>/Linux%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97%E7%BC%96%E8%AF%91/</url>
    <content><![CDATA[<h2 id="采用源码树外编译的方式"><a href="#采用源码树外编译的方式" class="headerlink" title="采用源码树外编译的方式"></a>采用源码树外编译的方式</h2><ol>
<li><p>编写模块代码：hello.c[1]</p>
</li>
<li><p>在同一目录下编写Makefile[2]</p>
</li>
<li><p>在同一目录下编译：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">make ARCH=arm CROSS_COMPILE=arm-linux-gnueabi- -C /vm/wangzhou/linux-iks  M=`pwd` modules</span><br></pre></td></tr></table></figure>
<p>ARCH：平台, CROSS_COMPILE：交叉编译器, -C：kernel源码路径, M=：module代码的路径,<br>modules：指明编译模块。这里编译arm平台下的module</p>
</li>
<li><p>在hello.c同一目录下将会生成hello.ko</p>
</li>
</ol>
<h2 id="在kernel的源码树中编译"><a href="#在kernel的源码树中编译" class="headerlink" title="在kernel的源码树中编译"></a>在kernel的源码树中编译</h2><ol>
<li><p>把写好的模块代码放入源码树的一个目录：linux-src/drivers/char/hello.c</p>
</li>
<li><p>在…/char/下的Makefile中加入：obj-m += hello.o</p>
<p>可以看到Makefile中的项目多是: obj-$(CONFIG_VIRTIO_CONSOLE)+= virtio_console.o</p>
</li>
<li><p>在linux-src/下：</p>
<p>make ARCH=arm CROSS_COMPILE=arm-linux-gnueabi- zImage -j 8 modules<br>内核中所有配置成模块的程序将得到编译, 本例子上在linux-src/drivers/char/下将会<br>看到编译好的hello.ko</p>
</li>
</ol>
<h2 id="模块由多个文件组成，放在一个目录里编译"><a href="#模块由多个文件组成，放在一个目录里编译" class="headerlink" title="模块由多个文件组成，放在一个目录里编译"></a>模块由多个文件组成，放在一个目录里编译</h2><ol>
<li><p>把文件夹放在源码树的一个目录下：</p>
<p>linux-src/drivers/char/hello/hello.c  …hello/add.c[3]<br>为了测试的目的, 需要在hello.c中加入调用add()的语句…</p>
</li>
<li><p>在…/char/的Makefile文件中加：obj-m += hello/ 表示该module在hello目录下</p>
</li>
<li><p>在…/hello/下创建Makefile文件[4]</p>
</li>
<li><p>同上面第三步, 在hello/下会生成hello_add.ko模块</p>
</li>
</ol>
<h2 id="利用Kconfig图形化配置kernel、module"><a href="#利用Kconfig图形化配置kernel、module" class="headerlink" title="利用Kconfig图形化配置kernel、module: "></a>利用Kconfig图形化配置kernel、module: </h2><p> 用make menuconfig可以图形化配置kernel。可以使用下面的方式, 把自己写的module加入<br> 配置菜单：</p>
<ol>
<li><p>把…/char/下的Makefile中之前加的行修改成：<br>obj-${CONFIG_HELLO_ADD} += hello/ 表示在最后在.config文件中显示的配置项是CONFIG_HELLO_ADD</p>
</li>
<li><p>把…/char/hello/下的Makefile修改成：<br>obj-${CONFIG_HELLO_ADD} += hello_add.o<br>hello_add-objs := hello.o add.o</p>
<p>可以看到, 编译时会根据CONFIG_HELLO_ADD的值去决定编译成什么, 下面要说的Kconfig<br>即用来设置CONFIG_HELLO_ADDde值</p>
</li>
<li><p>在…/char/下的Kconfig文件中加入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">config HELLO_ADD</span><br><span class="line">	tristate &quot;HELLO_ADD MODUEL(try the Kconfig)&quot;</span><br><span class="line">	default n</span><br><span class="line">	help</span><br><span class="line">	  something about help information of hello_add module</span><br></pre></td></tr></table></figure>
<p>tristate表示可以配置成n, y, m三种状态(不编译，直接编译入kernel，编译成module),<br>后面的字符串最后会显示到菜单上, default 指明默认的情况, config 指明配置的变量,<br>即CONFIG_HELLO_ADD</p>
</li>
<li><p>make menuconfig 依照下面路径, 将会看到HELLO_ADD模块的配置菜单</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Device Drivers ---&gt;</span><br><span class="line">          Character devices ---&gt;</span><br><span class="line">          ...</span><br><span class="line">          &lt;&gt; HELLO_ADD MODULE(try the Kconfig)</span><br><span class="line">          ...</span><br></pre></td></tr></table></figure></li>
<li><p>同(二)中第三步，在对应目录下即可得到hello_add.ko</p>
</li>
</ol>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>[1] 编写模块代码：hello.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* hello.c */</span><br><span class="line">#include &lt;linux/init.h&gt;</span><br><span class="line">#include &lt;linux/module.h&gt;</span><br><span class="line">MODULE_LICENSE(&quot;Dual BSD/GPL&quot;);</span><br><span class="line"></span><br><span class="line">static int hello_init(void)</span><br><span class="line">&#123;</span><br><span class="line">	printk(KERN_ALERT &quot;Hello, begin to test ...\n&quot;);</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static void hello_exit(void)</span><br><span class="line">&#123;</span><br><span class="line">	printk(KERN_ALERT&quot;Goodbye, cruel world\n&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module_init(hello_init);</span><br><span class="line">module_exit(hello_exit);</span><br><span class="line"></span><br><span class="line">MODULE_AUTHOR(&quot;***&quot;);</span><br><span class="line">MODULE_DESCRIPTION(&quot;A simple test Module&quot;);</span><br><span class="line">MODULE_ALIAS(&quot;a simplest module&quot;);</span><br></pre></td></tr></table></figure>

<p>[2] Makefile文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    obj-m := hello.o</span><br><span class="line">    clean:</span><br><span class="line">        rm  hello.ko hello.mod.c hello.mod.o hello.o modules.order Module.symvers</span><br></pre></td></tr></table></figure>

<p>[3] 编写文件: add.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    /* add.c */</span><br><span class="line">    int add(int a, int b)</span><br><span class="line">    &#123;</span><br><span class="line">return (a+b);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>[4] Makefile文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    obj-m := hello_add.o  /* 指明module的名字 */</span><br><span class="line">    hello_add-objs := add.o hello.o /* 指明hello_add是由几个文件链接到一起的*/</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核链表笔记</title>
    <url>/Linux%E5%86%85%E6%A0%B8%E9%93%BE%E8%A1%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>linux kernel中的链表实现在/linux/include/linux/list.h中。我们可以在一个c程序中<br>很快的实现一个链表，那么kernel中链表有什么独特之处呢？一般的我们这样定义一个链<br>表的节点：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct list_node_a &#123;</span><br><span class="line">	data_a; </span><br><span class="line">	struct list_node *next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样写的不足是，链表上的数据和链表本身紧密耦合在了一起。现在链表中每个节点存放的<br>是data_a, 如果又有一个data_b需要串成一个链表，那么就需要再建立一个struct list_note_b<br>这样的链表节点。简而言之，就是这样的通用性不强。</p>
<p>linux kernel中的实现是，首先实现一个</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct list_head &#123;</span><br><span class="line">	struct list_head *prev;</span><br><span class="line">	struct list_head *next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>的基础结构,这个是链表最核心的结构。然后再加上一些链表操作的接口函数。OK, 通用的<br>链表实现就完成了。</p>
<p>但是我要怎么使用呢？现在有一个结构 struct data 需要用一个链表管理起来(用一个链表<br>把一堆struct data串起来)。就在struct data的定义中嵌入一个struct list_head结构，<br>看起来是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct data &#123;</span><br><span class="line">	...;	</span><br><span class="line">	struct list_head node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>除此之外，一般还会定义一个struct list_head作为这个链表的表头节点。表头节点是一个<br>struct list_head结构, 代表整个链表，他不嵌入struct data中。最后组成的链表如下图<br>所示(其中struct A就是struct data)。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                   +--------------+        +--------------+        +--------------+    </span><br><span class="line">                   |___struct A___|        |___struct A___|        |___struct A___|</span><br><span class="line">                   | ...          |        | ...          |        | ...          |</span><br><span class="line">+----------+       | +----------+ |        | +----------+ |        | +----------+ |</span><br><span class="line">|list node_|       | |list node_| |        | |list node_| |        | |list node_| |</span><br><span class="line">|  prev    |&lt;--------|  prev    |&lt;-----------|  prev    |&lt;-----------|  prev    |&lt;------\</span><br><span class="line">|  next    |--------&gt;|  next    |-----------&gt;|  next    |-----------&gt;|  next    |-----\ |</span><br><span class="line">+----------+       | +----------+ |        | +----------+ |        | +----------+ |   | |</span><br><span class="line">   |  /|\          +--------------+        +--------------+        +--------------+   | |</span><br><span class="line">   |   \                                                                              | |</span><br><span class="line">   \   -------------------------------------------------------------------------------/ /</span><br><span class="line">    ------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<p>但是一个个的struct data节点是怎么被加入链表中的呢? 我们可去/linux/include/linux/list.h<br>找到答案。假设我们有空的链表 sample_list:<br>    LIST_HEAD(sample_list);<br>这里我们初始化了一个链表的表头节点sample_list, 他代表整个链表。在整个链表中的位置<br>就相当于上图中的最左面哪个list node, 只不过现在他的prev和next指针都是指向自己。</p>
<p>现在我们有一个struct data sample_data结构要加入sample_list链表, 可以调用list.h中的函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	list_add(struct list_head *new, struct list_head *head);</span><br><span class="line">实现：</span><br><span class="line">	list_add(&amp;sample_data.node, &amp;sample_list);</span><br></pre></td></tr></table></figure>
<p>可以想像知道struct data sample_data就知道了他里面的struct list_head node的指针，<br>又知道链表头节点的指针，把一个节点加入链表中是非常容易的。具体可以分析list.h中<br>的代码。</p>
<p>现在假设我们知道了链表表头节点的指针，我要遍历链表，读取其中的数据。其中关键的就是<br>知道struct data sample_data中的struct list_head node的指针p，怎么得到对应的<br>struct data sample_data的指针。没有关系，我们可以使用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">container_of(p, struct data, node);</span><br></pre></td></tr></table></figure>
<p>返回对应struct data sample_data的指针。</p>
<p>基本内容到这里就完了，当你发现一个结构中嵌入了一个struct list_head结构时，他一定<br>将要被连到某个链表中。当然也可能在一个结构中嵌入了多个不同的struct list_head结构，<br>那一定是这个结构同时被连入了多个链表中。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核scatterlist用法</title>
    <url>/Linux%E5%86%85%E6%A0%B8scatterlist%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<h2 id="为什么有scatterlist"><a href="#为什么有scatterlist" class="headerlink" title="为什么有scatterlist"></a>为什么有scatterlist</h2><p>   这里说的scatterlist，说的是用聚散表管理的内存，这是一个一般的概念。这个概念主要<br>   是配合硬件产生的。从物理上讲，我们可以分配一段连续的内存使用，但是，我们也<br>   可以分配很多段不连续的内存，然后用一个软件数据结构把这些数据结构串联起来使用。</p>
<p>   那在什么时候会用呢？CPU看到的内存可以通过MMU做映射，一段连续的虚拟内存可以被<br>   映射到不连续的物理内存上, 显然这里是不需要scatterlist这个概念支持的。当有了<br>   IOMMU这个概念的时候，设备看到的地址可以是一个连续的虚拟地址，这块地址通过IOMMU<br>   可以映射到不连续的物理地址上，这里也不需要scatterlist这个概念。可以注意到上面<br>   说的设备的DMA必须是支持连续地址的。</p>
<p>   但是有的时候，有的设备的DMA还支持scatterlist这种数据结构提供的内存模型。简单<br>   来讲，就是你可以配置这个设备，使得它的DMA可以一次DMA接入一串不连续的物理地址。</p>
<p>   这个时候内核的scatterlist数据结构就出现了。</p>
<h2 id="硬件结构"><a href="#硬件结构" class="headerlink" title="硬件结构"></a>硬件结构</h2><p>   如上所述，就是硬件寄存器可以配置一组离散的内存，设备启动DMA可以接入这组内存。</p>
<h2 id="API使用"><a href="#API使用" class="headerlink" title="API使用"></a>API使用</h2><p>   在使用硬件的这个特性的时候，需要首先分配好各个内存区域，当然，这个时候还只知道<br>   各个内存区域的cpu地址(cpu虚拟地址)</p>
<p>   然后把各个内存区域的信息填到scatterlist里，组成相应的内核数据结构。</p>
<p>   把scatterlist这个结构用dma_map_sg处理下，得到每段内存区域对应的dma地址(总线地址),<br>   注意设备发起DMA操作的时候用的就是这个地址。</p>
<p>   使用各个内存区域的dma地址, 配置具体的硬件。</p>
<h2 id="内核支持"><a href="#内核支持" class="headerlink" title="内核支持"></a>内核支持</h2><p>   …</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>scatterlist</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核镜像头分析</title>
    <url>/Linux%E5%86%85%E6%A0%B8%E9%95%9C%E5%83%8F%E5%A4%B4%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="Linux内核镜像头部格式"><a href="#Linux内核镜像头部格式" class="headerlink" title="Linux内核镜像头部格式"></a>Linux内核镜像头部格式</h2><p> Linux内核镜像的格式和启动它的bootloader有一定的关系。如果bootloader只是简单的跳到<br> 内核Image的第一条指令开始执行，那么内核Image直接从开始放指令就可以，如果bootloader<br> 对内核Image有一定的要求，一般是Image的头部符合一定的格式，那么内核Image就必须按照<br> 相关的格式排布，这意味着这样的bootloader可以解析内核Image的格式，内核的入口地址也<br> 不一定是内核Image的首地址。</p>
<p> Linux和windows上程序的格式是不一样的，Linux是ELF格式、windows上是PE/COFF格式，这里<br> 说的是应用程序，我们看下Linux内核的Image。编译Linux内核的log可以看见，编译内核时<br> 先生成vmlinux，然后用objcopy生成Image，vmlinux是ELF格式，bootloader需要理解ELF<br> 才能引导Linux内核，所以还要把ELF转变成bootloader可以理解的二进制格式。</p>
<p> 如上，如果bootloader直接从第一条指令执行，直接把内核的入口地址放在Image的开始就<br> 可以，现在可以看到opensbi就是这样的简单bootloader。对于有些复杂的bootloader，比如<br> uboot或者UEFI，它们支持的启动方式更多，有的启动方式对被启动的Image的格式就有约束。</p>
<p> UEFI中使用的EFI启动方式，uboot也支持这种启动方式。这种启动方式一开始在UEFI/windows<br> 中使用，Image head要求是PE/COFF格式的，Linux内核为了支持EFI启动，Image head也要<br> 是PE/COFF格式。</p>
<p> PE/COFF格式的head大概是这样的:<br> <img src="/Linux%E5%86%85%E6%A0%B8%E9%95%9C%E5%83%8F%E5%A4%B4%E5%88%86%E6%9E%90/PE_COFF_head.png" alt="PE/COFF head"></p>
<p> 其中，开头两个字节的二进制编码必须是0x5A4D，对应的ascii字符就是‘MZ’，0x3C偏移处<br> 是真实PE头在Image中的偏移，PE/COFF的格式为了兼容DOS的格式，在真实PE头前加了如上<br> 的DOS head、DOS STUB。AddressOfEntryPoint处放Image的入口信息，内容是入口地址相对<br> Image起始的偏移。</p>
<p> 下面具体看下riscv内核Image开始部分的代码。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#ifdef CONFIG_EFI</span><br><span class="line">        /*</span><br><span class="line">         * This instruction decodes to &quot;MZ&quot; ASCII required by UEFI.</span><br><span class="line">         */</span><br><span class="line">        c.li s4,-13</span><br><span class="line">        j _start_kernel</span><br><span class="line">#else</span><br><span class="line">        /* jump to start kernel */</span><br><span class="line">        j _start_kernel</span><br><span class="line">        /* reserved */</span><br><span class="line">        .word 0</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure>
<p> 编译好的riscv的Image要么支持EFI启动要么不支持。支持EFI启动的Image一开始放了一条<br> 指令，这条指令的二进制编码就是0x5A4D，后面在DOS head的空间直接放了一条跳转指令，<br> 这样EFI的内核Image也兼容了类似opensbi这种直接从Image第一条指令启动的bootloader，<br> c.li s4,-13这样的指令不会有什么副作用，第二条指令就直接跳到_start_kernel了，如果<br> bootloader按照EFI启动，会从PE头里的AddressOfEntryPoint域段得到入口地址，根本不会<br> 去执行j _start_kernel指令。</p>
<p> 对于EFI的Image，直接在如下的地方把这个PE头塞进来:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#ifdef CONFIG_EFI</span><br><span class="line">	.word pe_head_start - _start</span><br><span class="line">pe_head_start:</span><br><span class="line"></span><br><span class="line">	__EFI_PE_HEADER</span><br><span class="line">#else</span><br><span class="line">	.word 0</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure>
<p> 可以看到宏__EFI_PE_HEADER把PE头加了进来，AddressOfEntryPoint放的是__efistub_efi_pe_entry<br> 的偏移，所以EFI内核的代码执行路径和非EFI内核在这里还是不同的。</p>
<p> riscv上的__efistub_efi_pe_entry是定义在drivers/firmware/efi/libstub/efi-stub-entry.c里<br> 的efi_pe_entry这个函数，在编译的时候给这个函数加上了__efistub_这个前缀：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/firmware/efi/libstub/Makefile */</span><br><span class="line">STUBCOPY_FLAGS-$(CONFIG_RISCV)  += --prefix-alloc-sections=.init \</span><br><span class="line">                                   --prefix-symbols=__efistub_</span><br></pre></td></tr></table></figure>

<h2 id="opensbi启动内核"><a href="#opensbi启动内核" class="headerlink" title="opensbi启动内核"></a>opensbi启动内核</h2><p> 如上，opensbi只是简单跳到被启动对象的起始地址。</p>
<h2 id="uboot启动内核"><a href="#uboot启动内核" class="headerlink" title="uboot启动内核"></a>uboot启动内核</h2><p> Linux内核已经支持EFI方式启动，当初的内核patch的链接是<span class="exturl" data-url="aHR0cHM6Ly9sd24ubmV0L0FydGljbGVzLzgxMzU4OS8=">https://lwn.net/Articles/813589/<i class="fa fa-external-link-alt"></i></span><br> 这个patch是使用uboot + qemu来测试EFI启动的。</p>
<p> 用这个命令可以启动uboot:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-riscv64 -m 256m -machine virt -nographic -kernel path_to_u-boot/u-boot</span><br></pre></td></tr></table></figure>
<p> 启动的效果大概是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OpenSBI v1.1</span><br><span class="line">   ____                    _____ ____ _____</span><br><span class="line">  / __ \                  / ____|  _ \_   _|</span><br><span class="line"> | |  | |_ __   ___ _ __ | (___ | |_) || |</span><br><span class="line"> | |  | | &#x27;_ \ / _ \ &#x27;_ \ \___ \|  _ &lt; | |</span><br><span class="line"> | |__| | |_) |  __/ | | |____) | |_) || |_</span><br><span class="line">  \____/| .__/ \___|_| |_|_____/|____/_____|</span><br><span class="line">        | |</span><br><span class="line">        |_|</span><br><span class="line"></span><br><span class="line">Platform Name             : riscv-virtio,qemu</span><br><span class="line">Platform Features         : medeleg</span><br><span class="line">Platform HART Count       : 1</span><br><span class="line">Platform IPI Device       : aclint-mswi</span><br><span class="line">Platform Timer Device     : aclint-mtimer @ 10000000Hz</span><br><span class="line">Platform Console Device   : uart8250</span><br><span class="line">Platform HSM Device       : ---</span><br><span class="line">Platform Reboot Device    : sifive_test</span><br><span class="line">Platform Shutdown Device  : sifive_test</span><br><span class="line">Firmware Base             : 0x80000000</span><br><span class="line">Firmware Size             : 288 KB</span><br><span class="line">Runtime SBI Version       : 1.0</span><br><span class="line"></span><br><span class="line">Domain0 Name              : root</span><br><span class="line">Domain0 Boot HART         : 0</span><br><span class="line">Domain0 HARTs             : 0*</span><br><span class="line">Domain0 Region00          : 0x0000000002000000-0x000000000200ffff (I)</span><br><span class="line">Domain0 Region01          : 0x0000000080000000-0x000000008007ffff ()</span><br><span class="line">Domain0 Region02          : 0x0000000000000000-0xffffffffffffffff (R,W,X)</span><br><span class="line">Domain0 Next Address      : 0x0000000080200000</span><br><span class="line">Domain0 Next Arg1         : 0x000000008fe00000</span><br><span class="line">Domain0 Next Mode         : S-mode</span><br><span class="line">Domain0 SysReset          : yes</span><br><span class="line"></span><br><span class="line">Boot HART ID              : 0</span><br><span class="line">Boot HART Domain          : root</span><br><span class="line">Boot HART Priv Version    : v1.12</span><br><span class="line">Boot HART Base ISA        : rv64imafdch</span><br><span class="line">Boot HART ISA Extensions  : time,sstc</span><br><span class="line">Boot HART PMP Count       : 16</span><br><span class="line">Boot HART PMP Granularity : 4</span><br><span class="line">Boot HART PMP Address Bits: 54</span><br><span class="line">Boot HART MHPM Count      : 16</span><br><span class="line">Boot HART MIDELEG         : 0x0000000000001666</span><br><span class="line">Boot HART MEDELEG         : 0x0000000000f0b509</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">U-Boot 2023.01-rc4-00059-g8d6cbf5e6b (Jan 05 2023 - 15:59:10 +0800)</span><br><span class="line"></span><br><span class="line">CPU:   rv64imafdch_zicsr_zifencei_zihintpause_zba_zbb_zbc_zbs_sstc</span><br><span class="line">Model: riscv-virtio,qemu</span><br><span class="line">DRAM:  256 MiB</span><br><span class="line">Core:  25 devices, 12 uclasses, devicetree: board</span><br><span class="line">Flash: 32 MiB</span><br><span class="line">Loading Environment from nowhere... OK</span><br><span class="line">In:    serial@10000000</span><br><span class="line">Out:   serial@10000000</span><br><span class="line">Err:   serial@10000000</span><br><span class="line">Net:   No ethernet found.</span><br><span class="line">Working FDT set to 8f734950</span><br><span class="line">Hit any key to stop autoboot:  0 </span><br><span class="line">=&gt; </span><br></pre></td></tr></table></figure>
<p> (todo: 怎么继续启动内核)</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>opensbi</tag>
        <tag>uboot</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux巨页的使用</title>
    <url>/Linux%E5%B7%A8%E9%A1%B5%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>普通巨页的内核说明文档在linux/Documentation/admin-guide/mm/hugetlbpage.rst</p>
<p>使用巨页需要在内核编译的时候打开相关的配置选项：CONFIG_HUGETLBFS, CONFIG_HUGETLB_PAGE</p>
<p>使能系统的巨页可以通过内核的启动参数或者是/sys/kernel/mm/hugepages/下的文件。<br>我们具体看下sysfs下的相关目录。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hugepages/</span><br><span class="line">├── hugepages-1048576kB</span><br><span class="line">│   ├── free_hugepages</span><br><span class="line">│   ├── nr_hugepages</span><br><span class="line">│   ├── nr_hugepages_mempolicy</span><br><span class="line">│   ├── nr_overcommit_hugepages</span><br><span class="line">│   ├── resv_hugepages</span><br><span class="line">│   └── surplus_hugepages</span><br><span class="line">├── hugepages-2048kB</span><br><span class="line">│   ├── free_hugepages</span><br><span class="line">│   ├── nr_hugepages</span><br><span class="line">│   ├── nr_hugepages_mempolicy</span><br><span class="line">│   ├── nr_overcommit_hugepages</span><br><span class="line">│   ├── resv_hugepages</span><br><span class="line">│   └── surplus_hugepages</span><br><span class="line">├── hugepages-32768kB</span><br><span class="line">│   ├── free_hugepages</span><br><span class="line">│   ├── nr_hugepages</span><br><span class="line">│   ├── nr_hugepages_mempolicy</span><br><span class="line">│   ├── nr_overcommit_hugepages</span><br><span class="line">│   ├── resv_hugepages</span><br><span class="line">│   └── surplus_hugepages</span><br><span class="line">└── hugepages-64kB</span><br><span class="line">    ├── free_hugepages</span><br><span class="line">    ├── nr_hugepages</span><br><span class="line">    ├── nr_hugepages_mempolicy</span><br><span class="line">    ── nr_overcommit_hugepages</span><br><span class="line">    ├── resv_hugepages</span><br><span class="line">    └── surplus_hugepages</span><br></pre></td></tr></table></figure>
<p>可以看到在hugepages目录下为每个巨页大小都开了专门的目录。可以看到在我的ARM64<br>系统上，巨页有64KB，32KB，2MB，1GB四种类型。向nr_hugepages写数值可以创建指定数目<br>的巨页，读free_hugepages可以得到还没有使用的巨页数目。</p>
<p>一般巨页属于系统配置，我们只去使用，不去更改配置。使用的方法是在mmap的时候在<br>flags参数中加上MAP_HUGETLB。如果只用MAP_HUGETLB，巨页的分配算法是内核里定的，<br>比如，你要mmap 128KB的内存，在64KB和2MB都可以满足的时候，我们希望从64KB里搞两<br>页出来就好了，内核可能是从2MB里分配的，实际上，用5.10-rc2的内核，真的是从2MB的<br>页里分一页出来。</p>
<p>所以，针对巨页，mmap flags里还有宏可以指定从哪种大小的页里分巨页。man mmap下有：<br>MAP_HUGE_2MB, MAP_HUGE_1GB，不过直接用这个宏会报没有定义，这个是因为gcc版本比较<br>低，我们可以直接找见内核里定义的地方: linux/include/uapi/linux/mman.h</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define MAP_HUGE_64KB	HUGETLB_FLAG_ENCODE_64KB</span><br><span class="line">#define MAP_HUGE_2MB	HUGETLB_FLAG_ENCODE_2MB</span><br><span class="line"></span><br><span class="line">#define HUGETLB_FLAG_ENCODE_64KB	(16 &lt;&lt; HUGETLB_FLAG_ENCODE_SHIFT)</span><br><span class="line">#define HUGETLB_FLAG_ENCODE_2MB		(21 &lt;&lt; HUGETLB_FLAG_ENCODE_SHIFT)</span><br><span class="line"></span><br><span class="line">#define HUGETLB_FLAG_ENCODE_SHIFT	26</span><br></pre></td></tr></table></figure>
<p>可以看到16, 21正好是页大小以2为底的对数。所以，举个例子，我们可以如下指定从<br>64KB的巨页中申请内存：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int len = 128 * 1024;</span><br><span class="line">unsigned long *p;</span><br><span class="line">int i;</span><br><span class="line"></span><br><span class="line">p = mmap(NULL, len, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS |</span><br><span class="line">	 MAP_HUGETLB | (16 &lt;&lt; 26), -1, 0);</span><br><span class="line"></span><br><span class="line">for (i = 0; i &lt; len / sizeof(*p); i++) &#123;</span><br><span class="line">	*(p + i) = 20;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>之后再去看/sys/kernel/mm/hugepages/hugepages-64kB/free_hugepages, 会发现减少了2。<br>需要注意的是，mmap到的内存要去写下，内存才会真实分配，如果在mmap之后就去看<br>free_hugepages的值，其中还是原来的值。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux异步通知</title>
    <url>/Linux%E5%BC%82%E6%AD%A5%E9%80%9A%E7%9F%A5/</url>
    <content><![CDATA[<p>Linux系统中有很多内核和用户态程序通知的机制，比如event fd, netlink和异步通知。<br>通过这些机制内核可以主动给用户态程序发送消息。本文讨论异步通知的基本用法。</p>
<p>利用异步通知机制可以实现从内核中向设备文件绑定的进程发送特定信号。把异步通知用<br>起来需要内核中设备驱动和用户态程序的配合。在这里有一个实例程序可以直接下载，<br>然后在虚拟机里运行: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL3RyZWUvbWFzdGVyL2Zhc3luY190ZXN0">https://github.com/wangzhou/tests/tree/master/fasync_test<i class="fa fa-external-link-alt"></i></span></p>
<p>如这个示例中的驱动代码，为了支持异步通知，我们需要实现设备文件操作中的.fasync<br>回调，实现的方式也很简单，就是调用下标准的fasync_helper函数向内核注册一个<br>fasync_struct, 其中最后一个参数是一个fasync_struct结构的二维指针，一般设备驱动里<br>应该定义一个特定file相关的fasync_struct的指针，用于保存内核分配的fasync_struct<br>的地址, 其实前面所谓注册一个fasync_struct, 就是请求内核分配一个fasync_struct的<br>结构，然后返回该结构的地址给设备驱动。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int test_fasync(int fd, struct file *file, int mode)</span><br><span class="line">&#123;</span><br><span class="line">	return fasync_helper(fd, file, mode, &amp;async_queue);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>当内核要发送信号给用户态进程的时候需要调用下kill_fasync函数, 该函数会向<br>fasync_struct对应的fd所绑定的进程发送一个SIGIO信号。这里的SIGIO也可以换成其他的<br>信号，但是一般用SIGIO信号发异步通知。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void test_trigger_sigio(struct timer_list *tm)</span><br><span class="line">&#123;</span><br><span class="line">	kill_fasync(&amp;async_queue, SIGIO, POLL_IN);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看到在测试程序里，我们为了方便测试，其实是起了一个内核定时器，在测试内核模块<br>加载10s后，向fd绑定的用户态进程发送一个SIGIO信号。(注意，这个测试程序是在主线<br>内核5.1上调试的，主线内核在4.15更新了内核定时器的API，这里用的是新内核定时器API)</p>
<p>在用户态程序中，需要设置设备fd，使其和当前进程绑定，还要使其可以接受fasync信号。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ret = fcntl(fd, F_SETOWN, getpid());</span><br><span class="line">if (ret == -1) &#123;</span><br><span class="line">	printf(&quot;u fasync: fail to bind process\n&quot;);</span><br><span class="line">	return -2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fcntl(fd, F_SETFL, fcntl(fd, F_GETFL) | FASYNC);</span><br><span class="line">if (ret == -1) &#123;</span><br><span class="line">	printf(&quot;u fasync: fail to set fasync\n&quot;);</span><br><span class="line">	return -3;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样设置后在内核调用kill_fasync，内核就知道把信号发给哪个进程。不同shell可能对<br>于信号设定有所不同，这里在测试之前先把SIGIO信号unmask，避免SIGIO被shell默认mask<br>掉，然后子进程继承父进程的信号设定，也mask住SIGIO的情况。</p>
<p>Linux信号的实现基本逻辑是在进程的signal pending表里标记其他进程或者内核给自己发<br>的信号，然后在进程从内核态切回用户态的时候再去扫描signal pending表以响应信号。<br>如果用户态进程一直没有系统调用，那么内核态发的SIGIO会不会得不到即使的响应? 另外<br>是否其他的内核活动也会引起测试进程发生内核态向用户态切换的过程，其中一个最可能<br>的情况就是内核周期性的调度。测试程序中做了一个简单的测试，用户态程序在设置好<br>fd后就进入死循环，而内核设备驱动在加载10s后会给用户态进程发一个SIGIO信号。测试<br>的结果是用户态进程可以在10s左右收到内核发的SIGIO信号。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux抢占概念</title>
    <url>/Linux%E6%8A%A2%E5%8D%A0%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h2 id="抢占的概念"><a href="#抢占的概念" class="headerlink" title="抢占的概念"></a>抢占的概念</h2><p> linux内核支持抢占, 可以通过抢占相关的编译宏打开或者禁止抢占。</p>
<p> 一个线程在一个cpu上运行, 不是这个线程主动让出cpu导致的其他线程跑到这个cpu上<br> 的情况都是抢占。关抢占就是禁止这样的行为发生。</p>
<p> 线程主动让出cpu的行为有，比如一个线程发送了一个I/O任务，然后调用<br> wait_for_completion睡眠等待completion。其他的各种行为我们都认为是内核抢占行为，<br> 比如，周期性的tick中断中调度器把另外一个线程调度到cpu上跑；各种中断程序里，<br> 触发了调度把另一个线程调度到cpu上跑。</p>
<p> 所以，内核里一段程序之前关了内核抢占之后又把内核抢占打开，如果这块代码中没有<br> 主动让出cpu的行为，我们可以认为这段代码在一个cpu上会持续的执行完，中间不会被<br> 打断。但是，这里说的是单个cpu上的情况, 当这段代码里有访问全局的数据结构的时候，<br> 对于那个全局的数据结构，还是可能和其他cpu上的程序并发访问的。</p>
<h2 id="开关抢占的实现"><a href="#开关抢占的实现" class="headerlink" title="开关抢占的实现"></a>开关抢占的实现</h2><p> 可以看到禁止抢占这个宏就是给task_struct结构里的preempt count加引用计数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define preempt_disable() \</span><br><span class="line">do &#123; \</span><br><span class="line">	preempt_count_inc(); \</span><br><span class="line">	barrier(); \</span><br><span class="line">&#125; while (0)</span><br></pre></td></tr></table></figure>

<p> 开启抢占是先减引用计数, 然后执行调度。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define preempt_enable() \</span><br><span class="line">do &#123; \</span><br><span class="line">	barrier(); \</span><br><span class="line">	if (unlikely(preempt_count_dec_and_test())) \</span><br><span class="line">		__preempt_schedule(); \</span><br><span class="line">&#125; while (0)</span><br></pre></td></tr></table></figure>

<p> 是否可以抢占的判断标准除了如上preempt的计数外，还有中断的情况。如果中断是关闭的<br> 同样认为是不能抢占的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define preemptible()	(preempt_count() == 0 &amp;&amp; !irqs_disabled())</span><br></pre></td></tr></table></figure>
<p> 但是，从上面可以看出来，禁止抢占并没有关中断，在禁止抢占的上下文里，中断依然可以<br> 进来，中断处理完后内核会进行调度，在禁止抢占时，内核依然调度被中断打断的线程运行，<br> 这样从总体上看之前的线程没有被其它线程抢占。</p>
<h2 id="关抢占的用途"><a href="#关抢占的用途" class="headerlink" title="关抢占的用途"></a>关抢占的用途</h2><p> 关抢占的目的就是叫一段代码不被内核调度打断下执行完，因为有的时候换一个程序进来<br> 跑可能把原来程序的数据破坏掉。比如内核zswap里，在每一个cpu上分配了压缩解压缩<br> 的tfm，然后用per-cpu变量保存相应的内容，他在使用这些per-cpu变量的时候就需要<br> 关闭内核抢占, 不然中途换另一个线程在cpu上跑就会有数据不一致的问题。</p>
<p> 可以看到在使用per-cpu变量的宏里，一进来就把抢占关了:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define get_cpu_ptr(var)						\</span><br><span class="line">(&#123;									\</span><br><span class="line">	preempt_disable();						\</span><br><span class="line">	this_cpu_ptr(var);						\</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux文件引用计数的逻辑</title>
    <url>/Linux%E6%96%87%E4%BB%B6%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0%E7%9A%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>考虑这样一个场景，打开一个字符设备文件/dev/A，得到一个fd，用户态可以对这个<br>fd做相关的文件操作，包括ioctl, mmap, close等，内核如果保证close操作和其他<br>操作的同步，即不会出现close和其他文件并发执行，其他文件访问已经close掉的文件<br>这种情况。</p>
<p>内核是靠打开文件的引用计数来保证这一点的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kernel/fs/open.c</span><br><span class="line">filp_open</span><br><span class="line">  +-&gt; file_open_name</span><br><span class="line">    +-&gt; do_filp_open</span><br><span class="line">      +-&gt; path_openat</span><br><span class="line">        +-&gt; alloc_empty_file</span><br><span class="line">	这里在创建struct file结构的时候会把里面的f_count引用计数设置为1。</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kernel/fs/ioctl.c</span><br><span class="line">ksys_ioctl系统调用</span><br><span class="line">  +-&gt; fdget</span><br><span class="line">    +-&gt; __fdget</span><br><span class="line">      在rcu锁里得到file结构的指针</span><br><span class="line">      +-&gt; __fget_light</span><br><span class="line">        +-&gt; __fget</span><br><span class="line">	  +-&gt; get_file_rcu_many (atomic_long_add_unless(&amp;(x)-&gt;f_count, xx, 0))</span><br><span class="line">          这里只有在f_count非0的时候才会把引用计数加1。如果是0，表明已经file</span><br><span class="line">	  的引用计数已经是0。__fget会去files里查fd对应的file。</span><br><span class="line">  +-&gt; fdput</span><br><span class="line">    +-&gt; fput</span><br><span class="line">      +-&gt; fput_many</span><br><span class="line">        +-&gt; atomic_long_dec_and_test(&amp;file-&gt;f_count)</span><br><span class="line">	如果减到0，在另一个内核线程中，延迟执行delay_work：</span><br><span class="line">	  +-&gt; delayed_fput_work</span><br><span class="line">	    +-&gt; delayed_fput</span><br><span class="line">	      +-&gt; __fput</span><br><span class="line">	        +-&gt; file_free(file)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kernel/fs/open.c</span><br><span class="line">close系统调用</span><br><span class="line">  +-&gt; __close_fd</span><br><span class="line">    +-&gt; spin_lock(&amp;files-&gt;file_lock)</span><br><span class="line">    在锁里拿到fd对应的file结构的指针</span><br><span class="line">      +-&gt; filp_close</span><br><span class="line">        +-&gt; fput     </span><br><span class="line">        如上</span><br><span class="line">    +-&gt; spin_unlock(&amp;files-&gt;file_lock)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux设备驱动中DMA接口的使用</title>
    <url>/Linux%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E4%B8%ADDMA%E6%8E%A5%E5%8F%A3%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="Linux设备驱动中DMA接口的使用"><a href="#Linux设备驱动中DMA接口的使用" class="headerlink" title="Linux设备驱动中DMA接口的使用"></a>Linux设备驱动中DMA接口的使用</h2><p>-v0.1 2018.3.13 Sherlock init Westford</p>
<ol start="0">
<li><p>DMA的概念</p>
<p>DMA就是说设备可以直接进行内存的读写，不需要CPU的参与。当然，在设备启动DMA<br>进行读写之前，你需要通过CPU把读写的地址，大小等一些信息配置给设备。设备完成<br>数据读写后可以发一个中断告诉CPU，之后CPU就可以做相关的操作。但是，CPU要把<br>什么地址告诉设备呢？</p>
</li>
</ol>
<ol>
<li><p>几个地址的概念</p>
<p>在kernel/Documentation/DMA-API-HOWTO.txt里讲的比较清楚，它里面有一副图是<br>这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">             CPU                  CPU                  Bus</span><br><span class="line">           Virtual              Physical             Address</span><br><span class="line">           Address              Address               Space</span><br><span class="line">            Space                Space</span><br><span class="line"></span><br><span class="line">          +-------+             +------+             +------+</span><br><span class="line">          |       |             |MMIO  |   Offset    |      |</span><br><span class="line">          |       |  Virtual    |Space |   applied   |      |</span><br><span class="line">        C +-------+ --------&gt; B +------+ ----------&gt; +------+ A</span><br><span class="line">          |       |  mapping    |      |   by host   |      |</span><br><span class="line">+-----+   |       |             |      |   bridge    |      |   +--------+</span><br><span class="line">|     |   |       |             +------+             |      |   |        |</span><br><span class="line">| CPU |   |       |             | RAM  |             |      |   | Device |</span><br><span class="line">|     |   |       |             |      |             |      |   |        |</span><br><span class="line">+-----+   +-------+             +------+             +------+   +--------+</span><br><span class="line">          |       |  Virtual    |Buffer|   Mapping   |      |</span><br><span class="line">        X +-------+ --------&gt; Y +------+ &lt;---------- +------+ Z</span><br><span class="line">          |       |  mapping    | RAM  |   by IOMMU</span><br><span class="line">          |       |             |      |</span><br><span class="line">          |       |             |      |</span><br><span class="line">          +-------+             +------+</span><br></pre></td></tr></table></figure>
<p>这里有一堆地址概念，不同地址有不用的作用。硬件可以把物理的内存和设备的寄存器<br>空间映射(MMIO)到CPU物理地址空间, 这里的映射和kernel没有关系，我们可以认为固件<br>已经为我们做好了，代码里里直接访问对应的物理地址就可以了。CPU通过CPU虚拟地址<br>访问物理内存和设备的MMIO, CPU虚拟地址到实际地址的映射是MMU做的，当然如果在内核<br>的线性映射区，这个映射只是加上一个偏移。</p>
<p>从概念上说，设备看到的地址叫总线地址。一般，总线地址比较难以理解，这需要一点<br>体系结构的知识。一般，一个计算机系统类似这样的结构。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    +-----+   +------+</span><br><span class="line">    | CPU |   | CPU  |  ...</span><br><span class="line">    +--+--+   +--+---+</span><br><span class="line">       |         |          +-----+</span><br><span class="line">-------+---+-----+----------+ DDR |</span><br><span class="line">           |                +-----+</span><br><span class="line">   +-------+--------+</span><br><span class="line">   | Bus controller |</span><br><span class="line">   +----+-----------+</span><br><span class="line">        |</span><br><span class="line">   +----+----+</span><br><span class="line">   | Devices |</span><br><span class="line">   +---------+</span><br></pre></td></tr></table></figure>
<p>CPU, DDR，总线控制器连接的是系统总线，外设是连接到外部总线里的。两个总线域里<br>的物理信号，总线报文等都不一样。两个总线域是靠总线控制器联通的。所以，比较<br>容易理解，实际上CPU和外设是处在两个不同的地址空间里的。设备看到的地址，是外部<br>总线域里的地址，我们叫总线地址。其实CPU要访问外设，最终也是通过总线控制器，把<br>CPU地址翻译成总线地址，才能访问到，只不过是硬件把设备地址映射到了系统总线，<br>软件访问直接访问系统总线地址，如果落在映射的区域，总线控制器帮你翻译下，发给<br>设备。</p>
<p>同样的道理，设备做DMA访问，设备一开始发出的地址是总线地址，当这个访问到了总线<br>控制器，总线控制器帮忙翻译成为系统总线地址。(这里，我们可以认为IOMMU(ARM上<br>叫SMMU)也是总线控制器的一部分)</p>
<p>有了这样的认识，下面就好理解了。</p>
</li>
</ol>
<ol start="2">
<li><p>流式DMA和一致性DMA</p>
<p>所以，一个DMA操作，至少要有两个地址，一个CPU可以访问CPU虚拟地址，一个是设备<br>可以访问的设备总线地址(dma_addr_t)，他们其实对应的是一个物理地址。<br>(有回弹缓冲区的不是一个)</p>
<p>dma_alloc_coherent可以分配一段物理地址, 函数的返回是指向这段物理地址的CPU<br>虚拟地址和这段物理地址对应的总线地址。然后你就可以把这个总线地址配置给硬件。</p>
<p>dma_map_single和上面的一致性DMA分配不一样，假设我们已经分配好了一段物理地址，<br>要算出来这段地址对应的总线地址，我们就可以用dma_map_single这个函数。这种DMA<br>的使用方式，叫流式DMA。</p>
</li>
</ol>
<ol start="3">
<li><p>将DMA内存映射到用户态</p>
<p>可以注意到，你用一致性DMA分配”一段”物理内存。是根本不保证分配的物理内存在<br>内核的线性地址空间，而且不保证分配的物理内存是连续的。</p>
<p>那你想把这些物理内存映射到用户态，叫用户直接访问怎么才能做到？</p>
<p>dma_mmap_coherent这个API就做的是这个事情, 在驱动的mmap接口里调用这个函数就<br>可以了。这个函数把DMA物理区域映射到用户态的连续的虚拟地址上。</p>
</li>
</ol>
<ol start="4">
<li><p>聚散表DMA</p>
<p>有的时候，做DMA的数据在内存里是不连续存放的，而且设备也支持这种不连续内存的<br>DMA。这里的不连续，是指设备的DMA地址的描述就是一个聚散表类似的结构。</p>
<p>这时我们可以用内核数据结构struct scatterlist来描述数据的初始内存结构，随后用<br>dma_map_sg的到每一块的总线地址。然后再把这些总线地址配置到设备对应的数据结构<br>里。</p>
</li>
</ol>
<p>知道这些概念对我们编程有什么作用? 首先编程应该是基于正确语义的。你用get_free_page<br>或者是kmalloc分配一段地址给DMA，在特性的条件下或许没有问题。但是，语义完全是错的,<br>这些得到的地址都是CPU虚拟地址，CPU可以用这些地址访问数据。但是设备用这些地址发起<br>DMA操作，是很可能有问题的。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux线程学习- APUE11/12章</title>
    <url>/Linux%E7%BA%BF%E7%A8%8B%E5%AD%A6%E4%B9%A0-APUE11-12%E7%AB%A0/</url>
    <content><![CDATA[<h2 id="线程的创建和退出"><a href="#线程的创建和退出" class="headerlink" title="线程的创建和退出"></a>线程的创建和退出</h2><p> Linux系统下线程和进程的概念是比较模糊的。一般来说，线程是调度的单位，进程是资源<br> 的单位。本质上来说，内核看到都是一个个线程，但是线程之间可以通过共享资源，相互<br> 之间又划分到不同的进程里。</p>
<p> 从内核视角上看各种不同的ID会比较清楚。之前的<a href="https://wangzhou.github.io/APUE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B9%9D%E7%AB%A0/">这篇文章</a>对进程ID, 线程ID，进程组，<br> 线程组，会话已经有了简单描述。</p>
<p> 在一个进程的多个线程里调用getpid，这个是一个c库对getpid系统调用的封装，所以这<br> 个函数返回调用线程的进程ID, 这个ID其实就是这个线程组的线程组ID，也是这个线程组<br> 的主线程的内核pid，这个从kernel代码kernel/sys.c getpid的系统调用可以看的很清楚。<br> 使用gettid系统调用可以得到一个线程的对应内核pid。</p>
<p> 而所谓pthread库的pthread_self()得到的只是pthread库自定义的线程ID，这个东西和上<br> 面内核里真正的各种ID是完全不同的。</p>
<p> 线程的创建用pthread_create, 用的系统调用是clone。我们考虑线程结束的方法，线程执<br> 行完线程函数后自己会退出，这其中也包括线程函数自己调用pthread_exit把自己结束。<br> 线程也可以被进程中的其他线程取消，取消一个线程使用的函数是：pthread_cancel(pthread_t tid)。<br> 用strace跟踪下，可以发现pthread_cancel中的系统调用是tgkill(tgid, tid, SIGRTMIN),<br> 这个系统调用向线程组tgid里的tid线程发送SIGRTMIN信号。</p>
<p> 线程可以用pthread_jorn在阻塞等待相关线程结束，并且得到线程结束所带的返回值。</p>
<p> 线程可以注册退出的时候要调用的函数。使用pthread_cleanup_push, pthread_cleanup_pop。<br> 注册的函数在线程调用pthread_exit或者是线程被pthread_cancel的时候执行，线程正常<br> 执行结束时不执行。用strace -f跟踪进程中所有线程的系统调用可以发现，对应的系统调用<br> 是set_robust_list。</p>
<h2 id="线程同步的锁"><a href="#线程同步的锁" class="headerlink" title="线程同步的锁"></a>线程同步的锁</h2><p> 线程之间共享变量的时候需要加锁，以pthread库为例，我们有pthread_mutex, pthead_spinlock,<br> pthread读写锁。此外我们还可以自己实现锁，这个的好处是不受具体线程库的限制，<br> 不好的地方是，我们自己实现的锁一定没有pthread库实现的性能高。如下的测试代码中<br> 比较了pthread mutex/spinlock，以及自己实现的spinlock的性能:<br> <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL2xvY2tfdGVzdC90ZXN0LmM=">https://github.com/wangzhou/tests/blob/master/lock_test/test.c<i class="fa fa-external-link-alt"></i></span></p>
<p> apue这里的例子11-5很好的展示了一个引用计数的实现方式。<br> 一定要避免A-B/B-A这种交叉加锁的情况，但是注意这里只是加锁，减锁的顺序可以随意。</p>
<p> 如果一定要出现交叉加锁的情况，要做成如果第二把锁没有抢到(所以，第二把锁要用try<br> 锁)，那么已经锁上的第一把锁也要放开，要留出一条通道把可能的死锁情况放过去。过<br> 一段时间再锁上第一把锁，再try第二把锁。</p>
<p> 11-6的代码对于交叉加锁的处理方式是，需要上第二把锁的时候，直接放开第一把锁，<br> 然后先上第二把锁，再上第一把锁，由于这个时候有个时间的空隙，可能不满足之前的<br> 条件了，所以要再check下之前需要第二把锁的条件。当然apue举这个例子的目的这里是<br> 为了说明可以简化加锁，需要在复杂度和性能之间权衡。</p>
<h2 id="条件变量和信号量"><a href="#条件变量和信号量" class="headerlink" title="条件变量和信号量"></a>条件变量和信号量</h2><p> 条件变量和信号量都是为了线程/进程之间同步用的，提供了线程/进程之间相互等待、<br> 通知的机制。简单写一个测试:<br> <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL3B0aHJlYWQvdGhyZWFkLmM=">https://github.com/wangzhou/tests/blob/master/pthread/thread.c<i class="fa fa-external-link-alt"></i></span><br> <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL3B0aHJlYWQvc2VtLmM=">https://github.com/wangzhou/tests/blob/master/pthread/sem.c<i class="fa fa-external-link-alt"></i></span><br> 在thread.c和sem.c里都会出现同一个问题，如果不在enqueue_msg函数里unlock和发信号<br> 之间增加时延，会出现发送线程一直持有锁，接收线程得不到锁从而无法及时接收的情况。<br> 测试发现，在unlock和发送信号(比如thread.c的pthread_mutex_unlock和pthread_cond_signal)<br> 之间增加usleep(1)的时延就可以使如上的这种情况不出现。</p>
<h2 id="线程控制"><a href="#线程控制" class="headerlink" title="线程控制"></a>线程控制</h2><p> pthread库还有很多控制接口，这些接口可以改变如上接口的语义，或者增减新的功能。<br> 比如，每种锁的初始化接口都可以控制这些锁在嵌套加锁、不对称加锁等的语义，遇到<br> 这样的情况，可以配置成容许或者是返回错误值; 可以配置锁在线程之间还是进程之间<br> 共享; 可以配置线程栈的大小等; 可以申请线程的私有数据; 可以使得一个函数只执行<br> 一次。</p>
<p> 简单写一个线程私有数据的测试:<br> <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL3B0aHJlYWQvcHJpdi5j">https://github.com/wangzhou/tests/blob/master/pthread/priv.c<i class="fa fa-external-link-alt"></i></span><br> 可以看到，pthread_key_t key1的create没有和线程绑定。基本的逻辑是，pthread_key_create<br> 可以在任意一个线程里，pthread_setspecific和pthread_getspecific是在线程里对应的，<br> 在线程A里set的数据，可以在线程A里通过get拿到，但是在另外的一个线程B里对全局的<br> key1使用pthread_getspecific只能拿到NULL。apue里的例子把pthread_key_create放到<br> 了每个线程里，为了只调用一次pthread_key_create, 还介绍只跑一个函数一次的接口：<br> int pthread_once(pthread_once_t *initflag, void (*initfn)(void))，可以想象这个<br> 函数的实现是先上锁，然后检查initflag，initflag没有置上已经跑过的flag就调用initfn，<br> 如果已经跑过就不用跑了。这个只跑一次的接口在某些库的设计里是很有用的，比如一个<br> 库需要在进程使用的时候初始化一次，这里就可以用相似的设计或者直接用这个API。</p>
<p> 关于信号可以参考<a href="https://wangzhou.github.io/Linux%E4%BF%A1%E5%8F%B7%E7%AC%94%E8%AE%B0/">这里</a>，线程和信号的细节内容需要另外描述。</p>
<p> 线程在fork系统调用上遇到的问题，我们自己写一个独立的应用的时候比较难遇到，因为<br> 全局受自己的控制，我们只要一开始fork进程，做好规划就好。但是，当我们要写一个库，<br> 这个库可以被其他上层代码调用，我们的库里又要起独立的线程时，这个问题就会出来。<br> 这个问题的本质是自己写的库里向上层export出了全局的资源，比如，如果库里只出函数<br> 接口，就没有问题，所有库里的资源都在函数的栈上，但是，如果库里有了全局资源，相当<br> 于，我们的库向调用进程里增加了全局的资源，调用进程将需要考虑这些全局资源(当然库<br> 可以向上层的调用者提出诉求或者接口)。具体看，子进程会从父进程那里继承:<br> 全局变量(子进程在没有写之前，如果去读这个变量，依然得到的是父进程里的值，如果拿<br> 这个值去做判断就有可能出错)、各种锁、信号量和条件变量。如果fork出来的进程不是<br> 调用exec系列函数去执行一个新的程序，那么子进程里拥有和父进程一样的锁、信号量和<br> 条件变量。可以通过pthread_atfork提前挂上fork时候的回调函数进行处理，即一定是在<br> prepare回调里先获取所有的锁，在parent、child里再释放所有的锁，注意这里的获取释放<br> 的操作和父进程里的可能获取锁的行为做了互斥。</p>
<p> preaed/pwrite可以保证多线程对一个fd的操作是原子的。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>APUE</tag>
        <tag>Linux用户态编程</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux透明大页(THP)分析</title>
    <url>/Linux%E9%80%8F%E6%98%8E%E5%A4%A7%E9%A1%B5-THP-%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="THP是什么"><a href="#THP是什么" class="headerlink" title="THP是什么"></a>THP是什么</h2><p> Linux有两种使用大页的方式，一种是普通大页，另一种是透明大页。本文说的就是透明<br> 大页，透明大页的本意是系统在可以搞成大页的时候，自动的给你做大页的映射，这样<br> 有两个好处，一是减少缺页的次数，另一个是减少TLB miss的数量。</p>
<p> 可以在这里找见THP的说明文档：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">linux/Documentation/vm/transhuge.rst</span><br><span class="line">linux/Documentation/admin-guide/mm/transhuge.rst</span><br></pre></td></tr></table></figure>

<h2 id="THP的使用方式"><a href="#THP的使用方式" class="headerlink" title="THP的使用方式"></a>THP的使用方式</h2><p> 上面的内核文档已经详细介绍了THP的使用方式。简单总结，就是sysfs中提供了三大类<br> 接口去控制THP的使用方式。</p>
<p> 第一类接口配置THP的使用范围: /sys/kernel/mm/transparent_hugepage/enabled<br> alway是指在整个系统里使用THP, madvise是指可以用madvise指定使用THP的地址范围，<br> never是关掉THP。内核提供了CONFIG_TRANSPARENT_HUGEPAGE_ALWAYS以及<br> CONFIG_TRANSPARENT_HUGEPAGE_MADVISE去配置如上enable里的默认值。</p>
<p> 第二类接口配置生成大页的方式：/sys/kernel/mm/transparent_hugepage/defrag<br> always是在内存申请或者madvise的时候直接stall住内存申请，直到内核通过各种手段<br> 把大页搞出来。内存申请返回或者是madvise返回的时候，已经是大页了; defer是随后<br> 内核会异步的把大页给你搞好; defer+madvise是只对madvise是立即搞定大页，其他的<br> 情况还是按照defer的来; madvise是只对madvise立即搞定大页。</p>
<p> 第三类接口，配置内核khugepaged线程的一些参数，比如多少长时间做一次扫描, 具体<br> 可以参考上面的内核文档：/sys/kernel/mm/transparent_hugepage/khugepaged/*</p>
<p> 内核文档里也介绍了THP相关的一些统计参数，比如/proc/vmstat里的thp_*的各个参数，<br> /proc/meminfo、/proc/PID/smap里的AnonHugePages。从内核代码里还可以看到有debugfs<br> 下的接口/sys/kernel/debug/split_huge_pages。</p>
<p> 整个系统使用THP，打开enabled always就好。单独一段内存使用THP，需要用madvise打<br> 上一个MADV_HUGEPAGE, 类似:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">p = mmap(NULL, MEM_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE |</span><br><span class="line">	 MAP_ANONYMOUS, -1, 0);</span><br><span class="line">if (p == MAP_FAILED)</span><br><span class="line">	exit(1);</span><br><span class="line"></span><br><span class="line">ret = madvise(p, MEM_SIZE, MADV_HUGEPAGE);</span><br><span class="line">if (ret)</span><br><span class="line">	exit(1);</span><br><span class="line"></span><br><span class="line">munmap(p, MEM_SIZE);</span><br></pre></td></tr></table></figure>

<p> 实际上, 现在的内核代码和defrag中的定义已经没有严格对应。比如在enable: always,<br> defrag: madvise的时候，vma merge还是会把大于2MB的vma扔给khugepaged线程去扫描、<br> 然后触发huge page collapse。</p>
<h2 id="THP内核代码分析"><a href="#THP内核代码分析" class="headerlink" title="THP内核代码分析"></a>THP内核代码分析</h2><p> 透明巨页的内核配置是CONFIG_TRANSPARENT_HUGEPAGE，代码主要在mm/khugepages.c，<br> mm/huge_memory.c，相关的头文件在include/linux/huge_mm.h，include/linux/khugepaged.h。</p>
<p> THP的初始化在huge_memory.c的hugepage_init(), 这个函数初始化THP, 并且start<br> khugepaged内核线程。/sys/kernel/mm/transparent_hugepage/enabled里写always或者<br> madvise也可以start khugepaged内核线程。</p>
<p> 我们跟踪代码的从三个地方入手，一个是__transparent_hugepage_enabled，这个是内核<br> 缺页流程__handle_mm_fault里, 这里会判断系统有没有使能透明大页，如果使能了，会直<br> 接分配大物理页。另外一个是，系统会启动一个内核线程持续扫描系统里的页，进行大页<br> 的合并。代码分析依赖5.11-rc4。第三个地方是madvise系统调用。</p>
<p> mm/memory.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">__handle_mm_fault</span><br><span class="line">  +-&gt; __transparent_hugepage_enabled</span><br><span class="line">    /* 忽略细节，可以看到在下面的函数里申请page、安装页表 */</span><br><span class="line">    +-&gt; create_huge_pmd/pud</span><br><span class="line">      +-&gt; do_huge_pmd_anonymous_page</span><br><span class="line">        /* 分配内存 */</span><br><span class="line">        +-&gt; alloc_hugepage_vma</span><br><span class="line">	/* 安装页表并且处理和其他mm部件的关系 */</span><br><span class="line">	+-&gt; __do_huge_pmd_anonymous_page</span><br><span class="line"></span><br><span class="line">  /* ? */</span><br><span class="line">  +-&gt; handle_pte_fault</span><br></pre></td></tr></table></figure>
<p> khugepaged的线程函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">khugepaged</span><br><span class="line">  +-&gt; khugepaged_do_scan</span><br><span class="line">    /* 一般情况不会进去这里 */</span><br><span class="line">    +-&gt; collapse_pte_mapped_thp</span><br><span class="line">    /*</span><br><span class="line">     * 有这两个入口做小页换成大页的操作, 第一个和file-back的内存有关系，</span><br><span class="line">     * 下面调用的是: collapse_file。第二个处理匿名页或者是堆上的内存，</span><br><span class="line">     * 下面调用的是：collapse_huge_page。</span><br><span class="line">     */</span><br><span class="line">    +-&gt; khugepaged_scan_file</span><br><span class="line">    /* 先扫描整个pmd下的页，可以做collapase的时候，最后走到collapse_huge_page */</span><br><span class="line">    +-&gt; khugepaged_scan_pmd</span><br><span class="line">      /*</span><br><span class="line">       * 这个是小页换成大页的核心函数，先分配2MB连续页面，然后断开pmd，</span><br><span class="line">       * 同时做tlb invalidate，然后把小页的内存copy到大页，然后把pmd页表</span><br><span class="line">       * 安装上。其中涉及的同步逻辑在最后collapse_huge_page页表同步里</span><br><span class="line">       * 展开分析。</span><br><span class="line">       */</span><br><span class="line">      +-&gt; collapse_huge_page</span><br></pre></td></tr></table></figure>
<p> khugepaged的扫描需要内核的其他部分提供需要扫描的对象，这个入口函数是<br> khugepaged_enter。可以发现整个系统里有mm/mmap.c、mm/huge_memory.c、mm/shmem.c<br> 里调用了。</p>
<p> madvise系统调用，mm/madvise.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">madvise_behavior</span><br><span class="line">  +-&gt; hugepage_madvise</span><br><span class="line">   ...</span><br><span class="line">     /*</span><br><span class="line">      * 这个函数为对应的mm生成一个mm_slot，把mm_slot添加到khugepaged的</span><br><span class="line">      * 扫描链表里，然后wakeup khugepaged线程。</span><br><span class="line">      */</span><br><span class="line">   +-&gt; __khugepaged_enter</span><br></pre></td></tr></table></figure>
<p> mmap.c里的调用是在mmap或者brk系统调用，在不断的做vma_merge的时候，如果发现有<br> vma的range跨越了2MB的连续va，就会把对应的vma交给khugepaged扫描，做大页的替换。</p>
<p> collapse_huge_page页表同步分析:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    /* 分配大页 */</span><br><span class="line">+-&gt; khugepaged_alloc_page</span><br><span class="line"></span><br><span class="line">+-&gt; mmap_write_lock</span><br><span class="line"></span><br><span class="line">    /* 做secondory mmu tlbi */</span><br><span class="line">+-&gt; mmu_notifier_invalidate_range_start</span><br><span class="line"></span><br><span class="line">    /* 清空pmd页表项，做cpu tlbi */</span><br><span class="line">+-&gt; pmdp_collapse_flush</span><br><span class="line"></span><br><span class="line">+-&gt; mmu_notifier_invalidate_range_end</span><br><span class="line"></span><br><span class="line">    /* 清空pte页表, free相关页面 */</span><br><span class="line">+-&gt; __collapse_huge_page_isolate</span><br><span class="line"></span><br><span class="line">    /* 把小页里的内容copy到大页 */</span><br><span class="line">+-&gt; __collapse_huge_page_copy</span><br><span class="line"></span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">    /* 装大页页表 */</span><br><span class="line">+-&gt; set_pmd_at</span><br><span class="line"></span><br><span class="line">+-&gt; mmap_write_unlock</span><br></pre></td></tr></table></figure>
<p>  注意其他的cpu或者设备可以这个过程中还在写原来的小页内存，这些操作和如上页表<br>  变动的同步点在上面两个tlbi处，tlbi和随后的barrier可以保证之前正在总线上的相关<br>  地址操作都完成。这样在barrier后的新访存操作都会触发fault，这些fault都会在<br>  mmap_write_lock上排队。等到新访问的fault可以执行的时候，发现已经有大页，就可以<br>  做后续的处理。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux驱动软硬件兼容问题的考虑</title>
    <url>/Linux%E9%A9%B1%E5%8A%A8%E8%BD%AF%E7%A1%AC%E4%BB%B6%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98%E7%9A%84%E8%80%83%E8%99%91/</url>
    <content><![CDATA[<h2 id="面对的问题"><a href="#面对的问题" class="headerlink" title="面对的问题"></a>面对的问题</h2><p> 首先看我们的工作模型是怎么样。</p>
<p> 硬件设计一代接着一代进行，前后之间的设计可能有：1. 把一个功能砍了(硬件为了保<br> 证兼容性一般不会这样); 2. 新加了一个功能; 3. 改了一个功能(为了兼容性一般也不会<br> 这样); 4. 改了一个功能，但是为了兼容性还支持老的使用方式。</p>
<p> Linux驱动开发，我们一般先直接上传到Linux主线。然后，根据需要回合到各个实际使用<br> 的发行版本中。各个发型版本中因为要做质量控制，所以在发布后，有可能只回合bugfix，<br> 不做大特性的回合。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mainline: a -&gt; b -&gt; c -&gt; d -&gt; e -&gt; f -&gt; g</span><br><span class="line"></span><br><span class="line">distribution a: a -&gt; b -&gt; c</span><br><span class="line">distribution b: a -&gt; b -&gt; c -&gt; d -&gt; e</span><br></pre></td></tr></table></figure>
<p> 软件兼容性要求我们：1. 加一个新特性的时候，不能破坏老特性的使用; 2. 老的软件<br> 在新的硬件上使用时，老的特性还是可以使用的。</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p> 针对上面的，软件兼容性要求第一点，我们不能改动已有的对外接口，比如各种内核ABI<br> 接口(sysfs，debugfs，ioctl，mmap等等)。</p>
<p> 针对以上第二点，软件中不能用硬件版本号区分特性，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (harware version == X)</span><br><span class="line"> feature_one();</span><br></pre></td></tr></table></figure>
<p> 上面这样，在hardware version == X + 1硬件时，老的软件feature_one就无法使用了。</p>
<p> 一般的做法是, 通过预留的功能使能标记来判断是否有该功能。而hardware version只用<br> 来修复正常流程里的bug。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (feature_on_enable())</span><br><span class="line"> feature_one();</span><br></pre></td></tr></table></figure>

<p> 对于纯粹新加的硬件，我们只要原子的加上一个相关的支持补丁就可以，没有回合这个<br> 补丁就不支持新的功能。</p>
<p> 对于硬件设计中的第4点。我们可以把改动后的功能认为是正常流程，而把兼容之前的<br> 硬件设计作为一个bug用hardware version隔开。具体可以：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (harware version == X)</span><br><span class="line"> feature_one();</span><br><span class="line">if (feature_on_enable())</span><br><span class="line"> feature_one_new();</span><br></pre></td></tr></table></figure>
<p> 这里我们假设硬件在X + 1这个版本上对feature_one做了如上第4点里的改动。<br> 我们加一个原子的补丁，把硬件为了向前兼容做的努力看成是一个特殊硬件版本的bug，<br> 而新的改动，我们把它看成是主流程。这样，如果不合这个补丁，之前老的软件可以运行<br> 在新的硬件上。合入这个补丁，新的软件可以跑在新的硬件上，并且使用改动后的特性，<br> 新的软件也可以跑在老的硬件上，同时新的软件也可以兼容更新的硬件(只要硬件保证兼容)。</p>
]]></content>
      <tags>
        <tag>软件架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Macbook虚拟机磁盘扩容</title>
    <url>/Macbook%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/</url>
    <content><![CDATA[<p>本人使用的是MacBook air，CPU使用M1芯片，在上面安装了parallels虚拟机，在虚拟机里<br>安装了Ubuntu20.04 arm64版本。在使用过程中发现虚拟机的磁盘空间不够。于是想到可以<br>给虚拟机磁盘扩容。</p>
<p>具体步骤如下：</p>
<ol>
<li><p>先关闭虚拟机里的ubuntu系统。</p>
</li>
<li><p>在虚拟机配置里有磁盘选项，在这个选项里把磁盘容量配置成96GB。之前是64GB。</p>
<p>配置完成打开Ubuntu，发现磁盘的容量是变大了，但是可以使用的容量还是没有变动，<br>还是32GB。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sherlock@m1:~/notes$ lsblk</span><br><span class="line">NAME                      MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</span><br><span class="line">loop0                       7:0    0  48.9M  1 loop /snap/core18/2068</span><br><span class="line">loop1                       7:1    0  48.9M  1 loop /snap/core18/2127</span><br><span class="line">loop2                       7:2    0  94.8M  1 loop /snap/docker/1124</span><br><span class="line">loop3                       7:3    0  65.1M  1 loop /snap/gtk-common-themes/1515</span><br><span class="line">loop4                       7:4    0  59.6M  1 loop /snap/lxd/20330</span><br><span class="line">loop5                       7:5    0  28.1M  1 loop /snap/snapd/12707</span><br><span class="line">loop6                       7:6    0  28.1M  1 loop /snap/snapd/12886</span><br><span class="line">loop7                       7:7    0    62M  1 loop /snap/lxd/21032</span><br><span class="line">loop8                       7:8    0   1.2M  1 loop /snap/stress-ng/5933</span><br><span class="line">loop9                       7:9    0   1.2M  1 loop /snap/stress-ng/6068</span><br><span class="line">loop10                      7:10   0 104.3M  1 loop /snap/docker/800</span><br><span class="line">sda                         8:0    0    96G  0 disk </span><br><span class="line">├─sda1                      8:1    0   512M  0 part /boot/efi</span><br><span class="line">├─sda2                      8:2    0     1G  0 part /boot</span><br><span class="line">└─sda3                      8:3    0  94.5G  0 part </span><br><span class="line">  └─ubuntu--vg-ubuntu--lv 253:0    0  36.3G  0 lvm  /</span><br><span class="line">sr0                        11:0    1  1024M  0 rom  </span><br></pre></td></tr></table></figure>
<p>注意，如上是随后配置好时的情况，当时是ubuntu–vg-ubuntu–lv的总大小是32GB。</p>
<p>可以看出ubuntu系统安装的时候使用lvm，安装比较时间长了，当时的估计是选了lvm。</p>
</li>
<li><p>使用lvm的相关工具可以查看lvm的具体配置情况。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sherlock@m1:~/notes$ sudo pvdisplay</span><br><span class="line">[sudo] password for sherlock: </span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               /dev/sda3</span><br><span class="line">  VG Name               ubuntu-vg</span><br><span class="line">  PV Size               &lt;94.50 GiB / not usable 0   </span><br><span class="line">  Allocatable           yes </span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              24191</span><br><span class="line">  Free PE               14911</span><br><span class="line">  Allocated PE          9280</span><br><span class="line">  PV UUID               Ul0ZEe-A6d4-H6Q7-zAso-Zmqa-oTxz-R0jS7b</span><br><span class="line">   </span><br><span class="line">sherlock@m1:~/notes$ sudo vgdisplay </span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               ubuntu-vg</span><br><span class="line">  System ID             </span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        1</span><br><span class="line">  Metadata Sequence No  4</span><br><span class="line">  VG Access             read/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                1</span><br><span class="line">  Open LV               1</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                1</span><br><span class="line">  Act PV                1</span><br><span class="line">  VG Size               &lt;94.50 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              24191</span><br><span class="line">  Alloc PE / Size       9280 / 36.25 GiB</span><br><span class="line">  Free  PE / Size       14911 / &lt;58.25 GiB</span><br><span class="line">  VG UUID               0bjAYY-GG24-Nkv3-3Xng-OIY8-PLiM-ALwMbu</span><br><span class="line">   </span><br><span class="line">sherlock@m1:~/notes$ sudo lvdisplay </span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/ubuntu-vg/ubuntu-lv</span><br><span class="line">  LV Name                ubuntu-lv</span><br><span class="line">  VG Name                ubuntu-vg</span><br><span class="line">  LV UUID                m0cTmJ-CrAW-WOjG-0eTR-L19N-dkT1-t7o78d</span><br><span class="line">  LV Write Access        read/write</span><br><span class="line">  LV Creation host, time ubuntu-server, 2021-04-30 21:54:25 +0800</span><br><span class="line">  LV Status              available</span><br><span class="line">  # open                 1</span><br><span class="line">  LV Size                36.25 GiB</span><br><span class="line">  Current LE             9280</span><br><span class="line">  Segments               1</span><br><span class="line">  Allocation             inherit</span><br><span class="line">  Read ahead sectors     auto</span><br><span class="line">  - currently set to     256</span><br><span class="line">  Block device           253:0</span><br></pre></td></tr></table></figure>
<p>lvm里有物理卷，卷组和逻辑卷的概念。一般的使用逻辑是先用物理卷格式化磁盘分区，<br>多个或者一个物理卷可以组成一个卷组，再在卷组上创建逻辑卷。使用pvdisplay, vgdisplay,<br>lvdisplay可以查看物理卷、卷组和逻辑卷的相关信息。如上是扩容后的信息，可以看到，<br>我们在一个物理磁盘容量为96GB的磁盘上创建了一个96GB的pv，在这个pv上创建了一个<br>96GB的vg，但是在这个vg上创建的lv只有36.25GB。扩容之前是32GB。</p>
</li>
<li><p>明白这个底层逻辑后，我们只要搜索下lv扩容的命令就好。可以使用如下命令扩展lv。</p>
<p>lvextend -L +4GB /dev/ubuntu-vg/ubuntu-lv</p>
</li>
<li><p>用df -h看到上面逻辑卷的容量还没有扩大，这是因为df看的是文件系统的大小，还要对<br>逻辑卷上的文件系统扩容下，我这里的文件系统是ext4。可以使用如下命令扩容。</p>
<p>resize2fs /dev/ubuntu-vg/ubuntu-lv</p>
</li>
</ol>
<p>Note: ubuntu下有基于图形的分区工具GParted</p>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>LVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux软中断基本概念</title>
    <url>/Linux%E8%BD%AF%E4%B8%AD%E6%96%AD%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p>中断是CPU硬件上的基本概念，当一个中断发生的时候，CPU当前的执行流被打断，CPU被强<br>行切到预设好的地址继续执行，被打断的上下文通过CPU的一组寄存器向软件报告。</p>
<p>Linux内核里模拟硬件的这种行为，在软件层面也实现了类似的机制。内核需要预先定义一些<br>软中断以及软中断要执行的函数，软中断有是否触发的标记，内核有API配置软中断触发标记，<br>内核会在特定的点去检查是否有软中断的触发标记，如果有，就去调用软中断对应函数，执行<br>这个函数可以在当前上下文立即执行，也可以唤醒软中断相关的内核线程(ksoftirqd)，在相关<br>内核线程里执行软中断的回调函数。</p>
<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>硬件中断的调用流程，以riscv举例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* linux/arch/riscv/kernel/entry.S */</span><br><span class="line">handle_exception</span><br><span class="line">  +-&gt; generic_handle_arch_irq</span><br><span class="line">    +-&gt; irq_exit</span><br><span class="line">      +-&gt; __irq_exit_rcu</span><br><span class="line">        +-&gt; if (!in_interrupt() &amp;&amp; local_softirq_pending())</span><br><span class="line">       /* 注意这个函数会有两个版本，我们这里只看!CONFIG_PREEMPT_RT的版本 */</span><br><span class="line">   +-&gt; invoke_softirq()</span><br><span class="line">     +-&gt; wakeup_softirqd()</span><br></pre></td></tr></table></figure>
<p>如上，在硬中断退出的时候会检测有没有pending的软中断，如果有，就去执行软中断的回调<br>函数，在具体执行的时候，又会根据条件决定是马上调用软中断的处理函数还是唤醒软中断<br>内核线程去处理，判断条件是是否内核配置了强制在内核线程处理软中断，以及是否有ksoftirqd<br>这个per-cpu的结构。如果马上执行软中断处理函数，又用内核配置参数决定使用中断当前使用<br>的栈，还是使用软中断自己的栈。</p>
<p>软中断也提供了API，支持在内核的线程上下文里设置软中断，以及唤醒软中断内核线程处理<br>软中断。</p>
<p>内核现在已经不建议新增加软中断类型，目前的类型有：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">enum &#123;                                                                               </span><br><span class="line">        HI_SOFTIRQ=0,                                                           </span><br><span class="line">        TIMER_SOFTIRQ,                                                          </span><br><span class="line">        NET_TX_SOFTIRQ,                                                         </span><br><span class="line">        NET_RX_SOFTIRQ,                                                         </span><br><span class="line">        BLOCK_SOFTIRQ,                                                          </span><br><span class="line">        IRQ_POLL_SOFTIRQ,                                                       </span><br><span class="line">        TASKLET_SOFTIRQ,                                                        </span><br><span class="line">        SCHED_SOFTIRQ,                                                          </span><br><span class="line">        HRTIMER_SOFTIRQ,                                                        </span><br><span class="line">        RCU_SOFTIRQ,</span><br><span class="line">&#125;;                                                                              </span><br></pre></td></tr></table></figure>
<p>如果依然要使用软中断，可以用tasklet，它就是一种软中断的封装。</p>
<p>软中断使用local_bh_disable()/local_bh_enable()关闭和打开。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI FLR analysis</title>
    <url>/PCI-FLR-analysis/</url>
    <content><![CDATA[<p>关于FLR的硬件操作比较简单, 相关的硬件有:</p>
<ul>
<li>配置空间里device cap里的FLR capability bit, 这个表示设备是否支持FLR。</li>
<li>配置空间里device control里的BCR_FLR bit, 写这个bit可以触发FLR。</li>
</ul>
<p>检测是否支持FLR</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/pci/pci.c */</span><br><span class="line">pcie_has_flr(struct pci_dev *dev)</span><br></pre></td></tr></table></figure>

<p>Linux kernel里pcie_flr会被下面的三个函数调用到, 触发FLR</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/pci/pci.c: below tree functions will call __pci_reset_function_locked */</span><br><span class="line">pci_reset_function(struct pci_dev *dev)</span><br><span class="line">pci_reset_function_locked(struct pci_dev *dev)</span><br><span class="line">pci_try_reset_function(struct pci_dev *dev)</span><br><span class="line">    =&gt; __pci_reset_function_locked(struct pci_dev *dev)</span><br><span class="line">        -&gt; pcie_flr(struct pci_dev *dev)</span><br></pre></td></tr></table></figure>

<p>下面来看上面的三个函数在哪里会用到。</p>
<p>首先通过这个设备的sysfs接口可以触发FLR:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/pci/pci-sysfs.c */</span><br><span class="line">reset_store</span><br><span class="line">    -&gt; pci_reset_function(struct pci_dev *dev)</span><br></pre></td></tr></table></figure>

<p>另外，vfio里也提供的接口，可以供用户触发FLR。这些接口包括，vfio设备的enable,<br>disable, 以及一个vfio设备相关的ioctl。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/vfio/pci/vfio_pci_config.c */</span><br><span class="line">vfio_exp_config_write</span><br><span class="line">    -&gt; pci_try_reset_function</span><br><span class="line"></span><br><span class="line">/* drivers/vfio/pci/vfio_pci.c */</span><br><span class="line">vfio_pci_enable</span><br><span class="line">vfio_pci_disable</span><br><span class="line">vfio_pci_ioctl (cmd == VFIO_DEVICE_RESET)</span><br><span class="line">    =&gt; pci_try_reset_function(pdev);</span><br></pre></td></tr></table></figure>

<p>单独的FLR操作需要配合整个reset流程工作, 在上面的调用pcie_flr的函数里，他们基本<br>的处理流程都是, 先做reset_prepare, 再触发FLR，最后做reset后的恢复:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">the logic of pci_reset_function and its brother functions are:</span><br><span class="line">	- reset_prepare</span><br><span class="line">	- flr operation if supporting flr</span><br><span class="line">	- reset_done</span><br></pre></td></tr></table></figure>

<p>reset_prepare and reset_done callbacks are stored in pci_driver’s pci_error_handlers,<br>these callbacks should be offered by your device driver:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct pci_driver &#123;</span><br><span class="line">	...</span><br><span class="line">	const struct pci_error_handlers &#123;</span><br><span class="line">		...</span><br><span class="line">		void (*reset_prepare)(struct pci_dev *dev);</span><br><span class="line">		void (*reset_done)(struct pci_dev *dev);</span><br><span class="line">		...</span><br><span class="line">	&#125;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从上面的代码分析中可以看出，linux内核中的flr流程并不涉及到软件中设备结构的销毁。<br>所以，只要在发生flr这段时间不去下发硬件请求，用lspci看，设备一直都在。一般硬件<br>在实现flr的时候，在硬件层面都有pf到vf的通知方式, 这样可以保证在pf flr时候通知到<br>flr做必要的处理，当pf flr完成后，可以通知vf驱动做必要的硬件配置上的恢复。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI ACPI笔记1</title>
    <url>/PCI-ACPI%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<h2 id="ACPI"><a href="#ACPI" class="headerlink" title="ACPI"></a>ACPI</h2><p>For the basic knowledge, you can refer to “PCI Express 体系结构导读” by<br>WangQi, in charpter 14, there are related stuffs about ACPI.</p>
<p>And you can also refer to kernel source code linux/Documentation/acpi/ to find<br>ACPI related documents. ACPI Definition Blocks is as below, this figure is<br>copied from linux/Documentation/acpi/namespace.txt.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------+    +-------+    +--------+    +------------------------+</span><br><span class="line">|  RSDP   | +-&gt;| XSDT  | +-&gt;|  FADT  |    |  +-------------------+ |</span><br><span class="line">+---------+ |  +-------+ |  +--------+  +-|-&gt;|       DSDT        | |</span><br><span class="line">| Pointer | |  | Entry |-+  | ...... |  | |  +-------------------+ |</span><br><span class="line">+---------+ |  +-------+    | X_DSDT |--+ |  | Definition Blocks | |</span><br><span class="line">| Pointer |-+  | ..... |    | ...... |    |  +-------------------+ |</span><br><span class="line">+---------+    +-------+    +--------+    |  +-------------------+ |</span><br><span class="line">               | Entry |------------------|-&gt;|       SSDT        | |</span><br><span class="line">               +- - - -+                  |  +-------------------| |</span><br><span class="line">               | Entry | - - - - - - - -+ |  | Definition Blocks | |</span><br><span class="line">               +- - - -+                | |  +-------------------+ |</span><br><span class="line">                                        | |  +- - - - - - - - - -+ |</span><br><span class="line">                                        +-|-&gt;|       SSDT        | |</span><br><span class="line">                                          |  +-------------------+ |</span><br><span class="line">                                          |  | Definition Blocks | |</span><br><span class="line">                                          |  +- - - - - - - - - -+ |</span><br><span class="line">                                          +------------------------+</span><br><span class="line">                                                      |</span><br><span class="line">                                         OSPM Loading |</span><br><span class="line">                                                     \|/</span><br><span class="line">                                               +----------------+</span><br><span class="line">                                               | ACPI Namespace |</span><br><span class="line">                                               +----------------+</span><br></pre></td></tr></table></figure>
<h2 id="PCI-using-ACPI"><a href="#PCI-using-ACPI" class="headerlink" title="PCI using ACPI"></a>PCI using ACPI</h2><p>a. work flows:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* just register struct acpi_bus_type acpi_pci_bus to list: bus_type_list */</span><br><span class="line">acpi_pci_init() /* arch_initcall(acpi_pci_init) */</span><br><span class="line"></span><br><span class="line">acpi_init() /* subsys_initcall(acpi_init) */</span><br><span class="line">    --&gt; acpi_scan_init()</span><br><span class="line">        /*</span><br><span class="line">         * register pci_root_handler to list: acpi_scan_handlers_list.</span><br><span class="line">	 * the attach will be called in acpi_scan_attach_handler().</span><br><span class="line">	 * there attach is assigned as acpi_pci_root_add()</span><br><span class="line">	 */</span><br><span class="line">        --&gt; acpi_pci_root_init()</span><br><span class="line">	/*</span><br><span class="line">         * register pci_link_handler to list: acpi_scan_handlers_list.</span><br><span class="line">	 * this handler has relationship with PCI IRQ.</span><br><span class="line">	 */</span><br><span class="line">	--&gt; acpi_pci_link_init()</span><br><span class="line">	/* we facus on PCI-ACPI, ignore other handlers&#x27; init */</span><br><span class="line">	...</span><br><span class="line">        --&gt; acpi_bus_scan()</span><br><span class="line">	    /* create struct acpi_devices for all device in this system */</span><br><span class="line">	    --&gt; acpi_walk_namespace()</span><br><span class="line">	    --&gt; acpi_bus_attach()</span><br><span class="line">	        --&gt; acpi_scan_attach_handler()</span><br><span class="line">		    --&gt; acpi_scan_match_handler()</span><br><span class="line">		    --&gt; handler-&gt;attach /* attach is acpi_pci_root_add */</span><br><span class="line"></span><br><span class="line">acpi_pci_root_add()</span><br><span class="line">    /*</span><br><span class="line">     * in kernel, there are two pci_acpi_scan_root, they are in</span><br><span class="line">     * arch/ia64/pci/pci.c and arch/x86/pci/acpi.c.</span><br><span class="line">     * if we will implement PCI using ACPI in ARM64, we should implement</span><br><span class="line">     * another this kind of function in arch/arm64/kernel/pci.c.</span><br><span class="line">     * in pci_acpi_scan_root, will allocate struct pci_controller and</span><br><span class="line">     * struct pci_root_info.</span><br><span class="line">     */</span><br><span class="line">    --&gt; pci_acpi_scan_root()</span><br><span class="line">        --&gt; probe_pci_root_info()</span><br><span class="line">	    /*</span><br><span class="line">	     * will called twice, first for count_window, second for add window.</span><br><span class="line">	     * this function will get infomation from ACPI table.</span><br><span class="line">	     */</span><br><span class="line">	    --&gt; acpi_walk_resources() /* drivers/acpi/acpica/rsxface.c */</span><br></pre></td></tr></table></figure>
<p>b. basic structs:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">global list: acpi_scan_handlers_list /* drivers/acpi/scan.c */</span><br><span class="line">element: struct acpi_scan_handler</span><br><span class="line"></span><br><span class="line">static list: bus_type_list /* drivers/acpi */</span><br><span class="line">element: struct acpi_bus_type</span><br><span class="line"></span><br><span class="line">struct acpi_scan_handler:</span><br><span class="line">     struct acpi_device_id *ids;</span><br><span class="line">     attach;</span><br><span class="line">     ...</span><br><span class="line"></span><br><span class="line">struct acpi_bus_type</span><br><span class="line">struct acpi_device</span><br><span class="line"></span><br><span class="line">struct pci_controller</span><br><span class="line">struct pci_root_info:</span><br><span class="line">    struct pci_controller *controller;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI ACPI笔记2</title>
    <url>/PCI-ACPI%E7%AC%94%E8%AE%B02/</url>
    <content><![CDATA[<h2 id="Basic-points-about-PCI-ACPI"><a href="#Basic-points-about-PCI-ACPI" class="headerlink" title="Basic points about PCI ACPI"></a>Basic points about PCI ACPI</h2><p>There are three APCI tables involved in: DSDT, MCFG and IORT</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MCFG: should configure ECAM compliant configure space address(CPU address).</span><br><span class="line">|</span><br><span class="line">|    _CBA: MMCONFIG base address</span><br><span class="line"></span><br><span class="line">DSDT: we can configure IO/MEM range and some SoC specific info in DSDT.</span><br><span class="line">|</span><br><span class="line">|    _CRS indicates: MEM/IO space, bus range</span><br><span class="line">|</span><br><span class="line">|    ...</span><br><span class="line">|</span><br><span class="line">|    PNP0A03 ?</span><br><span class="line"></span><br><span class="line">IORT: build up the map among RID, stream ID and device ID.</span><br><span class="line">|</span><br><span class="line">|    ...</span><br><span class="line"></span><br><span class="line">_PRT: PCI routing table for INTx</span><br></pre></td></tr></table></figure>

<p>let’s see a sample of above tables in [2]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// for details, refer to p.42 in [4]</span><br><span class="line">EFI_ACPI_5_0_PCI_EXPRESS_MEMORY_MAPPED_CONFIGURATION_SPACE_TABLE Mcfg=</span><br><span class="line">&#123;</span><br><span class="line">  &#123;</span><br><span class="line">      &#123;</span><br><span class="line">        EFI_ACPI_5_0_PCI_EXPRESS_MEMORY_MAPPED_CONFIGURATION_SPACE_BASE_ADDRESS_DESCRIPTION_TABLE_SIGNATURE,</span><br><span class="line">        sizeof (EFI_ACPI_5_0_PCI_EXPRESS_MEMORY_MAPPED_CONFIGURATION_SPACE_TABLE),</span><br><span class="line">        ACPI_5_0_MCFG_VERSION,</span><br><span class="line">        0x00,                                             // Checksum will be updated at runtime</span><br><span class="line">        &#123;EFI_ACPI_ARM_OEM_ID&#125;,</span><br><span class="line">        EFI_ACPI_ARM_OEM_TABLE_ID,</span><br><span class="line">        EFI_ACPI_ARM_OEM_REVISION,</span><br><span class="line">        EFI_ACPI_ARM_CREATOR_ID,</span><br><span class="line">        EFI_ACPI_ARM_CREATOR_REVISION</span><br><span class="line">      &#125;,</span><br><span class="line">      0x0000000000000000,                                 //Reserved</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line"></span><br><span class="line">    // just a sample, so only list one Segment</span><br><span class="line">    &#123;</span><br><span class="line">      0xb0000000,                                         //Base Address</span><br><span class="line">      0x0,                                                //Segment Group Number</span><br><span class="line">      0x0,                                                //Start Bus Number</span><br><span class="line">      0x1f,                                               //End Bus Number</span><br><span class="line">      0x00000000,                                         //Reserved</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">so structure of MCFG table is:</span><br><span class="line">&#123;</span><br><span class="line">    EFI_ACPI_5_0_MCFG_TABLE_CONFIG</span><br><span class="line">    &#123;</span><br><span class="line">        EFI_ACPI_DESCRIPTION_HEADER</span><br><span class="line">        UINT64 Reserved1</span><br><span class="line">    &#125;</span><br><span class="line">    EFI_ACPI_5_0_MCFG_CONFIG_STRUCTURE[]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Scope(_SB)</span><br><span class="line">&#123;</span><br><span class="line">  // PCIe Root bus, for the ASL grammar, please refer to 19 charpter in [5]</span><br><span class="line">  Device (PCI0)                              // acpi_get_devices(&quot;HISI0080&quot;, ...) ?</span><br><span class="line">  &#123;</span><br><span class="line">    // for the details of items below, please refer to charpter 6 in [5]</span><br><span class="line">    Name (_HID, &quot;HISI0080&quot;)                  // PCI Express Root Bridge</span><br><span class="line">    Name (_CID, &quot;PNP0A03&quot;)                   // Compatible PCI Root Bridge, Compatible ID</span><br><span class="line">    Name(_SEG, 0)                            // Segment of this Root complex</span><br><span class="line">    Name(_BBN, 0)                            // Base Bus Number</span><br><span class="line">    Name(_CCA, 1)                            // cache coherence attribute ??</span><br><span class="line">    Method (_CRS, 0, Serialized) &#123;           // Root complex resources, _CRS: current resource setting</span><br><span class="line">                                             // Method is defined in 19.6.82 in [5]</span><br><span class="line">      Name (RBUF, ResourceTemplate () &#123;      // Name: 19.6.87, ResourceTemplate: 19.6.111,</span><br><span class="line">                                             // 19.3.3 in [5]</span><br><span class="line">        WordBusNumber (                      // Bus numbers assigned to this root,</span><br><span class="line">                                             // wordBusNumber: 19.6.144</span><br><span class="line">          ResourceProducer,</span><br><span class="line">          MinFixed,</span><br><span class="line">          MaxFixed,</span><br><span class="line">          PosDecode,</span><br><span class="line">          0,                                 // AddressGranularity</span><br><span class="line">          0x0,                               // AddressMinimum - Minimum Bus Number</span><br><span class="line">          0x1f,                              // AddressMaximum - Maximum Bus Number</span><br><span class="line">          0,                                 // AddressTranslation - Set to 0</span><br><span class="line">          0x20                               // RangeLength - Number of Busses</span><br><span class="line">        )</span><br><span class="line">        QWordMemory (                        // 64-bit BAR Windows</span><br><span class="line">          ResourceProducer,</span><br><span class="line">          PosDecode,</span><br><span class="line">          MinFixed,</span><br><span class="line">          MaxFixed,</span><br><span class="line">          Cacheable,</span><br><span class="line">          ReadWrite,</span><br><span class="line">          0x0,                               // Granularity</span><br><span class="line">          0xb2000000,                        // Min Base Address pci address</span><br><span class="line">          0xb7feffff,                        // Max Base Address</span><br><span class="line">          0x0,                               // Translate</span><br><span class="line">          0x5ff0000                          // Length</span><br><span class="line">        )</span><br><span class="line">        QWordIO (</span><br><span class="line">          ResourceProducer,</span><br><span class="line">          MinFixed,</span><br><span class="line">          MaxFixed,</span><br><span class="line">          PosDecode,</span><br><span class="line">          EntireRange,</span><br><span class="line">          0x0,</span><br><span class="line">          0x0,</span><br><span class="line">          0xffff,</span><br><span class="line">          0xb7ff0000,</span><br><span class="line">          0x10000</span><br><span class="line">        )</span><br><span class="line">      &#125;)                                      // Name(RBUF)</span><br><span class="line">      Return (RBUF)</span><br><span class="line">    &#125;                                         // Method(_CRS), this method return RBUF!</span><br><span class="line">  &#125; // Device(PCI0)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// did not include smmu related description, for the detail, please refer to [6]</span><br><span class="line">// IORT Head:</span><br><span class="line">[0004]                          Signature : &quot;IORT&quot;    [IO Remapping Table]</span><br><span class="line">[0004]                       Table Length : 0000029e</span><br><span class="line">[0001]                           Revision : 00</span><br><span class="line">[0001]                           Checksum : BC</span><br><span class="line">[0006]                             Oem ID : &quot;HISI &quot;</span><br><span class="line">[0008]                       Oem Table ID : &quot;D03&quot;</span><br><span class="line">[0004]                       Oem Revision : 00000000</span><br><span class="line">[0004]                    Asl Compiler ID : &quot;INTL&quot;</span><br><span class="line">[0004]              Asl Compiler Revision : 20150410</span><br><span class="line"></span><br><span class="line">[0004]                         Node Count : 00000003</span><br><span class="line">[0004]                        Node Offset : 00000034</span><br><span class="line">[0004]                           Reserved : 00000000</span><br><span class="line">[0004]                   Optional Padding : 00 00 00 00</span><br><span class="line"></span><br><span class="line">/* ITS 0, for dsa */</span><br><span class="line">[0001]                               Type : 00</span><br><span class="line">[0002]                             Length : 0018</span><br><span class="line">[0001]                           Revision : 00</span><br><span class="line">[0004]                           Reserved : 00000000</span><br><span class="line">[0004]                      Mapping Count : 00000000</span><br><span class="line">[0004]                     Mapping Offset : 00000000</span><br><span class="line"></span><br><span class="line">[0004]                           ItsCount : 00000001</span><br><span class="line">[0004]                        Identifiers : 00000000</span><br><span class="line"></span><br><span class="line">/* mbi-gen dsa  mbi0 - usb, named component */</span><br><span class="line">[0001]                               Type : 01</span><br><span class="line">[0002]                             Length : 0046</span><br><span class="line">[0001]                           Revision : 00</span><br><span class="line">[0004]                           Reserved : 00000000</span><br><span class="line">[0004]                      Mapping Count : 00000001</span><br><span class="line">[0004]                     Mapping Offset : 00000032</span><br><span class="line"></span><br><span class="line">[0004]                         Node Flags : 00000000</span><br><span class="line">[0008]                  Memory Properties : [IORT Memory Access Properties]</span><br><span class="line">[0004]                    Cache Coherency : 00000000</span><br><span class="line">[0001]              Hints (decoded below) : 00</span><br><span class="line">                                Transient : 0</span><br><span class="line">                           Write Allocate : 0</span><br><span class="line">                            Read Allocate : 0</span><br><span class="line">                                 Override : 0</span><br><span class="line">[0002]                           Reserved : 0000</span><br><span class="line">[0001]       Memory Flags (decoded below) : 00</span><br><span class="line">                                Coherency : 0</span><br><span class="line">                         Device Attribute : 0</span><br><span class="line">[0001]                  Memory Size Limit : 00</span><br><span class="line">[0017]                        Device Name : &quot;\_SB_.MBI0&quot;</span><br><span class="line">[0004]                            Padding : 00 00 00 00</span><br><span class="line"></span><br><span class="line">[0004]                         Input base : 00000000</span><br><span class="line">[0004]                           ID Count : 00000001</span><br><span class="line">[0004]                        Output Base : 00040080  // device id</span><br><span class="line">[0004]                   Output Reference : 00000034  // point to its dsa</span><br><span class="line">[0004]              Flags (decoded below) : 00000000</span><br><span class="line">                           Single Mapping : 1</span><br><span class="line"></span><br><span class="line">/* RC 0 */</span><br><span class="line">[0001]                               Type : 02</span><br><span class="line">[0002]                             Length : 0034</span><br><span class="line">[0001]                           Revision : 00</span><br><span class="line">[0004]                           Reserved : 00000000</span><br><span class="line">[0004]                      Mapping Count : 00000001</span><br><span class="line">[0004]                     Mapping Offset : 00000020</span><br><span class="line"></span><br><span class="line">[0008]                  Memory Properties : [IORT Memory Access Properties]</span><br><span class="line">[0004]                    Cache Coherency : 00000001</span><br><span class="line">[0001]              Hints (decoded below) : 00</span><br><span class="line">                                Transient : 0</span><br><span class="line">                           Write Allocate : 0</span><br><span class="line">                            Read Allocate : 0</span><br><span class="line">                                 Override : 0</span><br><span class="line">[0002]                           Reserved : 0000</span><br><span class="line">[0001]       Memory Flags (decoded below) : 00</span><br><span class="line">                                Coherency : 0</span><br><span class="line">                         Device Attribute : 0</span><br><span class="line">[0004]                      ATS Attribute : 00000000</span><br><span class="line">[0004]                 PCI Segment Number : 00000000</span><br><span class="line"></span><br><span class="line">// Input, output means BDF of pcie host as the device ID in ITS</span><br><span class="line">[0004]                         Input base : 00000000</span><br><span class="line">[0004]                           ID Count : 00002000       // the number of IDs in range</span><br><span class="line">[0004]                        Output Base : 00000000</span><br><span class="line">// refer to the a node, here refer to ITS node in 0x34 offset</span><br><span class="line">[0004]                   Output Reference : 00000034</span><br><span class="line">[0004]              Flags (decoded below) : 00000000</span><br><span class="line">                           Single Mapping : 0</span><br></pre></td></tr></table></figure>

<h2 id="Work-flow-of-ACPI-parse"><a href="#Work-flow-of-ACPI-parse" class="headerlink" title="Work flow of ACPI parse"></a>Work flow of ACPI parse</h2><p>based on kernel v4.8</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* from the view of ACPI, this function is the start */</span><br><span class="line">acpi_init</span><br><span class="line">    --&gt; acpi_bus_init</span><br><span class="line"></span><br><span class="line">        /* this is a week function */</span><br><span class="line">    --&gt; pci_mmcfg_late_init</span><br><span class="line">            /* parse the MCFG table: drivers/acpi/pci_mcfg.c */</span><br><span class="line">        --&gt; acpi_table_parse(ACPI_SIG_MCFG, pci_mcfg_parse) </span><br><span class="line">                /* add mcfg_entry which contain info in mcfg to pci_mcfg_list,</span><br><span class="line">                 * this will add all mcfg region to above list.</span><br><span class="line">                 */</span><br><span class="line">            --&gt; pci_mcfg_parse</span><br><span class="line"></span><br><span class="line">        /* this function is not in v4.8 now, it is used by PCIe controller to map</span><br><span class="line">         * itself to correct ITS and SMMU. This will be modified in future maybe.</span><br><span class="line">         *</span><br><span class="line">         * Now, it parses iort table in iort_table_detect. other moduldes like ITS</span><br><span class="line">         * will directly call the functions in iort_table_detect.</span><br><span class="line">         */</span><br><span class="line">    --&gt; iort_table_detect </span><br><span class="line"></span><br><span class="line">        /* this function will scan all ACPI resource, including PCI */</span><br><span class="line">    --&gt; acpi_scan_init</span><br><span class="line">	--&gt; acpi_pci_root_init</span><br><span class="line">	--&gt; acpi_pci_link_init</span><br><span class="line"></span><br><span class="line">	--&gt; acpi_bus_scan</span><br><span class="line">                /* each Device in DSDT will be a device below, which has a root</span><br><span class="line">                 * bus. normally, we configure related address translate hardware</span><br><span class="line">                 * unit in UEFI for this device(rp).</span><br><span class="line">                 */</span><br><span class="line">            --&gt; acpi_bus_attach(device)</span><br><span class="line"></span><br><span class="line">            ...     /* why we call acpi_pci_root_add, please refer to</span><br><span class="line">                     * https://wangzhou.github.io/PCI-ACPI笔记1/</span><br><span class="line">                     * scan PCI info in DSDT</span><br><span class="line">                     */</span><br><span class="line">	        --&gt; acpi_pci_root_add</span><br><span class="line">		    --&gt; pci_acpi_scan_root</span><br><span class="line">		        --&gt; acpi_pci_root_create</span><br><span class="line">			    --&gt; acpi_pci_probe_root_resources</span><br><span class="line">			            /* here parse _CRS info in DSDT */</span><br><span class="line">			        --&gt; acpi_dev_get_resources</span><br></pre></td></tr></table></figure>

<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul>
<li>[RFC PATCH v3 0/3] Add ACPI support for HiSilicon PCIe Host Controllers</li>
<li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29wZW4tZXN0dWFyeS91ZWZpLmdpdC9PcGVuUGxhdGZvcm1Qa2cvQ2hpcHMvSGlzaWxpY29uL1B2NjYwL1B2NjYwQWNwaVRhYmxlcy8=">https://github.com/open-estuary/uefi.git/OpenPlatformPkg/Chips/Hisilicon/Pv660/Pv660AcpiTables/<i class="fa fa-external-link-alt"></i></span></li>
<li><a href="https://wangzhou.github.io/PCI-ACPI%E7%AC%94%E8%AE%B01/">https://wangzhou.github.io/PCI-ACPI笔记1/</a></li>
<li>PCI Firmware Specification 3.0</li>
<li>ACPI 6.0 spec</li>
<li>IORT spec</li>
</ul>
]]></content>
      <tags>
        <tag>UEFI</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI MSI parse in ACPI</title>
    <url>/PCI-MSI-parse-in-ACPI/</url>
    <content><![CDATA[<h2 id="parse-IORT-table-and-build-ITS-interrupt-domain"><a href="#parse-IORT-table-and-build-ITS-interrupt-domain" class="headerlink" title="parse IORT table and build ITS interrupt domain"></a>parse IORT table and build ITS interrupt domain</h2><p>During PCI enumeration, pci_device_add will be called for each PCIe devices.<br>In pci_device_add, we will get the ITS irq domain for this PCIe device and store<br>this domain in this device’s struct(pci_dev).</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_device_add</span><br><span class="line">    --&gt; pci_set_msi_domain(dev)</span><br><span class="line">        --&gt; pci_dev_msi_domain(dev)</span><br><span class="line">            --&gt; pci_msi_get_device_domain</span><br><span class="line">                --&gt; iort_get_device_domain(&amp;pdev-&gt;dev, rid)</span><br><span class="line">                    --&gt; iort_dev_find_its_id(dev, req_id, 0, &amp;its_id)</span><br><span class="line">                            /* key point to find the RC node in IORT table */</span><br><span class="line">                        --&gt; iort_find_dev_node(dev)</span><br><span class="line">                            /*</span><br><span class="line">			     * if the device is a pci device, we get its related</span><br><span class="line">                             * root bus, and we scan the IORT table to get the</span><br><span class="line">                             * PCI_ROOT_COMPLEX node of this root bus.</span><br><span class="line">                             * </span><br><span class="line">                             * point is we use iort_match_node_callback to confirm</span><br><span class="line">                             * that we find the right PCI_ROOT_COMPLEX. however,</span><br><span class="line">                             * above function uses &quot;segment value&quot; to do the check</span><br><span class="line">                             * </span><br><span class="line">                             * if we configure IORT as:</span><br><span class="line">                             * RC0: segment0: ITS map1</span><br><span class="line">                             * RC1: segment0: ITS map2</span><br><span class="line">                             * </span><br><span class="line">                             * when we try to find a pcie device under RC1, finally</span><br><span class="line">                             * we will get ITS map1 under RC0.[2]</span><br><span class="line">                             * </span><br><span class="line">                             */</span><br><span class="line">                    ...</span><br><span class="line">                        /* return irq domain */</span><br><span class="line">                    --&gt; irq_find_matching_fwnode(handle, DOMAIN_BUS_PCI_MSI)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iort_dev_find_its_id</span><br><span class="line">       /* find related RC node in IORT table */</span><br><span class="line">    --&gt;iort_find_dev_node</span><br><span class="line">       /*</span><br><span class="line">        * find related ITS node in MADT table here. Is there a bug?</span><br><span class="line">        *</span><br><span class="line">        * This function supports multiple mappings in one RC[1]:</span><br><span class="line">        * 1. multiple ITS mappings.</span><br><span class="line">        * 2. multiple SMMU mappings?</span><br><span class="line">        * 3. multiple ITS/SMMU mappings</span><br><span class="line">        *</span><br><span class="line">        * One RC node in IORT table maps to one PCIe segment(one PCIe domain),</span><br><span class="line">        * we can have multiple PCIe host bridges in one PCIe segment.</span><br><span class="line">        *</span><br><span class="line">        */</span><br><span class="line">    --&gt;iort_node_map_rid(node, req_id, NULL, IORT_MSI_TYPE)</span><br></pre></td></tr></table></figure>
<h2 id="get-an-interrtup-in-PCIe-device-driver"><a href="#get-an-interrtup-in-PCIe-device-driver" class="headerlink" title="get an interrtup in PCIe device driver"></a>get an interrtup in PCIe device driver</h2><p>When PCIe device wants to apply interrupts, it will call some functions like: pci_enable_msi.<br>these kind of function will first get irq domain stored in pci_dev, then will<br>call ITS driver to get an interrupt resource.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><p>[1]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* PCIe0 */</span><br><span class="line">[0001]                               Type : 02</span><br><span class="line">[0002]                             Length : 0034</span><br><span class="line">[0001]                           Revision : 00</span><br><span class="line">[0004]                           Reserved : 00000000</span><br><span class="line">[0004]                      Mapping Count : 00000001</span><br><span class="line">[0004]                     Mapping Offset : 00000020</span><br><span class="line"></span><br><span class="line">[0008]                  Memory Properties : [IORT Memory Access Properties]</span><br><span class="line">[0004]                    Cache Coherency : 00000001</span><br><span class="line">[0001]              Hints (decoded below) : 00</span><br><span class="line">                                Transient : 0</span><br><span class="line">                           Write Allocate : 0</span><br><span class="line">                            Read Allocate : 0</span><br><span class="line">                                 Override : 0</span><br><span class="line">[0002]                           Reserved : 0000</span><br><span class="line">[0001]       Memory Flags (decoded below) : 00</span><br><span class="line">                                Coherency : 0</span><br><span class="line">                         Device Attribute : 0</span><br><span class="line">[0004]                      ATS Attribute : 00000000</span><br><span class="line">[0004]                 PCI Segment Number : 00000000</span><br><span class="line"></span><br><span class="line">[0004]                         Input base : 00000000</span><br><span class="line">[0004]                           ID Count : 00004000</span><br><span class="line">[0004]                        Output Base : 00000000</span><br><span class="line">[0004]                   Output Reference : 0000007c</span><br><span class="line">[0004]              Flags (decoded below) : 00000000</span><br><span class="line">                           Single Mapping : 0</span><br><span class="line"></span><br><span class="line">[0004]                         Input base : 00006000</span><br><span class="line">[0004]                           ID Count : 00000100</span><br><span class="line">[0004]                        Output Base : 00006000</span><br><span class="line">[0004]                   Output Reference : 0000007c</span><br><span class="line">[0004]              Flags (decoded below) : 00000000</span><br><span class="line">                           Single Mapping : 0</span><br></pre></td></tr></table></figure>
<p>[2]</p>
<p>This is our orignal design. But now it seems it is wrong. Now I think the concept<br>of RC should be mapped to PCIe domain or a segment. So multiple ITS maps for<br>one RC can be configured as in [1].</p>
<p>Same rule can be applied for SMMU configuration.</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>Makefile再学习</title>
    <url>/Makefile%E5%86%8D%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="make和Makefile的基本逻辑"><a href="#make和Makefile的基本逻辑" class="headerlink" title="make和Makefile的基本逻辑"></a>make和Makefile的基本逻辑</h2><p> make以及Makefile完成的最基本的功能是，根据定义的依赖关系，决定是否执行相应的命令。<br> 相关的依赖描述可以写在默认名字的文件里，也可以用make -f决定使用哪个makefile文件。</p>
<p> makefile文件只有一个最终目标target，是make解析makefile文件遇到的第一个target，<br> 但是，这并不是说只能编译出一个程序。可以像 all：target1 target2 target3 这样定义<br> 多个target，可以make all一次编译出所有target，也可以make target1，只编译出target1。</p>
<p> make也支持只有target没有被依赖对象的定义，比如我们看到的clean目标，这是这样的目标<br> 叫做伪目标，要执行伪目标对应的命令需要显示的使用make，比如make clean。伪目标一般<br> 放到makefile文件的后面，并用.PHONY这样的伪目标定义下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.PHONY: clean</span><br><span class="line">	rm *.o</span><br></pre></td></tr></table></figure>
<p> 伪目标可以支持和程序编译无关的一起操作，比如，程序的打包、安装、删除等等。</p>
<p> 可以想象要支持复杂的功能，make会定义自己的变量语法，流程控制的语法和函数。这些<br> 语法的定义都和shell的里的定义长的差不多。学习Makefile也就是对这些语法的学习。</p>
<p> make的官方指导文档在<span class="exturl" data-url="aHR0cHM6Ly93d3cuZ251Lm9yZy9zb2Z0d2FyZS9tYWtlL21hbnVhbC9odG1sX25vZGUv">这里<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p> 变量在makefile里可以理解为一个宏，make在运行时会做变量展开，这个变量展开的逻辑里<br> 有立即展开(immediate)和延后展开(deferred)两个概念，我们后面再具体看变量展开的细节。<br> 一般变量直接定义，使用$()来引用，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PREFIX=/usr/local</span><br><span class="line">SBINDIR=$(PREFIX)/sbin</span><br></pre></td></tr></table></figure>
<p> 变量的定义方式还可以用 := , ?= 和 +=。这些变量定义的方式定义变量的行为都有所<br> 不同，=定义变量，变量的右值可以出现在定义的后面，:=中变量的右值只能在定义之前，<br> 否则右值相当于是空的，?=会看下之前有没有定义过变量，如果有就覆盖掉之前的，覆盖的<br> 逻辑和=一样(和:=不一样)，+=是在变量上做追加。</p>
<p> 上面讲的各种不同定义变量的方式看起来有点神叨叨的，不过我们继续看下变量展开的内在<br> 逻辑，就可以理解这里的关键了。Makefile的运行逻辑大体分两个步骤，第一步是把Makefile<br> 文件读进来进行依赖解析，第二步是根据依赖关系执行对应的命令，所以变量的展开也分如上<br> 提到的立即展开和延后展开，立即展开是在第一步解析时就展开，延后展开是在随后的执行<br> 过程中展开。</p>
<p> 举个例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">B = C</span><br><span class="line">A := $(B)</span><br></pre></td></tr></table></figure>
<p> 如果如上第二个定义的右值是延后展开的话，第一步解析完A的值就是$(B)这个字符串本身，<br> 注意不是C。</p>
<p> 整个Makefile文件中，各个地方都可能定义变量，那每个地方的变量是立即展开还是延后<br> 展开? make具体定义了各种情况的展开方式，具体可以查make的<span class="exturl" data-url="aHR0cHM6Ly93d3cuZ251Lm9yZy9zb2Z0d2FyZS9tYWtlL21hbnVhbC9tYWtlLmh0bWwvI1JlYWRpbmctTWFrZWZpbGVz">文档<i class="fa fa-external-link-alt"></i></span>。可以看到变量的不同定义中<br> 展开方式是不一样的: immediate = deferred, immediate := immediate, immediate += deferred or immediate。<br> 可以看到=的方式中右值一定是deferred，而:=的右值一定是immediate，所以上面的例子里<br> A会在第一步解析后是C。</p>
<p> 变量有其相关的作用域。一般定义的变量的作用域就是本makefile。使用export可以把所有<br> 当前makefile中的变量export到子makefile中，也可以export A只把A变量export到子makefile中。<br> makefile里的变量，如果和环境变量一样，就会覆盖环境变量。还可以使用目标变量，把<br> 一个变量的作为范围只绑定到对应目标的命令上，目标变量的定义在下面pciutils makefile<br> 的分析中有提及。</p>
<p> 自动变量，$@, $&lt;, $^, 其中$@表示target的集合，$&lt;表示依赖中的第一个名字，$^表示<br> 依赖目标的集合。</p>
<p> make里的函数也可以看成是自定义的变量，和变量一样有各种定义形式，对应的是不同定义<br> 的展开方式不一样，比如=的定义rule是immediate，内容是deferred。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">define rule=</span><br><span class="line">xxxx</span><br><span class="line">xxxx</span><br><span class="line">endef</span><br></pre></td></tr></table></figure>
<p> 因为可以给自定义的函数传参，这种方式可以结合循环可以批量的定义规则。这里我们放<br> 一个makefile的小例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">_all=a b c d</span><br><span class="line">__all=$(patsubst %, %_lib, $(_all))</span><br><span class="line"></span><br><span class="line">all: $(_all) $(__all)</span><br><span class="line"></span><br><span class="line">define test_rule=</span><br><span class="line">$(1): test.o</span><br><span class="line">	gcc test.o -o $(1)</span><br><span class="line">$(1)_lib: test.o lib.o</span><br><span class="line">	gcc test.o lib.o -DLIB -o $(1)_lib</span><br><span class="line">endef</span><br><span class="line"></span><br><span class="line">$(foreach n,$(_all),$(eval $(call test_rule,$(n))))</span><br><span class="line"></span><br><span class="line">.PHONY: clean</span><br><span class="line">clean:</span><br><span class="line">	rm $(_all) $(__all) *.o</span><br></pre></td></tr></table></figure>
<p> 可以看见这里用一个自定义的函数，对a/b/c/d每一个字符都生成了x和x_lib的目标。</p>
<h2 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h2><p> makefile可以把另一个makefile include进来。比如下面pciutils makefile中的<br> -include lib/config.mk</p>
<p> make可以进入另外的目录执行。比如pciutils makefile中的$(MAKE) -C lib all<br> 这个会进入lib目录，运行lib里的Makefile，进入退出lib目录的时候会有log提示。</p>
<p> make中有分支语句。一般是，判断变量的值，然后根据判断结果决定是否执行命令，或者<br> 有相关的依赖条件。</p>
<p> make中有支持循环的函数。其语法是：$(foreach <var>,<list>,<text>)，其中list是变量<br> list，text是执行的表达式，var是每次取出的变量。比如：</text></list></var></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">names := a b c d</span><br><span class="line">files := $(foreach n,$(names),$(n).o)</span><br></pre></td></tr></table></figure>
<p> $(files)的值是 a.o b.o c.o d.o</p>
<p> make还有条件语句的支持，大概可以分为：1. 判断变量有无定义的条件语句(#ifdef-#else-#endif)，<br> 2. 判断两个变量关系的条件语句(#ifeq ($(xxx), $(xxx))-#else-#endif)。</p>
<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p> 我们把Makefile里常用的函数收集在这里。</p>
<p> patsubst</p>
<p>  $(patsubst <pattern>,<replacement>,<text>)，语意是把text里的符合pattern模式的<br>  单词替换成replacement，其中pattern/replacement里可以用通配符%表示任意字符串。</text></replacement></pattern></p>
<p>  比如，$(patsubst %.o, %.c, a.o b.o c.o)替换后的结果是a.c b.c c.c</p>
<p> call</p>
<p>  $(call expression,para1,para2,para3)，语意是把参数para1/2/3传给表达式里的$(1)/$(2)/$(3)。<br>  注意这里看起来是函数，其实就是自定义的一个表达式而已。关于自定义表达式的语法，<br>  可以参考上面的介绍。</p>
<p> eval</p>
<p>  $(eval expression)，展开变量或者函数。</p>
<p> foreach</p>
<p>  $(foreach var,list,text)，语意是从list里依次取出一个变量赋给var，然后进行text里<br>  定义的运算，返回运算的结果。比如，$(foreach n,a b c,$(n).o)返回a.o b.o c.o。</p>
<p> info/error/warning</p>
<p>  $(info xxx)，这样使用可以在打印xxx的内容，error/warning的用法类似。似乎info是在<br>  第一步解析Makefile时做打印的。</p>
<h2 id="一些高级用法"><a href="#一些高级用法" class="headerlink" title="一些高级用法"></a>一些高级用法</h2><p> make可以自动推导依赖关闭和要执行的命令，这叫make的隐含规则，其实就是对一些基本<br> 写法的省略表示。比如，对于.o的target，make自动推导依赖里有相同名字的.c存在，自动<br> 可以自动推导出基本的编译命令。</p>
<p> 比如，如果编译链条中需要通过file.c编译生成file.o，这个规则是全部不用写的，make<br> 会使用隐含规则自动都补上，但是make补齐的时候是按照一定的模版来的，比如这里make<br> 使用$(CC) $(CFLAGS) -c file.c -o file.o生成命令，所以如果你期望的编译命令不是这样<br> 的，就需要自己定义。</p>
<p> 你也可以自定义模版，就是自己定义规则，自己定义的规则还可以覆盖make的隐含规则。<br> 比如，可以把.c到.o的规则定义成如下，这样这个makefile里对应的隐含规则就被覆盖了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%.o: %.c</span><br><span class="line">	gcc --static -g -c $&lt; -o $@</span><br></pre></td></tr></table></figure>

<p> (持续增加…)</p>
<h2 id="pciutils-Makefile分析"><a href="#pciutils-Makefile分析" class="headerlink" title="pciutils Makefile分析"></a>pciutils Makefile分析</h2><p> 我们把pciutils Makefile copy到这里，然后逐行分析下。pciutils的github地址在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3BjaXV0aWxzL3BjaXV0aWxzLmdpdA==">这里<i class="fa fa-external-link-alt"></i></span>。<br> 我们直接用注释的方式写分析。用这个makefile做例子分析，有个缺点，是这个里面没有<br> 使用make的函数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Makefile for The PCI Utilities</span><br><span class="line"># (c) 1998--2020 Martin Mares &lt;mj@ucw.cz&gt;</span><br><span class="line"></span><br><span class="line">OPT=-O2</span><br><span class="line"></span><br><span class="line"># 定义编译参数。make的隐含规则会自动的把这个编译参数加到编译里。</span><br><span class="line"># 这里真是有点行业黑话的意思了 :)</span><br><span class="line">CFLAGS=$(OPT) -Wall -W -Wno-parentheses -Wstrict-prototypes -Wmissing-prototypes</span><br><span class="line"></span><br><span class="line">VERSION=3.7.0</span><br><span class="line">DATE=2020-05-31</span><br><span class="line"></span><br><span class="line"># Host OS and release (override if you are cross-compiling)</span><br><span class="line">HOST=</span><br><span class="line">RELEASE=</span><br><span class="line">CROSS_COMPILE=</span><br><span class="line"></span><br><span class="line"># Support for compressed pci.ids (yes/no, default: detect)</span><br><span class="line">ZLIB=</span><br><span class="line"></span><br><span class="line"># Support for resolving ID&#x27;s by DNS (yes/no, default: detect)</span><br><span class="line">DNS=</span><br><span class="line"></span><br><span class="line"># Build libpci as a shared library (yes/no; or local for testing; requires GCC)</span><br><span class="line">SHARED=no</span><br><span class="line"></span><br><span class="line"># Use libkmod to resolve kernel modules on Linux (yes/no, default: detect)</span><br><span class="line">LIBKMOD=</span><br><span class="line"></span><br><span class="line"># Use libudev to resolve device names using hwdb on Linux (yes/no, default: detect)</span><br><span class="line">HWDB=</span><br><span class="line"></span><br><span class="line"># ABI version suffix in the name of the shared library</span><br><span class="line"># (as we use proper symbol versioning, this seldom needs changing)</span><br><span class="line">ABI_VERSION=.3</span><br><span class="line"></span><br><span class="line"># Installation directories</span><br><span class="line">PREFIX=/usr/local</span><br><span class="line">SBINDIR=$(PREFIX)/sbin</span><br><span class="line">SHAREDIR=$(PREFIX)/share</span><br><span class="line">IDSDIR=$(SHAREDIR)</span><br><span class="line"></span><br><span class="line"># 通过shell调用shell命令，这里判断特定的安装目录是否存在。</span><br><span class="line">MANDIR:=$(shell if [ -d $(PREFIX)/share/man ] ; then echo $(PREFIX)/share/man ; else echo $(PREFIX)/man ; fi)</span><br><span class="line">INCDIR=$(PREFIX)/include</span><br><span class="line">LIBDIR=$(PREFIX)/lib</span><br><span class="line">PKGCFDIR=$(LIBDIR)/pkgconfig</span><br><span class="line"></span><br><span class="line"># Commands</span><br><span class="line">INSTALL=install</span><br><span class="line">DIRINSTALL=install -d</span><br><span class="line">STRIP=-s</span><br><span class="line">CC=$(CROSS_COMPILE)gcc</span><br><span class="line">AR=$(CROSS_COMPILE)ar</span><br><span class="line">RANLIB=$(CROSS_COMPILE)ranlib</span><br><span class="line"></span><br><span class="line"># Base name of the library (overridden on NetBSD, which has its own libpci)</span><br><span class="line">LIBNAME=libpci</span><br><span class="line"></span><br><span class="line"># 使用include关键字把lib下的配置文件包含进来。可以看到config.mk只是定义了一些变量。</span><br><span class="line"># 这里include之前的-表示忽略include过程中的报错。</span><br><span class="line">-include lib/config.mk</span><br><span class="line"></span><br><span class="line">PCIINC=lib/config.h lib/header.h lib/pci.h lib/types.h lib/sysdep.h</span><br><span class="line">PCIINC_INS=lib/config.h lib/header.h lib/pci.h lib/types.h</span><br><span class="line"></span><br><span class="line"># export这个makefile中的所有的变量到子make流程里，比如，下面使用$(MAKE) -C lib all</span><br><span class="line"># 进入lib目录，调用lib目录里的Makefile文件编译其中定义的target，lib Makefile里就</span><br><span class="line"># 可以看见本Makefile里定义的所有变量。</span><br><span class="line">export</span><br><span class="line"></span><br><span class="line"># 定义这个Makefile的总目标all，可以all又有一堆依赖，可以make all编译出所有，也可以</span><br><span class="line"># make加一个依赖，只编译出其中的一个target。</span><br><span class="line">all: lib/$(PCILIB) lspci setpci example lspci.8 setpci.8 pcilib.7 pci.ids.5 update-pciids update-pciids.8 $(PCI_IDS)</span><br><span class="line"></span><br><span class="line"># 这里的定义是，如果头文件有变动，那么重新编译lib。似乎也有道理，头文件不动的话，</span><br><span class="line"># 依然可以正常link，如果头文件都变动了，link一定会错。至于，头文件不同，lib的实现</span><br><span class="line"># 变动了的情况，上面也只是单独编译lib。</span><br><span class="line">#</span><br><span class="line"># force是不管依赖条件，强制都跑下面命令的意思。这里的force和下面的force是一起的</span><br><span class="line"># 逻辑，make的手册里相关的解释：gnu.org/software/make/manual/html_node/Force-Targets.html</span><br><span class="line">#</span><br><span class="line"># 简单讲就是force作为一个没有依赖，没有命令的伪目标，make认识force每次都update，</span><br><span class="line"># 所以依赖force的target也没有都要执行下。</span><br><span class="line">#</span><br><span class="line">lib/$(PCILIB): $(PCIINC) force</span><br><span class="line">	$(MAKE) -C lib all</span><br><span class="line"></span><br><span class="line">force:</span><br><span class="line"></span><br><span class="line"># 对于多目标的语法，一般拆开理解，比如下面的两个目标就可以拆开成：</span><br><span class="line"># lib/config.h:</span><br><span class="line"># 	cd lib &amp;&amp; ./configure</span><br><span class="line"># lib/config.mk:</span><br><span class="line"># 	cd lib &amp;&amp; ./configure</span><br><span class="line">#</span><br><span class="line"># 但是，拆开也比较费解?</span><br><span class="line">#</span><br><span class="line">lib/config.h lib/config.mk:</span><br><span class="line">	cd lib &amp;&amp; ./configure</span><br><span class="line"></span><br><span class="line"># 如下的编译命令都是由make的隐含规则推导出来的。</span><br><span class="line">lspci: lspci.o ls-vpd.o ls-caps.o ls-caps-vendor.o ls-ecaps.o ls-kernel.o ls-tree.o ls-map.o common.o lib/$(PCILIB)</span><br><span class="line">setpci: setpci.o common.o lib/$(PCILIB)</span><br><span class="line"></span><br><span class="line">LSPCIINC=lspci.h pciutils.h $(PCIINC)</span><br><span class="line">lspci.o: lspci.c $(LSPCIINC)</span><br><span class="line">ls-vpd.o: ls-vpd.c $(LSPCIINC)</span><br><span class="line">ls-caps.o: ls-caps.c $(LSPCIINC)</span><br><span class="line">ls-ecaps.o: ls-ecaps.c $(LSPCIINC)</span><br><span class="line">ls-kernel.o: ls-kernel.c $(LSPCIINC)</span><br><span class="line">ls-tree.o: ls-tree.c $(LSPCIINC)</span><br><span class="line">ls-map.o: ls-map.c $(LSPCIINC)</span><br><span class="line"></span><br><span class="line">setpci.o: setpci.c pciutils.h $(PCIINC)</span><br><span class="line">common.o: common.c pciutils.h $(PCIINC)</span><br><span class="line"></span><br><span class="line"># 这里的一个点是目标变量，他的语法是这样的：</span><br><span class="line">#</span><br><span class="line"># &lt;target&gt;: &lt;variable-assignment&gt;</span><br><span class="line"># 语义是限定变量的作用范围，比如，如下，lspci的所有相关依赖命令中LDLIBS都用这里</span><br><span class="line"># 赋予的值。CFLAGS的含义是一样的。</span><br><span class="line">#</span><br><span class="line"># LDLIBS, CFLAGS成了lspci，ls-kernel.o的相关命令的局部变量。</span><br><span class="line">lspci: LDLIBS+=$(LIBKMOD_LIBS)</span><br><span class="line">ls-kernel.o: CFLAGS+=$(LIBKMOD_CFLAGS)</span><br><span class="line"></span><br><span class="line"># 如果update-pciids.sh有变化，要重新生成下update-pciids, sed命令没有看懂？</span><br><span class="line">update-pciids: update-pciids.sh</span><br><span class="line">	sed &lt;$&lt; &gt;$@ &quot;s@^DEST=.*@DEST=$(IDSDIR)/$(PCI_IDS)@;s@^PCI_COMPRESSED_IDS=.*@PCI_COMPRESSED_IDS=$(PCI_COMPRESSED_IDS)@&quot;</span><br><span class="line">	chmod +x $@</span><br><span class="line"></span><br><span class="line"># The example of use of libpci</span><br><span class="line">example: example.o lib/$(PCILIB)</span><br><span class="line">example.o: example.c $(PCIINC)</span><br><span class="line"></span><br><span class="line"># 定义模式规则。所有，xxx: xxx.o的编译命令都使用如下的隐含规则。</span><br><span class="line">#</span><br><span class="line">%: %.o</span><br><span class="line">	$(CC) $(LDFLAGS) $(TARGET_ARCH) $^ $(LDLIBS) -o $@ --static -L../zlib</span><br><span class="line"></span><br><span class="line"># 多target + 定义模式规则。可以展开成:</span><br><span class="line">#</span><br><span class="line"># %.8: %.man</span><br><span class="line"># 	command</span><br><span class="line"># %.7: %.man</span><br><span class="line"># 	command</span><br><span class="line"># %.5: %.man</span><br><span class="line"># 	command</span><br><span class="line">#</span><br><span class="line">%.8 %.7 %.5: %.man</span><br><span class="line">	M=`echo $(DATE) | sed &#x27;s/-01-/-January-/;s/-02-/-February-/;s/-03-/-March-/;s/-04-/-April-/;s/-05-/-May-/;s/-06-/-June-/;s/-07-/-July-/;s/-08-/-August-/;s/-09-/-September-/;s/-10-/-October-/;s/-11-/-November-/;s/-12-/-December-/;s/\(.*\)-\(.*\)-\(.*\)/\3 \2 \1/&#x27;` ; sed &lt;$&lt; &gt;$@ &quot;s/@TODAY@/$$M/;s/@VERSION@/pciutils-$(VERSION)/;s#@IDSDIR@#$(IDSDIR)#&quot;</span><br><span class="line"></span><br><span class="line">ctags:</span><br><span class="line">	rm -f tags</span><br><span class="line">	find . -name &#x27;*.[hc]&#x27; -exec ctags --append &#123;&#125; +</span><br><span class="line"></span><br><span class="line">TAGS:</span><br><span class="line">	rm -f TAGS</span><br><span class="line">	find . -name &#x27;*.[hc]&#x27; -exec etags --append &#123;&#125; +</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">	rm -f `find . -name &quot;*~&quot; -o -name &quot;*.[oa]&quot; -o -name &quot;\#*\#&quot; -o -name TAGS -o -name core -o -name &quot;*.orig&quot;`</span><br><span class="line">	rm -f update-pciids lspci setpci example lib/config.* *.[578] pci.ids.gz lib/*.pc lib/*.so lib/*.so.* tags</span><br><span class="line">	rm -rf maint/dist</span><br><span class="line"></span><br><span class="line">distclean: clean</span><br><span class="line"></span><br><span class="line"># install伪目标依赖全部的目标。make install 会执行到下一个依赖之前。</span><br><span class="line">install: all</span><br><span class="line"># -c is ignored on Linux, but required on FreeBSD</span><br><span class="line">	$(DIRINSTALL) -m 755 $(DESTDIR)$(SBINDIR) $(DESTDIR)$(IDSDIR) $(DESTDIR)$(MANDIR)/man8 $(DESTDIR)$(MANDIR)/man7 $(DESTDIR)/$(MANDIR)/man5</span><br><span class="line">	$(INSTALL) -c -m 755 $(STRIP) lspci setpci $(DESTDIR)$(SBINDIR)</span><br><span class="line">	$(INSTALL) -c -m 755 update-pciids $(DESTDIR)$(SBINDIR)</span><br><span class="line">	$(INSTALL) -c -m 644 $(PCI_IDS) $(DESTDIR)$(IDSDIR)</span><br><span class="line">	$(INSTALL) -c -m 644 lspci.8 setpci.8 update-pciids.8 $(DESTDIR)$(MANDIR)/man8</span><br><span class="line">	$(INSTALL) -c -m 644 pcilib.7 $(DESTDIR)$(MANDIR)/man7</span><br><span class="line">	$(INSTALL) -c -m 644 pci.ids.5 $(DESTDIR)$(MANDIR)/man5</span><br><span class="line">ifeq ($(SHARED),yes)</span><br><span class="line">ifeq ($(LIBEXT),dylib)</span><br><span class="line">	ln -sf $(PCILIB) $(DESTDIR)$(LIBDIR)/$(LIBNAME)$(ABI_VERSION).$(LIBEXT)</span><br><span class="line">else</span><br><span class="line">	ln -sf $(PCILIB) $(DESTDIR)$(LIBDIR)/$(LIBNAME).$(LIBEXT)$(ABI_VERSION)</span><br><span class="line">endif</span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line">ifeq ($(SHARED),yes)</span><br><span class="line">install: install-pcilib</span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line">install-pcilib: lib/$(PCILIB)</span><br><span class="line">	$(DIRINSTALL) -m 755 $(DESTDIR)$(LIBDIR)</span><br><span class="line">	$(INSTALL) -c -m 644 lib/$(PCILIB) $(DESTDIR)$(LIBDIR)</span><br><span class="line"></span><br><span class="line">install-lib: $(PCIINC_INS) lib/$(PCILIBPC) install-pcilib</span><br><span class="line">	$(DIRINSTALL) -m 755 $(DESTDIR)$(INCDIR)/pci $(DESTDIR)$(PKGCFDIR)</span><br><span class="line">	$(INSTALL) -c -m 644 $(PCIINC_INS) $(DESTDIR)$(INCDIR)/pci</span><br><span class="line">	$(INSTALL) -c -m 644 lib/$(PCILIBPC) $(DESTDIR)$(PKGCFDIR)</span><br><span class="line">ifeq ($(SHARED),yes)</span><br><span class="line">ifeq ($(LIBEXT),dylib)</span><br><span class="line">	ln -sf $(LIBNAME)$(ABI_VERSION).$(LIBEXT) $(DESTDIR)$(LIBDIR)/$(LIBNAME).$(LIBEXT)</span><br><span class="line">else</span><br><span class="line">	ln -sf $(LIBNAME).$(LIBEXT)$(ABI_VERSION) $(DESTDIR)$(LIBDIR)/$(LIBNAME).$(LIBEXT)</span><br><span class="line">endif</span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line">uninstall: all</span><br><span class="line">	rm -f $(DESTDIR)$(SBINDIR)/lspci $(DESTDIR)$(SBINDIR)/setpci $(DESTDIR)$(SBINDIR)/update-pciids</span><br><span class="line">	rm -f $(DESTDIR)$(IDSDIR)/$(PCI_IDS)</span><br><span class="line">	rm -f $(DESTDIR)$(MANDIR)/man8/lspci.8 $(DESTDIR)$(MANDIR)/man8/setpci.8 $(DESTDIR)$(MANDIR)/man8/update-pciids.8</span><br><span class="line">	rm -f $(DESTDIR)$(MANDIR)/man7/pcilib.7</span><br><span class="line">ifeq ($(SHARED),yes)</span><br><span class="line">	rm -f $(DESTDIR)$(LIBDIR)/$(PCILIB) $(DESTDIR)$(LIBDIR)/$(LIBNAME).so$(ABI_VERSION)</span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">pci.ids.gz: pci.ids</span><br><span class="line">	gzip -9n &lt;$&lt; &gt;$@</span><br><span class="line"></span><br><span class="line">.PHONY: all clean distclean install install-lib uninstall force tags TAGS</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>Makefile</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI SMMU parse in ACPI</title>
    <url>/PCI-SMMU-parse-in-ACPI/</url>
    <content><![CDATA[<h2 id="acpi-smmu-parse"><a href="#acpi-smmu-parse" class="headerlink" title="acpi smmu parse"></a>acpi smmu parse</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">acpi_init</span><br><span class="line">    --&gt; acpi_iort_init</span><br><span class="line">            /* parse the SMMU node in IORT table */</span><br><span class="line">        --&gt; iort_init_platform_devices</span><br><span class="line">                /* create a platform_device for SMMU, and add it to platform_bus */</span><br><span class="line">            --&gt; iort_add_smmu_platform_device</span><br><span class="line">                    /* why call this function here? */</span><br><span class="line">                --&gt; acpi_dma_configure</span><br><span class="line">                    --&gt; iort_iommu_configure</span><br><span class="line"></span><br><span class="line">            /* call functions in section: __iort_acpi_probe_table ~ __iort_acpi_probe_table_end */</span><br><span class="line">        --&gt; acpi_probe_device_table(iort)</span><br></pre></td></tr></table></figure>
<h2 id="put-arm-smmu-init-in-above-section-in-compile-phase"><a href="#put-arm-smmu-init-in-above-section-in-compile-phase" class="headerlink" title="put arm_smmu_init in above section in compile phase"></a>put arm_smmu_init in above section in compile phase</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IORT_ACPI_DECLARE(arm_smmu_v3, ACPI_SIG_IORT, acpi_smmu_v3_init);</span><br><span class="line">arm_smmu_v3_init </span><br><span class="line">        /*</span><br><span class="line">         * add SMMU driver to platform_bus, this will trigger probe function to</span><br><span class="line">         * bind this driver and above SMMU platform_device.</span><br><span class="line">         */</span><br><span class="line">    --&gt; platform_driver_register(&amp;arm_smmu_driver)</span><br></pre></td></tr></table></figure>
<h2 id="PCIe-device-get-its-SMMU-device"><a href="#PCIe-device-get-its-SMMU-device" class="headerlink" title="PCIe device get its SMMU device"></a>PCIe device get its SMMU device</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_device_add</span><br><span class="line">    --&gt; pci_dma_configure(dev)</span><br><span class="line">        --&gt; acpi_dma_configure(&amp;dev-&gt;dev, attr)</span><br><span class="line"></span><br><span class="line">acpi_dma_configure</span><br><span class="line">       /* if dev is a pci device, first find related RC node in IORT table */</span><br><span class="line">    --&gt;iommu = iort_iommu_configure </span><br><span class="line">            /* RC node */</span><br><span class="line">        --&gt; node = iort_scan_node</span><br><span class="line">                /* parent here is SMMU node */</span><br><span class="line">            --&gt; parent = iort_node_map_rid</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI/SMMU ATS analysis note</title>
    <url>/PCI-SMMU-ATS-analysis-note/</url>
    <content><![CDATA[<h2 id="ATS"><a href="#ATS" class="headerlink" title="ATS"></a>ATS</h2><p>ATS is the short of Access Translation Service. In the PCIe spec, the ATS section<br>includes ATS, PRI and PASID. Here we only consider ATS.</p>
<p>Generally speaking, ATS just prefetch the PA(physical address) to store in PCIe<br>device’s side. Once this PCIe device sending a memory read/write operation<br>(e.g memory read/write TLP), it will bring a flag which means that the address<br>of this operation had already been translated. When this operation passes SMMU,<br>SMMU will parse this flag and know that this address is a PA and SMMU will do<br>nothing about this operation, SMMU then will just forward this operation to<br>read/write memory.</p>
<p>The hardware will do the prefetch automatically. But software must be involved<br>when we want to disable/invalidate a prefetched PA in PCIe device.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    +-------+</span><br><span class="line">    |  CPU  |</span><br><span class="line">    +---+---+</span><br><span class="line">        |</span><br><span class="line">--------+-----------------------------+----</span><br><span class="line">        |        PA   ^               |</span><br><span class="line">    +---+---+    ^    |            +--+--+</span><br><span class="line">    | SMMU  |    |    |            | DDR |</span><br><span class="line">    +---+---+    |    |            +-----+</span><br><span class="line">        |        VA   |</span><br><span class="line">    +---+---+         |</span><br><span class="line">    |  RP   |         |</span><br><span class="line">    +---+---+         |</span><br><span class="line">        |             |</span><br><span class="line">    +---+---+         PA</span><br><span class="line">    |  EP   |</span><br><span class="line">    +-------+</span><br></pre></td></tr></table></figure>

<p>From the above figure, with SMMU enable and without ATS enable, read/write<br>operation from EP will be with a VA, SMMU help to do the translation from VA to<br>PA. However, when enabling ATS, PA can be stored in EP ahead, EP can send<br>read/write operation with a PA directly, which will improve the performance.</p>
<h2 id="Hareware-support"><a href="#Hareware-support" class="headerlink" title="Hareware support"></a>Hareware support</h2><p> For EP that supports ATS must have ATS Extended Capability in its CFG. In ATS<br> cap, we have ATS capability registers and ATS controller registers. Now all<br> bits in ATS capability registers are RO, STU and Enable in ATS control registers<br> are RW. STU means smallest translation unit, which should be matched with SMMU<br> supported page size.</p>
<p> For SMMU, we should enable ATS by IDR0_ATS in SMMU_IDR0.ATS.</p>
<ul>
<li><p>ATS enable</p>
<p>We should enable SMMU_IDR0.ATS and enable bit in PCIe device’s ATS cap.</p>
</li>
<li><p>ATS request</p>
<p>I think the ATS request which will be followed by a ATS completion is triggered<br>automatically. PCIe ATS spec says “Host system software can not modify the ATC”,<br>which is used to store the PA in PCIe device.</p>
<p>I think the ATS request will be device specific and be implemented together<br>with the DMA of PCIe device. There is only one PCIe EP card(Mellanox ConnectX-5)<br>supported ATS, ATS in PCIe device(EP) maybe will implemented together with<br>EP’s DMA, which means DMA will still get a VA from SMMU as target address,<br>however, before DMA operation, EP will firstly send a ATS request to get the<br>related PA of above VA, then EP will send DMA read/write operation with this<br>PA. But how many does EP send ATS request? For example, if 8K size DMA range<br>has been requested, and STU is 4K for system, will EP send 2 ATS request 2 PA?<br>This also has relationship with the process of ATS invalidation. From the code<br>below, it will invalidate a range of iova as the inputs of arm_smmu_atc_inv_to_cmd<br>include iova and size. If the range of iova to iova + size covers multiple<br>stu/page, what will happen in the hardware?</p>
</li>
<li><p>ATS invalidate</p>
<p>When VA -&gt; PA changed in SMMU, PA in PCIe device’s ATC is not the correct<br>address, so host software should invalidate the cached PA by leting SMMU<br>send an ATS invalidation request. ATS invalidation completion will be sent<br>from the PCIe device later.</p>
<p>SMMU will use command CMD_ATC_INV to invalidate the cached PA in EP.</p>
</li>
</ul>
<h2 id="software-support"><a href="#software-support" class="headerlink" title="software support"></a>software support</h2><p> For analysis will base on [PATCH 0/7] Add PCI ATS support to SMMUv3, we can<br> find this in git://linux-arm.org/linux-jpb.git branch: svm/ats-v1.</p>
<ul>
<li><p>ATS enable:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">arm_smmu_add_device</span><br><span class="line">    --&gt; arm_smmu_enable_ats</span><br><span class="line">            /* will configure stu in this function, stu here comes from smmu */</span><br><span class="line">        --&gt; pci_enable_ats(pdev, stu)</span><br></pre></td></tr></table></figure></li>
<li><p>ATS invalidate:</p>
<p>There are two places to call ATC_INV:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*    arm_smmu_detach_dev</span><br><span class="line">         --&gt; arm_smmu_atc_inv_master_all</span><br><span class="line"></span><br><span class="line">*    arm_smmu_unmap(struct iommu_domain *domain, unsigned long iova, size_t size)</span><br><span class="line">         --&gt; arm_smmu_atc_inv_domain(smmu_domain, 0, iova, size)</span><br><span class="line">                 /* it has an algrithm to caculate the invalidate page */</span><br><span class="line">             --&gt; arm_smmu_atc_inv_to_cmd(ssid, iova, size, &amp;cmd)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">arm_smmu_unmap input: iommu domain, iova, size</span><br><span class="line">       /*</span><br><span class="line">        * we pick each device in this iommu domain to do the invalidation operation,</span><br><span class="line">        * normally, we have one device in one iommu domain.</span><br><span class="line">        * </span><br><span class="line">        * here struct arm_smmu_master_data is for every device under a SMMU, and</span><br><span class="line">        * it has been created in arm_smmu_add_device.</span><br><span class="line">        */</span><br><span class="line">    -&gt; arm_smmu_atc_inv_domain  input: smmu_domain, ssid, iova, size</span><br><span class="line">        --&gt; arm_smmu_atc_inv_master: </span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI parse MEM/IO range in _CRS in ACPI table</title>
    <url>/PCI-parse-MEM-IO-range-in-CRS-in-ACPI-table/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_acpi_scan_root(struct acpi_pci_root *root)</span><br><span class="line">    --&gt; root_ops-&gt;prepare_resources = pci_acpi_root_prepare_resources</span><br><span class="line">        --&gt; acpi_pci_root_create</span><br><span class="line">            ...</span><br><span class="line">            --&gt; ops-&gt;prepare_resources(info)</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">pci_acpi_root_prepare_resources</span><br><span class="line">    --&gt; acpi_pci_probe_root_resources(ci)</span><br><span class="line">        ...</span><br><span class="line">            /* this is the key function */</span><br><span class="line">        --&gt; acpi_dev_get_resources</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">/* will analyze acpi_dev_get_resources below */</span><br><span class="line">acpi_dev_get_resources(device, list, acpi_dev_filter_resource_type_cb, (void *)flags);</span><br><span class="line">    --&gt; c.list = list;</span><br><span class="line">	c.preproc = preproc; /* acpi_dev_filter_resource_type_cb */</span><br><span class="line">	c.preproc_data = preproc_data; /* (void *)flags */</span><br><span class="line">	c.count = 0;</span><br><span class="line">	c.error = 0;</span><br><span class="line">        /*</span><br><span class="line">         * will parse _CRS method for above device, acpi_walk_resources is defined</span><br><span class="line">         * in drivers/acpi/acpica/rsxface.c</span><br><span class="line">         *</span><br><span class="line">         * acpi_dev_process_resource will be called for every resources in _CRS,</span><br><span class="line">         * it parses a resource and adds it into the list in c.list.</span><br><span class="line">         *</span><br><span class="line">         * this means e.g. if there are 3 MEM/IO range, acpi_dev_process_resource</span><br><span class="line">         * will be called three times.</span><br><span class="line">         */</span><br><span class="line">    --&gt; acpi_walk_resources(adev-&gt;handle, METHOD_NAME__CRS, acpi_dev_process_resource, &amp;c);</span><br><span class="line"></span><br><span class="line">/* details of acpi_dev_process_resource */</span><br><span class="line">static acpi_status acpi_dev_process_resource(struct acpi_resource *ares, void *context)</span><br><span class="line">        /* filter the resource type to some kinds */</span><br><span class="line">    --&gt; c-&gt;preproc(ares, c-&gt;preproc_data)</span><br><span class="line">    ...</span><br><span class="line">        /*</span><br><span class="line">         * still do not know the difference of below two functions, prefetchable</span><br><span class="line">         * attribute will be parsed in acpi_decode_space function.</span><br><span class="line">         */</span><br><span class="line">    --&gt; acpi_dev_resource_memory(ares, res)</span><br><span class="line">    --&gt; acpi_dev_resource_address_space(ares, &amp;win)</span><br><span class="line">        --&gt; acpi_decode_space</span><br><span class="line">            --&gt; if (addr-&gt;info.mem.caching == ACPI_PREFETCHABLE_MEMORY)</span><br><span class="line">    ...</span><br><span class="line">        /* add resource to list */</span><br><span class="line">    --&gt; acpi_dev_new_resource_entry(&amp;win, c)</span><br><span class="line">        --&gt; resource_list_add_tail(rentry, c-&gt;list);</span><br></pre></td></tr></table></figure>

<h2 id="Some-ideas-about-PCI-prefetch-window-and-memory-type-attribute-of-AMRv8-1"><a href="#Some-ideas-about-PCI-prefetch-window-and-memory-type-attribute-of-AMRv8-1" class="headerlink" title="Some ideas about PCI prefetch window and memory type/attribute of AMRv8[1]"></a>Some ideas about PCI prefetch window and memory type/attribute of AMRv8[1]</h2><p>PCI prefetch and memory type/attribute has no relationship with each other,<br>as PCI prefetch is an attribute in PCI domain, and memory type/attribute is for<br>ARM bus domain. Further more, ATU(address translate unit: translate a CPU address<br>to PCI address) only translate address, do nothing about prefetch support.</p>
<p>Generally, when we get cpu physical address(PA) for a BAR, PCI device driver will<br>use ioremap(or other ioremap_*) to get a VA. memory type/attribute will be set<br>to MMU in ioremap kind of functions to support memory operations in ARM bus domain.</p>
<p>For ECAM, we use below work flow to get VA of ECAM, we can see nGnRE will be<br>set for ECAM memory type/attribute.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* will ioremap ECAM in below ioremap using nGnRE */</span><br><span class="line">pci_acpi_scan_root</span><br><span class="line">    --&gt; pci_acpi_scan_root</span><br><span class="line">        --&gt; pci_ecam_create</span><br><span class="line">            --&gt; cfg-&gt;win = ioremap</span><br></pre></td></tr></table></figure>
<p>Reference:<br>[1] ARM Cortex-A Series Programmer’s Guide for ARMv8-A(13: Memory Ordering) </p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe INTx parse in ACPI</title>
    <url>/PCIe-INTx-parse-in-ACPI/</url>
    <content><![CDATA[<p>When probing a PCIe device, it will assign INTx to it.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_device_probe</span><br><span class="line">        /* arch/arm64/kernel/pci.c */</span><br><span class="line">    --&gt; pcibios_alloc_irq</span><br><span class="line">        --&gt; acpi_pci_irq_enable(dev)</span><br><span class="line">            --&gt; pin = dev-&gt;pin</span><br><span class="line">                /* From a EP, we find the related INTx in the RP.</span><br><span class="line">                 * It will search from the EP which wants to be assigned a INTx</span><br><span class="line">                 * to find a host bridge which has the INTx configure(will be in</span><br><span class="line">                 * the host bridge&#x27;s DSDT device)</span><br><span class="line">                 * The device(RP or IEP) which directly connects the logic host</span><br><span class="line">                 * bridge and the pin number after swizzling will be passed to</span><br><span class="line">                 * check function to find correct irq number</span><br><span class="line">                 * (check function: acpi_pci_irq_check_entry in</span><br><span class="line">                 * acpi_pci_irq_find_prt_entry)</span><br><span class="line">                 */</span><br><span class="line">            --&gt; acpi_pci_irq_lookup(dev, pin)</span><br><span class="line">                --&gt; acpi_pci_irq_find_prt_entry(dev, pin, &amp;entry)</span><br><span class="line">                --&gt; while (bridge) &#123;</span><br><span class="line">                        --&gt; pin = pci_swizzle_interrupt_pin(dev, pin)</span><br><span class="line">                        --&gt; acpi_pci_irq_find_prt_entry(bridge, pin, &amp;entry)</span><br><span class="line">                    &#125;</span><br><span class="line">                /* get irq number in _PRT */</span><br><span class="line">            --&gt; gsi = entry-&gt;index</span><br><span class="line">                /* driver/acpi/irq.c */</span><br><span class="line">            --&gt; rc = acpi_register_gsi(&amp;dev-&gt;dev, gsi, triggering, polarity)</span><br><span class="line">                --&gt; fwspec.fwnode = acpi_gsi_domain_id</span><br><span class="line">                    /* This is the API to get virq */</span><br><span class="line">                --&gt; irq_create_fwspec_mapping(&amp;fwspec)</span><br><span class="line">                /* here rc is the related virq */</span><br><span class="line">            --&gt; dev-&gt;irq = rc</span><br></pre></td></tr></table></figure>
<p>If we use GICv3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gic_acpi_init</span><br><span class="line">    --&gt; acpi_set_irq_model(ACPI_IRQ_MODEL_GIC, domain_handle)</span><br><span class="line">        --&gt; acpi_irq_model = model</span><br><span class="line">        --&gt; acpi_gsi_domain_id = fwnode</span><br></pre></td></tr></table></figure>
<p>We can get GICD domain like above.</p>
<p>dev-&gt;pin had been set in below flow:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_setup_device</span><br><span class="line">    --&gt; pci_read_irq</span><br><span class="line">        --&gt; pci_read_config_byte(dev, PCI_INTERRUPT_PIN, &amp;irq)</span><br><span class="line">        --&gt; dev-&gt;pin = irq</span><br></pre></td></tr></table></figure>

<p>We can add INTx configure in ACPI DSDT as described in 6.2.13.1 in ACPI spec 6.1.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Name(_PRT, Package&#123;</span><br><span class="line">        Package&#123;0xFFFF,0,0,640&#125;,   // INT_A</span><br><span class="line">        Package&#123;0xFFFF,1,0,641&#125;,   // INT_B</span><br><span class="line">        Package&#123;0xFFFF,2,0,642&#125;,   // INT_C</span><br><span class="line">        Package&#123;0xFFFF,3,0,643&#125;    // INT_D</span><br><span class="line">&#125;)                |    | |  |</span><br><span class="line">    +-------------+    | |  +---------------+</span><br><span class="line">    |       -----------+ |                  |</span><br><span class="line"> All_BDF   pin   interrup_controller   hw_irq_number</span><br></pre></td></tr></table></figure>
<p>If interrup_controller field is 0, we will use “global interrupt pool” mentioned<br>in ACPI spec. In ARM world, this “global interrupt pool” will be GICD which is<br>defined in above gic_acpi_init</p>
<p>If our hardware topology is like this:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">system bus  </span><br><span class="line">     |         </span><br><span class="line">     +----- RP0(device number is 0)</span><br><span class="line">     |</span><br><span class="line">     +----- RP1(device number is 1)</span><br><span class="line">     |</span><br><span class="line">     +----- RP2(device number is 2)</span><br><span class="line">     |</span><br><span class="line">     +----- IEP(device number is y)</span><br><span class="line">     |</span><br></pre></td></tr></table></figure>
<p>We can configure the INTx _RPT as:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Name(_PRT, Package&#123;</span><br><span class="line">        Package&#123;0xFFFF,0,0,RP0_INTA&#125;,</span><br><span class="line">        Package&#123;0xFFFF,1,0,RP0_INTB&#125;,</span><br><span class="line">        Package&#123;0xFFFF,2,0,RP0_INTC&#125;,</span><br><span class="line">        Package&#123;0xFFFF,3,0,RP0_INTD&#125;,</span><br><span class="line">        Package&#123;0x1FFFF,0,0,RP1_INTA&#125;,</span><br><span class="line">        Package&#123;0x1FFFF,1,0,RP1_INTB&#125;,</span><br><span class="line">        Package&#123;0x1FFFF,2,0,RP1_INTC&#125;,</span><br><span class="line">        Package&#123;0x1FFFF,3,0,RP1_INTD&#125;,</span><br><span class="line">        Package&#123;0x2FFFF,0,0,RP2_INTA&#125;,</span><br><span class="line">        Package&#123;0x2FFFF,1,0,RP2_INTB&#125;,</span><br><span class="line">        Package&#123;0x2FFFF,2,0,RP2_INTC&#125;,</span><br><span class="line">        Package&#123;0x2FFFF,3,0,RP2_INTD&#125; </span><br><span class="line">        Package&#123;0xyFFFF,1,0,IEP_INTA&#125; </span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>NOTE:<br> 把dts下INTx中断的解析也放到这个文档里吧，不想另外起文档了。调用链大致是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_device_add()</span><br><span class="line">    --&gt;pcibios_add_device(dev)</span><br><span class="line">        --&gt;dev-&gt;irq = of_irq_parse_and_map_pci(dev, 0, 0)</span><br><span class="line">	    --&gt;of_irq_parse_and_map_pci()</span><br><span class="line">	           /* check if pci_dev has a device_node */</span><br><span class="line">	        --&gt;pci_device_to_OF_node(pdev); ?</span><br><span class="line">		   /* what is pin for? */</span><br><span class="line">		--&gt;pci_read_config_byte(pdev, PCI_INTERRUPT_PIN, &amp;pin)</span><br><span class="line">		--&gt;of_irq_parse_raw</span><br><span class="line">		      /* just parse interrupt-cell, interrupt-map,</span><br><span class="line">		       * interrupt-map-mask</span><br><span class="line">		       */</span><br><span class="line">		   --&gt; of_get_property(...,&quot;interrupt-cell&quot;,...&quot;)</span><br><span class="line"></span><br><span class="line">	    --&gt;irq_create_of_mapping()</span><br></pre></td></tr></table></figure>
<p>可见，PCI的核心代码pci_device_add()会扫面dts中的信息，然后给对应的中断分配<br>中断号资源。分配好中断号(virq)会写到pci_dev-&gt;irq中，供pci设备驱动注册中断handler<br>的时候使用。各个pci设备中注册的中断handler有时会共享一个INTx中短线(e.g. INTa)。<br>这时一旦一个INTx中断被触发，不同设备上的中断handler都会被调用到。可见注册的时候，<br>这些中断handler都应该是shareable的。</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe PRI分析</title>
    <url>/PCIe-PRI%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="协议逻辑"><a href="#协议逻辑" class="headerlink" title="协议逻辑"></a>协议逻辑</h2><p> PRI依赖ATS，带有PRI功能的设备，可以给IOMMU发page request，IOMMU为设备申请物理页，<br> 并建立页表后，给设备发PRG response的消息。</p>
<p> PCIe协议里定义了PCIe设备和IOMMU之间的request/repsponse消息格式，并且定义了<br> PRI capability的格式，后者是软件可以读写的软硬件接口。</p>
<p> 先看PRI capability的详细定义，这个cap只在PF里有：</p>
<ul>
<li><p>cap head</p>
<p>常规cap头。</p>
</li>
<li><p>status register</p>
<p>response failure bit: 读到1表示收到一个failure的response。RW1C寄存器，写1清0。</p>
<p>unexpected page request group index bit: 读到1表示收到一个非法PRI group index。<br>RW1C寄存器，写1清。</p>
<p>stopped bit: 只在enable clear的时候有效。这个是一个只读的指示位，为0时或者设备<br>根本没有停，或者设备停了，但是链路上还有PRI request，当为1时，表示之前发出去的<br>request都完成了(对应的response都回来了?)</p>
<p>PRG response PASID required bit: 只读的指示位，PRG response带不带PASID，使能<br>PASID的时候这个都应该是1？</p>
</li>
<li><p>control register</p>
<p>enable bit: 控制是否可以发PRI request。从non-enable到enable的操作，清空status<br>flags。enable和stopped同时处于clear，可能设备和IOMMU的连接通路上还有PRI request。</p>
<p>reset bit:  只有在enable clear的时候有用，这个时候对reset写1，会clear page<br>request credit counter和pending request state。</p>
<p>估计上面这两个是硬件内部为了维护PRI request和response的具体实现, reset把这些<br>东西清掉，使得设备回到复位状态，结合上面enable clear时只是disable PRI request<br>的主动行为，并不保证清理链路上和设备里已经发出去的请求，这里的逻辑应该是这样。</p>
</li>
<li><p>outstanding page request cap</p>
<p>定义page request的最大outstanding。page request的个数是资源，PRG index的个数<br>也是资源，这里限定的是前者。</p>
</li>
<li><p>outstanding page request allocation</p>
<p>用来配置实际使用的最大page request outstanding数。提供这个接口给软件的目的是<br>要和IOMMU的处理能力做匹配，如果IOMMU的缺页处理能力比设备小，会造成PRI request<br>失败。所以，这里对IOMMU就有了限制，可以直接想到的有: IOMMU处理PRI的队列最好独立，<br>这样方便和设备的PRI outstanding能力做匹配; IOMMU需要考虑处理PRI请求队列溢出时，<br>如何恢复; IOMMU的驱动需要考虑合理的匹配逻辑。</p>
</li>
</ul>
<p> 协议中定义的各种PRI相关的消息，这一部分软件无法直接感知。相关的硬件消息包括：</p>
<ul>
<li><p>Page Request Message(PRM)</p>
<p>设备发出的消息，一组消息可以组成一个group，group中的最后一个message有last标记<br>去标记。</p>
</li>
<li><p>Stop Marker Message</p>
<p>由设备发给IOMMU，告诉IOMMU，设备不再使用相关PASID了。</p>
</li>
<li><p>Page Request Group Response Message(PRGRM)</p>
<p>IOMMU处理完一组PRM，对设备返回一个PRGRM，使用ID路由，携带pasid，group index，<br>请求完成状态信息。注意这里是不带请求到的PA的。以group为单位返回状态信息，group<br>里一个request failure，整个group就failure了。</p>
<p>所以，PRI对IOMMU的诉求就是IOMMU要给软件提供下发PRGRM的接口，这个接口至少要有bdf，<br>pasid，group index，PRM group处理返回值。</p>
</li>
</ul>
<p> 基于cap各个域段的分析，整体流程已经比较清楚了。唯一要注意的是PRGRM只返回处理<br> 结果，需要接着再发ATS请求拿到PA，然后再发地址翻译过的请求。因为PRI翻译建立的页表<br> 是可能变动的，如果收到PRGRM后直接用va访问可能会异常，所以接着发ATS拿到PA，依赖<br> ATS的同步机制保证发出的访问请求成功。</p>
<h2 id="Linux驱动分析"><a href="#Linux驱动分析" class="headerlink" title="Linux驱动分析"></a>Linux驱动分析</h2><p> 目前Linux内核主线(Linux-5.14-rc4)已经支持了PCIe PRI的基本使能函数。相关函数在<br> drivers/pci/ats.c中，这些只是一些PCIe cap的操作函数。在业务中使用PRI功能的情况<br> 还没有。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe ATS协议分析</title>
    <url>/PCIe-ATS%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="translation-request"><a href="#translation-request" class="headerlink" title="translation request"></a>translation request</h2><ul>
<li><p>Length</p>
<p>这个地方的数值表示的是dword的个数，因为一个tranlation request是两个dword，所以<br>如果Length是2，就表示请求untranslated address开始的一个STU大小的翻译，如果，<br>大于2就表示Length/2个STU大小的翻译。可见Length总是偶数。</p>
<p>Spec里说，这样申请的多个翻译需要是一样的页大小，以STU为大小，从untranslated<br>address起始。</p>
<p>Length的最大取值是RCB，就是128，那么一次申请的翻译最大只有64个SUT。</p>
</li>
<li><p>Untranslated address</p>
<p>需要翻译的VA，size大小是 (2^(STU + 12)) * (Length / 2) Byte。STU就是ATS<br>capability里的STU域段的数值。这个VA总是要和STU表示的大小对齐。</p>
</li>
</ul>
<h2 id="translation-completion"><a href="#translation-completion" class="headerlink" title="translation completion"></a>translation completion</h2><p> 翻译的结果用这个消息返回给PCIe设备。如果翻译成功，在completion的报文中就带有<br> payload，payload中有Translated address，S，N，Global，W，R等域段。</p>
<ul>
<li><p>Status</p>
<p>表示翻译的结果。</p>
</li>
<li><p>Translated address和S</p>
<p>翻译后的PA，这个地址覆盖的size需要S域段和地址本身确定。S为0表示size的值是4096，<br>S为1就要看PA其中的bit了，bit12为0表示8KB，bit13/bit12为0/1表示16KB，依次类推<br>Spec上定义了2M，1G，4G的表示方法。</p>
</li>
<li><p>Global</p>
<p>表示翻译得到的结果的作用范围，如果是1，表示是整个设备，如果是0，设备需要配合<br>PASID给这个翻译项创建ATC，这样范围就被限定在了PASID的范围内。</p>
</li>
</ul>
<p> 按照如上的逻辑，translation request的size只能和STU的大小一样，而STU是ATS capability<br> 里在设备初始化时配置好的，所以size的大小在设备的使用过程中是固定的。</p>
<h2 id="invalidate-request"><a href="#invalidate-request" class="headerlink" title="invalidate request"></a>invalidate request</h2><p> 无效化一段VA在ATC的缓存。</p>
<ul>
<li><p>Untranslated address和S</p>
<p>地址和S域段和上面的意思一样。不同的是无效化的size只能是2的幂次方倍个4096。<br>Spec中没有明确说Length域段怎么和其对应起来。</p>
<p>SMMUv3 Spec的CMD_ATC_INV命令，使用4096 * 2^size来指定无效化的大小，和PCIe这里<br>是配套的。这样设定无效化的size，一般是和实际需要无效化的size是不一致的，需要<br>软件去匹配一下。</p>
</li>
<li><p>Global</p>
<p>和上面的意思一样，表示PASID这个因素要不要加到invalidate里来。</p>
</li>
</ul>
<h2 id="ATS-capability"><a href="#ATS-capability" class="headerlink" title="ATS capability"></a>ATS capability</h2><ul>
<li><p>cap head</p>
<p>常规head</p>
</li>
<li><p>Capability register</p>
<ul>
<li>Invalidate Queue Depth，不清楚暴露给软件的目的？</li>
</ul>
</li>
<li><p>Control register</p>
<ul>
<li>STU</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe memory base and memory limit</title>
    <url>/PCIe-memory-base-and-memory-limit/</url>
    <content><![CDATA[<p>在PCIe typeI 配置空间中有mem base, mem limit, prefetch mem base, prefetch mem limit,<br>upper 32 bit prefetch base, upper 32 bit prefetch limit，io base, io limit等一些<br>寄存器。</p>
<p>这些寄存器都是在PCI域中路由TLP包的时候用的一些窗口寄存器。如果一个TLP mem 读/写<br>的地址正好落入mem base ~ mem base + mem limit的范围内，则该TLP包从上游总线转到下游<br>总线上；如果一个TLP包到达桥的下游总线时，其访问的地址不在mem base ~ mem base + mem limit<br>的范围内时，该TLP包被传送到上游总线。一个访问从CPU域被转换到PCI域，应该使用和具体<br>实现相关的硬件转换模块，比如ATU。</p>
<p>prefetch的情况和上面的是类似的，只是prefetch可以是32bit地址也可以是64bit地址。<br>而非prefetch只有是32bit地址的。</p>
<p>所有的BAR分配的地址都是在上面的窗口中的，其实更准确的讲是先确定BAR的类型和大小然后<br>再根据一条总线下各类BAR占的总资源，填写上面的寄存器。</p>
<p>也就说BAR的类型可以有：non-prefetch 32 bit mem bar.</p>
<pre><code>                   prefetch 64 bit mem bar
                   prefetch 32 bit mem bar ?

                   16 bit io bar  io都是non-prefetch
                   32 bit io bar
</code></pre>
<p>在PCIe中，prefetch的bar(prefetch bit 置位)必须是64bit mem bar<a href="22.1.16">1</a>, 和上面<br>prefetch 32 bit mem bar是冲突的(Intel 82599上有该类型的BAR)</p>
<p>non-prefetch bar只可以被分到non-prefetch的window里，prefetch bar可以被分到non-prefetch<br>的window里。64bit bar可以被分到32bit window里。这里的窗口只的都是PCI域的窗口。</p>
<p>[1] [PCI.EXPRESS系统体系结构标准教材].(美)Pavi.Budruk,Don.Anderson,Tom.Shanley</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe userspace tools: lspci, setpci and sysfs</title>
    <url>/PCIe-userspace-tools-lspci-setpci-and-sysfs/</url>
    <content><![CDATA[<h2 id="lspci"><a href="#lspci" class="headerlink" title="lspci"></a>lspci</h2><p> lspci</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">00:00.0 Host bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 DMI2 (rev 02)</span><br><span class="line">00:01.0 PCI bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 PCI Express Root Port 1 (rev 02)</span><br><span class="line">00:02.0 PCI bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 PCI Express Root Port 2 (rev 02)</span><br><span class="line">00:02.2 PCI bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 PCI Express Root Port 2 (rev 02)</span><br><span class="line">00:03.0 PCI bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 PCI Express Root Port 3 (rev 02)</span><br><span class="line">00:03.2 PCI bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 PCI Express Root Port 3 (rev 02)</span><br></pre></td></tr></table></figure>
<p>A general show of PCIe devices in system.</p>
<p> lspci -t</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[...]</span><br><span class="line"> \-[0000:00]-+-00.0</span><br><span class="line">             +-01.0-[01]----00.0</span><br><span class="line">             +-02.0-[02]--+-00.0</span><br><span class="line">             |            +-00.1</span><br><span class="line">             |            +-00.2</span><br><span class="line">             |            \-00.3</span><br><span class="line">             +-02.2-[03]--</span><br><span class="line">             +-03.0-[04]--</span><br><span class="line">             +-03.2-[05]--</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p>Show PCIe topology in tree picture. As showed in above picture, 0000:00 means<br>domain 0 bus 0; 01.0 means a device under bus 0, so its BDF should be 0000:00:01.0;<br>01.0-[01] here [01] means the bus number under device 00:01.0, some times we may<br>get [xx-yy] which means the bus range under this device, apparently this device<br>is a PCIe bridge; 01.0-[01]—-00.0 here 00.0 means a device which device:function<br>is 00.0, so together with its father bus, its BDF should be 0000:01:00.0.</p>
<p> lspci -s ff:0f.1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ff:0f.1 System peripheral: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 Buffered Ring Agent (rev 02)</span><br></pre></td></tr></table></figure>
<p>To see the specific device information, use lspci -s BDF</p>
<p> lspci -vvv</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">80:05.4 PIC: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 I/O APIC (rev 02) (prog-if 20 [IO(X)-APIC])</span><br><span class="line">	Subsystem: Device 19e5:2060</span><br><span class="line">	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-</span><br><span class="line">	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-</span><br><span class="line">	Latency: 0</span><br><span class="line">	Region 0: Memory at c8000000 (32-bit, non-prefetchable) [size=4K]</span><br><span class="line">	Capabilities: &lt;access denied&gt;</span><br></pre></td></tr></table></figure>
<p>To see more information, use -vvv/-vv/-v. You can see BAR in “Region”, different<br>bridge window range and different capabilities.</p>
<p> lspci -xxx</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">7f:13.1 System peripheral: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 Integrated Memory Controller 0 Target Address, Thermal &amp; RAS Registers (rev 02)</span><br><span class="line">00: 86 80 71 2f 00 00 10 00 02 00 80 08 00 00 80 00</span><br><span class="line">10: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">20: 00 00 00 00 00 00 00 00 00 00 00 00 e5 19 60 20</span><br><span class="line">30: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br></pre></td></tr></table></figure>
<p>To see configure space as bit value, use -xxx/-xx/-x</p>
<h2 id="setpci"><a href="#setpci" class="headerlink" title="setpci"></a>setpci</h2><p> setpci -s BDF 20.L=0x12345678</p>
<p> Above command set 0x20 offset of configure space of a device which BDF is BDF<br> to 0x12345678. L here means 32bit, W will mean 16bit, B will mean 8bit.</p>
<h2 id="sysfs"><a href="#sysfs" class="headerlink" title="sysfs"></a>sysfs</h2><ul>
<li><p>remove</p>
<p>We can remove a device by writing 1 to its remove file.</p>
</li>
<li><p>rescan</p>
<p>We can rescan a device/bus by writing 1 to its rescan file, we can also rescan<br>whole PCIe system by echo 1 &gt; /sys/bus/pci/rescan (need to check…).</p>
<p>When we do rescan a device, we find its father bus, and pass this bus to PCIe<br>enumeration process.</p>
<p>However, if we rescan a RP or PCIe bridge, as the structure of related RP or<br>PCIe bridge is still there, Linux kernel will do nothing about MEM/IO window<br>of related RP or PCIe bridge.(however, writing/reading MEM/IO operation will<br>be done to checkout if this bridge’s MEM/IO window register is working)</p>
</li>
<li><p>reset</p>
<p>(to do…)</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe enumeration for SR-IOV PCIe device</title>
    <url>/PCIe-enumeration-for-SR-IOV-PCIe-device/</url>
    <content><![CDATA[<h2 id="Parse-VF-PF-BARs"><a href="#Parse-VF-PF-BARs" class="headerlink" title="Parse VF/PF BARs"></a>Parse VF/PF BARs</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* In PCI enumeration, once we find SR-IOV capability, sriov_init will be called */</span><br><span class="line">pci_device_add</span><br><span class="line">    --&gt; pci_init_capabilities</span><br><span class="line">        --&gt; pci_iov_init</span><br><span class="line">                /* parse sr-iov capability in pf, get bar sizef of vf */</span><br><span class="line">            --&gt; sriov_init(struct pci_dev *dev, int pos)</span><br><span class="line">                ...</span><br><span class="line">                    /* get bar_i size of vf */</span><br><span class="line">                --&gt; __pci_read_base(dev, pci_bar_unknown, res, pos + PCI_SRIOV_BAR + i * 4);</span><br><span class="line">                ...</span><br><span class="line">                    /* get bar_i size for all vf */</span><br><span class="line">                --&gt; res-&gt;end = res-&gt;start + resource_size(res) * total - 1;</span><br></pre></td></tr></table></figure>
<p>so at this step, we will see BAR0~BAR5, ROM_BAR, VF_BAR0~VF_BAR5…, the sizes<br>of them will be known. VF_BAR0 includes BAR0 in VF1,2,…. the resouces assignment<br>below will allocate resources for VF BARs ahead. Here all BAR0 in VF1,2… has<br>been seen as one block. you can see pci_enable_sriov below will directly assign<br>related BAR of certain VF to related pci_dev, as we already caculate and allocate<br>the resouces here.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * so if we get the size of bar of vf here, assignment will happen in standard</span><br><span class="line"> * assignment in PCI subsystem.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * this function will be called in specific PCIe device driver. e.g.</span><br><span class="line"> * drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c, when user in userspace triggers</span><br><span class="line"> * vf by sysfs or loads modules with vf parameters.</span><br><span class="line"> */</span><br><span class="line">pci_enable_sriov</span><br><span class="line">    --&gt; sriov_enable(dev, nr_virtfn)</span><br><span class="line">        --&gt; pci_iov_add_virtfn</span><br><span class="line">            ...</span><br><span class="line">                /*</span><br><span class="line">                 * As we already got all VFs&#x27; BARs been assigned above, so here</span><br><span class="line">                 * just fill the BAR resource to each VF&#x27;s pci_dev.</span><br><span class="line">                 */</span><br><span class="line">            --&gt; virtfn-&gt;resource[i].start = res-&gt;start + size * id</span><br></pre></td></tr></table></figure>
<h2 id="Note"><a href="#Note" class="headerlink" title="Note:                            "></a>Note:                            </h2><ol>
<li>BARs in PF/VF<pre><code>  +--------------+    +--------&gt;+----------+
  | PF CFG       |    |         | VF1 BAR0 |
  +--------------+    |         | VF2 BAR0 |
  | PCI cfg head:|    |         | ...      |
  | ...          |    |         | VF5 BAR0 |
  | BAR0 ~ BAR5  |    |         +----------+
  | ...          |    |  +-----&gt;+----------+
  +--------------+    |  |      | VF1 BAR1 |
  | SR-IOV cap:  |    |  |      | VF2 BAR1 |
  | ...          |    |  |      | ...      |
  | VF BAR0      +----+  |      | VF5 BAR1 |
  | VF BAR1      +-------+      +----------+
  | ...          |              ...              
  | VF BAR5      +-------------&gt;+----------+
  +--------------+              | VF1 BAR5 |
                                | VF2 BAR5 |
                                | ...      |
                                | VF5 BAR5 |
                                +----------+
</code></pre>
</li>
</ol>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe学习笔记(一)</title>
    <url>/PCIe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/</url>
    <content><![CDATA[<p> 本文是学习linux kernel中PCI子系统代码的一个笔记。PCI子系统的代码最主要的就是<br> 实现整个PCI树的枚举和资源的分配。本文先总体介绍，然后主要分析pci_create_root_bus<br> 函数，该函数实现pci_bus结构和pci_host_bridge结构的分配。本文分析的代码版本为内核<br> 3.18-rc1</p>
<p> pci_scan_root_bus()完成整个PCI树的枚举和资源分配。主要实现在下面的三个函数中：<br>    –&gt;pci_create_root_bus()<br>    –&gt;pci_scan_child_bus(b)<br>    –&gt;pci_assign_unassigned_bus_resources(b)</p>
<p> pci_create_root_bus 建立相应的数据结构pci_bus, pci_host_bridge等<br> pci_scan_child_bus 枚举整个pci总线上的设备<br> pci_assign_unassigned_bus_resources 分配总线资源</p>
<p> 下面直观的给出PCI(PCIe)硬件和软件的相互对应，也可以看出软件对硬件是怎么做抽象的。<br> 硬件结构:<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	|   root bus: 0   ----&gt;  struct pci_bus</span><br><span class="line">	|</span><br><span class="line">+----------------+        ----&gt;  struct pci_host_bridge</span><br><span class="line">| pcie root port |        ----&gt;  struct pci_dev</span><br><span class="line">+----------------+        </span><br><span class="line">	|   bus: 1        ----&gt;  struct pci_bus</span><br><span class="line">	|</span><br><span class="line">+----------------+</span><br><span class="line">| pcie net cards |        ----&gt;  struct pci_dev</span><br><span class="line">+----------------+</span><br></pre></td></tr></table></figure></p>
<p>先从硬件的角度说明PCIe总线系统的工作大致流程。PCIe总线系统是一个局部总线系统，<br>目的在于沟通内存和外设的存储空间。总结起来完成: 1. CPU访问外设的存储空间；2.<br>外设访问系统内存空间。一般介绍PCIe的时候说的RC，switch, EP，这里以arm Soc为例说明.<br>一般的, RC可以理解成PCIe host bridge, 有时也叫PCIe控制器，完成CPU域地址到PCI域<br>地址的转换，RC在Soc的内部。switch是一个独立的器件，和RC的接口相连，提供扩展。EP<br>是具有PCIe接口的网卡，SATA控制器等。PCI中还有一个概念是PCI桥, 实际的PCI桥存在<br>PCI总线中(不是PCIe总线), 完成系统的扩展，和host桥不同的是，PCI桥没有地址翻译的功能.<br>PCI总线中的switch每个端口相当于一个PCI桥(虚拟PCI桥), 完成PCI桥类似的功能。</p>
<p>每个PCI设备，包括host桥、PCI桥和PCI设备都有一个配置空间。PCIe中，这个配置空间的<br>大小是4K。CPU和外设根据配置空间的一些寄存器, 访问相应的地址。关键的寄存器有:<br>BAR, mem base/limit, I/O base/limit. mem base/limit, I/O base/limit指定当前PCI桥<br>mem空间和I/O空间的起始和大小, 在PCI桥中使用. BAR指示的是PCI设备的mem空间和I/O空间.<br>比如, 下图中PCIe net card的BAR空间(BAR指示的一段地址), 就可以存放网卡本身的寄存器.<br>一般情况, PCI桥的BAR是用不到的.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                +----------------+ ----&gt; PCIe host bridge</span><br><span class="line">                | pcie root port |</span><br><span class="line">                +----------------+ ----&gt; in Soc</span><br><span class="line">                        |</span><br><span class="line">--------------------------------------------------- ----&gt; switch</span><br><span class="line">|                    +----------------+           |</span><br><span class="line">|                    |   pci bridge   |           |</span><br><span class="line">|                    +----------------+           |</span><br><span class="line">|                            |                    |</span><br><span class="line">|         -------------------------------         |</span><br><span class="line">|         |                             |         |</span><br><span class="line">| +----------------+           +----------------+ |</span><br><span class="line">| |   pci bridge   |           |   pci bridge   | |</span><br><span class="line">| +----------------+           +----------------+ |</span><br><span class="line">---------------------------------------------------</span><br><span class="line">          |</span><br><span class="line">  +----------------+</span><br><span class="line">  |  PCIe net card |</span><br><span class="line">  +----------------+</span><br></pre></td></tr></table></figure>

<p>整个pci枚举的过程最主要的就是配置pci桥和pci设备的BAR和mem、I/O base/limit<br>下面以此为主线分析整个pci枚举的过程。</p>
<p>在分析代码之前，先介绍基本的数据结构。代码围绕这些数据结构构成。<br>这几个核心的数据结构是: struct pci_bus, struct pci_dev, struct pci_host_bridge<br>每一个pci总线对应一个struct pci_bus结构，每一个pci设备(可以是host bridge,<br>普通pci device)对应一个pci_dev, 一个pci host bridge对应一个pci_host_bridge,<br>一般一个pci总线体系中有一个pci host bridge, 这一个pci总线系统也叫一个pci domain,<br>各个pci domain不可以直接相互访问。有些时候一个系统会有多个pcie root port, 这时<br>每个root port和下面的pci device组成各自的pci domain, 下面介绍各个结构中关键条目。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct pci_bus:</span><br><span class="line">        /* 指向该总线上游的pci桥的pci_dev结构 */</span><br><span class="line">        struct pci_dev *self;</span><br><span class="line">        /* 存储该总线的mem、I/O，prefetch mem等资源。由总线上游pci桥的</span><br><span class="line">         * pci_dev结构中的resource中的第PCI_BRIDGE_RESOURCES到</span><br><span class="line">         * PCI_BRIDGE_RESOURCES + PCI_BRIDGE_RESOURCE_NUM -1个元素复制得到</span><br><span class="line">         * 发生于pci_scan_bridge()</span><br><span class="line">         *           --&gt;pci_add_new_bus()</span><br><span class="line">         *              --&gt;pci_alloc_child_bus()</span><br><span class="line">         */</span><br><span class="line">        struct resource *resource[PCI_BRIDGE_RESOURCE_NUM];</span><br><span class="line">        struct list_head resources;</span><br><span class="line"></span><br><span class="line">struct pci_host_bridge:</span><br><span class="line">        /* 整个pci系统的mem, I/O资源作为一个个链表元素 */</span><br><span class="line">        struct list_head windows;</span><br><span class="line"></span><br><span class="line">struct pci_dev:</span><br><span class="line">        /* 若该设备是pci桥，指向该桥的下游总线 */</span><br><span class="line">        struct pci_bus *subordinate;</span><br><span class="line">        /* 存该pci设备的BAR等资源, 在__pci_read_base()中初始化</span><br><span class="line">         * pci_scan_device() </span><br><span class="line">         *    --&gt; pci_setup_device() ...</span><br><span class="line">         *        --&gt; __pci_read_base</span><br><span class="line">         * still do know where to init resource[PCI_BRIDGE_RESOURCES] ?</span><br><span class="line">         */</span><br><span class="line">        struct resource resource[DEVICE_COUNT_RESOURCE];</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe学习笔记(三)</title>
    <url>/PCIe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_scan_child_bus(b)</span><br></pre></td></tr></table></figure>
<p>这个函数完成pci总线的枚举，完成整个pci树各个总线号的分配。但是并没有分配各个pci桥，<br>pci device的BAR和mem, I/O, prefetch mem的base/limit寄存器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">unsigned int pci_scan_child_bus(struct pci_bus *bus)</span><br><span class="line">&#123;</span><br><span class="line">	unsigned int devfn, pass, max = bus-&gt;busn_res.start;</span><br><span class="line">	struct pci_dev *dev;</span><br><span class="line"></span><br><span class="line">	/* 开始pci枚举, 一个pci bus上最多有256个设备，每个设备最多有8个function</span><br><span class="line">	 * 所以这里最多可以扫描32个device。实际上每个function也是有配置空间的</span><br><span class="line">	 * 所以function可以看作是个逻辑的设备</span><br><span class="line">	 */</span><br><span class="line">	for (devfn = 0; devfn &lt; 0x100; devfn += 8)</span><br><span class="line">		pci_scan_slot(bus, devfn);</span><br><span class="line"></span><br><span class="line">	/* pci虚拟化相关的东西 */</span><br><span class="line">	max += pci_iov_bus_range(bus);</span><br><span class="line"></span><br><span class="line">	...</span><br><span class="line">	if (!bus-&gt;is_added) &#123;</span><br><span class="line">		dev_dbg(&amp;bus-&gt;dev, &quot;fixups for bus\n&quot;);</span><br><span class="line">		/* 对于arm64来说，是个空函数 */</span><br><span class="line">		pcibios_fixup_bus(bus);</span><br><span class="line">		bus-&gt;is_added = 1;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	for (pass = 0; pass &lt; 2; pass++)</span><br><span class="line">		list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list) &#123;</span><br><span class="line">			if (pci_is_bridge(dev))</span><br><span class="line">			    	/* 开始递归的做pci枚举，在下面的函数中会再次</span><br><span class="line">				 * 调用pci_scan_child_bus。总线号的确定也在这个</span><br><span class="line">				 * 函数中</span><br><span class="line">				 */</span><br><span class="line">				max = pci_scan_bridge(bus, dev, max, pass);</span><br><span class="line">		&#125;</span><br><span class="line">	...</span><br><span class="line">	return max;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_scan_slot(struct pci_bus *bus, int devfn)</span><br><span class="line">    ...</span><br><span class="line">    --&gt; pci_scan_single_device(bus, devfn)</span><br><span class="line">        ...</span><br><span class="line">	    /* 扫描devfn设备, 探测设备：读vendor id, 读BAR的大小 。</span><br><span class="line">	     * 软件行为：为device分配struct pci_dev; 根据探测设备得到的数据，填</span><br><span class="line">	     * 写pci_dev里的一些值。若是按照笔记1中的硬件拓扑图现在扫描的device</span><br><span class="line">	     * 应该是pcie root port</span><br><span class="line">	     */</span><br><span class="line">	--&gt; pci_scan_device(bus, devfn)</span><br><span class="line">	        /* 读devfn的vendor id, 会调用到pci_scan_root_bus的参数ops传入的read操作 */</span><br><span class="line">	    --&gt; pci_bus_read_dev_vendor_id()</span><br><span class="line">	        /* 分配pci_dev结构 */</span><br><span class="line">	    --&gt; pci_alloc_dev(bus)</span><br><span class="line">	    ...</span><br><span class="line">	        /* 处理pci配置头中的class code, Header Type, Interrupt Pin</span><br><span class="line">		 * Interrupt Line等内容，并设定新分配的pci_dev中的对应项。这里</span><br><span class="line">		 * 也会探测device的BAR大小，并设定pci_dev-&gt;resource[]。我们关注</span><br><span class="line">		 * 的重点也是pci_bus,pci_dev中相应的元素怎么变化。</span><br><span class="line">		 */</span><br><span class="line">	    --&gt; pci_setup_device(dev)</span><br><span class="line">	        ...</span><br><span class="line">	--&gt; pci_device_add(dev, bus)	</span><br><span class="line"></span><br><span class="line">int pci_setup_device(struct pci_dev *dev)</span><br><span class="line">    ...</span><br><span class="line">        /* 的到device配置空间中head type的值，可以是普通pci device, pci桥等</span><br><span class="line">	 * 这个值在下面的switch-case中决定程序是走哪个分支。如果当前的设备是</span><br><span class="line">	 * pcie root port，那么走PCI_HEADER_TYPE_BRIDGE分支。设备配置空间</span><br><span class="line">	 * head type的值一般是pci host controller驱动中配置过的, 可以参考</span><br><span class="line">	 * /drivers/pci/host/pcie-designware.c</span><br><span class="line">	 */</span><br><span class="line">    --&gt; pci_read_config_byte(dev, PCI_HEADER_TYPE, &amp;hdr_type)</span><br><span class="line">        /* device的parent是上游总线之上的那个pci桥 */</span><br><span class="line">    --&gt; dev-&gt;dev.parent = dev-&gt;bus-&gt;bridge;</span><br><span class="line">        /* 读出class revision的值 */</span><br><span class="line">    --&gt; pci_read_config_dword(dev, PCI_CLASS_REVISION, &amp;class)</span><br><span class="line">	dev-&gt;revision = class &amp; 0xff;</span><br><span class="line">	dev-&gt;class = class &gt;&gt; 8;		    /* upper 3 bytes */</span><br><span class="line"></span><br><span class="line">	dev-&gt;current_state = PCI_UNKNOWN;</span><br><span class="line"></span><br><span class="line">	class = dev-&gt;class &gt;&gt; 8;</span><br><span class="line"></span><br><span class="line">	switch (dev-&gt;hdr_type) &#123;		    /* header type */</span><br><span class="line">	case PCI_HEADER_TYPE_NORMAL:		    /* standard header */</span><br><span class="line">		if (class == PCI_CLASS_BRIDGE_PCI)</span><br><span class="line">			goto bad;</span><br><span class="line">		pci_read_irq(dev);</span><br><span class="line">		/*</span><br><span class="line">		 * 得到BAR的大小，并存在pci_dev-&gt;resource[]中，主要是对pci</span><br><span class="line">		 * device起作用。当输入参数dev是pci桥时，得到的值似乎对后面影响</span><br><span class="line">		 * 不大，这点需要确认下？</span><br><span class="line">		 */</span><br><span class="line">		pci_read_bases(dev, 6, PCI_ROM_ADDRESS);</span><br><span class="line">		pci_read_config_word(dev, PCI_SUBSYSTEM_VENDOR_ID, &amp;dev-&gt;subsystem_vendor);</span><br><span class="line">		pci_read_config_word(dev, PCI_SUBSYSTEM_ID, &amp;dev-&gt;subsystem_device);</span><br><span class="line"></span><br><span class="line">			break;</span><br><span class="line"></span><br><span class="line">	case PCI_HEADER_TYPE_BRIDGE:		    /* bridge header */</span><br><span class="line">		if (class != PCI_CLASS_BRIDGE_PCI)</span><br><span class="line">			goto bad;</span><br><span class="line">		pci_read_irq(dev);</span><br><span class="line">		dev-&gt;transparent = ((dev-&gt;class &amp; 0xff) == 1);</span><br><span class="line">		pci_read_bases(dev, 2, PCI_ROM_ADDRESS1);</span><br><span class="line">		set_pcie_hotplug_bridge(dev);</span><br><span class="line">		pos = pci_find_capability(dev, PCI_CAP_ID_SSVID);</span><br><span class="line">		if (pos) &#123;</span><br><span class="line">			pci_read_config_word(dev, pos + PCI_SSVID_VENDOR_ID, &amp;dev-&gt;subsystem_vendor);</span><br><span class="line">			pci_read_config_word(dev, pos + PCI_SSVID_DEVICE_ID, &amp;dev-&gt;subsystem_device);</span><br><span class="line">		&#125;</span><br><span class="line">		break;</span><br><span class="line">		...</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * 直接读硬件配置空间中的interrupt line and pin，然后写到pci_dev-&gt;line、pci_dev-&gt;irq中</span><br><span class="line"> */</span><br><span class="line">pci_read_irq(dev)</span><br><span class="line"></span><br><span class="line">/* 依次读取6个bar的内容，相关内容写入pci_dev下的struct resource resource[] */</span><br><span class="line">pci_read_bases(dev, 6, PCI_ROM_ADDRESS);</span><br><span class="line">    --&gt;__pci_read_bases</span><br><span class="line">        --&gt;mask = type ? PCI_ROM_ADDRESS_MASK : ~0;</span><br><span class="line">        /* 最初探测bar的大小，向bar空间写入全1 */</span><br><span class="line">        --&gt;pci_write_config_dword(dev, pos, l | mask);</span><br><span class="line">        /* 读出提示信息，写入sz缓存 */</span><br><span class="line">	--&gt;pci_read_config_dword(dev, pos, &amp;sz);</span><br><span class="line">        /* 如果全1, 设备工作不正常，或是没有这个bar */</span><br><span class="line">        --&gt;if (sz == 0xffffffff)</span><br><span class="line">		sz = 0;</span><br><span class="line"></span><br><span class="line">	--&gt;if (type == pci_bar_unknown) &#123;</span><br><span class="line">                /* 根据上面sz的值，得到是io/mem bar, 32/64bits, prefectch/</span><br><span class="line">                 * non-prefectch。这些信息都会写入res-&gt;flags中的对应bit</span><br><span class="line">                 */ </span><br><span class="line">		/* 如果l = 0, 解析的结果，kernel会认为这是一个32bit,</span><br><span class="line">		 * non-prefetchable, mem bar</span><br><span class="line">		 */</span><br><span class="line">		--&gt;res-&gt;flags = decode_bar(dev, l);</span><br><span class="line">		res-&gt;flags |= IORESOURCE_SIZEALIGN;</span><br><span class="line">		if (res-&gt;flags &amp; IORESOURCE_IO) &#123;</span><br><span class="line">                    ...</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">                        /* 下面先把低32bits写到64bits变量的低32bits */</span><br><span class="line">			/* 如果l = 0, l64 = 0 */</span><br><span class="line">			l64 = l &amp; PCI_BASE_ADDRESS_MEM_MASK;</span><br><span class="line">			/* sz是写入全1后从bar中得到的值，如果是0xf000000f</span><br><span class="line">			 * sz64 = 0xf0000000</span><br><span class="line">			 */</span><br><span class="line">			sz64 = sz &amp; PCI_BASE_ADDRESS_MEM_MASK;</span><br><span class="line">                        /* ul的变量是多少bits? */</span><br><span class="line">			/* 应该是0xfffffff0 */</span><br><span class="line">			mask64 = (u32)PCI_BASE_ADDRESS_MEM_MASK;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">            ...</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (res-&gt;flags &amp; IORESOURCE_MEM_64) &#123;</span><br><span class="line">		pci_read_config_dword(dev, pos + 4, &amp;l);</span><br><span class="line">		pci_write_config_dword(dev, pos + 4, ~0);</span><br><span class="line">		pci_read_config_dword(dev, pos + 4, &amp;sz);</span><br><span class="line">		pci_write_config_dword(dev, pos + 4, l);</span><br><span class="line"></span><br><span class="line">		l64 |= ((u64)l &lt;&lt; 32);</span><br><span class="line">		sz64 |= ((u64)sz &lt;&lt; 32);</span><br><span class="line">		mask64 |= ((u64)~0 &lt;&lt; 32);</span><br><span class="line">	&#125;</span><br><span class="line">        ...</span><br><span class="line">	sz64 = pci_size(l64, sz64, mask64);</span><br><span class="line">		/* size = 0xf000_0000 */</span><br><span class="line">		--&gt; size = sz64 &amp; mask64;</span><br><span class="line">		/* size = 0x0fff_ffff, and return this */</span><br><span class="line">		--&gt; size = (size &amp; ~(size-1)) - 1;</span><br><span class="line"></span><br><span class="line">	/* struct pci_bus_region, local struct. indicate a bar&#x27;s resource? */</span><br><span class="line">	region.start = l64; /* l64 = 0 */</span><br><span class="line">	region.end = l64 + sz64; /* 0x0fff_ffff */</span><br><span class="line"></span><br><span class="line">	pcibios_bus_to_resource(dev-&gt;bus, res, &amp;region);</span><br><span class="line">		/* 首先找到host中的对应资源 */</span><br><span class="line">		--&gt; resource_list_for_each_entry(window, &amp;bridge-&gt;windows) &#123;</span><br><span class="line">			--&gt; if (resource_type(res) != resource_type(window-&gt;res))</span><br><span class="line">			/* 找到对应到pci域中的起始地址 */</span><br><span class="line">			--&gt; bus_region.start = window-&gt;res-&gt;start - window-&gt;offset;</span><br><span class="line">			--&gt; bus_region.end = window-&gt;res-&gt;end - window-&gt;offset;</span><br><span class="line"></span><br><span class="line">	pcibios_resource_to_bus(dev-&gt;bus, &amp;inverted_region, res);</span><br></pre></td></tr></table></figure>

<p>现在回到pci_scan_child_bus中的pci_scan_bridge()函数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * 如果一个pci_dev是pci桥的话，以它的上游总线bus和这个设备dev本身为参数，扫描这个</span><br><span class="line"> * 桥。这个函数中实现pci树的递归扫描，在这个函数中为整个pci树上的各个子总线分配</span><br><span class="line"> * 总线号。这里pci_scan_bridge会调用两次，第一次处理BIOS中已经配置的东西，不清楚</span><br><span class="line"> * intel在BIOS中做了怎样的处理。</span><br><span class="line"> */</span><br><span class="line">pci_scan_bridge(bus, dev, max, pass);</span><br><span class="line">    ...</span><br><span class="line">    /* 这里上来就读桥设备的主bus号，是因为有些体系架构下可能在BIOS中已经对这些</span><br><span class="line">     * 做过配置。在ARM64中暂时没有看到有这样做的情况。</span><br><span class="line">     */</span><br><span class="line">    --&gt; pci_read_config_dword(dev, PCI_PRIMARY_BUS, &amp;buses);</span><br><span class="line">    ...</span><br><span class="line">    /* 配置了在kernel中分配bus号，应该不会进入到if中 */</span><br><span class="line">    --&gt; if ((secondary || subordinate) &amp;&amp; !pcibios_assign_all_busses() &amp;&amp;</span><br><span class="line">	    !is_cardbus &amp;&amp; !broken) &#123;</span><br><span class="line">	...</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">	        /* 第一次掉用pci_scan_bridge就此返回了 */</span><br><span class="line">		if (!pass) &#123;</span><br><span class="line">		        ...</span><br><span class="line">			goto out;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		/* Clear errors */</span><br><span class="line">		pci_write_config_word(dev, PCI_STATUS, 0xffff);</span><br><span class="line"></span><br><span class="line">		/* 确定是否已用max+1这个bus号 */</span><br><span class="line">		child = pci_find_bus(pci_domain_nr(bus), max+1);</span><br><span class="line">		if (!child) &#123;</span><br><span class="line">		        /* 分配新的struct pci_bus */</span><br><span class="line">			child = pci_add_new_bus(bus, dev, max+1);</span><br><span class="line">			if (!child)</span><br><span class="line">				goto out;</span><br><span class="line">			pci_bus_insert_busn_res(child, max+1, 0xff);</span><br><span class="line">		&#125;</span><br><span class="line">		max++;</span><br><span class="line">		/* 合成新的bus号，准备写入pci桥的配置空间 */</span><br><span class="line">		buses = (buses &amp; 0xff000000)</span><br><span class="line">		      | ((unsigned int)(child-&gt;primary)     &lt;&lt;  0)</span><br><span class="line">		      | ((unsigned int)(child-&gt;busn_res.start)   &lt;&lt;  8)</span><br><span class="line">		      | ((unsigned int)(child-&gt;busn_res.end) &lt;&lt; 16);</span><br><span class="line">		...</span><br><span class="line">		/* 写入该pci桥primary bus, secondary bus, subordinate bus */</span><br><span class="line">		pci_write_config_dword(dev, PCI_PRIMARY_BUS, buses);</span><br><span class="line"></span><br><span class="line">		if (!is_cardbus) &#123;</span><br><span class="line">			child-&gt;bridge_ctl = bctl;</span><br><span class="line">			/* 递归调用pci_scan_child_bus, 扫描这个子总线下的设备*/</span><br><span class="line">			max = pci_scan_child_bus(child);</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">		        /* 不关心cardbus */</span><br><span class="line">			...</span><br><span class="line">		&#125;</span><br><span class="line">		/*</span><br><span class="line">		 * Set the subordinate bus number to its real value.</span><br><span class="line">		 * 每次递归结束把实际的subordinate bus写入pci桥的配置空间</span><br><span class="line">		 * subordinate bus表示该pci桥下最大的总线号</span><br><span class="line">		 */</span><br><span class="line">		pci_bus_update_busn_res_end(child, max);</span><br><span class="line">		pci_write_config_byte(dev, PCI_SUBORDINATE_BUS, max);</span><br><span class="line">	&#125;</span><br><span class="line">	...</span><br><span class="line">out:</span><br><span class="line">	pci_write_config_word(dev, PCI_BRIDGE_CONTROL, bctl);</span><br><span class="line"></span><br><span class="line">	return max;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe学习笔记(四)</title>
    <url>/PCIe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9B%9B/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_assign_unassigned_bus_resources(b)</span><br><span class="line">void pci_assign_unassigned_bus_resources(struct pci_bus *bus)</span><br><span class="line">&#123;</span><br><span class="line">	struct pci_dev *dev;</span><br><span class="line">	LIST_HEAD(add_list); /* list of resources that</span><br><span class="line">					want additional resources */</span><br><span class="line"></span><br><span class="line">	down_read(&amp;pci_bus_sem);</span><br><span class="line">	list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list)</span><br><span class="line">		if (pci_is_bridge(dev) &amp;&amp; pci_has_subordinate(dev))</span><br><span class="line">		    		/* 这里传入的参数是bus 1对应的pci_bus */</span><br><span class="line">				__pci_bus_size_bridges(dev-&gt;subordinate,</span><br><span class="line">							 &amp;add_list);</span><br><span class="line">	up_read(&amp;pci_bus_sem);</span><br><span class="line">	/* 配置各个桥和设备的BAR，配置桥的MEM，I/O，prefetch MEM base/limit */</span><br><span class="line">	__pci_bus_assign_resources(bus, &amp;add_list, NULL);</span><br><span class="line">	BUG_ON(!list_empty(&amp;add_list));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__pci_bus_size_bridges(struct pci_bus *bus, struct list_head *realloc_head)</span><br><span class="line">    ...</span><br><span class="line">       /* 在当前的连接状态下，list中的代码不会执行。下面的代码层层递归，直到</span><br><span class="line">        * 最底层设备的上的pci_bus，这时最底层设备是没有下一级的pci_bus的。</span><br><span class="line">        * 所以，继续执行后面的代码。</span><br><span class="line">        */</span><br><span class="line">    --&gt;list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list) &#123;</span><br><span class="line">	    ...</span><br><span class="line">	case PCI_CLASS_BRIDGE_PCI:</span><br><span class="line">	default:</span><br><span class="line">		__pci_bus_size_bridges(b, realloc_head);</span><br><span class="line">		break;</span><br><span class="line">       &#125;</span><br><span class="line">    ...</span><br><span class="line">       /* 当前pci_bus的桥对应的pci_dev, 这里应该是host */</span><br><span class="line">    --&gt;switch (bus-&gt;self-&gt;class &gt;&gt; 8) &#123;</span><br><span class="line">        ...</span><br><span class="line">        case PCI_CLASS_BRIDGE_PCI:</span><br><span class="line">            /* 会执行这里 */</span><br><span class="line">    	    pci_bridge_check_ranges(bus);</span><br><span class="line">	    ...</span><br><span class="line">	default:</span><br><span class="line">		/* 这个函数改变了pci_bus-&gt;resource[]中的值。start对齐4K，size是该bus下</span><br><span class="line">		 * 所有I/O空间的总和。但是似乎realloc_head</span><br><span class="line">		 * list似乎没有node添加上去</span><br><span class="line">		 */</span><br><span class="line">		pbus_size_io(bus, realloc_head ? 0 : additional_io_size,</span><br><span class="line">			     additional_io_size, realloc_head);</span><br><span class="line"></span><br><span class="line">		/*</span><br><span class="line">		 * If there&#x27;s a 64-bit prefetchable MMIO window, compute</span><br><span class="line">		 * the size required to put all 64-bit prefetchable</span><br><span class="line">		 * resources in it.</span><br><span class="line">		 */</span><br><span class="line">		b_res = &amp;bus-&gt;self-&gt;resource[PCI_BRIDGE_RESOURCES];</span><br><span class="line">		mask = IORESOURCE_MEM;</span><br><span class="line">		prefmask = IORESOURCE_MEM | IORESOURCE_PREFETCH;</span><br><span class="line">		if (b_res[2].flags &amp; IORESOURCE_MEM_64) &#123;</span><br><span class="line">			prefmask |= IORESOURCE_MEM_64;</span><br><span class="line">			ret = pbus_size_mem(bus, prefmask, prefmask,</span><br><span class="line">				  prefmask, prefmask,</span><br><span class="line">				  realloc_head ? 0 : additional_mem_size,</span><br><span class="line">				  additional_mem_size, realloc_head);</span><br><span class="line"></span><br><span class="line">			/*</span><br><span class="line">			 * If successful, all non-prefetchable resources</span><br><span class="line">			 * and any 32-bit prefetchable resources will go in</span><br><span class="line">			 * the non-prefetchable window.</span><br><span class="line">			 */</span><br><span class="line">			if (ret == 0) &#123;</span><br><span class="line">				mask = prefmask;</span><br><span class="line">				type2 = prefmask &amp; ~IORESOURCE_MEM_64;</span><br><span class="line">				type3 = prefmask &amp; ~IORESOURCE_PREFETCH;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		/*</span><br><span class="line">		 * If there is no 64-bit prefetchable window, compute the</span><br><span class="line">		 * size required to put all prefetchable resources in the</span><br><span class="line">		 * 32-bit prefetchable window (if there is one).</span><br><span class="line">		 */</span><br><span class="line">		if (!type2) &#123;</span><br><span class="line">			prefmask &amp;= ~IORESOURCE_MEM_64;</span><br><span class="line">			ret = pbus_size_mem(bus, prefmask, prefmask,</span><br><span class="line">					 prefmask, prefmask,</span><br><span class="line">					 realloc_head ? 0 : additional_mem_size,</span><br><span class="line">					 additional_mem_size, realloc_head);</span><br><span class="line"></span><br><span class="line">			/*</span><br><span class="line">			 * If successful, only non-prefetchable resources</span><br><span class="line">			 * will go in the non-prefetchable window.</span><br><span class="line">			 */</span><br><span class="line">			if (ret == 0)</span><br><span class="line">				mask = prefmask;</span><br><span class="line">			else</span><br><span class="line">				additional_mem_size += additional_mem_size;</span><br><span class="line"></span><br><span class="line">			type2 = type3 = IORESOURCE_MEM;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		/*</span><br><span class="line">		 * Compute the size required to put everything else in the</span><br><span class="line">		 * non-prefetchable window.  This includes:</span><br><span class="line">		 *</span><br><span class="line">		 *   - all non-prefetchable resources</span><br><span class="line">		 *   - 32-bit prefetchable resources if there&#x27;s a 64-bit</span><br><span class="line">		 *     prefetchable window or no prefetchable window at all</span><br><span class="line">		 *   - 64-bit prefetchable resources if there&#x27;s no</span><br><span class="line">		 *     prefetchable window at all</span><br><span class="line">		 *</span><br><span class="line">		 * Note that the strategy in __pci_assign_resource() must</span><br><span class="line">		 * match that used here.  Specifically, we cannot put a</span><br><span class="line">		 * 32-bit prefetchable resource in a 64-bit prefetchable</span><br><span class="line">		 * window.</span><br><span class="line">		 */</span><br><span class="line">		pbus_size_mem(bus, mask, IORESOURCE_MEM, type2, type3,</span><br><span class="line">				realloc_head ? 0 : additional_mem_size,</span><br><span class="line">				additional_mem_size, realloc_head);</span><br><span class="line">		break;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__pci_bus_assign_resources(bus, &amp;add_list, NULL)</span><br><span class="line">        /* bus:0, 会对该bus上的所有device分别调用__dev_sort_resource</span><br><span class="line">	 * 然后统一调用一个__assign_resources_sorted()。之后程序进入</span><br><span class="line">	 * 下面的list中，又会嵌套进入__pci_bus_assign_resources, 这时</span><br><span class="line">	 * bus:1。重复上面的过程。在bus:1是__pci_bus_assign_resources</span><br><span class="line">	 * 在处理处理完pbus_assign_resources_sorted后不回往下执行,返回</span><br><span class="line">	 * 上层。这时bus:0, 进入pci_setup_bridge执行。</span><br><span class="line">	 * 其中，会在pbus_assign_resources_sorted中配置BAR，在</span><br><span class="line">	 * __pci_setup_bridge中配mem, I/O, prefetch mem的base/limit</span><br><span class="line">	 */</span><br><span class="line">    --&gt; pbus_assign_resources_sorted(bus, realloc_head, fail_head);</span><br><span class="line">    --&gt; list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list)</span><br><span class="line">        --&gt; __pci_bus_assign_resources(b, realloc_head, fail_head);</span><br><span class="line">	--&gt; switch (dev-&gt;class &gt;&gt; 8)</span><br><span class="line">		case PCI_CLASS_BRIDGE_PCI:</span><br><span class="line">		    --&gt; pci_setup_bridge(b);</span><br><span class="line"></span><br><span class="line">static void pbus_assign_resources_sorted()</span><br><span class="line">    --&gt; LIST_HEAD(head);</span><br><span class="line">    --&gt; list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list)</span><br><span class="line">		__dev_sort_resources(dev, &amp;head);</span><br><span class="line">                    /* 把pci_dev中的resource[]从大到小排序, 存入链表head中 */</span><br><span class="line">		    --&gt; pdev_sort_resources(dev, head);</span><br><span class="line">    --&gt; __assign_resources_sorted(&amp;head, realloc_head, fail_head);</span><br><span class="line">            /* 因为realloc_head为空链表，所以直接到requested_and_reassign */</span><br><span class="line">	--&gt; if (!realloc_head || list_empty(realloc_head))</span><br><span class="line">		goto requested_and_reassign;</span><br><span class="line">	...</span><br><span class="line">        requested_and_reassign:</span><br><span class="line">	--&gt; assign_requested_resources_sorted(head, fail_head);</span><br><span class="line">	        /* dev_res(pci_dev_resource)存储一个device中一个配置空间</span><br><span class="line">		 * 的资源(一个设备可有多个mem或I/O配置空间)</span><br><span class="line">		 */</span><br><span class="line">	    --&gt; list_for_each_entry(dev_res, head, list)</span><br><span class="line">		--&gt; resource_size(res) &amp;&amp;</span><br><span class="line">		pci_assign_resource(dev_res-&gt;dev, idx)</span><br><span class="line">	           --&gt; _pci_assign_resource(dev, resno, size, align);</span><br><span class="line">		   --&gt; pci_update_resource(dev, resno);</span><br><span class="line"></span><br><span class="line">        --&gt; reassign_resources_sorted(realloc_head, head);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PLKA第三章读书笔记</title>
    <url>/PLKA%E7%AC%AC%E4%B8%89%E7%AB%A0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="基础数据结构"><a href="#基础数据结构" class="headerlink" title="基础数据结构"></a>基础数据结构</h2><p>一个NUMA的描述数据结构是：struct pglist_data(include/linux/mmzone.h)。一个NUMA里<br>有多个不同的域，比如DMA，Normal，高端内存等，每一个这样的域用struct zone描述(mmzone.h)。<br>每个zone里的伙伴系统内存用struct free_area(mmzone.h)描述，free_area是用order索引的<br>一个数组，数组里又按MIGRATE_TYPE分为若干个链表。如果用一幅图，大概表示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> numa node 0:                                         numa node 1:</span><br><span class="line">+-------------------------------------------------+ +-------------------------+</span><br><span class="line">|struct pglist_data &#123;			          | | struct pglist_data &#123;    |</span><br><span class="line">| +----------------------------------------------+| |                         |</span><br><span class="line">| |struct zone &#123;                                 || |   [...]                 |</span><br><span class="line">| | +-------------------------------------------+|| |                         |</span><br><span class="line">| | |/* order 0 */                              ||| |                         |</span><br><span class="line">| | |struct free_area &#123;                         ||| |                         |</span><br><span class="line">| | |  struct list_head free_list[MIGRATE_TYPES]||| |                         |</span><br><span class="line">| | |&#125;                                          ||| |                         |</span><br><span class="line">| | +-------------------------------------------+|| |                         |</span><br><span class="line">| | +-------------------------------------------+|| |                         |</span><br><span class="line">| | |/* order 1 */                              ||| |                         |</span><br><span class="line">| | |struct free_area &#123;                         ||| |                         |</span><br><span class="line">| | |  [...]                                    ||| |                         |</span><br><span class="line">| | |&#125;                                          ||| |                         |</span><br><span class="line">| | +-------------------------------------------+|| |                         |</span><br><span class="line">| |  [...]                                       || |                         |</span><br><span class="line">| | +-------------------------------------------+|| |                         |</span><br><span class="line">| | |/* order MAX_ORDER - 1 */                  ||| |                         |</span><br><span class="line">| | |struct free_area &#123;                         ||| |                         |</span><br><span class="line">| | |  [...]                                    ||| |                         |</span><br><span class="line">| | |&#125;                                          ||| |                         |</span><br><span class="line">| | +-------------------------------------------+|| |                         |</span><br><span class="line">| |&#125;                                             || |                         |</span><br><span class="line">| +----------------------------------------------+| |                         |</span><br><span class="line">| +----------------------------------------------+| |                         |</span><br><span class="line">| |struct zone &#123;                                 || |                         |</span><br><span class="line">| |  [...]                                       || |                         |</span><br><span class="line">| |&#125;                                             || |                         |</span><br><span class="line">| +----------------------------------------------+| |                         |</span><br><span class="line">| +----------------------------------------------+| |                         |</span><br><span class="line">| |struct zone &#123;                                 || |                         |</span><br><span class="line">| |  [...]                                       || |                         |</span><br><span class="line">| |&#125;                                             || |                         |</span><br><span class="line">| +----------------------------------------------+| |                         |</span><br><span class="line">|  [...]                                          | |                         |</span><br><span class="line">|&#125;                                                | |&#125;                        |</span><br><span class="line">+-------------------------------------------------+ +-------------------------+</span><br></pre></td></tr></table></figure>
<p>可以看出struct zone这个数据结构是比较核心的，除了上面所示，zone中还有很多其他的<br>数据结构，比如用于管理页面回收的struct lruvec，最新的内核活跃页相关的链表都封装到了<br>lurvec结构中，用户管理冷热页的数据封装在struct per_cpu_pageset __percpu *pageset。<br>物理内存都是用page管理的，核心的数据结构是struct page (include/linux/mm_types.h)。<br>虚拟地址到物理地址翻译要用到页表，内核里也定义了一些页表相关的数据结构：PGD，PUD，<br>PMD，PTE是四级页表结构里的几个域段，与之相关的有一堆宏定义，各个体系结构需要自己<br>定义相关的内容，比如riscv64的定义在arch/riscv/include/asm/pgtable-64.h</p>
<h2 id="内存管理初始化"><a href="#内存管理初始化" class="headerlink" title="内存管理初始化"></a>内存管理初始化</h2><p>把start_kernel流程里关于内存初始化相关的步骤提取出来。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start_kernel</span><br><span class="line">  -&gt; page_address_init</span><br><span class="line">     /* 看下riscv里的实现：arch/riscv/kernel/setup.c */</span><br><span class="line">  -&gt; setup_arch</span><br><span class="line">     /*</span><br><span class="line">      * 解析device tree里定义的内存节点的内容，这里我用riscv qemu平台来调试，</span><br><span class="line">      * 所以memory的base在qemu/hw/riscv/virt.c里的VIRT_DRAM定义，是0x80000000，</span><br><span class="line">      * size的大小在qemu启动cmdline里定义，比如，-m 1024M，定义1GB的内存。</span><br><span class="line">      */</span><br><span class="line">    -&gt; parse_dtb()</span><br><span class="line">      -&gt; early_init_dt_scan</span><br><span class="line">        -&gt; early_init_dt_scan_nodes</span><br><span class="line">          -&gt; of_scan_flat_dt(early_init_dt_scan_memory, NULL)</span><br><span class="line">            -&gt; early_init_dt_add_memory_arch(base, size)</span><br><span class="line">              -&gt; memblock_add(base, size)</span><br><span class="line"></span><br><span class="line">    -&gt; setup_bootmem()</span><br><span class="line">    -&gt; paging_init()</span><br><span class="line">         /* zone里面内存的初始化从这里进入 */</span><br><span class="line">      -&gt; zone_sizes_init()</span><br><span class="line">    -&gt; swiotlb_init()</span><br><span class="line">    -&gt; kasan_init()</span><br><span class="line">  -&gt; build_all_zonelists()</span><br><span class="line">     /* lru和vm统计有关系 */</span><br><span class="line">  -&gt; page_alloc_init()</span><br><span class="line">  -&gt; mm_init()</span><br><span class="line">    -&gt; mem_init()</span><br><span class="line">         /* 这个函数把物理内存加到伙伴系统里 */</span><br><span class="line">      -&gt; memblock_free_all()</span><br><span class="line">        -&gt; free_low_memory_core_early()</span><br><span class="line">          -&gt; __free_memory_core()</span><br><span class="line">            /*</span><br><span class="line">             * 这里把若干个页搞成一个compound页，然后加到伙伴系统里。先都加到</span><br><span class="line">             * order最高的链表里。可以看到伙伴系统的初始化复用了free page的函数，</span><br><span class="line">             * free_one_page正式伙伴系统free page最核心的函数，具体分析在下面的</span><br><span class="line">             * 章节。</span><br><span class="line">             */</span><br><span class="line">            -&gt; free_one_page</span><br><span class="line">  -&gt; kmem_cache_init_late()</span><br><span class="line">  -&gt; setup_per_cpu_pageset()</span><br><span class="line">  -&gt; numa_policy_init()</span><br><span class="line">  -&gt; anon_vma_init()</span><br><span class="line">  -&gt; buffer_init()</span><br><span class="line">  -&gt; pagecache_init()</span><br></pre></td></tr></table></figure>

<h2 id="伙伴系统"><a href="#伙伴系统" class="headerlink" title="伙伴系统"></a>伙伴系统</h2><p>内核的伙伴系统可以作为分析Linux内存管理的线索，按照伙伴系统的逻辑基本可以把内存<br>管理的所有基本概念贯穿起来。总的来看，alloc_pages就可以贯穿起来，alloc_pages的<br>快速路径上，可以直接分配出想要的页，但是慢速路径上，也就是说伙伴系统里管理的内存<br>不够的时候，就会做各种内存回收的操作，如果有swap分区，就尝试把现有内存的内容写入<br>swap分区来腾出内存，使用内存紧缩(compact)来移动碎片化的内存，这样可以腾出来更大的<br>连续内存空间，在compact的时候就会使用到内存迁移(migrate page)，当这些都不行的时候，<br>还可以把page cache里的内存回收(reclaim)，这样还没有内存就只有杀死占用内存过多的<br>进程来回收内存了(oom)，在内存迁移的时候，因为一个page可能被map到了多个虚拟地址上，<br>所以要找到一个page对应的所有虚拟地址，这就要用到反向映射(reverse mapping)，迁移<br>的时候我们要断开VA到PA的映射，并重新建立VA到新PA的映射。内核用来分配小于一页内存<br>的内存池slab分配器。伙伴系统的内存最开始来自memblock，memblock解析dts或者acpi里的<br>信息得到系统中物理内存的配置。内存回收的时候要优先回收不用的冷页，相关的算法是LRU。</p>
<p>从虚拟地址的角度，对于每一块虚拟地址，在内核里有vma数据结构来管理，申请虚拟内存<br>可有用brk和mmap，申请的虚拟地址在第一次写的时候会触发fault，进程fork时对于虚拟地址<br>的管理也是同样的道理(COW)，触发fault后，内核会分配物理页并且建立页表(page table)，<br>这里的页有可能是大页(又分传统大页和透明大页)。</p>
<p>下面看伙伴系统具体的实现。</p>
<p>我们知道伙伴系统的基本模型是搞了很多不同order的链表，用这些链表存不同order的连续<br>物理内存，分配的时候如果小的order的内存不够了，就拆开打的order内存用，内存放回伙伴<br>系统里的时候，就放入对应order的链表，如果正好有他的“伙伴”也在伙伴系统中，就把他们<br>合并起来，放入更高order的链表。</p>
<p>如上所说的，伙伴系统中分配页的核心函数分为快速路径和慢速路径，慢速路径里要先做内存<br>回收等，然后再尝试分配页，所以要分析这里就需要把我们的分析范围不断扩大。我们一个<br>一个逻辑的看，这里先看page cache和他的回收，需要搞清楚的逻辑有，1. page cache<br>的创建逻辑，2. page cache回收的逻辑，就是有一堆页都在page cache里，优先回收谁。</p>
<p>page cache的创建逻辑可以顺着有文件背景的内存使用看下，分为对普通文件通过read/write<br>系统调用读写和对普通文件通过mmap把文件内容映射到内存进行访问，内核对于这两种读文件<br>的访问都会做预读，比如要读4KB的内容，内核可以读8KB，把多读的内容在page cache里先<br>存起来。我们已mmap一个文件的逻辑看下具体的调用关系。(这一部分需要配合8.5章节来看)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mmap</span><br><span class="line">  -&gt; ksys_mmap_pgoff</span><br><span class="line">    -&gt; vm_mmap_pgoff</span><br><span class="line">      -&gt; do_mmap</span><br><span class="line">         -&gt; mmap_region</span><br><span class="line">           -&gt; call_mmap</span><br><span class="line">             -&gt; file-&gt;f_op-&gt;mmap</span><br><span class="line">/*</span><br><span class="line"> * 具体的文件系统会实现mmap这个回调函数，我们以ext2为例看下：ext2_file_mmap</span><br><span class="line"> * 代码的位置在fs/ext2/file.c</span><br><span class="line"> */</span><br><span class="line">ext2_file_mmap</span><br><span class="line">     /*</span><br><span class="line">      * 这个函数里为相关的vma挂上了generic_file_vm_ops，并没有做其他的事情。</span><br><span class="line">      * 可以看出mmap文件的系统调用完成时，文件内容并没有被copy到内存里，知道用户</span><br><span class="line">      * 真是的读mmap文件的内容是，会进入fault流程。下面继续看fault流程。</span><br><span class="line">      */</span><br><span class="line">  -&gt; generic_file_mmap</span><br><span class="line"></span><br><span class="line">filemap_fault</span><br><span class="line">     /* 先去page cache里找 */</span><br><span class="line">  -&gt; find_get_page</span><br><span class="line">  -&gt; do_sync_mmap_readahead</span><br><span class="line">    -&gt; page_cache_sync_ra</span><br><span class="line">      -&gt; ondemand_readahead</span><br><span class="line">        -&gt; do_page_cache_ra</span><br><span class="line">             /* page cache 预读核心函数 */</span><br><span class="line">          -&gt; page_cache_ra_unbounded</span><br><span class="line">               /*</span><br><span class="line">                * 这里以ext2文件系统为例，其没有readpages回调，所以走到了这个分支。</span><br><span class="line">                * add_to_page_cache_lru是把page加入到lru链表的核心函数。</span><br><span class="line">                * (ext2 address_space_operations定义在fs/ext2/inode.c)</span><br><span class="line">                */</span><br><span class="line">            -&gt; add_to_page_cache_lru</span><br><span class="line">            -&gt; read_pages</span><br><span class="line"></span><br><span class="line">  -&gt; filemap_read_page</span><br><span class="line">     /* ext2_readahead, fs/ext2/inode.c */</span><br></pre></td></tr></table></figure>

<p>我们再看page cache的回收逻辑，就是内存不足了，先回收那些page的cache。这里关键是<br>要确定那些page我们认为是不经常访问的。struct page里的flags定义了一堆page状态的标记<br>位，这些标记位的查看函数是拿宏拼起来的，定义在include/linux/page-flags.h。struct page<br>里对page的引用计数有atomic_t _mapcount、atomic_t _refcount，其中_refcount是对page<br>最基础的引用计数，对page的引用，包括虚拟地址到page的映射，以及内核里内存管理相关<br>结构管理page是对其的引用都会增加_refcount的值，虚拟地址到page的映射也会增加_mapcount<br>的引用，_mapcount在逆向映射里使用。</p>
<p>先看下__alloc_pages里慢速路径里的内存回收的入口，具体分析需要在其他文档里展开。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">__alloc_pages</span><br><span class="line">  -&gt; __alloc_pages_slowpath</span><br><span class="line">    -&gt; wake_all_kswapds</span><br><span class="line">    -&gt; __alloc_pages_direct_compact</span><br><span class="line">      /* 直接内存回收在这里 */</span><br><span class="line">    -&gt; __alloc_pages_direct_reclaim</span><br><span class="line">      -&gt; __perform_reclaim</span><br><span class="line">           /* linux/mm/vmscan.c */</span><br><span class="line">        -&gt; try_to_free_pages</span><br><span class="line">          -&gt; do_try_to_free_pages</span><br></pre></td></tr></table></figure>

<h2 id="slab分配器"><a href="#slab分配器" class="headerlink" title="slab分配器"></a>slab分配器</h2><p>slab分配器的逻辑比较独立，我们先分析slab分配器的使用方式，再分析其实现。</p>
<p>创建和销毁slab内存池：kmem_cache_create/kmem_cache_free<br>从slab内存池里分配和释放内存：kmem_cache_alloc/kmem_cache_free</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>Linux内核</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>Parsing ranges item in pcie-designware.c</title>
    <url>/Parsing-ranges-item-in-pcie-designware-c/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int __init dw_pcie_host_init(struct pcie_port *pp)</span><br><span class="line">&#123;</span><br><span class="line">	...</span><br><span class="line">	/*</span><br><span class="line">	 * read out value of &quot;#address-cells&quot; as na and #size-cells as ns.</span><br><span class="line">	 * as you can see in dw pcie binding document, address-cells should</span><br><span class="line">	 * be set as 3 and size-cells should be set as 2 which are we already</span><br><span class="line">	 * done in pcie dts node. so here na = 3, np = 2.</span><br><span class="line">	 *</span><br><span class="line">	 * in fact, address-cells indicates how many bytes do we use to describe</span><br><span class="line">	 * a pci address: of cause thit is 3, one byte is bits flage which</span><br><span class="line">	 * indicates if it is a mem/io/cfg/pref, other two bytes are for a 64</span><br><span class="line">	 * bits address(pci address can be a 64 bits address). </span><br><span class="line">	 * size-cells indicate how many bytes do we use to describe a size of</span><br><span class="line">	 * above pci address, of cause it should be at least 2 to describe a</span><br><span class="line">	 * 64 bits address.</span><br><span class="line">	 */</span><br><span class="line">	of_property_read_u32(np, &quot;#address-cells&quot;, &amp;na);</span><br><span class="line">	ns = of_n_size_cells(np);</span><br><span class="line"></span><br><span class="line">	cfg_res = platform_get_resource_byname(pdev, IORESOURCE_MEM, &quot;config&quot;);</span><br><span class="line">	if (cfg_res) &#123;</span><br><span class="line">		pp-&gt;cfg0_size = resource_size(cfg_res)/2;</span><br><span class="line">		pp-&gt;cfg1_size = resource_size(cfg_res)/2;</span><br><span class="line">		pp-&gt;cfg0_base = cfg_res-&gt;start;</span><br><span class="line">		pp-&gt;cfg1_base = cfg_res-&gt;start + pp-&gt;cfg0_size;</span><br><span class="line"></span><br><span class="line">		/* Find the untranslated configuration space address */</span><br><span class="line">		index = of_property_match_string(np, &quot;reg-names&quot;, &quot;config&quot;);</span><br><span class="line">		addrp = of_get_address(np, index, NULL, NULL);</span><br><span class="line">		pp-&gt;cfg0_mod_base = of_read_number(addrp, ns);</span><br><span class="line">		pp-&gt;cfg1_mod_base = pp-&gt;cfg0_mod_base + pp-&gt;cfg0_size;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		dev_err(pp-&gt;dev, &quot;missing *config* reg space\n&quot;);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (of_pci_range_parser_init(&amp;parser, np)) &#123;</span><br><span class="line">		/* na indicates pci address address cells, ns is related size */</span><br><span class="line">		--&gt; const int na = 3, ns = 2;</span><br><span class="line">		/*</span><br><span class="line">		 * so parser-&gt;pna is cpu address cell number which can be found</span><br><span class="line">		 * in parent dts node. take hip05.dtsi as an example, it should</span><br><span class="line">		 * be 2, as:</span><br><span class="line">		 *   	peripherals &#123;</span><br><span class="line">		 *		compatible = &quot;simple-bus&quot;;</span><br><span class="line">		 *		#address-cells = &lt;2&gt;;</span><br><span class="line">		 *		#size-cells = &lt;2&gt;;</span><br><span class="line">		 *		...</span><br><span class="line">		 * note: pcie dts nodes should under peripherals node.</span><br><span class="line">		 * above size-cells and address-cells should be both 2 for</span><br><span class="line">		 * 64bits cpu address. all are wrong in hulk-3.19/hulk-4.1</span><br><span class="line">		 * hip05.dtsi.</span><br><span class="line">		 *</span><br><span class="line">		 * for a 32bits Soc, above address-cells and size-cells both</span><br><span class="line">		 * are 1, so parser-&gt;pna is 1</span><br><span class="line">		 */</span><br><span class="line">		--&gt; parser-&gt;pna = of_n_addr_cells(node);</span><br><span class="line">			--&gt; if (np-&gt;parent)</span><br><span class="line">				np = np-&gt;parent;</span><br><span class="line">			--&gt; ip = of_get_property(np, &quot;#address-cells&quot;, NULL);</span><br><span class="line">		/*</span><br><span class="line">		 * ranges =</span><br><span class="line">		 * &lt;0x82000000 0 0xb5100000 0x240 0x00000000 0 0x00f00000&gt;;</span><br><span class="line">		 * |----&gt; pci address &lt;----|--&gt; cpu addr &lt;--|--&gt; size &lt;--|</span><br><span class="line">		 */</span><br><span class="line">		--&gt; parser-&gt;np = parser-&gt;pna + na + ns;</span><br><span class="line">		/*</span><br><span class="line">		 * ranges = &lt;0x00000800 0 0x01f00000 0x01f00000 0 0x00080000</span><br><span class="line"> 		 * 	     0x81000000 0 0          0x01f80000 0 0x00010000</span><br><span class="line"> 		 *	     0x82000000 0 0x01000000 0x01000000 0 0x00f00000&gt;;</span><br><span class="line">		 *</span><br><span class="line">		 * like above ranges, parser-&gt;range will point to 0x00000800,</span><br><span class="line">		 * parser-&gt;end will point to 0x00f00000.</span><br><span class="line">		 */</span><br><span class="line">		--&gt; parser-&gt;range = of_get_property(node, &quot;ranges&quot;, &amp;rlen);</span><br><span class="line">		--&gt; parser-&gt;end = parser-&gt;range + rlen / sizeof(__be32);</span><br><span class="line"></span><br><span class="line">		dev_err(pp-&gt;dev, &quot;missing ranges property\n&quot;);</span><br><span class="line">		return -EINVAL;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	/* Get the I/O and memory ranges from DT */</span><br><span class="line">	for_each_of_pci_range(&amp;parser, &amp;range) &#123;</span><br><span class="line">	  --&gt; of_pci_range_parser_one(parser, range)</span><br><span class="line">	    /* first byte in ranges in each line */</span><br><span class="line">	    --&gt; range-&gt;pci_space = parser-&gt;range[0];</span><br><span class="line">	    /*</span><br><span class="line">	     * from below function, we can see only mem/io/32bits/64bits/</span><br><span class="line">	     * prefetch bits are valid. so even we set other bits in dts,</span><br><span class="line">	     * code does not parse them.</span><br><span class="line">	     *</span><br><span class="line">	     * for each bit&#x27;s meaning, you can refer to:</span><br><span class="line">	     * http://www.devicetree.org/Device_Tree_Usage</span><br><span class="line">	     */</span><br><span class="line">	    --&gt; range-&gt;flags = of_bus_pci_get_flags(parser-&gt;range);</span><br><span class="line">	    /* point to pci address */</span><br><span class="line">	    --&gt; range-&gt;pci_addr = of_read_number(parser-&gt;range + 1, ns);</span><br><span class="line">	    /* point to cpu address, not very clear below function */</span><br><span class="line">	    --&gt; range-&gt;cpu_addr = of_translate_address(parser-&gt;node,</span><br><span class="line">				  parser-&gt;range + na);</span><br><span class="line">	    /* point to size section */</span><br><span class="line">	    --&gt; range-&gt;size = of_read_number(parser-&gt;range + parser-&gt;pna + na, ns);</span><br><span class="line">	    /*</span><br><span class="line">	     * after below, parser-&gt;range points to 0x81000000 for example,</span><br><span class="line">	     * next time, we start to parse the second line of ranges.</span><br><span class="line">	     */</span><br><span class="line">	    --&gt; parser-&gt;range += parser-&gt;np;</span><br><span class="line">	    /*</span><br><span class="line">	     * in below loop, it find the ranges sub-item which has same type </span><br><span class="line">	     * and are contiguous.</span><br><span class="line">	     */</span><br><span class="line">	    --&gt; while (parser-&gt;range + parser-&gt;np &lt;= parser-&gt;end) &#123;</span><br><span class="line">			...</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line">		...</span><br><span class="line">		if (restype == IORESOURCE_MEM) &#123;</span><br><span class="line">			of_pci_range_to_resource(&amp;range, np, &amp;pp-&gt;mem);</span><br><span class="line">			pp-&gt;mem.name = &quot;MEM&quot;;</span><br><span class="line">			pp-&gt;mem_size = resource_size(&amp;pp-&gt;mem);</span><br><span class="line">			pp-&gt;mem_bus_addr = range.pci_addr;</span><br><span class="line"></span><br><span class="line">			/*</span><br><span class="line">			 * take mem as example, ranges item as below:</span><br><span class="line">			 * ranges =</span><br><span class="line">			 * &lt;0x82000000 0 0xb5100000 0x240 0x00000000 0 0x00f00000&gt;;</span><br><span class="line">			 * now parser.range points to 0x00f00000,</span><br><span class="line">			 * parser.rang - parser.np shifts points to 0x82000000.</span><br><span class="line">			 * &quot;+ na&quot; will add pci address offset, so now point to</span><br><span class="line">			 * start of cpu address: 0x240</span><br><span class="line">			 *</span><br><span class="line">			 * the problem of v3 patch: of_n_addr_cells(np) - 5 + na</span><br><span class="line">			 * is ok, but there went wrong in parser_range_end.</span><br><span class="line">			 * if there is only one line in ranges item which is</span><br><span class="line">			 * my test case, it is ok. but if there are more than</span><br><span class="line">			 * one line in ranges item, parser_range_end points to</span><br><span class="line">			 * the end of ranges item which should points to each</span><br><span class="line">			 * line end.</span><br><span class="line">			 *</span><br><span class="line">			 * pp-&gt;mem_mod_base = of_read_number(parser_range_end -</span><br><span class="line">			 *		of_n_addr_cells(np) - 5 + na, ns);</span><br><span class="line">			 *</span><br><span class="line">			 * there is one problem: mem_mod_base is same with</span><br><span class="line">			 * mem_base, why do we use mem_base directly??</span><br><span class="line">			 */</span><br><span class="line">			pp-&gt;mem_mod_base = of_read_number(parser.range -</span><br><span class="line">							  parser.np + na, ns);</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line">		if (restype == 0) &#123;</span><br><span class="line">			of_pci_range_to_resource(&amp;range, np, &amp;pp-&gt;cfg);</span><br><span class="line">			pp-&gt;cfg0_size = resource_size(&amp;pp-&gt;cfg)/2;</span><br><span class="line">			pp-&gt;cfg1_size = resource_size(&amp;pp-&gt;cfg)/2;</span><br><span class="line">			pp-&gt;cfg0_base = pp-&gt;cfg.start;</span><br><span class="line">			pp-&gt;cfg1_base = pp-&gt;cfg.start + pp-&gt;cfg0_size;</span><br><span class="line"></span><br><span class="line">			/* Find the untranslated configuration space address */</span><br><span class="line">			pp-&gt;cfg0_mod_base = of_read_number(parser.range -</span><br><span class="line">							   parser.np + na, ns);</span><br><span class="line">			pp-&gt;cfg1_mod_base = pp-&gt;cfg0_mod_base +</span><br><span class="line">					    pp-&gt;cfg0_size;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>SMMU SSV的逻辑</title>
    <url>/SMMU-SSV%E7%9A%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>SMMU的STE表里的S1DSS=0b10时(目前的Linux主线内核代码是这样配置)，对于外设报文的<br>处理逻辑是：</p>
<ul>
<li><p>a transction without a Substream ID is accepted and uses the CD of Substream 0.</p>
</li>
<li><p>a transction that arrive with Substream ID 0 are aborted and an event recorded.</p>
</li>
</ul>
<p>上面SMMU判断一个外设上来的请求是否带有Substream ID的方法是看SMMU和外设之间的一个<br>叫SSV的信号有没有使能。这个SSV信号一般是外设可以配置的，如果一个外设把这个SSV<br>配置成1, 发给SMMU的请求中的PASID有是0的话, 就会报SMMU event错误。</p>
<p>常见硬件设计错误是把这个SSV的配置搞成设备全局的。这样这个设备只能要么工作在内核<br>态，要么工作在用户态，不能一部分资源工作在内核态，一部分工作在用户态。</p>
<p>正确的硬件设计是，把SSV的配置粒度减小。对于在内核态使用的资源，软件把对应的SSV配置<br>成0，对于在用户态使用的资源，软件把对应的SSV配置成1。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>SMMU</tag>
      </tags>
  </entry>
  <entry>
    <title>SMMU TLBI分析</title>
    <url>/SMMU-TLBI%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="SMMU-E2H支持"><a href="#SMMU-E2H支持" class="headerlink" title="SMMU E2H支持"></a>SMMU E2H支持</h2><p> SMMU SVA支持的patchset里有一个补丁：iommu/arm-smmu-v3: Add support for VHE<br> 这个补丁的基本逻辑是, CPU支持VHE，且内核支持时，host kernel工作在EL2，在SVA<br> 下，因为需要做DVM tlbi广播，且tlbi广播在同一个EL里生效，所以需要SMMU这边的TLB<br> 也打上EL2的标记。这个是通过设置设备对应的STE里的STRW生效的，这个配置对SMMU tlb<br> 影响是全局的。</p>
<p> 这里面还有三个逻辑：</p>
<ol>
<li><p>要使得DVM tlbi的广播生效，还要设置下SMMU CR2.E2H寄存器(使得ASID标记生效)。<br>这个补丁里配置了E2H。</p>
</li>
<li><p>因为STRW的配置是STE全局的，SMMU管理的内核地址的TLB也打上了EL2的标记，这就<br>需要把原来对内核地址的tlbi命令从TLBI_NH改成TLBI_VA。</p>
</li>
<li><p>原来invalidate内核地址的tlbi命令又可以分为TLBI_VA和TLBI_ASID，所以，上面<br>一步还要把原来的TLBI_NH_VA/TLBI_NH_ASID都改成TLBI_EL2_VA/TLBI_EL2_ASID。</p>
</li>
</ol>
<p> 这个补丁做了上述的修改，使得SVA和内核地址的tlbi在VHE使能的时候都可以正常使用。</p>
<h2 id="SMMU-tlbi相关命令"><a href="#SMMU-tlbi相关命令" class="headerlink" title="SMMU tlbi相关命令"></a>SMMU tlbi相关命令</h2><p> SMMU spec 4.4章节介绍了tlbi相关的command, 上面提到的相关tlbi命令在这一章节中<br> 都有详细的描述。EL2和NH的标记在上面已经介绍。关于VA和ASID标记的区别在于做tlb<br> 无效化的时候作用的范围是不一样的。简单的讲带ASID的tlbi会无效化对应asid的tlb，<br> 理论上讲，SVA需要用这个，但是现在SVA用的是DVM，还没有用到带ASID的tlbi，在实际<br> 分析的时候，还看到no-strict时使用的是带ASID的tlbi，这个下面结合代码来分析。<br> 直接带VA的tlbi可以对一段地址做tlb无效化，相关的command域段里的值有: scale,<br> num, asid, address, tg, ttl, leaf。当SMMU硬件支持tlbi by range(IDR3_RIL = 1)时，<br> 还可以用带VA的tlbi实现tlbi by range.</p>
<h2 id="linux-SMMU驱动里的实现"><a href="#linux-SMMU驱动里的实现" class="headerlink" title="linux SMMU驱动里的实现"></a>linux SMMU驱动里的实现</h2><p> 因为SVA没有用tlbi，所以我们主要分析内核dma内存中的tlbi就好了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/iommu/dma-iommu.c */</span><br><span class="line">iommu_dma_free</span><br><span class="line">  +-&gt; __iommu_dma_unmap</span><br><span class="line">    /* 用来init iotlb_gather，为以后做tlbi by range做准备 */</span><br><span class="line">    +-&gt; iommu_iotlb_gather_init</span><br><span class="line">    +-&gt; iommu_unmap_fast</span><br><span class="line">      ...</span><br><span class="line"> +-&gt; arm_smmu_unmap</span><br><span class="line">   /* drivers/iommu/io-pgtable-arm.c, 这一步跳的有点大:) */</span><br><span class="line">   +-&gt; arm_lpae_unmap </span><br><span class="line">     +-&gt; __arm_lpae_unmap</span><br><span class="line">       +-&gt; io_pgtable_tlb_add_page</span><br><span class="line">  /* drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c */</span><br><span class="line">  +-&gt; arm_smmu_tlb_inv_page_nosync </span><br><span class="line">    /*</span><br><span class="line">     * 不断的收集tlbi的range，遇到不相连的range就做tlbi</span><br><span class="line">     */</span><br><span class="line">    +-&gt; iommu_iotlb_gather_add_page</span><br><span class="line">      +-&gt; iommu_tlb_sync</span><br><span class="line">        /* arm-smmu-v3.c */</span><br><span class="line">        +-&gt; arm_smmu_iotlb_sync</span><br><span class="line">	  /* 使用VA tlbi，range invalidate的逻辑也在这里 */</span><br><span class="line">	  +-&gt; arm_smmu_tlb_inv_range</span><br><span class="line"></span><br><span class="line">    +-&gt; iommu_tlb_sync (如果没有fq_domain，即使用strict mode)</span><br><span class="line">      +-&gt; arm_smmu_iotlb_sync</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p> 使用no-strict模式会起一个定时器, 在队列满或者定时器到时做tlbi。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/iommu/iova.c */</span><br><span class="line">iova_domain_flush</span><br><span class="line">  /* drivers/iommu/dma-iommu.c */</span><br><span class="line">  +-&gt; iommu_dma_flush_iotlb_all</span><br><span class="line">    /* arm-smmu-v3.c */</span><br><span class="line">    +-&gt; arm_smmu_flush_iotlb_all</span><br><span class="line">      /* 这个函数里会下发带ASID的tlb */</span><br><span class="line">      +-&gt; arm_smmu_tlb_inv_context</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe学习笔记(二)</title>
    <url>/PCIe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C/</url>
    <content><![CDATA[<p> 调用关系：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_scan_root_bus</span><br><span class="line">    --&gt; pci_create_root_bus</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * device说明见下文，bus是根总线号，ops是配置空间读写函数的接口，需要驱动作者</span><br><span class="line"> * 传入回调函数, 会在pci_scan_child_bus-&gt;pci_scan_slot-&gt;pci_scan_single_device-&gt;</span><br><span class="line"> * pci_scan_device-&gt;pci_bus_read_dev_vendor_id调用到该ops中的read函数。sysdata</span><br><span class="line"> * 传入私有数据。resources链表的元素是struct pci_host_bridge_window, 是dts上</span><br><span class="line"> * 读上来的总线号，mem空间，I/O空间的信息, 一般一个pci_host_bridge_window对应</span><br><span class="line"> * 一个信息</span><br><span class="line"> */</span><br><span class="line">struct pci_bus *pci_create_root_bus(struct device *parent, int bus,</span><br><span class="line">		struct pci_ops *ops, void *sysdata, struct list_head *resources)</span><br><span class="line">&#123;</span><br><span class="line">	...</span><br><span class="line">	/*</span><br><span class="line">	 * 分配 struct pci_host_bridge, 初始化其中的windows链表</span><br><span class="line">	 * windows链表上的存的结构是：struct pci_host_bridge_window</span><br><span class="line">	 * struct pci_host_bridge_window &#123;</span><br><span class="line">	 * 	struct list_head list;</span><br><span class="line">	 * 	struct resource *res;	/* host bridge aperture (CPU address) */</span><br><span class="line">	 * 	resource_size_t offset;	/* bus address + offset = CPU address */</span><br><span class="line">	 * &#125;;</span><br><span class="line">	 */</span><br><span class="line">	bridge = pci_alloc_host_bridge();</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * 输入参数parent来自pci host驱动中pci host核心结构的struct device *dev,</span><br><span class="line">	 * dev来自 platform_device 中的dev。可以以drivers/pci/host下的pci-mvebu.c</span><br><span class="line">	 * 作为例子, 其中所谓的pci host核心结构是：struct mvebu_pcie</span><br><span class="line">	 */</span><br><span class="line">	bridge-&gt;dev.parent = parent;</span><br><span class="line"></span><br><span class="line">	/* 分配 struct pci_bus */</span><br><span class="line">	b = pci_alloc_bus(NULL);</span><br><span class="line"></span><br><span class="line">	b-&gt;sysdata = sysdata;</span><br><span class="line">	b-&gt;ops = ops;</span><br><span class="line">	b-&gt;number = b-&gt;busn_res.start = bus;</span><br><span class="line">	/* 在pcie dts节点中找见domain字段, 加入pci_bus的domain_nr */</span><br><span class="line">	pci_bus_assign_domain_nr(b, parent);</span><br><span class="line">	/*</span><br><span class="line">	 * 在pci_root_buses全局链表中找相应domain下的bus, 首次调用的时候返回NULL</span><br><span class="line">	 * 上面分配的pci_root_buses是在当前函数的最后才加入pci_root_buses中的，现在该</span><br><span class="line">	 * 全局链表为空</span><br><span class="line">	 */</span><br><span class="line">	b2 = pci_find_bus(pci_domain_nr(b), bus);</span><br><span class="line">	/*</span><br><span class="line">	 * 上面两行处理有关pci domain的信息，kernel pci子系统怎么处理pci domain</span><br><span class="line">	 * 的呢？ 首先数据结构是全局的链表：pci_root_buses, 局部链表：pci_domain_busn_res_list</span><br><span class="line">	 * pci_root_buses中存放每个pci domain的根总线，根总线在pci_create_root_bus</span><br><span class="line">	 * 函数的结尾被添加到pci_root_buses链表中。pci_domain_busn_res_list存放</span><br><span class="line">	 * 各个domain的信息, 包括domain号、domain包含的bus号范围, 该链表上存放</span><br><span class="line">	 * 存放的结构是：struct pci_domain_busn_res, 在函数get_pci_domain_busn_res</span><br><span class="line">	 * 中查找相应domain号的pci_domain_busn_res, 如果没有就分配一个新的</span><br><span class="line">	 * pci_domain_busn_res, 然后加到pci_domain_busn_res_list上</span><br><span class="line">	 */</span><br><span class="line"></span><br><span class="line">	bridge-&gt;bus = b;</span><br><span class="line">	dev_set_name(&amp;bridge-&gt;dev, &quot;pci%04x:%02x&quot;, pci_domain_nr(b), bus);</span><br><span class="line">	error = pcibios_root_bridge_prepare(bridge);</span><br><span class="line"></span><br><span class="line">	error = device_register(&amp;bridge-&gt;dev);</span><br><span class="line"></span><br><span class="line">	b-&gt;bridge = get_device(&amp;bridge-&gt;dev);</span><br><span class="line">	device_enable_async_suspend(b-&gt;bridge);</span><br><span class="line">	pci_set_bus_of_node(b);</span><br><span class="line"></span><br><span class="line">	if (!parent)</span><br><span class="line">		set_dev_node(b-&gt;bridge, pcibus_to_node(b));</span><br><span class="line"></span><br><span class="line">	b-&gt;dev.class = &amp;pcibus_class;</span><br><span class="line">	/* b-&gt;bridge 为对应pci_host_bridge中struct device dev的指针 */</span><br><span class="line">	b-&gt;dev.parent = b-&gt;bridge;</span><br><span class="line">	dev_set_name(&amp;b-&gt;dev, &quot;%04x:%02x&quot;, pci_domain_nr(b), bus);</span><br><span class="line">	error = device_register(&amp;b-&gt;dev);</span><br><span class="line"></span><br><span class="line">	pcibios_add_bus(b);</span><br><span class="line"></span><br><span class="line">	/* Create legacy_io and legacy_mem files for this bus */</span><br><span class="line">	pci_create_legacy_files(b);</span><br><span class="line"></span><br><span class="line">	...</span><br><span class="line">	/*</span><br><span class="line">	 * 取出pci_create_root_bus函数传入的链表resources中的pci_host_bridge_window,</span><br><span class="line">	 * 把每个pci_host_bridge_window加入pci_host_bridge中的window链表中</span><br><span class="line">	 */</span><br><span class="line">	list_for_each_entry_safe(window, n, resources, list) &#123;</span><br><span class="line">		list_move_tail(&amp;window-&gt;list, &amp;bridge-&gt;windows);</span><br><span class="line">		res = window-&gt;res;</span><br><span class="line">		offset = window-&gt;offset;</span><br><span class="line">		if (res-&gt;flags &amp; IORESOURCE_BUS)</span><br><span class="line">		    	/*</span><br><span class="line">			 * 一般的，resources链表上有bus number, MEM space, I/O</span><br><span class="line">			 * space的节点，如果是bus number节点则调用以下函数。该</span><br><span class="line">			 * 函数会找到当前pci_bus的父结构，生成pci_bus中的busn_res</span><br><span class="line">			 * 并和父结构中的struct resource busn_res建立联系。</span><br><span class="line">			 * 如果父子在bus号上存在冲突，则返回冲突的bus号[1]</span><br><span class="line">			 */</span><br><span class="line">			pci_bus_insert_busn_res(b, bus, res-&gt;end);</span><br><span class="line">		else</span><br><span class="line">			/*</span><br><span class="line">			 * 向pci_bus中的链表resources中加入struct pci_bus_resource</span><br><span class="line">			 * 记录mem, io的资源</span><br><span class="line">			 */</span><br><span class="line">			pci_bus_add_resource(b, res, 0);</span><br><span class="line">		if (offset) &#123;</span><br><span class="line">			if (resource_type(res) == IORESOURCE_IO)</span><br><span class="line">				fmt = &quot; (bus address [%#06llx-%#06llx])&quot;;</span><br><span class="line">			else</span><br><span class="line">				fmt = &quot; (bus address [%#010llx-%#010llx])&quot;;</span><br><span class="line">			snprintf(bus_addr, sizeof(bus_addr), fmt,</span><br><span class="line">				 (unsigned long long) (res-&gt;start - offset),</span><br><span class="line">				 (unsigned long long) (res-&gt;end - offset));</span><br><span class="line">		&#125; else</span><br><span class="line">			bus_addr[0] = &#x27;\0&#x27;;</span><br><span class="line">		dev_info(&amp;b-&gt;dev, &quot;root bus resource %pR%s\n&quot;, res, bus_addr);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	down_write(&amp;pci_bus_sem);</span><br><span class="line">	/* 把创建的pci_bus加入全局链表pci_root_buses中 */</span><br><span class="line">	list_add_tail(&amp;b-&gt;node, &amp;pci_root_buses);</span><br><span class="line">	up_write(&amp;pci_bus_sem);</span><br><span class="line"></span><br><span class="line">	return b;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>[1] 关于linux中resource结构对资源的管理可以参看: <span class="exturl" data-url="aHR0cDovL3d3dy5saW51eGlkYy5jb20vTGludXgvMjAxMS0wOS80MzcwOC5odG0=">http://www.linuxidc.com/Linux/2011-09/43708.htm<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>Some tips to help to boot an installed Linux distribution manually</title>
    <url>/Some-tips-to-help-to-boot-an-installed-Linux-distribution-manually/</url>
    <content><![CDATA[<ol>
<li><p>enther UEFI shell: D06&gt;</p>
</li>
<li><p>enter “start fs1:\EFI\redhat\grubaa64.efi”</p>
<p>this step is to load grubaa64.efi. fs1 here is the partition<br>in which there is a grubaa64.efi. you maybe need to try other<br>partitions like: fs0, fs1, fs2…</p>
<p>for linux, the EFI partition will be mounted under /boot/efi/</p>
</li>
<li><p>then you will enter the shell of grub: grub&gt;<br>(or your grub will load grub.conf directly, then you can directly<br> go into grub menu)</p>
</li>
<li><p>find where is grub.cfg in grub shell, then enter:</p>
<p>configfile grub.cfg<br>(my case is: configfile (hd1,gpt1)/efi/redhat/grub.conf)</p>
<p>in this step, you may need go around to see where is the grub.conf,<br>you can use “ls” in grub shell, and “tab” works also in grub shell,<br>path can be completed by “tab”</p>
<p>then enter grub menu</p>
</li>
<li><p>choose which kernel to boot</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>UEFI</tag>
      </tags>
  </entry>
  <entry>
    <title>TPH analysis</title>
    <url>/TPH-analysis/</url>
    <content><![CDATA[<h2 id="TPH-analysis"><a href="#TPH-analysis" class="headerlink" title="TPH analysis"></a>TPH analysis</h2><p>-v0.1 2017.6.1 Sherlock init</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">               +-----+</span><br><span class="line">               | CPU |</span><br><span class="line">               +--+--+</span><br><span class="line">                  |</span><br><span class="line">             +----+-----+</span><br><span class="line">             | L3 cache |</span><br><span class="line">             +----+-----+</span><br><span class="line">                  |                 +-----+</span><br><span class="line">----------+-------+-----------------+ DDR |</span><br><span class="line">          |                         +-----+</span><br><span class="line">          |</span><br><span class="line">        +-+--+ &lt;--- stash table</span><br><span class="line">        | RP |</span><br><span class="line">        +-+--+</span><br><span class="line">          |</span><br><span class="line">        +-+--+ &lt;--- TH, PH, ST</span><br><span class="line">        | EP |</span><br><span class="line">        +----+</span><br></pre></td></tr></table></figure>

<p>TPH is a feature by which we can controll if TLP from EP can read/write to DDR<br>or L3 cache directly. As the attribute of read/write flow is different, e.g.<br>some data will be read soon by CPU, after a DMA write to DDR/Cache, for this<br>kind of date, it is better to put them directly to cache.</p>
<p>Physically, RP will do the read/write to DDR/cach. TH, PH, ST in PCIe TLP<br>will be used to control above operation. TH bit(one bit) in TLP indicates if we<br>enable TPH feature. PH(2 bits) in TLP indicates which kind of flow it is. ST is<br>a hardware specific design, which can be stored in MSI-X table or TPH capability.</p>
<p>So the software interface about TPH is:</p>
<pre><code>    PR: Device Capability 2(offset 0x24): TPH Completer Supported(12,13bit RO)
    EP: TPH Requester Capability: Header, capability register(RO), controler register(RW),
        ST table.(ST table can be in MSI-X table)
    EP&#39;s DMA descriptor: must have a place to offer PH related information.
</code></pre>
<p>In our system, we have above registers, ST table is in EP’s MSI-X table. Further<br>more, we have a stash table in RP. We implement ST table with 8 bit ST entry in EP,<br>which indicates which core/cluster one flow is expected to go to. The stash table<br>is a self-defined table, which is used to translate PH/ST info to stash info.</p>
<p>So from software view, we can controll TPH from EP’s DMA descriptor, ST table,<br>stash table. Now ST table and stash table have relationship, it is hard to modify<br>a PCIe device driver according to our chip’s ST table definition :(</p>
<p>But currently there is no clear definition about ST in PCIe spec and ARM spec.</p>
<p>Above is the first consideration, which means we may need private patch to do<br>optimization in the future.</p>
<p>The second consideration is as stach table will be implemented in BIOS, which<br>is hard to modified, we only can use ST and DMA descriptor to controll TPH behavior.</p>
<p>Then third is that we should firstly configure cacheable attribute, then enable<br>TPH to put data to L3 cache directly, e.g. we should firstly enable CCA=1 for RP,<br>then enable TPH feature(e.g. we can enable SMMU and CCA=1 to enable cacheable attribute).</p>
<p>The last is integrated PCIe devices, e.g. networking, also have TPH like feature.</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>UEFI/Linux系统加载过程</title>
    <url>/UEFI-Linux%E7%B3%BB%E7%BB%9F%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>一般一个从硬盘启动的linux系统的启动顺序是：UEFI-&gt;GRUB-&gt;Linux。其中，我们一般称<br>UEFI是固件，GRUB是bootloader. </p>
<p>顾名思义，一般我们认为固件是固化在系统里的，启动会自动加载、执行的一段代码，这<br>里我们不关心固件的存储位置。一般，bootloader和linux kernel镜像都是放在磁盘上<br>(我们这里只看磁盘启动的情况，不关心网络启动，e.g. PXE)。</p>
<p>UEFI在加载bootloader(e.g. grub)的时候会从EFI system分区寻找grub程序(e.g. grub.efi).<br>这个程序是一个UEFI环境中的可执行程序。一般，UEFI里会在EFI system分区上的一组路径<br>上搜索grub.efi，这组路径是提前在UEFI静态配置好的。EFI system分区必须被格式化成<br>FAT文件系统，这样UEFI才可以读取其中的文件。grub.efi加载后，会找见它对应的配置<br>文件grub.cfg. 在grub.cfg中，我们可以配置grub到哪里去加载kernel镜像, 以及到哪里去<br>加载根文件系统. 一个grub.cfg的配置类似:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Sample GRUB configuration file</span><br><span class="line">#</span><br><span class="line"># Boot automatically after 5 secs.</span><br><span class="line">set timeout=5</span><br><span class="line"># By default, boot the Estuary with Centos filesystem</span><br><span class="line">set default=d05_centos_sata_console</span><br><span class="line"># For booting GNU/Linux</span><br><span class="line"></span><br><span class="line">menuentry &quot;D05 Centos SATA(CONSOLE)&quot; --id d05_centos_sata_console &#123;</span><br><span class="line">        search --no-floppy --fs-uuid --set=root &lt;UUID&gt;</span><br><span class="line">                linux /Image pcie_aspm=off pci=pcie_bus_perf rootwait root=PARTUUID=&lt;PARTUUID&gt; rw</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中上面的UUID是存放Image的分区的UUID, PARTUUID是存放根文件系统的分区的PARTUUID.<br>UUID, PARTUUID可以事先用blkid得到，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# blkid</span><br><span class="line">/dev/sdc4: UUID=&quot;87b76c0a-7c76-4d2a-9414-6b52e6a00b1c&quot; TYPE=&quot;xfs&quot; PARTUUID=&quot;511f3dc7-4b6b-4991-975e-1d60e5e3e616&quot; </span><br><span class="line">/dev/sdc2: UUID=&quot;2ee48246-43a2-4014-a176-5d723e6be5b4&quot; TYPE=&quot;xfs&quot; PARTUUID=&quot;c724693a-751a-40c7-a8bd-d3bdd56311e1&quot; </span><br><span class="line">/dev/sdc1: SEC_TYPE=&quot;msdos&quot; UUID=&quot;80A6-A4F2&quot; TYPE=&quot;vfat&quot; PARTLABEL=&quot;EFI System Partition&quot; PARTUUID=&quot;f768b945-a14d-47d0-a5c0-371d4a163316&quot; </span><br><span class="line">/dev/sdc3: UUID=&quot;417d3354-a5f8-4a2c-81e4-ef03b9d51e94&quot; TYPE=&quot;swap&quot; PARTUUID=&quot;d9ae07de-fdad-4fcc-8621-deeadace67c5&quot; </span><br><span class="line">/dev/sdc5: UUID=&quot;c03a5dee-f751-4ff5-bc7c-f6026388a676&quot; TYPE=&quot;xfs&quot; PARTUUID=&quot;29860a29-e084-4105-815b-1a94c296a473&quot; </span><br></pre></td></tr></table></figure>

<p>按照上面的配置做的系统只是一个临时可用的系统，在系统启动之后，如果不加入其他的<br>操作，启动分区(上面存放Image, grub.efi, grub.cfg)是没有办法挂在到根文件系统上的。<br>这样，如果我们想把内核编译成一个RPM包，然后用rpm -i kernel-package安装到系统，<br>使得下次系统启动的时候可以用新安装的内核，这样是不可能的。</p>
<p>rpm -i kernel-package的时候会把相关的配置加载grub.cfg里。to do…</p>
<p>实际上一个标准的ISO安装是这样的, 安装过成应该会自动的分区和在启动分区里放置相应<br>的文件，并且更改/etc/fstab里的内容，实现开机自动挂在启动分区到/boot. to do…</p>
<p>一个例子:</p>
<ul>
<li>磁盘分区: fdisk [4]</li>
<li>拷贝grub.efi, grub.cfg, kernel Image到对应的EFI system分区</li>
<li>更改grub.cfg里的参数, 主要是修改上面提到的UUID和PARTUUID</li>
<li>拷贝文件系统到PARTUUID对应的分区里</li>
</ul>
<p>reference:</p>
<p>[1] <span class="exturl" data-url="aHR0cDovL3d3dy5yb2RzYm9va3MuY29tL2xpbnV4LXVlZmkv">http://www.rodsbooks.com/linux-uefi/<i class="fa fa-external-link-alt"></i></span><br>[2] <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRUZJX3N5c3RlbV9wYXJ0aXRpb24=">https://en.wikipedia.org/wiki/EFI_system_partition<i class="fa fa-external-link-alt"></i></span> (install bootloader)<br>[3] <span class="exturl" data-url="aHR0cDovL3d3dy5yb2RzYm9va3MuY29tL2VmaS1ib290bG9hZGVycy9pbnN0YWxsYXRpb24uaHRtbA==">http://www.rodsbooks.com/efi-bootloaders/installation.html<i class="fa fa-external-link-alt"></i></span><br>[4] <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29wZW4tZXN0dWFyeS9lc3R1YXJ5L2Jsb2IvbWFzdGVyL2RvYy9EZXBsb3lfTWFudWFsLjREMDUubWQjMy4z">https://github.com/open-estuary/estuary/blob/master/doc/Deploy_Manual.4D05.md#3.3<i class="fa fa-external-link-alt"></i></span><br>[4] <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29wZW4tZXN0dWFyeS9lc3R1YXJ5L2Jsb2IvbWFzdGVyL2RvYy9EZXBsb3lfTWFudWFsLjREMDUubWQjMy4z">https://github.com/open-estuary/estuary/blob/master/doc/Deploy_Manual.4D05.md#3.3<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>UEFI</tag>
      </tags>
  </entry>
  <entry>
    <title>SMMU stalled transaction with device</title>
    <url>/SMMU-stalled-transaction-with-device/</url>
    <content><![CDATA[<p>Currently when process dies, there is no way(software callback function) to<br>control device DMA stop. The software flow is as below:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     +-&gt; exit_mmap</span><br><span class="line">       +-&gt; mmu_notifier_release</span><br><span class="line">// iommu-sva.c: </span><br><span class="line">iommu_mmu_notifier_ops</span><br><span class="line">  +-&gt; io_mm_release</span><br><span class="line">    +-&gt; io_mm-&gt;ops-&gt;clear</span><br><span class="line">    // arm-smmu-v3.c:</span><br><span class="line">    arm_smmu_mm_ops-&gt;arm_smmu_mm_clear</span><br><span class="line">      +-&gt; arm_smmu_write_ctx_desc</span><br><span class="line">        +-&gt; __arm_smmu_write_ctx_desc</span><br><span class="line">	  +-&gt; if (cd == &amp;invalid_cd) </span><br><span class="line">	    +-&gt; CD.S = CD.R = 0; EPD0 = 1;</span><br><span class="line">	  +-&gt; arm_smmu_sync_cd</span><br><span class="line">	    +-&gt; CD CFGI</span><br></pre></td></tr></table></figure>
<p>This will stop smmu stall and stop recore events in SMMU event q, and prevent<br>page table walk. Note that: CD.S = CD.R = 0; EPD0 = 1 does not prevent translation.<br>So if there is a transaction from device can SMMU TLB hit, this transaction can<br>still be sent out SMMU to system bus. Another Note: CD.S = CD.R = 0; EPD0 = 1<br>works after CD CFGI.</p>
<p>Above flow may happen together with normal SMMU fault flow, consider this case:<br>We use “exit” as above flow, and “dma” as normal SMMU fault flow.</p>
<p>  SMMU page fault triggered by device DMA(dma) -&gt; software fault handling(dma) -&gt;<br>  SMMU CD modification(exit) -&gt; software sending CMD resume retry(dma).</p>
<p>In above case, the last CMD resume retry will fail, hardware should terminate<br>former stalled transaction, otherwise there will be transaction which is stalled<br>in SMMU hardware for ever, which will cause device sending the transaction die.</p>
<p>sva unbind flow:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iommu_sva_unbind_device</span><br><span class="line">  [...]</span><br><span class="line">  +-&gt; arm_smmu_write_ctx_desc</span><br><span class="line">    +-&gt; __arm_smmu_write_ctx_desc</span><br><span class="line">      +-&gt; if (!cd)</span><br><span class="line">        +-&gt; cd val = 0;</span><br><span class="line">      +-&gt; arm_smmu_sync_cd</span><br><span class="line">        +-&gt; CD CFGI</span><br></pre></td></tr></table></figure>
<p>This will invalid CD(CD.valid = 0), if device DMA arrive SMMU here, maybe<br>a SMMU bad CD event(0xa) will report. however, in uacce driver, the release<br>callback will firstly stop q, then do sva unbind. So if q is stopped, we will<br>not see SMMU bad CD event.</p>
]]></content>
      <tags>
        <tag>SMMU</tag>
      </tags>
  </entry>
  <entry>
    <title>SMMU的BBML语意</title>
    <url>/SMMU%E7%9A%84BBML%E8%AF%AD%E6%84%8F/</url>
    <content><![CDATA[<p>SMMUv3 spec 3.21里定义了BBML的基本逻辑。软件在修改页表的时候，硬件可能还在访问，<br>这个会带来同步问题。如果，我们的世界里只有CPU，这个问题比较容易解决，如果是单核，<br>没有同步问题，如果是多核，一个核改页表的时候，另外一个core可能在使用页表做翻译，<br>这就有同步问题。解决办法应该是，先加锁，再清页表，再做tlbi，这样同步的页表翻译在<br>tlbi之前都是可以的，在tlbi以及barrier之后触发fault，fault在之前的锁上排队。</p>
<p>但是，这个过程里，如果有IO设备也在访问相关的页表，这个同步应该怎么做? 其基本逻辑<br>也是一样的，只不过tlbi上增加了针对设备一层tlb的无效化。</p>
<p>但是，IO设备访问memory分为两种情况，一种是像在内核中一个设备的dma访问一段内存，<br>这个dma访问是不能stop的; 一种是像SVA中一个设备的dma访问一段内存，页表变动的时候，<br>可以进入fault流程，fault流程可以在锁上排队，也就是dma是可有stop的。对于可以stop<br>的dma，我们使用如上的办法可以解决页表切换时的同步问题。</p>
<p>但是对于内核dma的情况，就搞不定了。首先，一般情况下在做内核dma的时候，页表不会<br>变动，但是在做虚拟化热迁移的时候，需要把大页解开成小页，就可能遇到在做内核dma<br>的时候需要变动页表。</p>
<p>SMMU BBML定义了0、1、2三种level，0就是需要软件保证整个同步，如上我们已经描述的。<br>level1的基本逻辑是这样的, 我们只看他从大页拆分成小页的过程，首先改变block页表的<br>nTbit到1(这个flag的含义是不在tlb里缓存这个block的tlb)，然后做tlbi把tlb清掉，然后<br>在原子的把block页表换成散开的page页表，这个过程中如果有IO一侧访问页表都是没有同步<br>问题的，因为如上的过程通过nT和tlbi配合把可能的tlb先清掉，然后换页表又是原子的。</p>
<p>注意SMMU spec上说，这里的页表变动是只有大页拆小页的变动，也就是说，页的物理位置，<br>属性相关的都不能在这个过程中有变化。</p>
<p>Level2和level1的使用场景是一致的，只是level2不需要nTbit的帮助。他要求硬件设计的<br>时候在出现一个地址对应多个tlb的时候先选择一个使用，然后用一个tlbi把相关的tlb都<br>清掉，然后page table walk从新的页表里得到tlb。</p>
]]></content>
      <tags>
        <tag>SMMU</tag>
      </tags>
  </entry>
  <entry>
    <title>Use coccinelle to check patch in Ubuntu 14.04</title>
    <url>/Use-coccinelle-to-check-patch-in-Ubuntu-14-04/</url>
    <content><![CDATA[<ol>
<li><p>install from its git repo: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NvY2NpbmVsbGUvY29jY2luZWxsZS5naXQ=">https://github.com/coccinelle/coccinelle.git<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>install the packedge mention in install.txt</p>
</li>
<li><p>./autogen<br>./configure<br>./make<br>./make install</p>
</li>
<li><p>cd in a kernel directory, run:</p>
<p>make coccicheck MODE=report M=arch/arm64/mm</p>
<p>It will check all the patch under, e.g. arch/arm64/mm</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>Unix/Linux编程实践教程笔记(11-15)</title>
    <url>/Unix-Linux%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-11-15/</url>
    <content><![CDATA[<p>从获取服务的服务的角度讲，一个进程可以直接调用函数，也可以通过和另个一个进程互动<br>获取相关的服务。从进程间通信的角度讲，socket是一种不同物理机器上进程间通信的方式。</p>
<p>本机间的进程间通信的方式有匿名管道，命名管道(mkfifo), Unix domain socket, 通过<br>相同的文件通信，共享内存。不同物理机器之间的进程见通信一般用socket。</p>
<h2 id="匿名管道"><a href="#匿名管道" class="headerlink" title="匿名管道"></a>匿名管道</h2><p>匿名管道的一般用法是，在父进程中创建管道，fork出子进程，这样子进程也继承了父进程<br>中的管道的读文件描述符和写文件描述符，如果是父进程往子进程写数据，那么父进程需要<br>需要关闭读读描述符，子进程需要关闭写描述符，反之亦然，通信的时候父进程向写文件<br>描述符写入信息，子进程读读文件描述符读出信息。</p>
<h2 id="命名管道"><a href="#命名管道" class="headerlink" title="命名管道"></a>命名管道</h2><p>命名管道的方式，需要首先使用mkfifo创建管道文件，然后写进程向管道里写入数据，读<br>进程从管道里读出数据。命名管道可以使用在独立的进程之间，而匿名管道必须在父子进程<br>之间使用。(如何查看系统里的管道?)</p>
<h2 id="文件通信"><a href="#文件通信" class="headerlink" title="文件通信"></a>文件通信</h2><p>文件通信需要使用文件锁保证一致性</p>
<h2 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h2><p>使用ipcs -m可以查看系统里现在的共享内存，共享内存是用户态的一块内存，两个进程可以<br>通过一块共享内存交换信息。使用共享内存会有同步问题，需要对共享内存加锁，做到各个<br>进程对共享内存访问的互斥，这个可以用信号量机制，信号量是一个系统机制。</p>
<p>  产生数据的进程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">seg_id = shmget(TIME_MEM_KEY, SEG_SIZE, IPC_CREAT | 0777);</span><br><span class="line"></span><br><span class="line">mem_ptr = shmat(seg_id, NULL, 0);</span><br><span class="line"></span><br><span class="line">for (n = 0; n &lt; 60; n++) &#123;</span><br><span class="line">	time(&amp;now);			/* get the time	*/</span><br><span class="line">	strcpy(mem_ptr, ctime(&amp;now));	/* write to mem */</span><br><span class="line">	sleep(1);			/* wait a sec   */</span><br><span class="line">&#125;</span><br><span class="line">	</span><br><span class="line">/* now remove it */</span><br><span class="line">shmctl(seg_id, IPC_RMID, NULL);</span><br></pre></td></tr></table></figure>
<p>  消耗数据的进程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">seg_id = shmget(TIME_MEM_KEY, SEG_SIZE, 0777);</span><br><span class="line"></span><br><span class="line">mem_ptr = shmat(seg_id, NULL, 0);</span><br><span class="line"></span><br><span class="line">printf(&quot;The time, direct from memory: ..%s&quot;, mem_ptr);</span><br><span class="line"></span><br><span class="line">shmdt( mem_ptr );</span><br></pre></td></tr></table></figure>

<h2 id="socket"><a href="#socket" class="headerlink" title="socket"></a>socket</h2><p>关于socket通信，这几章介绍了基于TCP的socket，数据报socket(基于UDP), 和Unix domain<br>socket。可以使用select和poll响应多个阻塞的文件。</p>
<ol>
<li><p>流式socket</p>
<p>client端连接建立的流程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct sockaddr_in  servadd;        /* the number to call */</span><br><span class="line">struct hostent      *hp;            /* used to get number */</span><br><span class="line">int    sock_id, sock_fd;            /* the socket and fd  */</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * AF_INET表示Internet域，SOCK_STREAM表示流式socket</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">sock_id = socket(AF_INET, SOCK_STREAM, 0);    /* get a line */</span><br><span class="line">if (sock_id == -1) </span><br><span class="line">	oops( &quot;socket&quot; );</span><br><span class="line"></span><br><span class="line">/* servadd用来描述server的地址, 地址由IP和port组成 */</span><br><span class="line">bzero(&amp;servadd, sizeof(servadd));   /* zero the address */</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * 这个函数返回host对应的IP地址，入参是host name或者域名。测试返现，</span><br><span class="line"> * 如果是host name，得到的是/etc/hosts下对应的IP; 如果是域名，会得到对应</span><br><span class="line"> × 的IP。</span><br><span class="line"> */</span><br><span class="line">hp = gethostbyname(av[1]);          /* lookup host&#x27;s ip # */</span><br><span class="line">if (hp == NULL) </span><br><span class="line">	oops(av[1]);                /* or die */</span><br><span class="line">bcopy(hp-&gt;h_addr, (struct sockaddr *)&amp;servadd.sin_addr, hp-&gt;h_length);</span><br><span class="line"></span><br><span class="line">servadd.sin_port = htons(atoi(av[2]));  /* fill in port number */</span><br><span class="line"></span><br><span class="line">servadd.sin_family = AF_INET ;          /* fill in socket type */</span><br><span class="line"></span><br><span class="line">/* now dial */</span><br><span class="line">if (connect(sock_id,(struct sockaddr *)&amp;servadd, sizeof(servadd)) != 0)</span><br><span class="line">       oops( &quot;connect&quot; );</span><br></pre></td></tr></table></figure>
<p>server端(省略了一些错误处理)：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct  sockaddr_in   saddr;   /* build our address here */</span><br><span class="line">struct	hostent		*hp;   /* this is part of our    */</span><br><span class="line">char	hostname[HOSTLEN];     /* address 	         */</span><br><span class="line">int	sock_id,sock_fd;       /* line id, file desc     */</span><br><span class="line">FILE	*sock_fp;              /* use socket as stream   */</span><br><span class="line"></span><br><span class="line">sock_id = socket(PF_INET, SOCK_STREAM, 0);    /* get a socket */</span><br><span class="line">bzero((void *)&amp;saddr, sizeof(saddr)); /* clear out struct     */</span><br><span class="line"></span><br><span class="line">gethostname(hostname, HOSTLEN);         /* where am I ?         */</span><br><span class="line">hp = gethostbyname(hostname);           /* get info about host  */</span><br><span class="line">                                        /* fill in host part    */</span><br><span class="line">bcopy((void *)hp-&gt;h_addr, (void *)&amp;saddr.sin_addr, hp-&gt;h_length);</span><br><span class="line">saddr.sin_port = htons(PORTNUM);        /* fill in socket port  */</span><br><span class="line">saddr.sin_family = AF_INET ;            /* fill in addr family  */</span><br><span class="line"></span><br><span class="line">if (bind(sock_id, (struct sockaddr *)&amp;saddr, sizeof(saddr)) != 0)</span><br><span class="line">       oops(&quot;bind&quot;);</span><br><span class="line"></span><br><span class="line">if (listen(sock_id, 1) != 0) </span><br><span class="line">	oops(&quot;listen&quot;);</span><br><span class="line"></span><br><span class="line">while (1) &#123;</span><br><span class="line">	sock_fd = accept(sock_id, NULL, NULL); /* wait for call */</span><br><span class="line">        if (sock_fd == -1)</span><br><span class="line">                oops(&quot;accept&quot;);         /* error getting calls  */</span><br><span class="line"></span><br><span class="line">	/* fdopen把一个文件fd和一个文件流建立联系 */</span><br><span class="line">        sock_fp = fdopen(sock_fd,&quot;w&quot;);  /* we&#x27;ll write to the   */</span><br><span class="line">        if (sock_fp == NULL)            /* socket as a stream   */</span><br><span class="line">                oops(&quot;fdopen&quot;);         /* unless we can&#x27;t      */</span><br><span class="line"></span><br><span class="line">        thetime = time(NULL);           /* get time             */</span><br><span class="line">					/* and convert to strng */</span><br><span class="line">        fprintf(sock_fp, &quot;The time here is ..&quot;);</span><br><span class="line">        fprintf(sock_fp, &quot;%s&quot;, ctime(&amp;thetime)); </span><br><span class="line">        fclose(sock_fp);              /* release connection   */</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>数据报socket</p>
<p>数据报socket不需要提前建立链路，所以流式socket里的connect，listen，accept都<br>没有。数据报socket用sendto和recvfrom来发送和接收数据, recvfrom可以得到发送<br>者的地址，这样就可以用sendto给发送者发送信息。</p>
<p>(数据报可以直接对socket fd做read操作么？)</p>
</li>
<li><p>Unix domain socket</p>
<p>和上面的使用IP加端口号的地址表示方式不同，Uninx domain socket用主机上的一个<br>文件表示地址。</p>
<p>client:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int	           sock;</span><br><span class="line">struct sockaddr_un addr;</span><br><span class="line"></span><br><span class="line">sock = socket(PF_UNIX, SOCK_DGRAM, 0);</span><br><span class="line"></span><br><span class="line">addr.sun_family = AF_UNIX;</span><br><span class="line">/* sockname是一个文件名字 */</span><br><span class="line">strcpy(addr.sun_path, sockname);</span><br><span class="line">addrlen = strlen(sockname) + sizeof(addr.sun_family);</span><br><span class="line"></span><br><span class="line">sendto(sock, msg, strlen(msg), 0, (struct sockaddr *)&amp;addr, addrlen);</span><br></pre></td></tr></table></figure>
<p>server:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int	sock;			/* read messages here	*/</span><br><span class="line">struct sockaddr_un addr;	/* this is its address	*/</span><br><span class="line"></span><br><span class="line">/* build an address */</span><br><span class="line">addr.sun_family = AF_UNIX;		/* note AF_UNIX */</span><br><span class="line">strcpy(addr.sun_path, sockname);	/* filename is address */</span><br><span class="line">addrlen = strlen(sockname) + sizeof(addr.sun_family);</span><br><span class="line"></span><br><span class="line">sock = socket(PF_UNIX, SOCK_DGRAM, 0);	/* note PF_UNIX  */</span><br><span class="line"></span><br><span class="line">/* bind the address */</span><br><span class="line">bind(sock, (struct sockaddr *) &amp;addr, addrlen);</span><br><span class="line"></span><br><span class="line">while (1) &#123;</span><br><span class="line">	/* 直接read socket fd */</span><br><span class="line">	l = read(sock, msg, MSGLEN);	/* read works for DGRAM	*/</span><br><span class="line">	msg[l] = &#x27;\0&#x27;;			/* make it a string 	*/</span><br><span class="line"></span><br><span class="line">	/* do your job... */</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>select poll</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>Linux用户态编程</tag>
      </tags>
  </entry>
  <entry>
    <title>cache基础概念</title>
    <url>/cache%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<p>cache的存在是为了弥补CPU和内存间性能的gap. 一般的，有数据cache，指令cache。按照<br>级别分，有L1 cache，L2 cache, L3 cache。下面讲的cache是一个广泛的cache概念，仅仅<br>用于梳理cache里的其他的基本概念。</p>
<p>Cache，比如L1 data cache, 可以被细分为cache set，中文叫cache组。cache中的每个<br>cache组大小相等, 每个cache组可以包含一定数目的cache line, 一个set里包含的cache<br>line的个数和cache way(中文翻译为“路”)的数值是一样的(后面会讲cache way这个概念)，<br>每个cache line 会包含一定byte, 比如一个cache line 32 byte, 64 byte 或者有可能是<br>128 byte. 下面是cache set，way, line的示意图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     ---   +---+-----+-----------------------------------+   </span><br><span class="line">      ^    | v | tag | cache line                        |  每个组的cache line</span><br><span class="line">      |    +---+-----+-----------------------------------+  的个数也是这个cache</span><br><span class="line">set   |    | v | tag | cache line                        |  way(路)的数目</span><br><span class="line">      |    +---+-----+-----------------------------------+</span><br><span class="line">      |    ...</span><br><span class="line">      v    | v | tag | cache line                        |</span><br><span class="line">     ---   +---+-----+-----------------------------------+</span><br><span class="line"></span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           ...</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line"></span><br><span class="line">           ...     </span><br><span class="line"></span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           ...</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line"></span><br><span class="line">           假设这里有S个set, 每个cache line包含B个bypt</span><br></pre></td></tr></table></figure>
<p>基础概念介绍完了，我们看看一个特定内存地址是怎么被映射到cache中的。假设一个内存<br>地址有m位, 那么中间的s位用来对应set的编号，指定去哪个set中找cache, 其中 2^s = S.<br>低b bit对应cache line中bypt的偏移, 其中 2^b = B。高 m-s-b bit 就是上面的tag域段。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  tag(m-s-b)       index(s bit)        offset(b bit)</span><br><span class="line">|            |                      |                |</span><br><span class="line">|&lt;----------&gt;|&lt;--------------------&gt;|&lt;--------------&gt;|</span><br><span class="line">|            |                      |                |</span><br></pre></td></tr></table></figure>
<p>去找一个内存地址对应的cache时，首先根据index域的值, 找到相应的cache set。然后<br>根据内存地址tag域的值和cache里tag的值匹配，找到相应的cache line。至于怎么找到<br>匹配的cache line, 以及找不到匹配的cache line的时候的cache line替换策略, 本文将<br>不去涉及, 这个主题涉及的范围远超过本文的内容。找到对应的cache line后, 再根据<br>offset的值找到对应的byte。</p>
<p>有了上述的铺垫，现在可以讲清全相连cache，组相连cache，直接映射cache这几个概念了。<br>直接映射cache就是只有一个set的cache; 全相连cache就是一个set里只有一个cache line<br>的cache; 组相连cache就是一个set里有多个cache line的cache。</p>
<p>在ARMv8的编程指导手册cache的章节有如下的示意图。我们现在把上面的概念和下面ARMv8<br>手册上的示意图联系到一起。下图里，每一行(连同它的背影里的行)组成一个cache set。<br>所以，下面的图里有几个立体的行，就是有多少个set。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">      --- +----------------------------------------------+</span><br><span class="line">       ^  |                                              |</span><br><span class="line">       |  |                                              |</span><br><span class="line">       |  |                                              |</span><br><span class="line">       |  |                                              |</span><br><span class="line">       |  |      +----------------------------------------------+</span><br><span class="line">       |  |      |                                              |</span><br><span class="line">       |  |      |   +----------------------------------------------+</span><br><span class="line">       |  |      |   |                                              |</span><br><span class="line">       |  |      |   |   +---+-----+-----------------------------------+  ---</span><br><span class="line">       |  |      |   |   | v | tag | cache line                        |   ^</span><br><span class="line">       |  |      |   |   +---+-----+-----------------------------------+   |</span><br><span class="line">       |  |      |   |   | v | tag | cache line                        |   |</span><br><span class="line">       |  |      |   |   +---+-----+-----------------------------------+   |</span><br><span class="line">       |  |      |   |   | v | tag | cache line                        |   |</span><br><span class="line">       v  |      |   |   +---+-----+-----------------------------------+   |  每一行是一个set，</span><br><span class="line">     ---&gt; +------|   |   | v | tag | cache line                        |   |  一共有S行</span><br><span class="line">        \  .     |   |   +---+-----+-----------------------------------+   |</span><br><span class="line">         \  .    |   |   | v | tag | cache line                        |   |</span><br><span class="line">          \  .   |   |   +---+-----+-----------------------------------+   |</span><br><span class="line">           \     |   |   ...                                               |</span><br><span class="line">=&gt; one      \    +---|   | v | tag | cache line                        |   |</span><br><span class="line">   set       \       |   +---+-----+-----------------------------------+   |</span><br><span class="line">              \     -+---| v | tag | cache line                        |   |</span><br><span class="line">               \         +---+-----+-----------------------------------+   |</span><br><span class="line">                \        | v | tag | cache line                        |   v</span><br><span class="line">               ----&gt;     +---+-----+-----------------------------------+  ---</span><br></pre></td></tr></table></figure>
<p>下面再对全相连cache，组相连cache，直接映射cache做下解释。直接映射的cache只有一个<br>cache set, 所以每次去cache里找一个地址对应的cache都要遍历整个cache。这样，cache<br>小还可以，cache大了，效率会很差。直接映射cache的最直接的应用是TLB，TLB是MMU页表<br>的cache(缓存)。一般的DDR的cache都是组相连cache，全相连cache有cache颠簸的潜在性能<br>影响, 画个具体的图比较容易明白:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   +------------+              +------------+</span><br><span class="line">A  | ddr        |  ---------&gt;  | cache line |  全相连cache，一个set一个cache line</span><br><span class="line">   +------------+         /    +------------+</span><br><span class="line">   | ddr        |  ------/--&gt;  | cache line |</span><br><span class="line">   +------------+       / /    +------------+</span><br><span class="line">   | ddr        |  ----/-/--&gt;  | cache line |</span><br><span class="line">   +------------+     / / /    +------------+</span><br><span class="line">   | ddr        |  --/-/-/--&gt;  | cache line |</span><br><span class="line">   +------------+   / / / /    +------------+</span><br><span class="line">B  | ddr        |  / / / /</span><br><span class="line">   +------------+   / / /</span><br><span class="line">   | ddr        |  / / /</span><br><span class="line">   +------------+   / /</span><br><span class="line">   | ddr        |  / /</span><br><span class="line">   +------------+   /</span><br><span class="line">   | ddr        |  /</span><br><span class="line">   +------------+</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure>
<p>上面的内存块都以是cache line粒度的。可以看到B地址对应的cache line和A地址对应的<br>cache line是相同的。当对A地址引用后，再对B地址引用，由于cache line中还是A地址<br>对应的内容，那么对B地址的引用必然会引起cache替换。所以如果有这样的程序，那么性能<br>将会很差:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for (i = 0; i &lt; NUM; i++) &#123;</span><br><span class="line">	tmp = *(A + i) * *(B + i);</span><br><span class="line">	value += tmp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>组相连cache因为一个set里有多个cache line, 还可以在set这一维度引入很多处理算法，<br>减少cache颠簸的发生。</p>
<p>最后以一个具体的cache的示例结束这篇笔记，一个32K 一个cache line 32 byte 4路组相连<br>cache的示意图: 先计算地址里s, b的数值，2 ^ b = 32, b = 5; S = 32K / (32 * 4) =<br>256, 2 ^ s = S, s = 8。所以一个内存地址的划分是:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|bit 31 ~ bit 13                        |bit 12 ~ bit 5 |bit 4 ~  bit 0|</span><br><span class="line">+---------------------------------------+---------------+--------------+</span><br><span class="line">|       tag                             |   index       |   offset     |</span><br></pre></td></tr></table></figure>
<p>这个具体的cache有256个组，各个组4路。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>ARM64</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title>cpio compress and extract</title>
    <url>/cpio-compress-and-extract/</url>
    <content><![CDATA[<p>we often see a file is a *.cpio.gz, normally it is a gzip compressed data.</p>
<p>“gunzip file.cpio.gz” to extract it to a file.cpio, whichi is a ASCII cpio archive.<br>“cpio -ivmd &lt; file.cpio” can extract it finally.</p>
<p>we can use “find . | cpio -o –format=newc &gt; ../file.cpio” to compress a directory<br>to file.cpio. this command should be run in the root of the directory.</p>
<p>then we use “gzip -c file.cpio &gt; file.cpio.gz” to get orignal gzip compressed file.</p>
<p>Above initrd file system should be cpio.gz type. for example, we can run a qemu<br>besed on memory initrd fiel system like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 -machine virt -cpu cortex-a57 \</span><br><span class="line">-m 300M \</span><br><span class="line">-kernel ~/repos/linux/arch/arm64/boot/Image \</span><br><span class="line">-initrd ~/rootfs/rootfs.cpio.gz \</span><br><span class="line">-append &quot;console=ttyAMA0 root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">-nographic \</span><br></pre></td></tr></table></figure>
<p>Note: as we run rootfs in memory, -m parametre above should carefully choose.<br>      too small is a bad idea.</p>
]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>autoconf/automake使用笔记</title>
    <url>/autoconf-automake%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li><p>写hello.c程序</p>
</li>
<li><p>autoscan 生成：autoscan.log  configure.scan  hello.c<br>   cat configure.scan:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#                                               -*- Autoconf -*-</span><br><span class="line"># Process this file with autoconf to produce a configure script.</span><br><span class="line"></span><br><span class="line">AC_PREREQ([2.68])</span><br><span class="line">AC_INIT([FULL-PACKAGE-NAME], [VERSION], [BUG-REPORT-ADDRESS])</span><br><span class="line">AC_CONFIG_SRCDIR([hello.c])</span><br><span class="line">        AC_CONFIG_HEADERS([config.h])</span><br><span class="line"></span><br><span class="line"># Checks for programs.</span><br><span class="line">AC_PROG_CC</span><br><span class="line"></span><br><span class="line"># Checks for libraries.</span><br><span class="line"></span><br><span class="line"># Checks for header files.</span><br><span class="line"></span><br><span class="line"># Checks for typedefs, structures, and compiler characteristics.</span><br><span class="line"></span><br><span class="line"># Checks for library functions.</span><br><span class="line"></span><br><span class="line">AC_OUTPUT</span><br></pre></td></tr></table></figure>
<p>修改成：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   #                                               -*- Autoconf -*-</span><br><span class="line">   # Process this file with autoconf to produce a configure script.</span><br><span class="line"></span><br><span class="line">   AC_PREREQ([2.68])</span><br><span class="line">   AC_INIT([autoconf_test, [0.1]) # change this line!</span><br><span class="line">   AC_CONFIG_SRCDIR([hello.c])</span><br><span class="line">   AC_CONFIG_HEADERS([config.h])</span><br><span class="line">   AM_INIT_AUTOMAKE([autoconf], [0.1]) # add this line!</span><br><span class="line"></span><br><span class="line">   # Checks for programs.</span><br><span class="line">   AC_PROG_CC</span><br><span class="line"></span><br><span class="line">   # Checks for libraries.</span><br><span class="line"></span><br><span class="line">   # Checks for header files.</span><br><span class="line"></span><br><span class="line">   # Checks for typedefs, structures, and compiler characteristics.</span><br><span class="line"></span><br><span class="line">   # Checks for library functions.</span><br><span class="line"></span><br><span class="line">   AC_OUTPUT(Makefile) # change this line！</span><br></pre></td></tr></table></figure>
<p>   把文件名改成: configure.in</p>
</li>
<li><p>aclocal 生成：<br>   aclocal.m4  autom4te.cache  configure.in  hello.c <br>   (主要是生成aclocal.m4)</p>
</li>
<li><p>autoconf 生成：<br>   aclocal.m4  autom4te.cache  configure  configure.in  hello.c</p>
</li>
<li><p>autoheader 生成：<br>   aclocal.m4  autom4te.cache  config.h.in  configure  configure.in  hello.c</p>
</li>
<li><p>创建Makefile.am:<br>   AUTOMAKE_OPTIONS=foreign<br>   bin_PROGRAMS=hello<br>   hello_SOURCES=hello.c</p>
</li>
<li><p>automake –add-missing <br>   过程信息为：<br>   configure.in:8: installing <code>./install-sh&#39;    configure.in:8: installing </code>./missing’<br>   Makefile.am: installing `./depcomp’<br>   生成文件：<br>   aclocal.m4  autom4te.cache  config.h.in  configure  configure.in  depcomp  <br>   hello.c  install-sh  Makefile.am  Makefile.in  missing<br>   (makefile.in是这步生成的关键文件)</p>
</li>
<li><p>./configure 生成最终的Makefile文件(该步骤中可能需要指定编译器：export CC=gcc)</p>
</li>
<li><p>make 生成最终的可执行的程序：hello, ./hello运行输出：test autoconf!</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	printf(&quot;test autoconf!\n&quot;);</span><br><span class="line">	</span><br><span class="line">	#ifdef CONFIG_H_TEST</span><br><span class="line">	printf(&quot;test autoconf: test config.h\n&quot;);</span><br><span class="line">	#endif</span><br><span class="line">	</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <tags>
        <tag>软件开发</tag>
        <tag>autoconf</tag>
      </tags>
  </entry>
  <entry>
    <title>cmake使用速记</title>
    <url>/cmake%E4%BD%BF%E7%94%A8%E9%80%9F%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="cmake基本逻辑"><a href="#cmake基本逻辑" class="headerlink" title="cmake基本逻辑"></a>cmake基本逻辑</h2><p>cmake根据cmake的配置文件生成Makefile, 然后用户可以用make命令进行软件的构建。<br>我们先从软件构件的角度自己想下cmake要描述怎么样的场景，然后一一看看cmake的描述<br>方法。我们需要：</p>
<ol>
<li><p>指定要编译生成的二进制文件，以及生成这些二进制文件的源文件。这些二进制文件可以<br>是可执行文件，静态库和动态库。</p>
</li>
<li><p>指定编译参数，指定链接参数。链接参数里要可以指定要链接的库。</p>
</li>
<li><p>支持多目录层次的构建。</p>
</li>
<li><p>支持Makfile里伪目标的创建，比如，make clean/install/uninstall等。</p>
</li>
<li><p>支持系统环境的检测，类似autoconfigure/automake里的功能。</p>
</li>
<li><p>支持环境参数的定义，以及条件构建。</p>
</li>
<li><p>cmake的基本用法。</p>
</li>
</ol>
<h2 id="描述二进制文件的构建"><a href="#描述二进制文件的构建" class="headerlink" title="描述二进制文件的构建"></a>描述二进制文件的构建</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">project(hello_demo)</span><br><span class="line"></span><br><span class="line">add_executable(hello hello.c hello.h)</span><br><span class="line"></span><br><span class="line">add_subdirectory(lib)</span><br><span class="line"></span><br><span class="line">target_link_libraries(hello add)</span><br></pre></td></tr></table></figure>
<p>如上，在工程目录里的CMakeLists.txt文件里描述如何构建。project指定工程的名字，<br>add_executable指定要编译的二进制文件和它对应的源文件，这里是hello和hello.c, hello.h。<br>add_subdirectory指定要包含的下一级构建的目录，比如我们这里加了一个lib的目录，里面<br>是一个实例的库，lib目录下同样要有CMakeLists.txt：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add_library(add SHARED add.c)</span><br></pre></td></tr></table></figure>
<p>add_library指定要编译生成一个libadd的动态库，SHARED就是指定生成动态库，这个动态<br>库的源代码是add.c。</p>
<p>继续看顶层的CMakeLists.txt, target_link_libraries指定hello要链接libadd这个库。</p>
<p>如上是一个最简单的认识，那么cmake是怎么知道使用哪个编译器编译的呢？比如系统里有<br>默认的gcc编译器，之后又在其他地方安装了其他版本的gcc，通过什么控制cmake对编译器<br>的选择。首先，cmake将选择系统默认的gcc编译器，路径在/usr/bin，库的路径在/lib，<br>通过修改CMAKE_C_COMPILER或者CMAKE_CXX_COMPILER的配置，可以改变默认的编译器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set(CMAKE_C_COMPILER &quot;/opt/gcc/bin/gcc&quot;)</span><br></pre></td></tr></table></figure>
<p>如上，会把编译器设定为/opt/gcc下的gcc。但是，链接还是标准路径的glibc库，如何把glibc<br>库也修改成自定义的库？</p>
<h2 id="指定编译链接参数"><a href="#指定编译链接参数" class="headerlink" title="指定编译链接参数"></a>指定编译链接参数</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set(CMAKE_C_FLAGS &quot;-O2 -g&quot;)</span><br><span class="line">set(CMAKE_C_FLAGS &quot;$&#123;CMAKE_C_FLAGS&#125; -fprofiles-arcs -ftest-coverage&quot;)</span><br></pre></td></tr></table></figure>
<p>如上，先设置编译参数是-O2 -g，然后在此基础上加上-fprofiles-arcs -ftest-coverage</p>
<p>下面来看链接一个第三方库的方法，比如hello.c里使用了一个numa_max_node()的函数，这个<br>函数是libnuma里的库函数。现在考虑把libnuma链接进来。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">target_link_libraries(hello add numa)</span><br></pre></td></tr></table></figure>
<p>如上，可以把-lnuma这个参数加到链接参数里。dpdk -L libnuma1可以发现，当前测试系统<br>上这个库的位置在/usr/lib/aarch64-linux-gnu/libnuma.so.1.0.0，所以，编译器使用默认<br>路径就可以搜索到。可以使用如下的方式把更多的链接参数配置给cmake:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set(CMAKE_EXE_LINKER_FLAGS &quot;-L/opt -I/opt/include/&quot;)</span><br></pre></td></tr></table></figure>

<h2 id="多层次目录构建"><a href="#多层次目录构建" class="headerlink" title="多层次目录构建"></a>多层次目录构建</h2><p>如上，使用add_subdirectory()把下下一级目录加进来。</p>
]]></content>
      <tags>
        <tag>cmake</tag>
        <tag>编译链接</tag>
      </tags>
  </entry>
  <entry>
    <title>dpdk mempool的逻辑</title>
    <url>/dpdk-mempool%E7%9A%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>dpdk里有块内存池的支持，用户可以调用相关接口创建固定block大小的内存池，然后从这<br>个内存池里申请和释放内存。</p>
<p>这个内存池叫做rte_mempool, 相关的实现代码在dpdk/lib/librte_mempool/*. dpdk里<br>提供了rte_mempool相关的测试app，在这个地方app/test/test_mempool.c, 顺着这个测试<br>app可以大概看出如何使用rte_mempool。</p>
<h2 id="相关的接口有："><a href="#相关的接口有：" class="headerlink" title="相关的接口有："></a>相关的接口有：</h2><ol>
<li><p>rte_mempool_create</p>
</li>
<li><p>rte_mempool_cache_create</p>
</li>
<li><p>rte_mempool_generic_get</p>
</li>
<li><p>rte_mempool_generic_put</p>
</li>
</ol>
<h2 id="基本的构架是："><a href="#基本的构架是：" class="headerlink" title="基本的构架是："></a>基本的构架是：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">          +------------+</span><br><span class="line">cpu0      |local_cache |----+</span><br><span class="line">          +------------+    |</span><br><span class="line">                            |</span><br><span class="line">          +------------+    +-&gt; +---------+           +-------------+</span><br><span class="line">cpu1      |local_cache |------&gt; |rte_ring |----------&gt;|memory block |</span><br><span class="line">          +------------+    +-&gt; +---------+           +-------------+</span><br><span class="line">                            |</span><br><span class="line">                            |</span><br><span class="line">                            |</span><br><span class="line">                            |</span><br><span class="line">                            |</span><br><span class="line">          +------------+    |</span><br><span class="line">cpuN      |local_cache |----+</span><br><span class="line">          +------------+</span><br></pre></td></tr></table></figure>
<p>支持mempool的基本数据结构式，local_cache, rte_ring, memzone。memzone是分配的<br>内存(包括管理需要的内存)，一般是大页内存。rte_ring是一个无锁队列，用来管理内存,<br>rte_ring的entry个数等于memory里block的个数。rte_ring用cmp and change的原子指令<br>实现无锁队列，但是，在多核的时候，这个指令的开销也很大，所以为每一个cpu建立了<br>block内存的cache，cpu先从自己的cache里取block内存，不够的时候再去rte_ring里批量取。</p>
<h2 id="具体实现的关键点："><a href="#具体实现的关键点：" class="headerlink" title="具体实现的关键点："></a>具体实现的关键点：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rte_mempool_create</span><br><span class="line">  /*</span><br><span class="line">   * flags指示具体挂哪个ops, ops的定义在dpdk/drivers/mempool/ring/rte_mempool_ring.c</span><br><span class="line">   * ring的初始化调用alloc，从ring里申请释放内存调用dequeue/enqueue</span><br><span class="line">   */</span><br><span class="line">  +-&gt; rte_mempool_set_ops_name</span><br><span class="line">  +-&gt; rte_mempool_populate_default</span><br><span class="line">    +-&gt; mempool_ops_alloc_onece</span><br><span class="line">      +-&gt; rte_mempool_ops_alloc</span><br><span class="line">        +-&gt; ops-&gt;alloc</span><br><span class="line">  /* 被管理的memory memzone的申请 */</span><br><span class="line">  +-&gt; rte_mempool_populate_default </span><br><span class="line"></span><br><span class="line">在dpdk/drivers/mempool/ring/rte_mempool_ring.c里有alloc的实现：</span><br><span class="line">ring_alloc</span><br><span class="line">  +-&gt; rte_ring_create</span><br><span class="line">    +-&gt; rte_ring_create_elem</span><br><span class="line">       /* ring memzone的申请在这个地方 */</span><br><span class="line">       +-&gt; rte_memzone_reserve_aligned</span><br><span class="line">       /* ring的初始化 */</span><br><span class="line">       +-&gt; rte_ring_init</span><br><span class="line"></span><br><span class="line">rte_mempool_generic_get</span><br><span class="line">  +-&gt; /* get from local cache */</span><br><span class="line">  ...</span><br><span class="line">  +-&gt; /* ring_dequeue: 这个地方是从ring里分配内存 */</span><br><span class="line">    +-&gt; rte_mempool_ops_dequeue_bluk</span><br><span class="line">      +-&gt; ops-&gt;dequeue</span><br><span class="line"></span><br><span class="line">在dpdk/drivers/mempool/ring/rte_mempool_ring.c里有dequeue的实现：</span><br><span class="line">e.g. rte_ring_sc_dequeue</span><br><span class="line">       +-&gt; rte_ring_sc_dequeue_bulk</span><br><span class="line">       ...</span><br><span class="line">         /* 具体申请bluck的逻辑 */</span><br><span class="line">         +-&gt; __rte_ring_do_dequeue_elem</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>dpdk</tag>
      </tags>
  </entry>
  <entry>
    <title>ftp服务器设置</title>
    <url>/ftp%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<p>做嵌入式调试需要在主机上建立一个ftp服务器，然后通过网线将主机上的程序下载到嵌入<br>式开发板上，主机的ftp服务器需要有用户名和密码。以上是所知道的信息，怎么利用现有<br>的资源快速解决这个简单的问题？上网搜索是必要的，但是漫无目的的搜索效率并不是很<br>大；还有一种方法是系统的把ftp服务器的知识学一下，这样有太慢。这里提供几个工具和<br>思路，或许会比较有用。</p>
<ol>
<li><p>man -k ftp，man命令加-k选项可以列出所有ftp相关的帮助信息，我们可以从中选择<br>运行这个命令后可以看到ftp，tftp，vsftpd等相关项目。可以以上面的信息作为基础<br>再在网上搜索</p>
</li>
<li><p>可以看到vsftpd是一个FTP服务器，man 8 vsftpd可以查看和他相关的信息</p>
</li>
<li><p>在google中搜vsftpd的信息，输入ubuntu vsftpd，第一条就得到下面的信息：<br><span class="exturl" data-url="aHR0cHM6Ly9oZWxwLnVidW50dS5jb20vMTAuMDQvc2VydmVyZ3VpZGUvZnRwLXNlcnZlci5odG1s">https://help.ubuntu.com/10.04/serverguide/ftp-server.html<i class="fa fa-external-link-alt"></i></span></p>
<p>文件的头几行就得到这样的信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Access to an FTP server can be managed in two ways:</span><br><span class="line">Anonymous</span><br><span class="line">Authenticated</span><br></pre></td></tr></table></figure>
<p>上面告诉我们FTP服务器分为两大类：匿名的(就是直接ftp &lt;ip&gt;就可以登陆的)，需要<br>输入用户名、密码的。结合我们的需要，我们的搜索词变成了ftp，authenticated，但是<br>这篇文档已经介绍了需要怎么设置，我们就不需要去别的地方搜了。下面的”User Authenticated<br>FTP Configuration”说明要设置/etc/vsftpd.conf中的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">local_enable=YES</span><br><span class="line">write_enable=YES</span><br></pre></td></tr></table></figure>
<p>然后在重启vsftpd:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo /etc/init.d/vsftpd restart</span><br></pre></td></tr></table></figure></li>
<li><p>说到这里我们的pc(ubuntu系统)上还没有vsftpd啊，试试ubuntu的软件下载管理工具<br>sudo apt-get install vsftpd(可以自动补全)，果然可以。</p>
</li>
<li><p>vsftpd服务器有了，开始配置/etc/vsftpd.conf。我们打开对应的文件<br>sudo vi /etc/vsftpd.conf<br>仔细看，发现文档已经是充满注释了。找个和我们目的相关的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Allow anonymous FTP? (Beware - allowed by default if you comment this out).</span><br><span class="line">anonymous_enable=NO</span><br></pre></td></tr></table></figure>
<p>后面的注释说，这个是默认带开的，我们用的是authenticate，所以这里选NO</p>
</li>
<li><p>按这样的设置，然后重启服务器，用ftp 127.0.0.1登陆自己的服务器，发现要输入的<br>自己ubuntu系统的用户名和密码作为vsftpd的用户名和密码，但是到了哪个目录中了呢？<br>用get &lt;自己home中的文件&gt;，发现可以把自己home中的文件拉到当前目录下，看来默认<br>vsftpd的目录就是自己的/home/XXX</p>
</li>
<li><p>既然/etc/vsftpd.conf文件注释很好，那就到该文件中看看怎么设置，设置：<br>local_root=/home/XXX/your_ftpboot<br>chmod 777 /home/XXX/your_ftpboot<br>重启服务器，发现vsftpd可以使用了，根目录就是上面设置的</p>
<p>至于想更好的用好ftp服务器，就是研究、尝试/etc/vsftpd.conf的事了</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>运维</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>git am 冲突解决技巧</title>
    <url>/git-am-%E5%86%B2%E7%AA%81%E8%A7%A3%E5%86%B3%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<p>使用git am合patch的时候可能有冲突出现，这个时候，手动解决的办法是看看冲突在哪里，<br>然后手动的把那个patch和入。手动合入需要的时间太长.</p>
<p>我们可以用git apply –reject patch的方式合入。这里需要注意几个问题。</p>
<p>git apply只会看到文件，它把patch里的一个个diff段拆出来, 然后合入相应的文件里,<br>而且git apply只会合入当前目录下的diff段，所以上面的命令要到所有diff段的最大的<br>一个目录里去执行，一般为了方便就在代码的根目录里执行。git apply后相当于修改了<br>原文见，所以要git add，git commit下。–reject的这个参数会把有冲突的段保存在一个<br>.rej的文件里。</p>
<p>所以，一般git am合patch的步骤可以是这样的：</p>
<ol>
<li><p>git am patch     –&gt; 没有conflict，over！</p>
</li>
<li><p>有冲突的时候： cd code_root/</p>
<pre><code>   git apply --reject patch
</code></pre>
</li>
<li><p>在.rej文件里找见冲突的diff段，手动修改对应的代码</p>
</li>
<li><p>git add related_files</p>
</li>
<li><p>git am –resolved</p>
</li>
</ol>
<p>注意最后一个操作, 我们现在已经把git am的冲突解决，用git am –resovled可以继续git<br>am的操作把commit log也自动的打上！</p>
]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>docker笔记</title>
    <url>/docker%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>docker的三个最基本的概念是镜像(Image)，容器(Container)和仓库(Repository)。本文<br>简单介绍有关他们的基础操作。</p>
<h2 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h2><p>Image可以看成是一个配置过的发行版(e.g. 带apach配置的ubuntu发行版)。我们可以自己<br>生成一个镜像，或者是直接下载已有的镜像使用。比如下载最新的ubuntu docker iamge:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wangzhou@EstBuildSvr1:~$ docker pull ubuntu</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/ubuntu</span><br><span class="line">8aec416115fd: Pull complete </span><br><span class="line">695f074e24e3: Pull complete </span><br><span class="line">946d6c48c2a7: Pull complete </span><br><span class="line">bc7277e579f0: Pull complete </span><br><span class="line">2508cbcde94b: Pull complete </span><br><span class="line">Digest: sha256:71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130950</span><br><span class="line">Status: Downloaded newer image for ubuntu:latest</span><br></pre></td></tr></table></figure>
<p>注意上面下载的Image和ubuntu官方发行版还是有一定的不同的, 希望一直用docker image<br>的同学要对这个问题有准备。</p>
<p>查看系统中已有的docker image可以使用docker images, 比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wangzhou@EstBuildSvr1:~$ docker images</span><br><span class="line">REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">ubuntu                  latest              f49eec89601e        3 weeks ago         129.5 MB</span><br><span class="line">ubuntu                  14.04.1             4bf30b29cec4        4 weeks ago         284.6 MB</span><br><span class="line">ubuntu                  14.04               3f755ca42730        8 weeks ago         188 MB</span><br><span class="line">compile/ubuntu/server   14.04               09637d6f7c6f        3 months ago        540.2 MB</span><br><span class="line">&lt;none&gt;                  &lt;none&gt;              bd3d4369aebc        5 months ago        126.6 MB</span><br><span class="line">build/ubuntu/server     14.04               ab01bc6b6a57        7 months ago        1.663 GB</span><br></pre></td></tr></table></figure>

<h2 id="container"><a href="#container" class="headerlink" title="container"></a>container</h2><p>假设我们现在已经有了ubuntu的image，可以运行以下的命令在容器中运行这个镜像。</p>
<p>sudo docker run -t -i ubuntu:latest /bin/bash</p>
<p>在容器中:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@518aa47b44c4:~#   /* here 518aa47b44c4 is the ID of this container */</span><br></pre></td></tr></table></figure>
<p>ubuntu的环境下可以用apt-get安装相应的软件：<br>apt-get update<br>then install basic programs: ping, vim, mutt, fetchmail, msmtp, procmail</p>
<p>但是，上面的改动只是在容器中的，容器退出我们的改动就不存在了，所以，容器退出后<br>要把相关的改动提交，从而形成一个新的docker image, 下次我们在容器里运行这个image,<br>就包含了这次提交的信息。具体如下节。</p>
<h2 id="build-a-docker-image"><a href="#build-a-docker-image" class="headerlink" title="build a docker image"></a>build a docker image</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wangzhou@EstBuildSvr1:~$ docker commit -m &quot;Add: basic tools: vim, mutt, fetchmail, procmail, msmtp&quot; -a &quot;Docker basic&quot; 518aa47b44c4 ubuntu/latest</span><br><span class="line">sha256:c8ea18e453cf756dcbb5acaa21a96e78baf18f7b7fbdd5cf8b44edd606702bf0</span><br><span class="line">wangzhou@EstBuildSvr1:~$ docker images</span><br><span class="line">REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">ubuntu/latest           latest              c8ea18e453cf        14 seconds ago      301.3 MB</span><br><span class="line">ubuntu                  latest              f49eec89601e        3 weeks ago         129.5 MB</span><br><span class="line">ubuntu                  14.04.1             4bf30b29cec4        4 weeks ago         284.6 MB</span><br><span class="line">ubuntu                  14.04               3f755ca42730        8 weeks ago         188 MB</span><br><span class="line">compile/ubuntu/server   14.04               09637d6f7c6f        3 months ago        540.2 MB</span><br><span class="line">&lt;none&gt;                  &lt;none&gt;              bd3d4369aebc        5 months ago        126.6 MB</span><br><span class="line">build/ubuntu/server     14.04               ab01bc6b6a57        7 months ago        1.663 GB</span><br></pre></td></tr></table></figure>
<p>下次再运行image的时候，可以指定image的ID运行，可以用repo:tag的方式运行, 比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker run -t -i c8ea18e453cf7</span><br><span class="line">sudo docker run -t -i ubuntu/latest</span><br><span class="line">sudo docker run -t -i ubuntu/latest:latest</span><br></pre></td></tr></table></figure>
<p>更改docker image的tag:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker tag 6b46dcca842d docker_test/with_net_tools:add_ping_change_tag</span><br><span class="line">sherlock@T440:~/notes$ sudo docker images</span><br><span class="line">REPOSITORY                   TAG                   IMAGE ID            CREATED             SIZE</span><br><span class="line">docker_test/with_net_tools   add_ping              6b46dcca842d        4 hours ago         170.9 MB</span><br><span class="line">docker_test/with_net_tools   add_ping_change_tag   6b46dcca842d        4 hours ago         170.9 MB</span><br><span class="line">docker_test/with_net_tools   latest                4b48ccb6135c        4 hours ago         167.2 MB</span><br><span class="line">ubuntu                       latest                f753707788c5        2 weeks ago         127.2 MB</span><br><span class="line">ubuntu                       12.04                 6e0ef8cc1b8a        12 months ago       136 MB</span><br></pre></td></tr></table></figure>

<p>从一个容器退出后，容器还没有彻底消亡。用sudo docker ps -a 可以查看现在系统里的容器,<br>STATUS一栏显示容器的状态，’Up XXX days’表示这个容器已经运行XXX天了，现在还在运行，<br>Exited (0) XXX days ago表示这个容器现在是退出状态。对于退出状态的容器，可以<br>docker start -i 容器id重新启动这个容器。对于运行状态的容器，可以docker attach 容器id<br>继续接入这个容器, 这时接入这个容器的所有终端将同步显示容器中运行的程序。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* remove a contrainor */</span><br><span class="line">sudo docker rm</span><br><span class="line">/*</span><br><span class="line"> * remove a image, if a image was used by a stopped container, should firstly</span><br><span class="line"> * remove related stopped container</span><br><span class="line"> */</span><br><span class="line">sudo docker rmi</span><br></pre></td></tr></table></figure>
<h2 id="upload-a-docker-image"><a href="#upload-a-docker-image" class="headerlink" title="upload a docker image"></a>upload a docker image</h2><p>为了保存，传播image，我们可以把一个docker image上传到DockerHub的仓库。当然首先<br>要在DockerHub上注册有账户。在上传之前，先在本机上用docker login登录下DockerHub<br>账户, 用户名、密码用对答的方式输入。然后就是运行：docker push repo_name[:tag]<br>的方式上传了。值得注意的是，repo_name要用DockerHub_account/XXX的形式, 不然可能<br>无法上传。</p>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title>git submodule的使用</title>
    <url>/git-submodule%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> 当一个代码仓库以代码的方式完整包含另一个仓库的代码时，我们就可以使用git中submodule<br> 的方式来组织代码仓库，其中，被包含的仓库叫submodule/subproject, 包含他的仓库叫<br> superproject，所谓submodule的配置都是针对superproject的。</p>
<p> qemu就是用submodule的方式管理各种firmware的，比如，qemu把opensbi就作为它的一个<br> submodule。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                            +-----------+</span><br><span class="line">                            | project_a |</span><br><span class="line">                            +-----------+</span><br><span class="line">                             /</span><br><span class="line">                            /</span><br><span class="line">+-----------------------+  /</span><br><span class="line">| superproject          | /</span><br><span class="line">|   |                   |/</span><br><span class="line">|   +----               |</span><br><span class="line">|   |                  /|</span><br><span class="line">|   +----             / |</span><br><span class="line">|   |                /  |</span><br><span class="line">|   +----- ./project_a  |</span><br><span class="line">+-----------------------+</span><br></pre></td></tr></table></figure>
<p> 如上示意图，superproject和project_a各自都是独立的git仓库。所谓superproject把<br> project_a作为一个子仓库，只是在superproject中加入project_a的配置信息，比如，project_a<br> 的URL/branch/commit，这些信息可以确定作为superproject子仓库的project_a的一个精确<br> 的提交。</p>
<p> 同时superproject中会有一个目录存放project_a的代码。</p>
<h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><p> 我们下面具体看怎么把一个独立的仓库作为子仓库添加给superproject。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sherlock@m1:~/tests/git_submodule/superproject$ git submodule add https://github.com/wangzhou/notes  submodule/notes</span><br><span class="line">Cloning into &#x27;/home/sherlock/tests/git_submodule/superproject/submodule/notes&#x27;...</span><br><span class="line">remote: Enumerating objects: 1397, done.</span><br><span class="line">remote: Counting objects: 100% (120/120), done.</span><br><span class="line">remote: Compressing objects: 100% (86/86), done.</span><br><span class="line">remote: Total 1397 (delta 68), reused 81 (delta 34), pack-reused 1277</span><br><span class="line">Receiving objects: 100% (1397/1397), 5.82 MiB | 852.00 KiB/s, done.</span><br><span class="line">Resolving deltas: 100% (680/680), done.</span><br><span class="line">sherlock@m1:~/tests/git_submodule/superproject$ </span><br></pre></td></tr></table></figure>
<p> 我们这里把名字叫notes的这个仓库(<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L25vdGVzKSVFNCVCRCU5QyVFNCVCOCVCQXN1cGVycHJvamVjdA==">https://github.com/wangzhou/notes)作为superproject<i class="fa fa-external-link-alt"></i></span><br> 的一个子仓库。</p>
<p> 如上添加命令后，我们可以看到superproject的目录下多了.gitmodules文件，submodule/notes<br> 目录里也多了notes仓库的代码。当我给superproject加notes这个子仓库的时候，git顺着<br> notes的URL找到notes，并把notes的当前分支上的最新提交作为子仓库的检出点。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sherlock@m1:~/tests/git_submodule/superproject$ cat .gitmodules </span><br><span class="line">[submodule &quot;submodule/notes&quot;]</span><br><span class="line">        path = submodule/notes</span><br><span class="line">        url = https://github.com/wangzhou/notes</span><br><span class="line">sherlock@m1:~/tests/git_submodule/superproject$ git submodule status</span><br><span class="line"> e78b0444f95c05e6cc9a2aa953dfd135e6cde1ef submodule/notes (heads/master)</span><br></pre></td></tr></table></figure>

<p> 当我们把.gitmodules和submodule/notes提交superproject后，会发现submodule/notes<br> 的改动就一行，它指向子仓库的一个提交。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sherlock@m1:~/tests/git_submodule/superproject$ git status</span><br><span class="line">On branch master</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage)</span><br><span class="line">        new file:   .gitmodules</span><br><span class="line">        new file:   submodule/notes</span><br><span class="line"></span><br><span class="line">sherlock@m1:~/tests/git_submodule/superproject$ git add .gitmodules submodule/notes/</span><br><span class="line">sherlock@m1:~/tests/git_submodule/superproject$ git commit -s -m &quot;add note as a submodule&quot;</span><br><span class="line">[master 95fe141] add note as a submodule</span><br><span class="line"> 2 files changed, 4 insertions(+)</span><br><span class="line"> create mode 100644 .gitmodules</span><br><span class="line"> create mode 160000 submodule/notes</span><br><span class="line"></span><br><span class="line">sherlock@m1:~/tests/git_submodule/superproject$ git log -p</span><br><span class="line">[...]</span><br><span class="line">diff --git a/.gitmodules b/.gitmodules</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000..5f6b923</span><br><span class="line">--- /dev/null</span><br><span class="line">+++ b/.gitmodules</span><br><span class="line">@@ -0,0 +1,3 @@</span><br><span class="line">+[submodule &quot;submodule/notes&quot;]</span><br><span class="line">+       path = submodule/notes</span><br><span class="line">+       url = https://github.com/wangzhou/notes</span><br><span class="line">diff --git a/submodule/notes b/submodule/notes</span><br><span class="line">new file mode 160000</span><br><span class="line">index 0000000..e78b044</span><br><span class="line">--- /dev/null</span><br><span class="line">+++ b/submodule/notes</span><br><span class="line">@@ -0,0 +1 @@</span><br><span class="line">+Subproject commit e78b0444f95c05e6cc9a2aa953dfd135e6cde1ef</span><br></pre></td></tr></table></figure>

<p>git submodule里有和子仓库相关的各种自命令，比如，我们可以用如下的命令更新子仓库，<br>可以看见superproject的子仓库的提交也更新了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sherlock@m1:~/tests/git_submodule/superproject$ git submodule update --remote </span><br><span class="line">remote: Enumerating objects: 5, done.</span><br><span class="line">remote: Counting objects: 100% (5/5), done.</span><br><span class="line">remote: Compressing objects: 100% (1/1), done.</span><br><span class="line">remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0</span><br><span class="line">Unpacking objects: 100% (3/3), 289 bytes | 96.00 KiB/s, done.</span><br><span class="line">From https://github.com/wangzhou/notes</span><br><span class="line">   9ed1125..a24c0b1  master     -&gt; origin/master</span><br><span class="line">Submodule path &#x27;submodule/notes&#x27;: checked out &#x27;a24c0b112dd342e2c00ec79dfe4f5f3fbfcd09b4&#x27;</span><br><span class="line">sherlock@m1:~/tests/git_submodule/superproject$ git diff</span><br><span class="line">diff --git a/submodule/notes b/submodule/notes</span><br><span class="line">index e78b044..a24c0b1 160000</span><br><span class="line">--- a/submodule/notes</span><br><span class="line">+++ b/submodule/notes</span><br><span class="line">@@ -1 +1 @@</span><br><span class="line">-Subproject commit e78b0444f95c05e6cc9a2aa953dfd135e6cde1ef</span><br><span class="line">+Subproject commit a24c0b112dd342e2c00ec79dfe4f5f3fbfcd09b4</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>ftrace学习笔记1</title>
    <url>/ftrace%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ol>
<li><p>ftrace是一个内核调试工具，调试内核运行，是一个trace工具。它可以查看函数的<br>  调用关系、函数的执行时间、系统中断的最大关闭时间等等</p>
</li>
<li><p>需编译内核时进行配置，使用时挂载虚拟文件系统debugfs：<br>  mount -t debugfs nodev /sys/kernel/debug</p>
</li>
<li><p>通过对该文件系统中文件的读写完成所用功能</p>
</li>
</ol>
<h2 id="配置与编译"><a href="#配置与编译" class="headerlink" title="配置与编译"></a>配置与编译</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Kernel hacking  ---&gt;</span><br><span class="line">[*] Tracers ---&gt;</span><br><span class="line">-*- Tracers</span><br><span class="line">...(配置相应的跟踪器)</span><br></pre></td></tr></table></figure>

<h2 id="具体使用"><a href="#具体使用" class="headerlink" title="具体使用"></a>具体使用</h2><p>下面显示的就是ftrace使用时的目录了(pc ubuntu12.04)，所有的操作都可以通过读写这些<br>文件完成，读写的时候用到的命令多为echo, cat之类进入/sys/kernel/debug/tracing,<br>ubuntu12.04默认已经把ftrace编译入了内核, ls /sys/kernel/debug/tracing得到:</p>
<p>vailable_events            kprobe_profile      stack_trace<br>available_filter_functions  options             trace<br>available_tracers           per_cpu             trace_clock<br>buffer_size_kb              printk_formats      trace_marker<br>buffer_total_size_kb        README              trace_options<br>current_tracer              saved_cmdlines      trace_pipe<br>dyn_ftrace_total_info       set_event           trace_stat<br>enabled_functions           set_ftrace_filter   tracing_cpumask<br>events                      set_ftrace_notrace  tracing_enabled<br>free_buffer                 set_ftrace_pid      tracing_max_latency<br>function_profile_enabled    set_graph_function  tracing_on<br>kprobe_events               stack_max_size      tracing_thresh</p>
<p>可以 cat vailable_tracers 查看现在支持的tracer, 一般的tracer有：</p>
<p>function, 可以打印出内核函数的调用过程<br>function_graph, 以函数调用的格式打印函数调用过程，看起来要方便很多<br>irqsoff, 打印出禁止中断的时间，对于系统响应不及时的问题，可以用这个查看<br>wakeup，这个可以打印进程从ready到run的latency<br>sched_swich，显示的是关于调度的信息</p>
<p>cat current_tracer 查看当前的tracer是什么，所以要用一个tracer的时候，首先要<br>把它写到这个文件中，比如要用function，就写 echo function &gt; current_tracer</p>
<p>下面具体看一个一个tracer的用法和由此引出来的东西:</p>
<ul>
<li>function:</li>
</ul>
<p>因为测试的时候常常挂死（cat trace）, 这里就把function的使用写成了一个脚<br>本，我们一行一行看下，echo 0 &gt; tracing_on， 暂停跟踪器，参考网上的资料，<br>一般会有一个tracing_enabled的文件(但是有些系统上可能没有),关于tracing_on<br>和tracing_enabled的区别，现在的理解是，tracing_on是暂停跟踪器，此时跟踪<br>器还在跟踪内核的运行，只是不再向文件 trace 中写入跟踪信息，<br>echo 1 &gt; tracing_on 是可以继续当前的跟踪的，而tracing_enabled用来开关<br>ftrace。另外，ftrace提供了内核函数tracing_on()，这个东西可以直接写在内<br>核里，放在你想要停起的地方，当tracing_on()执行的时候，你在外面cat trace，<br>显示的就是附近的信息，如果你在用户态暂停，trace中的内容会离你想要定位的<br>地方远</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#! /system/bin/sh </span><br><span class="line"># change to /sys/kernel/debug/tracing </span><br><span class="line"></span><br><span class="line">dir=&quot;/sys/kernel/debug/tracing/&quot; </span><br><span class="line">echo 0 &gt; $&#123;dir&#125;tracing_on </span><br><span class="line">echo function &gt; $&#123;dir&#125;current_tracer          # 写入function</span><br><span class="line">echo 1 &gt; $&#123;dir&#125;tracing_on                     # 运行tracer</span><br><span class="line">sleep 5                                       # 叫tracer运行一段时间</span><br><span class="line">echo 0 &gt; $&#123;dir&#125;tracing_on                     # 暂停tracer</span><br><span class="line">cat $&#123;dir&#125;trace | head -10                    # 显示跟踪的函数内容</span><br></pre></td></tr></table></figure>
<p>截取了一段现实的结果放在这里</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tracer: function </span><br><span class="line">#                       </span><br><span class="line"># entries-in-buffer/entries-written: 222082/295723   #P:4 </span><br><span class="line">#                                               （online cpu number: 4）</span><br><span class="line">#                              _-----=&gt; irqs-off </span><br><span class="line">#                             / _----=&gt; need-resched </span><br><span class="line">#                            | / _---=&gt; hardirq/softirq </span><br><span class="line">#                            || / _--=&gt; preempt-depth </span><br><span class="line">#                            ||| /     delay </span><br><span class="line">#           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION </span><br><span class="line">#              | |       |   ||||       |         | </span><br><span class="line"> SurfaceFlinger-255   [001] d... 14810.259416: gic_handle_irq &lt;-__irq_usr </span><br><span class="line"> SurfaceFlinger-255   [001] d... 14810.259483: irq_find_mapping &lt;-gic_handle_irq </span><br></pre></td></tr></table></figure>
<ul>
<li> function_graph:</li>
</ul>
<p>同样的操作，看看function_graph的输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  # tracer: function_graph </span><br><span class="line">  # </span><br><span class="line">  # CPU  DURATION                  FUNCTION CALLS </span><br><span class="line">  # |     |   |                     |   |   |   | </span><br><span class="line">   3)               |  __do_fault() &#123; </span><br><span class="line">3)               |    i915_gem_fault() &#123; </span><br><span class="line">3)               |      i915_mutex_lock_interruptible() &#123; </span><br><span class="line">3)               |        mutex_lock_interruptible() &#123; </span><br><span class="line">3)   0.339 us    |          _cond_resched(); </span><br><span class="line">3)   1.212 us    |        &#125; </span><br><span class="line">3)   1.887 us    |      &#125; 3)               |      i915_gem_object_unbind() &#123; </span><br><span class="line">3)   0.114 us    |        i915_gem_object_finish_gpu(); </span><br><span class="line">3)   0.120 us    |        i915_gem_release_mmap();</span><br></pre></td></tr></table></figure>
<p>这里 echo __do_fault &gt; set_graph_function了一下，所以输出是针对<br>_do_fault()的检测</p>
<ul>
<li>irqsoff:</li>
</ul>
<p>下面截取一段irqsoff的输出信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tracer: irqsoff </span><br><span class="line"># </span><br><span class="line"># irqsoff latency trace v1.1.5 on 3.9.0-rc7-00004-g70df926-dirty </span><br><span class="line"># -------------------------------------------------------------------- </span><br><span class="line"># latency: 17790 us, #323/323, CPU#1 | (M:server VP:0, KP:0, SP:0 HP:0 #P:4) </span><br><span class="line">#    ----------------- </span><br><span class="line">#    | task: ProcessStats-502 (uid:1000 nice:0 policy:0 rt_prio:0) </span><br><span class="line">#    ----------------- </span><br><span class="line">#  =&gt; started at: __lock_task_sighand </span><br><span class="line">#  =&gt; ended at:   _raw_spin_unlock_irqrestore </span><br><span class="line"># </span><br><span class="line"># </span><br><span class="line">                  _------=&gt; CPU#            </span><br><span class="line"> / _-----=&gt; irqs-off        </span><br><span class="line"> | / _----=&gt; need-resched    </span><br><span class="line"> || / _---=&gt; hardirq/softirq </span><br><span class="line"> ||| / _--=&gt; preempt-depth   </span><br><span class="line"> |||| /     delay             </span><br><span class="line">        # cmd     pid      ||||| time  |   caller      </span><br><span class="line">      \   /          |||||  \       |   /           </span><br><span class="line">ProcessS-502     1d...   49us!: __lock_task_sighand </span><br><span class="line">ProcessS-502     1d...  154us!: _raw_spin_lock &lt;-__lock_task_sighand </span><br><span class="line">ProcessS-502     1d...  933us+: thread_group_cputime_adjusted &lt;-do_task_stat </span><br><span class="line">ProcessS-502     1d... 1016us+: thread_group_cputime &lt;-thread_group_cputime_adjusted </span><br><span class="line">ProcessS-502     1d... 1106us+: task_sched_runtime &lt;-thread_group_cputime </span><br><span class="line">ProcessS-502     1d... 1195us+: task_rq_lock &lt;-task_sched_runtime </span><br><span class="line">        ProcessS-502     1d... 1286us!: _raw_spin_lock_irqsave &lt;-task_rq_lock</span><br></pre></td></tr></table></figure>
<p>上面找到的是中断关闭时间最长的进程（ProcessStats-502），具体显示出关闭<br>和开中断的函数，CPU# 进程运行在哪个CPU上，irqs-off ‘d’表示中断关闭<br>(上半部)，need-resched设置时中断出来之后，执行一次调度，hardirq/softirq<br>表示硬中断/软中断正在运行。latency: 17790us表示的是关中断最长的时间</p>
<ul>
<li>Wakeup_rt:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  # tracer: wakeup_rt </span><br><span class="line">  # </span><br><span class="line">  # wakeup_rt latency trace v1.1.5 on 3.9.0-rc7-00004-g70df926-dirty </span><br><span class="line">  # -------------------------------------------------------------------- </span><br><span class="line">  # latency: 4155 us, #269/269, CPU#0 | (M:server VP:0, KP:0, SP:0 HP:0 #P:4) </span><br><span class="line">  #    ----------------- </span><br><span class="line">  #    | task: watchdog/0-11 (uid:0 nice:0 policy:1 rt_prio:99) </span><br><span class="line">  #    ----------------- </span><br><span class="line">  # </span><br><span class="line">  #                  _------=&gt; CPU#            </span><br><span class="line">  #                 / _-----=&gt; irqs-off        </span><br><span class="line">  #                | / _----=&gt; need-resched    </span><br><span class="line">  #                || / _---=&gt; hardirq/softirq </span><br><span class="line">  #                ||| / _--=&gt; preempt-depth   </span><br><span class="line">  #                |||| /     delay             </span><br><span class="line">  #  cmd     pid   ||||| time  |   caller      </span><br><span class="line">  #     \   /      |||||  \    |   /           </span><br><span class="line">    &lt;idle&gt;-0       0d.h.   32us+:      0:120:R   + [000]    11:  0:R watchdog/0 </span><br><span class="line">&lt;idle&gt;-0       0d.h.   99us+: 0 </span><br><span class="line">&lt;idle&gt;-0       0d.h.  114us+: check_preempt_curr &lt;-ttwu_do_wakeup </span><br><span class="line">&lt;idle&gt;-0       0d.h.  126us+: resched_task &lt;-check_preempt_curr </span><br><span class="line">&lt;idle&gt;-0       0dNh.  141us+: task_woken_rt &lt;-ttwu_do_wakeup </span><br><span class="line">&lt;idle&gt;-0       0dNh.  157us+: _raw_spin_unlock_irqrestore &lt;-try_to_wake_up </span><br><span class="line">&lt;idle&gt;-0       0dNh.  172us+: ktime_get &lt;-watchdog_timer_fn </span><br><span class="line">…</span><br><span class="line">&lt;idle&gt;-0       0dN.. 4064us+: put_prev_task_idle &lt;-__schedule &lt;idle&gt;-0       0dN.. 4077us+: pick_next_task_stop &lt;-__schedule </span><br><span class="line">&lt;idle&gt;-0       0dN.. 4090us+: pick_next_task_rt &lt;-__schedule </span><br><span class="line">&lt;idle&gt;-0       0dN.. 4103us+: dequeue_pushable_task &lt;-pick_next_task_rt </span><br><span class="line">&lt;idle&gt;-0       0d... 4126us+: __schedule </span><br><span class="line">&lt;idle&gt;-0       0d... 4138us :      0:120:R ==&gt; [000]    11:  0:R watchdog/0</span><br></pre></td></tr></table></figure>
可以看到最长实时进程的latency是4155us, 其中很多时间是消耗在过程函数的统<br>计上，echo 0 &gt; /sys/kernel/debug/tracing/options/function-trace <br>可以关闭对过程函数的统计, 有些系统上没有上面的目录或文件</li>
</ul>
<p>设定<br>sysctl  kernel.ftrace_enabled=1   或者 <br>echo 1 &gt; /proc/sys/kernel/ftrace_enabled<br>也可以关掉对过程函数的跟踪, 下面是使用wakeup tracer时，不记录过程函数时<br>的测试结果：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tracer: wakeup </span><br><span class="line"># </span><br><span class="line"># wakeup latency trace v1.1.5 on 3.2.0-41-generic </span><br><span class="line"># -------------------------------------------------------------------- </span><br><span class="line"># latency: 8307 us, #4/4, CPU#2 | (M:desktop VP:0, KP:0, SP:0 HP:0 #P:4) </span><br><span class="line">#    ----------------- </span><br><span class="line">#    | task: alsa-sink-22445 (uid:1002 nice:-11 policy:2 rt_prio:5) </span><br><span class="line">#    ----------------- </span><br><span class="line"># </span><br><span class="line">#                  _------=&gt; CPU#            </span><br><span class="line">#                 / _-----=&gt; irqs-off        </span><br><span class="line">#                | / _----=&gt; need-resched    </span><br><span class="line">#                || / _---=&gt; hardirq/softirq </span><br><span class="line">#                ||| / _--=&gt; preempt-depth   </span><br><span class="line">#                |||| /     delay             </span><br><span class="line">#  cmd     pid   ||||| time  |   caller      </span><br><span class="line">#     \   /      |||||  \    |   /           </span><br><span class="line">   Xorg-22173   2d.h1    0us :  22173:120:R   + [002] 22445: 94:R alsa-sink </span><br><span class="line">Xorg-22173   2d.h1    1us!: ttwu_do_activate.constprop.179 &lt;-sched_ttwu_pending </span><br><span class="line">Xorg-22173   2d... 8306us : probe_wakeup_sched_switch &lt;-__schedule </span><br><span class="line">Xorg-22173   2d... 8307us :  22173:120:R ==&gt; [002] 22445: 94:R alsa-sink </span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>ftrace</tag>
      </tags>
  </entry>
  <entry>
    <title>gdb用法收集</title>
    <url>/gdb%E7%94%A8%E6%B3%95%E6%94%B6%E9%9B%86/</url>
    <content><![CDATA[<p>最简单的用法就是在本地用gdb, gdb + 被调试的程序，注意被调试程序的参数要通过<br>set args xxx这样传给gdb。</p>
<p>我们启动一个实际的调试看看，这里的调试程序选qemu-riscv64，这个是一个qemu的用户态<br>模式命令行工具，用他可以跨平台的执行用户态程序，它的代码在qemu/linux-user下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sherlock@m1:~/repos/qemu/build$ gdb qemu-riscv64 </span><br><span class="line">GNU gdb (Ubuntu 9.2-0ubuntu1~20.04) 9.2</span><br><span class="line">Copyright (C) 2020 Free Software Foundation, Inc.</span><br><span class="line">License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;</span><br><span class="line">This is free software: you are free to change and redistribute it.</span><br><span class="line">There is NO WARRANTY, to the extent permitted by law.</span><br><span class="line">Type &quot;show copying&quot; and &quot;show warranty&quot; for details.</span><br><span class="line">This GDB was configured as &quot;aarch64-linux-gnu&quot;.</span><br><span class="line">Type &quot;show configuration&quot; for configuration details.</span><br><span class="line">For bug reporting instructions, please see:</span><br><span class="line">&lt;http://www.gnu.org/software/gdb/bugs/&gt;.</span><br><span class="line">Find the GDB manual and other documentation resources online at:</span><br><span class="line">    &lt;http://www.gnu.org/software/gdb/documentation/&gt;.</span><br><span class="line"></span><br><span class="line">For help, type &quot;help&quot;.</span><br><span class="line">Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;...</span><br><span class="line">Reading symbols from qemu-riscv64...</span><br><span class="line">warning: File &quot;/home/sherlock/repos/qemu/.gdbinit&quot; auto-loading has been declined by your `auto-load safe-path&#x27; set to &quot;$debugdir:$datadir/auto-load&quot;.</span><br><span class="line">To enable execution of this file add</span><br><span class="line">        add-auto-load-safe-path /home/sherlock/repos/qemu/.gdbinit</span><br><span class="line">line to your configuration file &quot;/home/sherlock/.gdbinit&quot;.</span><br><span class="line">To completely disable this security protection add</span><br><span class="line">        set auto-load safe-path /</span><br><span class="line">line to your configuration file &quot;/home/sherlock/.gdbinit&quot;.</span><br><span class="line">For more information about this security protection see the</span><br><span class="line">--Type &lt;RET&gt; for more, q to quit, c to continue without paging--</span><br><span class="line">&quot;Auto-loading safe path&quot; section in the GDB manual.  E.g., run from the shell:</span><br><span class="line">        info &quot;(gdb)Auto-loading safe path&quot;</span><br><span class="line">(gdb) set args ~/a.out</span><br><span class="line">(gdb) b tcg_gen_code</span><br><span class="line">Breakpoint 1 at 0x179e68: file ../tcg/tcg.c, line 4159.</span><br><span class="line">(gdb) r</span><br><span class="line">Starting program: /home/sherlock/repos/qemu/build/qemu-riscv64 ~/a.out</span><br><span class="line">[Thread debugging using libthread_db enabled]</span><br><span class="line">Using host libthread_db library &quot;/lib/aarch64-linux-gnu/libthread_db.so.1&quot;.</span><br><span class="line">[New Thread 0xfffff7971150 (LWP 548388)]</span><br><span class="line"></span><br><span class="line">Thread 1 &quot;qemu-riscv64&quot; hit Breakpoint 1, tcg_gen_code (s=0xaaaaaae88e88 &lt;tcg_init_ctx&gt;, tb=tb@entry=0xffffe8000080 &lt;code_gen_buffer+52&gt;)</span><br><span class="line">    at ../tcg/tcg.c:4159</span><br><span class="line">warning: Source file is more recent than executable.</span><br><span class="line">4159    &#123;</span><br><span class="line">(gdb) bt</span><br><span class="line">#0  tcg_gen_code (s=0xaaaaaae88e88 &lt;tcg_init_ctx&gt;, tb=tb@entry=0xffffe8000080 &lt;code_gen_buffer+52&gt;) at ../tcg/tcg.c:4159</span><br><span class="line">#1  0x0000aaaaaac1a3b4 in tb_gen_code (cpu=cpu@entry=0xaaaaaaec7d90, pc=pc@entry=66376, cs_base=cs_base@entry=0, flags=flags@entry=24832, cflags=0)</span><br><span class="line">    at ../accel/tcg/translate-all.c:1496</span><br><span class="line">#2  0x0000aaaaaac563fc in cpu_exec (cpu=cpu@entry=0xaaaaaaec7d90) at ../accel/tcg/cpu-exec.c:945</span><br><span class="line">#3  0x0000aaaaaab5e224 in cpu_loop (env=0xaaaaaaed0110) at ../linux-user/riscv/cpu_loop.c:37</span><br><span class="line">#4  0x0000aaaaaab484b8 in main (argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, envp=&lt;optimized out&gt;) at ../linux-user/main.c:885</span><br><span class="line">(gdb)</span><br></pre></td></tr></table></figure>
<p>如上，使用gdb qemu-riscv64启动调试，set args ~/a.out设置qemu-riscv64的参数，b tcg_gen_code<br>在tcg_gen_code上打一个断点，r把程序跑起来，程序会在断点处停下来，bt可以把这个时候<br>的调用栈打出来。c可以叫程序继续跑起来，一直到下一个断点处停下来。</p>
<p>我们也可以用.gdbinit把gdb启动后需要执行的命令先写到这个配置文件里，gdb启动后就会<br>自动调用。.gdbinit可以放到自己的home目录作为默认配置文件，也可以放到运行gdb的当前<br>目录，.gdbinit里可以用file把要跑的命令加上，用set args把命令的参数加上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">file ./qemu_path/qemu-system-riscv</span><br><span class="line">set args -m 2048M  \</span><br><span class="line">	qemu parameters \</span><br><span class="line">break main</span><br><span class="line">run</span><br></pre></td></tr></table></figure>

<p>打断点的方式还有很多变种：</p>
<ol>
<li>b file:line      文件名 + 文件行号</li>
<li>b line           直接用行号</li>
<li>b function       直接用函数名</li>
<li>b file:function  文件名 + 函数名</li>
<li>b xxxx if cond   满足后面的条件才会断住程序</li>
</ol>
<p>info命令可以查看gdb的信息，比如info b查看当前所有设定的断点，info registers查看<br>寄存器的值。</p>
<p>delete number可以删去对应的断点，这里的number就是info b里显示的断点的编号。</p>
<p>gdb可以单步执行程序，或者跳入跳出函数。用n单步执行，这个是以函数为单位的，要想<br>调入函数执行需要用s，退出函数回到上层执行用finish。如果单步执行到一个循环里，可以<br>用u直接把循环一次都执行完。在gdb输入enter会重复输入上次的命令。</p>
<p>用p可以打出参数的值，如果是一个地址，可以p *arg打印地址上的数据，对于静态数组可以<br>直接用”p 数组名”打印数组成员，对于动态申请的内存可以”p 地址@size”打印这个地址开始<br>的长度为size的数据，p file::variable、p function::variable可以给变量限定作用域。</p>
<p>watch cond可以监控变量和表达式的值没有改变，只要改变，程序就会停止。</p>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>gdb</tag>
      </tags>
  </entry>
  <entry>
    <title>glib option简易使用</title>
    <url>/glib-option%E7%AE%80%E6%98%93%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>直接在代码里加注释说明基本的使用方式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static gboolean opt_hugepage = FALSE;</span><br><span class="line">static gint opt_size = 128;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * GOptionEntry定义一个输入参数项。每项的定义依次是：长参数名字，短参数名字，flag，</span><br><span class="line"> * 参数类型，参数存放地址，参数项描述，参数描述。</span><br><span class="line"> *</span><br><span class="line"> * 简单使用时，flag用G_OPTION_FLAG_NONE就好，最后一个参数描述会在长参数后输出，</span><br><span class="line"> * 可以参看最下面的--help输出。</span><br><span class="line"> */</span><br><span class="line">static GOptionEntry entries[] = &#123;</span><br><span class="line">	&#123; &quot;size&quot;, &#x27;s&#x27;, G_OPTION_FLAG_NONE, G_OPTION_ARG_INT, &amp;opt_size,</span><br><span class="line">	  &quot;size of dma copy in normal page case&quot;, &quot;size&quot; &#125;,</span><br><span class="line">	&#123; &quot;hugepage&quot;, &#x27;h&#x27;, G_OPTION_FLAG_NONE, G_OPTION_ARG_NONE, &amp;opt_hugepage,</span><br><span class="line">	  &quot;use hugepage(one 2M page for src, one for dts)&quot;, NULL &#125;,</span><br><span class="line">	&#123; NULL &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">static void handle_options(int argc, char *argv[])</span><br><span class="line">&#123;</span><br><span class="line">	GError *err = NULL;</span><br><span class="line">	/* 参数项描述的上下文 */</span><br><span class="line">	GOptionContext *context;</span><br><span class="line"></span><br><span class="line">	/* 创建参数项描述的上下文 */</span><br><span class="line">	context = g_option_context_new(&quot;- test devmmu pasid&quot;);</span><br><span class="line"></span><br><span class="line">	/* 把如上定义的各个参数项放到context中 */</span><br><span class="line">	g_option_context_add_main_entries(context, entries, NULL);</span><br><span class="line"></span><br><span class="line">	/* 调用这个函数解析输入值 */</span><br><span class="line">	g_option_context_parse(context, &amp;argc, &amp;argv, &amp;err);</span><br><span class="line"></span><br><span class="line">	/* 释放context */</span><br><span class="line">	g_option_context_free(context);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(int argc, char *argv[])</span><br><span class="line">&#123;</span><br><span class="line">	handle_options(argc, argv);</span><br><span class="line"></span><br><span class="line">	/* ... */</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如上的配置，–help的输出:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># ghms_test --help</span><br><span class="line">Usage:</span><br><span class="line">  ghms_test [OPTION*] - test devmmu pasid</span><br><span class="line"></span><br><span class="line">Help Options:</span><br><span class="line">  -?, --help          Show help options</span><br><span class="line"></span><br><span class="line">Application Options:</span><br><span class="line">  -s, --size=size     size of dma copy in normal page case</span><br><span class="line">  -h, --hugepage      use hugepage(one 2M page for src, one for dts)</span><br></pre></td></tr></table></figure>

<p>使用如上的函数可以简单的把glib option使用起来，更多使用方法需要去查glib的使用手册。</p>
]]></content>
      <tags>
        <tag>Linux用户态编程</tag>
        <tag>glib</tag>
      </tags>
  </entry>
  <entry>
    <title>hisi perf uncore event</title>
    <url>/hisi-perf-uncore-event/</url>
    <content><![CDATA[<p>你可以使用 perf list来列出系统支持的perf事件，有一类perf事件可以用来统计CPU的<br>L3 cache, HHA和DDRC的事件，他们统一叫uncore事件。他们对应的内核驱动是在<br>linux/drivers/perf/hisilicon/*。这些uncore event的命名是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hisi_sccl1_ddrc0/act_cmd/                          [Kernel PMU event]</span><br><span class="line">hisi_sccl1_ddrc0/flux_rcmd/                        [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_ddrc1/act_cmd/                          [Kernel PMU event]</span><br><span class="line">hisi_sccl1_ddrc1/flux_rcmd/                        [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_ddrc2/act_cmd/                          [Kernel PMU event]</span><br><span class="line">hisi_sccl1_ddrc2/flux_rcmd/                        [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_ddrc3/act_cmd/                          [Kernel PMU event]</span><br><span class="line">hisi_sccl1_ddrc3/flux_rcmd/                        [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_hha2/bi_num/                            [Kernel PMU event]</span><br><span class="line">hisi_sccl1_hha2/edir-hit/                          [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_hha3/bi_num/                            [Kernel PMU event]</span><br><span class="line">hisi_sccl1_hha3/edir-hit/                          [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_l3c10/back_invalid/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/prefetch_drop/                    [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_l3c11/back_invalid/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c11/prefetch_drop/                    [Kernel PMU event]</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>

<p>我们在做性能分析的时候，首先要看懂这些统计，把这些项目和程序运行的CPU对应上。<br>现在依次介绍相关的概念。一个完整的服务器CPU系统(我们这里不看IO)，是由物理CPU，<br>物理CPU中的CPU die, CPU die上的一个个CPU core组成的。一般，物理CPU支持多个互联<br>在一起，我们下面用chip表示一个物理CPU, 一个物理CPU里可以有多个CPU die, 在uncore<br>event里CPU die我们叫做sccl<n>, 这里的n是sccl的编号，其中chip0(主片)上的CPU die<br>分别叫sccl1和sccl3, chip1(从片)上的叫sccl5、sccl7，注意我们这里举例的系统一个<br>物理CPU里有两个CPU die。</n></p>
<p>一个sccl中的CPU core是四个聚集在一起成一个cluster，一般一个sccl里有6个cluster,<br>那么一个sccl就有24个core，一个物理CPU有48个core, 一个2P系统就有96个core。这些core<br>和DDR的连接如下图, 他们通过HHA和DDRC连接，DDRC和DIM条连接。sccl之间通过HHA相连。<br>这些core和L3 cache的连接关系(这里先不考虑L1, L2 cache)是一个sccl和一大块L3相连，<br>在使用上，把这一大块L3 cache分成几个partition, 一般是有几个cluster就分几个partion,<br>一个cluster里的core优先使用自己cluster对应的L3 partition，当然也可以使用其他的<br>L3 partion。</p>
<p>这样，我们很好看懂上面的event，比如:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hisi_sccl1_l3c11/back_invalid/                     [Kernel PMU event]</span><br></pre></td></tr></table></figure>
<p>就表示，chip0上sccl1这个CPU die上L3 partition编号是11的back_invalid事件。一般，<br>一个sccl对应一个NUMA node节点，一个l3c后面的编号在一个sccl上是顺序增加的。<br>比如，sccl1上的的各个l3c的编号是，l3c10, l3cll, l3c12, l3c13, l3c14, l3c15，那么<br>l3c11对应的就是这个sccl1上的core4~core7。一般，sccl1对应的就是系统里的node0,<br>sccl3对应node1，sccl5对应node2，sccl7对应node3。</p>
<p>使用numactl -H可以确定各个NUMA node里的CPU编号，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">available: 4 nodes (0-3)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31</span><br><span class="line">node 0 size: 0 MB</span><br><span class="line">node 0 free: 0 MB</span><br><span class="line">node 1 cpus: 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63</span><br><span class="line">node 1 size: 31912 MB</span><br><span class="line">node 1 free: 30275 MB</span><br><span class="line">node 2 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95</span><br><span class="line">node 2 size: 0 MB</span><br><span class="line">node 2 free: 0 MB</span><br><span class="line">node 3 cpus: 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127</span><br><span class="line">node 3 size: 32097 MB</span><br><span class="line">node 3 free: 31514 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0   1   2   3 </span><br><span class="line">  0:  10  12  20  22 </span><br><span class="line">  1:  12  10  22  24 </span><br><span class="line">  2:  20  22  10  12 </span><br><span class="line">  3:  22  24  12  10 </span><br></pre></td></tr></table></figure>
<p>注意这个系统是128core的系统，无非是系统拓扑基本上不变，一个sccl里多了2个cluster。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                              sccl1      sccl3                 sccl5         sccl7</span><br><span class="line">    DIM0 DIM1  DIM2 DIM3        /         /</span><br><span class="line">      |  |       |  |          /  ...    /      chip0                               chip1</span><br><span class="line">+-----+--+-------+--+---------/---------/------------+   +-------------------------------+</span><br><span class="line">| +---+--+-------+--+--------/-+ +-----/-----------+ |   |                               |</span><br><span class="line">| | +-----+    +-----+      /  | |    /            | |   |                               |</span><br><span class="line">| | |DDRC0|    |DDRC1|     /   | |   /             | |   |                               |</span><br><span class="line">| | +---+-+    +-+---+    /    | |  /              | |   |                               |</span><br><span class="line">| |     |        |             | |                 | |   |                               |</span><br><span class="line">| |     +---+ +--+             | |                 | |   |                               |</span><br><span class="line">| |         | |                | |  ...            | |   |    ...                        |</span><br><span class="line">| |      +--+-+-+              | |                 | |   |                               |</span><br><span class="line">| |      | HHA0 +--------------+-+--               | |   |                               |</span><br><span class="line">| |      +--+---+              | |                 | |   |                               |</span><br><span class="line">| |         |         +------+ | |                 | |   |                               |</span><br><span class="line">| |   +-----+-----+   |      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |   |core0|core1|   |l3c&lt;n&gt;| | | |core0|core1|   | |   | |core0|core1|   |core0|core1| |</span><br><span class="line">| |   +-----+-----+---+      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |   |core2|core3|   +------+ | | |core2|core3|   | |   | |core2|core3|   |core2|core3| |</span><br><span class="line">| |   +-----+-----+   |      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |                   |...   | | |                 | |   |                               |</span><br><span class="line">| |   ...             |      | | | ...             | |   | ...             ...           |</span><br><span class="line">| |                   |      | | |                 | |   |                               |</span><br><span class="line">| |   +-----+-----+   |      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |   |core0|core1|   +------+ | | |core0|core1|   | |   | |core0|core1|   |core0|core1| |</span><br><span class="line">| |   +-----+-----+---+      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |   |core2|core3|   |l3c&lt;n&gt;| | | |core2|core3|   | |   | |core2|core3|   |core2|core3| |</span><br><span class="line">| |   +-----+-----+   |      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |         |         +------+ | |                 | |   |                               |</span><br><span class="line">| |      +--+---+              | |                 | |   |                               |</span><br><span class="line">| |      | HHA1 +--------------+-+--               | |   |                               |</span><br><span class="line">| |      +--+-+-+              | |  ...            | |   |    ...                        |</span><br><span class="line">| |         | |                | |                 | |   |                               |</span><br><span class="line">| |     +---+ +--+             | |                 | |   |                               |</span><br><span class="line">| |     |        |             | |                 | |   |                               |</span><br><span class="line">| | +---+-+   +--+--+          | |                 | |   |                               |</span><br><span class="line">| | |DDRC2|   |DDRC3|          | |                 | |   |                               |</span><br><span class="line">| | +-----+   +-----+          | |                 | |   |                               |</span><br><span class="line">| +--+--+-------+--+-----------+ +-----------------+ |   |                               |</span><br><span class="line">+----+--+-------+--+---------------------------------+   +-------------------------------+</span><br><span class="line">     |  |       |  |               ...                     ...</span><br><span class="line">   DIM0 DIM1  DIM2 DIM3</span><br></pre></td></tr></table></figure>
<p>下面我们先看下L3 cache的各个event的含义：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hisi_sccl1_l3c10/back_invalid/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/prefetch_drop/                    [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/rd_cpipe/                         [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/rd_hit_cpipe/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/rd_hit_spipe/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/rd_spipe/                         [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/retry_cpu/                        [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/retry_ring/                       [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/victim_num/                       [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/wr_cpipe/                         [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/wr_hit_cpipe/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/wr_hit_spipe/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/wr_spipe/                         [Kernel PMU event]</span><br></pre></td></tr></table></figure>
<p>  rd_cpipe, rd_spipe可以表示CPU发出的所有请求数。<br>  rd_hit_cpipe, rd_hit_spipe可以表示CPU发出的请求hit该L3 partition的数目。</p>
]]></content>
      <tags>
        <tag>软件性能</tag>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>git使用笔记</title>
    <url>/git%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="chaper-2"><a href="#chaper-2" class="headerlink" title="chaper 2"></a>chaper 2</h2><p>git add -u 加入缓冲区所有的改动文件<br>        -A 把所有增加/删除的文件加入缓冲区<br>    -i 提供一个添加文件的交互界面</p>
<p>git diff 显示改动，只显示没有提交到缓冲区的改动<br>         –cached 显示缓冲区中的改动<br>     old-ID new-ID 显示两次提交的改动</p>
<p>git stash/git stash pop 工作的时候往往要临时切换到一个分支，这时可以用git stash<br>    不需要git add, 当处理完另一个分支时，切换会原来的分支，使用git stash pop<br>    即可以回到原来的工作区</p>
<h2 id="chapter-3"><a href="#chapter-3" class="headerlink" title="chapter 3"></a>chapter 3</h2><p> 中文：UTF-8字符集(一般)</p>
<h2 id="chapter-4"><a href="#chapter-4" class="headerlink" title="chapter 4"></a>chapter 4</h2><p> git config -e           改当前库的配置(.git/config)<br> git config -e –global  改当前用户的配置(/home/***/.gitconfig)<br> git config -e –system  改整个系统的配置(/etc/gitconfig)</p>
<h2 id="chapter-5"><a href="#chapter-5" class="headerlink" title="chapter 5"></a>chapter 5</h2><p> git log –oneline 精简模式显示提交信息<br> git log –prety=fuller 显示AuthorData和Commit Date ?<br> git ls-tree -l HEAD 查看版本库中的最新提交<br> git ls-files 查看缓冲区的最新内容</p>
<h2 id="chapter-6"><a href="#chapter-6" class="headerlink" title="chapter 6"></a>chapter 6</h2><p> git cat-file -t <ID> 显示ID对应的类型(type)：commit, tree, blob…<br>              -p <ID> 显示ID对应的内容<br> git rev-parse HEAD/master… 显示对应的ID<br> git log –graph –all 显示commit的历史，加上–all可以显示所有分支</ID></ID></p>
<p> HEAD: 保存当前分支的路径，若当前分支test, HEAD内容是 ref: refs/heads/test<br> .git/refs/heads/master : heads目录下存放所有的分支，若此时git中还有一个分支test<br>                          在heads下会发现：master，test. master, test文件中放的<br>              是ID，是对应分支最新提交的ID<br> .git/refs/tags/*** : …</p>
<h2 id="chapter-7"><a href="#chapter-7" class="headerlink" title="chapter 7"></a>chapter 7</h2><p> git reset –hard HEAD^ 整个HEAD切换到他的父提交, 若现在有如下的提交:<br> A–&gt;B–&gt;C–&gt;D<br> 使用上述命令后，使用git log将只看到: A–&gt;B–&gt;C<br> git reset –hard 将版本库，缓存区，工作区全部切换到相应的版本(C), D版本相当于<br> 被丢掉了(没有显示出来)</p>
<p> git reset –soft HEAD^ 只把版本库切到了父提交，也就是回到了，上次git add ***<br> git commit *** 之前的状态，使用git status可以证明这点</p>
<p> git reset HEAD^ 把版本库，缓存区切到了父提交，也就是回到了上次编辑过工作区，<br> git add *** 之前的状态，使用git status可以看到这点</p>
<p> git reset/git reset HEAD 依照上面的分析，相当于缓冲区切到父提交，就是把git add<br> 加入缓冲区的东西去掉，是git add 的逆操作</p>
<p> git reset – filename 是git add filename的逆操作</p>
<p> git reset –hard HEAD^ 之后的挽救措施：（想恢复原来的提交）<br> 在.git/log/logs/HEAD中记录着每次HEAD的改动，找到想要的ID用来恢复<br> 更简单的方法：<br> git reflog show 找到要恢复的版本<br> git reset –hard master@{***} 即可</p>
<p> 注: git reset –hard HEAD^<br>     do some change…<br>     git add …<br>     git commit … (version E)<br>     实际是存储是：(其中version D是不可见的)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A--&gt;B--&gt;C--&gt;D</span><br><span class="line">         \--&gt;E</span><br></pre></td></tr></table></figure>
<pre><code> git reset 没有改变HEAD的内容，而是改变了.git/refs/head/... 的内容
</code></pre>
<h2 id="chapter-8"><a href="#chapter-8" class="headerlink" title="chapter 8"></a>chapter 8</h2><p> git checkout ID 检出ID所对应的提交. 比如；<br> A–&gt;B–&gt;C–&gt;D<br> git checkout ID(C) ID(C)表示C对应的ID<br> 这是用git branch察看所在的分支，会显示当前处于no branch的状态，实际上察看<br> .git/HEAD会发现其中的内容不是指向一个分支(如：ref:refs/heads/master), 而是一个<br> 具体提交的ID. 在这种no branch的状态可以查看代码，做验证，但是不能提交修改。<br> 其实也是可以在提交的，只是再从当前的状态切回某个分支(如：git checkout master),<br> 之前的提交不可见了:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">              /-- master</span><br><span class="line">A--&gt;B--&gt;C--&gt;D </span><br><span class="line">     \</span><br><span class="line">      E -- git checkout ID(B), git commit E</span><br></pre></td></tr></table></figure>
<p> 如上在no branch上提交了E，然后git checkout master切回了master这时候E不可见了.<br> 用git reflog show 查看提交的历史，然后git reset –hard HEAD@{…} 可以把HEAD<br> 指向E，这时 A–&gt;B–&gt;E 成了master分支，C、D不可见了</p>
<p> git checkout -b branch_name 创建新的分支，名字是branch_name<br> git checkout 改变HEAD的内容</p>
<h2 id="chapter-12"><a href="#chapter-12" class="headerlink" title="chapter 12"></a>chapter 12</h2><p> git cherry-pick id 把id对应的commit向当前的HEAD提交<br> A–&gt;B–&gt;C–&gt;D–&gt;E git checkout id(C) git cherry-pich id(E)会把E向C提交:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                 /-- master</span><br><span class="line">A--&gt;B--&gt;C--&gt;D--&gt;E </span><br><span class="line">         \</span><br><span class="line">          E &lt;--HEAD</span><br></pre></td></tr></table></figure>
<p> 这时HEAD分离的情况，HEAD不对应任何分支,可以建立新的branch, 也可以git reset可以<br> 把master的内容指向E, 这时D和其后的E将显示不出来</p>
<p> git cherry-pick id -e 可以修改commit的签名中的内容(邮箱)</p>
<p> git rebase</p>
<h2 id="chapter-15"><a href="#chapter-15" class="headerlink" title="chapter 15"></a>chapter 15</h2><p> git pull/push<br> git push 有时无法成功，可能是因为git push对应的git仓库不是bare的，直接推送会<br> 改变工作区。这可以配置对应的远程仓库：git config receive.denyCurrentBranch ignore<br> 这时可以成功push</p>
<h2 id="chapter-16"><a href="#chapter-16" class="headerlink" title=" chapter 16"></a> chapter 16</h2><p> -other<br> git commit –amend –author=’your name <email-box>‘ 可以修改commit中author一行的内容</email-box></p>
<p> patch的subject这一行有时不只是[PATCH], 比如在询问意见时可以是[PATCH RFC ***], 在第3版<br> patch时subject可以是[PATCH v3 <em><strong>]. 如何改变subject这一行的内容：可以在生成patch<br> 的时候加–subject-prefix=”</strong></em>“, 比如, git format-patch -s -2 –subject-prefix=”PATCH RFC”<br> 生成的patch subject为：[PATCH RFC 0/3], [PATCH RFC 1/3], [PATCH RFC 2/3], [PATCH RFC 3/3]</p>
<p> git send-email 使用git send-email发送patches, 组成的patches是一个系列的。<br> 用git format-patch生成patches, 然后一个个用普通邮箱发出，给出的patches是一个个分裂的。<br> git send-email *.patch 即可把当前目录里的patch都发送出去，而且git send-email提供一个<br> 对话是的发送过程，只要在过程中填入发送的邮箱即可。对于cc的邮箱可以在一开始的命令中给出：<br> git send-email *.patch --cc=<span class="exturl" data-url="bWFpbHRvOiYjMTIxOyYjeDZmOyYjeDc1OyYjeDcyOyYjeDVmOyYjeDY1OyYjMTA5OyYjOTc7JiMxMDU7JiMxMDg7JiM5NTsmI3g2MjsmI3g2ZjsmI3g3ODsmIzY0OyYjNDk7JiM1MDsmIzU0OyYjeDJlOyYjeDYzOyYjMTExOyYjMTA5Ow==">&#121;&#x6f;&#x75;&#x72;&#x5f;&#x65;&#109;&#97;&#105;&#108;&#95;&#x62;&#x6f;&#x78;&#64;&#49;&#50;&#54;&#x2e;&#x63;&#111;&#109;<i class="fa fa-external-link-alt"></i></span></p>
<p> Message-ID to be used as In-Reply-To?</p>
<h2 id="git-pull-note"><a href="#git-pull-note" class="headerlink" title="git pull note"></a>git pull note</h2><ol>
<li><p>git repo A:<br>branch: master, test</p>
<p>git repo B;<br>branch: master, test(all pull from repo A)</p>
<p>若在repo A上test分支加一个提交, 在repo B的master分支上用git pull, reop<br>B的test分支将不会更新，repo B切换到test分支上，再使用git pull,<br>则可以更新test分支。</p>
</li>
<li><p>还是上面的场景，在repo A test分支上加一个commit new。在repo B中git fetch,<br>git checkout origin/test, git log, 会发现现在repo B的远程分支origin/test<br>有了repo A test分支上的commit new</p>
<p>在repo B的test分支上，git merge orgin/test，即可把git fetch得到的repo B<br>origin/test分支和test合并。这也就是常说的git pull = git fetch + git merge</p>
<p>可以看出repo B在本地是有origin/master, origin/test的远程分支的完整拷贝，也有<br>本地分支master, test。在git fetch操作时，只是把repo A上的新提交加到repo B的<br>origin/test“分支”上。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                    git clone</span><br><span class="line">repo A: A--&gt;B--&gt;C   ========&gt;   repo B: A--&gt;B--&gt;C </span><br><span class="line">                \                                    \</span><br><span class="line">               master                           master(也是origin/master)</span><br><span class="line"></span><br><span class="line">                     new commit</span><br><span class="line">                    /  </span><br><span class="line">repo A: A--&gt;B--&gt;C--&gt;D           repo B: A--&gt;B--&gt;C </span><br><span class="line">                    \                            \</span><br><span class="line">                     master                       master(也是origin/master)</span><br><span class="line"></span><br><span class="line">                     new commit        git fetch    origin/master</span><br><span class="line">                    /                               / </span><br><span class="line">repo A: A--&gt;B--&gt;C--&gt;D           repo B: A--&gt;B--&gt;C--&gt;D</span><br><span class="line">                    \                            \</span><br><span class="line">                     master                      master</span><br><span class="line"></span><br><span class="line">                     new commit       git merge     origin/master</span><br><span class="line">                    /                               / </span><br><span class="line">repo A: A--&gt;B--&gt;C--&gt;D           repo B: A--&gt;B--&gt;C--&gt;D</span><br><span class="line">                    \                                \</span><br><span class="line">                         master                                master</span><br><span class="line"></span><br><span class="line">如果在第三步中在repo B的master分支上又作了几次提交,比如:</span><br><span class="line">                     new commit                     origin/master</span><br><span class="line">                    /                               / </span><br><span class="line">repo A: A--&gt;B--&gt;C--&gt;D           repo B: A--&gt;B--&gt;C--&gt;D</span><br><span class="line">                    \                            \</span><br><span class="line">                    master                        --&gt;E--&gt;F  master</span><br><span class="line">                   </span><br><span class="line">那么在git merge会如下, master分支中会加入E, F两个提交</span><br><span class="line">                     new commit        git merge    origin/master</span><br><span class="line">                    /                               / </span><br><span class="line">repo A: A--&gt;B--&gt;C--&gt;D           repo B: A--&gt;B--&gt;C--&gt;D-------</span><br><span class="line">                    \                            \          \</span><br><span class="line">                    master                        --&gt;E--&gt;F--&gt;G  master</span><br></pre></td></tr></table></figure></li>
<li><p>git branch显示本地分支，git branch -r显示远程分支，<br>git checkout orgin/test -b local_branch 建立一个本地分支local_branch跟踪<br>远程分支</p>
</li>
<li><p>在repo B .git中的config文件中有这样的配置条目:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[remote &quot;origin&quot;]</span><br><span class="line"> url = /home/example/git_test_client/../git_test</span><br><span class="line"> fetch = +refs/heads/*:refs/remotes/origin/*</span><br><span class="line">[branch &quot;master&quot;]</span><br><span class="line"> remote = origin</span><br><span class="line"> merge = refs/heads/master</span><br><span class="line">[branch &quot;test_client&quot;]</span><br><span class="line"> remote = origin</span><br><span class="line"> merge = refs/heads/test</span><br></pre></td></tr></table></figure>
<p> 其中第一条[remote “origin”], url表示远程仓库的url, fetch表示做git fetch<br> 的时候远程仓库中的各个分支，对应本地仓库中的refs/remotes/orgin/下的各个“分支”。<br> 本地仓库的origin/master等严格的讲并不是一个分支，使用git checkout origin/master<br> 会显示处于头指针分离状态。<br> 后面的[branch “master”]条目表示，当时候git pull时，会把git fetch得到的orgin/master<br> merge到本地的master分支中。</p>
</li>
</ol>
<h2 id="给branch添加描述信息"><a href="#给branch添加描述信息" class="headerlink" title="给branch添加描述信息"></a>给branch添加描述信息</h2><p>   git branch –edit-description 可以添加当前git分支的说明，说明文字被添加到.git/config<br>   所以可以用git config -l 查看当前分支的说明，如果有说明的话。</p>
<h2 id="一些开发中有用的小技巧"><a href="#一些开发中有用的小技巧" class="headerlink" title="一些开发中有用的小技巧"></a>一些开发中有用的小技巧</h2><ol>
<li><p>已经知道了可以下载代码的git服务器的地址，比如：git://git.linaro.org/kernel.git<br>可以使用：</p>
<pre><code>git clone git://git.linaro.org/kernel.git
</code></pre>
<p>下在代码</p>
<p>要是你的本地电脑上已经有了之前clone的一个kernel的git仓库，可以使用：</p>
<pre><code>git clone git://git.linaro.org/kernel.git --reference /path/to/kernel.git
</code></pre>
<p>提高下载的速度，新的下载的git库将会重用以前已有的git库</p>
<p>下载好git库后，可以使用：</p>
<pre><code>git branch -r 
</code></pre>
<p>显示所用的远程分支, 然后用：</p>
<pre><code>git checkout branchname
</code></pre>
<p>提取出需要的分支，然后用；</p>
<pre><code>git branch
</code></pre>
<p>就可以看到上面提出来的分支的名字了</p>
</li>
<li><p>显示远程仓库的网址和名字：</p>
<pre><code>git remote -v
</code></pre>
<p>修改远程仓库，发现远程仓库的地址错了，导致一直下载不下来代码，需要添加正确<br>的地址：</p>
<pre><code>git remote add hilt ssh://git.linaro.org/kernel.git
</code></pre>
<p>其中hilt是这个远程仓库的名字</p>
<pre><code>git remote rm origin
</code></pre>
<p>其中orgin是原来错误远程仓库的名字，之后把远程仓库的名字从hilt改成origin：</p>
<pre><code>git remote rename hilt origin
</code></pre>
</li>
<li><p>在开发的时候会出现很多中间版本，这些版本做的改动对别人是无意义的, 比如有<br>version_1—&gt;version_2—&gt;version_3, 怎么把这三个版本合并成一个版本(一次提交):</p>
<pre><code>git rebase -i HEAD~3
</code></pre>
<p>其中3表示把最近的3次提交合并成一次提交</p>
<p>如果commit的log message写的不好，也可以用：</p>
<pre><code>git commit --amend
</code></pre>
<p>重写commit的log message</p>
</li>
<li><p>代码改好了，需要制作patch，可以使用：</p>
<pre><code>git format-patch -s -1
</code></pre>
<p>其中s表示patch中会加上签名项, 1表示对最近一次提交生成patch. 如果把1变成2，那么<br>会生成两个patch, 以version_1—&gt;version_2—&gt;version_3为例，这两个patch是<br>version_3对version_2的patch、version_2对version_1的patch</p>
</li>
</ol>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>软件开发</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>how to compile perf tool statically</title>
    <url>/how-to-compile-perf-tool-statically/</url>
    <content><![CDATA[<ol>
<li><p>D05 machine + CentOS7.4</p>
</li>
<li><p>kernel code</p>
</li>
<li><p>cd kernel/tools<br>make LDFLAGS=-static perf</p>
</li>
<li><p>you can find perf statically in kernel/tools/perf</p>
</li>
</ol>
<p>Note:</p>
<ol>
<li>you may need to: yum install glibc-static</li>
<li>you need install slang to enable perf report -tui. In ubuntu20.04, you need<br>sudo apt install libslang2-dev. you can use ldd perf to see if perf already<br>links slang</li>
<li>you may need to install a lot other libs to compile a full function perf.<br>(libelf-dev)</li>
</ol>
]]></content>
      <tags>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>how to test RoCE driver</title>
    <url>/how-to-test-RoCE-driver/</url>
    <content><![CDATA[<h2 id="how-to-test-RoCE-driver"><a href="#how-to-test-RoCE-driver" class="headerlink" title="how to test RoCE driver"></a>how to test RoCE driver</h2><p>-v0.1 2018..27 Sherlock init</p>
<p>This document shares how to do a sanity test for a RoCE driver. If you are a<br>new guy for RoCE, it is for you.</p>
<p>Here is what I had done to test RoCE driver:</p>
<ol>
<li><p>I had a ARM64 based machine installed a Redhat system in it.</p>
</li>
<li><p>download perftest, this is a open source RoCE test cases:</p>
<p> git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xzZ3VudGgvcGVyZnRlc3QuZ2l0">https://github.com/lsgunth/perftest.git<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>download RoCE’s user space library, rdma-core:</p>
<p> git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xpbnV4LXJkbWEvcmRtYS1jb3JlLmdpdA==">https://github.com/linux-rdma/rdma-core.git<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>download kernel which includes the RoCE driver to test.</p>
</li>
<li><p>build perftest tools followed by the README.</p>
<p>Here in a Redhat(CentOS) system, when you did ./configure, it maybe show<br>that you lack ibverbs head files. you can install rdma-core-devel to solve it:</p>
<p> yum install rdma-core-devel.aarch64</p>
<p>if it is successful, you will get ib_send_bw …. tools in perftest.</p>
</li>
<li><p>build rdma-core followed by its README. In a Redhat(CentOS) system, maybe you<br>need:</p>
<p> yum install libnl-devel.aarch64</p>
<p>if it is successful, you will get user space library under rdma-core/build/lib</p>
</li>
<li><p>Copy the libraries under rdma-core/build/lib/* to /lib64 in your system.<br>(I also did the test in ubuntu, you should copy the libraries to /lib)</p>
</li>
<li><p>build your kernel together with your RoCE drivers.<br>In my case, RoCE driver will be built into some kernel modules.</p>
<p>install modules: make modules_install<br>install kernel image: make install<br>(before build kernel, you can add a localversion file to help to tell your kernel)</p>
</li>
<li><p>reboot system, then boot up using your built kernel.</p>
</li>
<li><p>in my case, RoCE kernel can not be loaded automatically, so I need to do:</p>
<p>modprobe hns-roce-hw-v2</p>
</li>
<li><p>then you can find your roce device in /sys/class/infiniband. Or use the tools<br>in rdma-core/build/bin: ibv_devices, ibv_devinfo to see your RoCE devices.</p>
</li>
<li><p>up your RoCE’s networking interface, and keep its link status as: yes<br>you can find RoCE’s networking interface by searching, e.g.:</p>
<p>/sys/class/infiniband/hns_0/ports/1/gid_attrs/ndevs</p>
</li>
<li><p>in a redhat system, you need do:</p>
<p>echo “driver hns” &gt; /etc/libibverbs.d/hns.driver</p>
<p>but in a ubuntu system, you need do:</p>
<p>echo “driver hns” &gt; /usr/local/etc/libibverbs.d/hns.driver</p>
</li>
<li><p>finally you can use ib_send_bw, ib_read_bw, ib_write_bw… to do the sanity<br>test. Every time you do the test, you should create a RoCE server and create<br>a RoCE client to access and send date to the server.</p>
<p>you can do as:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost perftest]# ./ib_write_bw -n 5 -d hns_0 &amp;</span><br><span class="line">[1] 10352</span><br><span class="line">[root@localhost perftest]# </span><br><span class="line">************************************</span><br><span class="line">* Waiting for client to connect... *</span><br><span class="line">************************************</span><br><span class="line"></span><br><span class="line">[root@localhost perftest]# ./ib_write_bw -n 5 -d hns_0 192.168.2.198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">                    RDMA_Write BW Test</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line">                    RDMA_Write BW Test</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> TX depth        : 5</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> Max inline data : 0[B]</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line"> Max inline data : 0[B]</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line"> local address: LID 0000 QPN 0x0022 PSN 0x16566e RKey 0x000300 VAddr 0x00ffff93c9b000</span><br><span class="line"> local address: LID 0000 QPN 0x0023 PSN 0x1fd9e RKey 0x000400 VAddr 0x00ffffbe24d000</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> remote address: LID 0000 QPN 0x0023 PSN 0x1fd9e RKey 0x000400 VAddr 0x00ffffbe24d000</span><br><span class="line"> remote address: LID 0000 QPN 0x0022 PSN 0x16566e RKey 0x000300 VAddr 0x00ffff93c9b000</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> 65536      5                8169.77            8161.23		   0.130580</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> 65536      5                8169.77            8161.23		   0.130580</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">[1]+  Done                    ./ib_write_bw -n 5 -d hns_0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@localhost perftest]# ./ib_read_bw -n 5 -d hns_0 &amp;</span><br><span class="line">[1] 10339</span><br><span class="line">[root@localhost perftest]# ---------------------------------------------------------------------------------------</span><br><span class="line">Device not recognized to implement inline feature. Disabling it</span><br><span class="line"></span><br><span class="line">************************************</span><br><span class="line">* Waiting for client to connect... *</span><br><span class="line">************************************</span><br><span class="line"></span><br><span class="line">[root@localhost perftest]# ./ib_read_bw -n 5 -d hns_0 192.168.2.198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">Device not recognized to implement inline feature. Disabling it</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">                    RDMA_Read BW Test</span><br><span class="line">                    RDMA_Read BW Test</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> TX depth        : 5</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> Outstand reads  : 128</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line"> Outstand reads  : 128</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line"> local address: LID 0000 QPN 0x0020 PSN 0xe6d890 OUT 0x80 RKey 0x000300 VAddr 0x00ffffb0250000</span><br><span class="line"> local address: LID 0000 QPN 0x0021 PSN 0x87bda4 OUT 0x80 RKey 0x000400 VAddr 0x00ffff9dbe8000</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> remote address: LID 0000 QPN 0x0020 PSN 0xe6d890 OUT 0x80 RKey 0x000300 VAddr 0x00ffffb0250000</span><br><span class="line"> remote address: LID 0000 QPN 0x0021 PSN 0x87bda4 OUT 0x80 RKey 0x000400 VAddr 0x00ffff9dbe8000</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> 65536      5                8002.35            8002.35		   0.128038</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> 65536      5                8002.35            8002.35		   0.128038</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">[1]+  Done                    ./ib_read_bw -n 5 -d hns_0</span><br><span class="line"></span><br><span class="line">[root@localhost perftest]# ./ib_send_bw -n 5 -d hns_0 &amp;</span><br><span class="line">[1] 10300</span><br><span class="line">[root@localhost perftest]# </span><br><span class="line">************************************</span><br><span class="line">* Waiting for client to connect... *</span><br><span class="line">************************************</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@localhost perftest]# ./ib_send_bw -n 5 -d hns_0 192.168.2.198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">                    Send BW Test</span><br><span class="line">                    Send BW Test</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> RX depth        : 6</span><br><span class="line"> TX depth        : 5</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> Max inline data : 0[B]</span><br><span class="line"> Max inline data : 0[B]</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line"> local address: LID 0000 QPN 0x001e PSN 0xab3cfa</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> local address: LID 0000 QPN 0x001f PSN 0xdefae7</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> remote address: LID 0000 QPN 0x001f PSN 0xdefae7</span><br><span class="line"> remote address: LID 0000 QPN 0x001e PSN 0xab3cfa</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> 65536      5                8054.00            8051.92		   0.128831</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> 65536      5                0.00               15340.76		   0.245452</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">[1]+  Done                    ./ib_send_bw -n 5 -d hns_0</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <tags>
        <tag>RDMA</tag>
      </tags>
  </entry>
  <entry>
    <title>linux O_CLOEXEC标志位笔记</title>
    <url>/linux-O-CLOEXEC%E6%A0%87%E5%BF%97%E4%BD%8D%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>在linux系统中，open一个文件可以带上O_CLOEXEC标志位，这个表示位和用fcntl设置的<br>FD_CLOEXEC有同样的作用，都是在fork的子进程中用exec系列系统调用加载新的可执行<br>程序之前，关闭子进程中fork得到的fd。</p>
<p>fork之后，子进程得到父进程的完整拷贝，对于父进程已经open的文件，子进程也可以得<br>到一样的fd。内核里，子进程只是把fd对应的file指针指向父进程fd对应的struct file，<br>并且把file的引用加1。对于设置了这个标志位的fd来说，内核执行exec时，在加载二进制<br>可执行文件之前会调用filp_close关闭子进程的fd。可以看到filp_close的意图是关闭<br>子进程里的fd，但是因为父进程还持有fd的引用计数，所以这个关闭的动作只会执行诸如<br>文件fops对应的flush回调函数，并没有真正调用到fops的release回调函数把struct file<br>release掉。</p>
<p>这里有一个简单的测试程序：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL3RyZWUvbWFzdGVyL2ZvcmtfZXhlYw==">https://github.com/wangzhou/tests/tree/master/fork_exec<i class="fa fa-external-link-alt"></i></span><br>从test_log里可以看到，子进程执行execlp会触发设备驱动里的flush函数。</p>
<p>可以看到，如果这里的flush函数有对硬件的操作将有可能出错。比如这个设备正被<br>open在父进程里使用，子进程里调用到的flush函数也会操作到相同的struct file，可能<br>破坏这个正在打开的设备里的资源。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title>how to test crypto accelerator engine</title>
    <url>/how-to-test-crypto-accelerator-engine/</url>
    <content><![CDATA[<p>简单的讲，就是在你写的crypto驱动注册到crypto子系统的时候，注册函数会调用crypto_chain<br>注册链表上的回调函数。</p>
<p>如果使能了crypto/algboss.c这个驱动，这个驱动初始化的时候会向crypto_chain里注册相关<br>的测试代码, 这些测试代码会启动一个内核线程去测试刚刚注册的crypto算法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cryptomgr_schedule_test</span><br><span class="line">	--&gt; kthread_run(cryptomgr_test, param, &quot;cryptomgr_test&quot;)</span><br><span class="line"></span><br><span class="line">static int cryptomgr_test(void *data)</span><br><span class="line">&#123;</span><br><span class="line">	struct crypto_test_param *param = data;</span><br><span class="line">	u32 type = param-&gt;type;</span><br><span class="line">	int err = 0;</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_CRYPTO_MANAGER_DISABLE_TESTS</span><br><span class="line">	goto skiptest;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">	if (type &amp; CRYPTO_ALG_TESTED)</span><br><span class="line">		goto skiptest;</span><br><span class="line"></span><br><span class="line">	err = alg_test(param-&gt;driver, param-&gt;alg, type, CRYPTO_ALG_TESTED);</span><br><span class="line"></span><br><span class="line">skiptest:</span><br><span class="line">	crypto_alg_tested(param-&gt;driver, err);</span><br><span class="line"></span><br><span class="line">	kfree(param);</span><br><span class="line">	module_put_and_exit(0);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到要想要执行到测试程序alg_test, 需要把CONFIG_CRYPTO_MANAGER_DISABLE_TESTS<br>设置成n.</p>
<p>alg_test(crypto/testmgr.c)会根据算法的名字调用事先准备好的测试代码, 可以搜索<br>struct alg_test_desc alg_test_descs<a href="crypto/testmgr.c"></a>这个数组找到所有事先放好<br>的测试代码。</p>
<p>比如，我们可以找到deflate压缩解压缩相关的测试代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">	.alg = &quot;deflate&quot;,</span><br><span class="line">	.test = alg_test_comp,</span><br><span class="line">	.fips_allowed = 1,</span><br><span class="line">	.suite = &#123;</span><br><span class="line">		.comp = &#123;</span><br><span class="line">			.comp = __VECS(deflate_comp_tv_template),</span><br><span class="line">			.decomp = __VECS(deflate_decomp_tv_template)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>可以看到，alg_test_comp函数会调用crypto子系统提供的API做相应算法的测试，比如，<br>这里会用crypto_alloc_comp, crypto_comp_compress, crypto_free_comp(comp)<br>等crypto API对新注册的支持deflate算法的驱动做测试(假设你写的crypto算法的驱动支持<br>deflate算法), 当然测试使用的压缩解压缩的数据就是上面deflate_comp_tv_template,<br>deflate_decomp_tv_template中的数据。</p>
<p>但是，这里对于压缩解压缩算法似乎存在一个问题, 那就硬件加速器压缩解压缩得到的<br>数据和软件压缩解压缩可能是不一样的。也就是说，你为硬件压缩解压缩写一个crypto的<br>驱动，注册在crypto系统上，然后上面crypto自带的测试程序测试，由于硬件压缩，软件<br>压缩出来的结果是不一样的，可能你的硬件压缩的结果是对的，但是上面的测试是失败的。</p>
<p>对于压缩解压这个问题，可以通过用软件把硬件压缩完的数据解压一下，然后对比是否和<br>原来的压缩前的数据一致。具体实现的话，可以直接用crypto API申请一个基于软件的压缩<br>解压算法进行验证。也可以用内核lib/zlib_deflate, lib/zlib_inflate库来进行软件的<br>压缩和解压缩。对于第一种验证方法，我们可以修改内核的alg_test_comp函数来实现，<br>这个修改可以考虑upstream到内核主线。对于第二种验证方法，我们可以在硬件的crypto<br>驱动中直接调用zlib_deflate/zlib_inflate相关函数来做，基本的使用方法是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct z_stream_s stream;</span><br><span class="line">char decomp_result[COMP_BUF_SIZE];</span><br><span class="line"></span><br><span class="line">stream.workspace  = kmalloc(zlib_inflate_workspacesize(), GFP_KERNEL);</span><br><span class="line"></span><br><span class="line">ret = zlib_inflateInit2(&amp;stream, MAX_WINDOW_BIT);</span><br><span class="line"></span><br><span class="line">stream.next_in = dst;</span><br><span class="line">stream.avail_in = dlen;</span><br><span class="line">stream.total_in = 0;</span><br><span class="line">stream.next_out = decomp_result;</span><br><span class="line">stream.avail_out = COMP_BUF_SIZE;</span><br><span class="line">stream.total_out = 0;</span><br><span class="line"></span><br><span class="line">ret = zlib_inflate(&amp;stream, Z_FINISH);</span><br><span class="line"></span><br><span class="line">memcmp(decomp_result, src, slen);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件测试</tag>
        <tag>Linux内核</tag>
        <tag>crypto</tag>
      </tags>
  </entry>
  <entry>
    <title>mtrace使用笔记</title>
    <url>/mtrace%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="使用过程："><a href="#使用过程：" class="headerlink" title="使用过程："></a>使用过程：</h1><ol>
<li>程序中需要包含头文件mchech.h, 在程序开始处调用mtrace()</li>
<li>设定环境变量 export MALLOC_TRACE=”mtrace.out”</li>
<li>编译运行程序, 会生成mtrace.out文件</li>
<li>mtrace a.out mtrace.out得到内存泄露信息</li>
</ol>
<h2 id="Memory-not-freed"><a href="#Memory-not-freed" class="headerlink" title="Memory not freed:"></a>Memory not freed:</h2><p>Address     Size     Caller<br>0x0000000001650490     0x28  at /vm/<em><strong>/src/mtrace_test/mtrace_test.c:11<br>0x00000000016504f0     0x28  at /vm/</strong></em>/src/mtrace_test/mtrace_test.c:13<br>0x0000000001650550      0xa  at /vm/***/src/mtrace_test/mtrace_test.c:15</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* mtrace_test.c */</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;mcheck.h&gt;</span><br><span class="line">#</span><br><span class="line">int main(void)</span><br><span class="line">&#123;</span><br><span class="line">	mtrace(); </span><br><span class="line">	</span><br><span class="line">	int i;</span><br><span class="line">	int *p_0 = (int*)malloc(sizeof(int)*10);</span><br><span class="line">	int *p_1 = (int*)malloc(sizeof(int)*10);</span><br><span class="line">	int *p_2 = (int*)malloc(sizeof(int)*10);</span><br><span class="line">	int *p_3 = (int*)malloc(sizeof(int)*10);</span><br><span class="line">	int *p_4 = (int*)malloc(sizeof(int)*10);</span><br><span class="line">	char *p_char = (char*)malloc(sizeof(char)*10);</span><br><span class="line">	</span><br><span class="line">	for (i = 0; i &lt; 10; i++) &#123;</span><br><span class="line">		p_0[i] = i;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	for (i = 0; i &lt; 10; i++) &#123;</span><br><span class="line">		p_char[i] = &#x27;w&#x27;;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	free(p_0);</span><br><span class="line">	/* 制造人为的内存泄漏 */</span><br><span class="line">	//free(p_1)</span><br><span class="line">	free(p_2);</span><br><span class="line">	//free(p_3)</span><br><span class="line">	free(p_4);</span><br><span class="line">	//free(p_char);</span><br><span class="line">	</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><p>[1] <span class="exturl" data-url="aHR0cDovL3d3dy5nbnUub3JnL3NvZnR3YXJlL2xpYmMvbWFudWFsL2h0bWxfbm9kZS9BbGxvY2F0aW9uLURlYnVnZ2luZy5odG1s">http://www.gnu.org/software/libc/manual/html_node/Allocation-Debugging.html<i class="fa fa-external-link-alt"></i></span><br>[2] <span class="exturl" data-url="aHR0cDovL3d3dy5nbnUub3JnL3NvZnR3YXJlL2xpYmMvbWFudWFsL2h0bWxfbm9kZS9Ib29rcy1mb3ItTWFsbG9jLmh0bWw=">http://www.gnu.org/software/libc/manual/html_node/Hooks-for-Malloc.html<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>软件调试</tag>
      </tags>
  </entry>
  <entry>
    <title>opensbi设备驱动模型</title>
    <url>/opensbi%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> 分析代码之前可以先想下，如果自己要写一个设备驱动的框架出来，大概要考虑的地方。</p>
<p> 我们首先看可能的需求，opensbi作为一个支持多平台多设备的BIOS，它需要支持多个不同<br> 的硬件平台，每个平台上会有不同的硬件设备，不同的硬件设备需要与之匹配的驱动，这里<br> 可以看出，首先要有描述设备的软件结构和描述驱动的软件结构。设备需要有一定的发现<br> 机制，这样才能和硬件解耦合，一个好的办法是软件检测dts中定义的设备，然后为之创建<br> 如上的设备软件结构。设备和驱动之间需要有一定的匹配机制，这样，驱动可以和相关设备<br> 建立关联，进而驱动设备。对于一类设备驱动，需要抽象出一个对外的公共接口层，用户<br> 统一通过这个公共接口层访问设备，这样可以屏蔽底层硬件的差异。</p>
<p> 如下是一个大概的示意图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-----+      +---------------+</span><br><span class="line">| DTS |&lt;-----| device struct | &lt;---+</span><br><span class="line">+-----+      +---------------+     |</span><br><span class="line">                     ^             |</span><br><span class="line">                     | match       | </span><br><span class="line">                     v             |  </span><br><span class="line">             +---------------+     |</span><br><span class="line">             | driver struct |-----+</span><br><span class="line">             +---------------+</span><br><span class="line">                     |</span><br><span class="line">             registe |</span><br><span class="line">                     v</span><br><span class="line">           +-------------------+</span><br><span class="line">           | common interfaces |</span><br><span class="line">           +-------------------+</span><br></pre></td></tr></table></figure>

<h2 id="看一个例子"><a href="#看一个例子" class="headerlink" title="看一个例子"></a>看一个例子</h2><p> opensbi里用platform这个全局结构体收集特定平台上所有设备的初始化入口和特定平台的<br> 配置参数，对于一个编译好的二进制，因为平台是确定的，这个platform结构也是唯一的。</p>
<p> 不同类的设备要在platform里有不同的初始化入口函数，这样在opensbi的主流程里，调用<br> 对应的初始化函数就可以初始化对应的设备。</p>
<p> 我们具体看下串口这类设备是怎么搞的，我们具体看generic这个平台的实现。fdt_serial_init<br> 是generic platform里的串口类设备初始化函数，这个函数扫描dts里stdout-path域段，<br> 找见dts中的串口描述节点，然后查看opensbi fdt_serial_drivers这个表格，按照dts<br> compatible域段的信息和驱动做匹配，所有串口相关的驱动要静态的放到fdt_serial_drivers<br> 这个表里，找见相匹配的驱动后，就调用驱动的init函数初始化串口设备，驱动的初始化<br> 函数同时把具体设备注册到串口的公共接口里：sbi_console_set_device。</p>
<p> 这里使用一个console_dev的全局变量表示当前的显示设备，显示公共接口就是直接调用<br> console_dev里的输入输出回调函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sbi_getc</span><br><span class="line">  +-&gt; console_dev-&gt;console_getc</span><br><span class="line"></span><br><span class="line">sbi_putc</span><br><span class="line">  +-&gt; console_dev-&gt;console_putc</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>opensbi</tag>
      </tags>
  </entry>
  <entry>
    <title>linux kernel vfio mdev arch</title>
    <url>/linux-kernel-vfio-mdev-arch/</url>
    <content><![CDATA[<p>Linux kernel adds a vfio mdev system in /driver/vfio/mdev. This sub-system can<br>create virtual devices and export their DMA outside to user space.</p>
<p>This subsystem creates one new bus called: struct bus_type mdev_bus_type.<br>And it uses a gobal list to store parent device, here we call orginal device as<br>parent device and virtual device created from orginal device as mdev device,<br>e.g. a NIC is a parent device and one of NIC’s queue can be a mdev device to<br>fullfil a seperated package sending/receiving work.</p>
<p>vfio mdev exports a API:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int mdev_register_device(struct device *dev, const struct mdev_parent_ops *ops)</span><br></pre></td></tr></table></figure>
<p>to let a device register to itself.</p>
<p>And vfio mdev exports a set of sysfs files to help user to create a mdev device,<br>here we call a mdev device as child device.</p>
<p>A child device will be added into mdev_bus_type, and a driver which belongs to<br>the mdev_bus_type in vfio_mdev.c will bind with related child device.</p>
<p>The probe of this vfio_mdev is very simple, it just call:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio_add_group_dev(dev, &amp;vfio_mdev_dev_ops, mdev)</span><br></pre></td></tr></table></figure>
<p>It just create a vfio_group and related vfio_device. So the vfio mdev will reuse<br>all the concept of vfio system.</p>
<p>The probe of mdev_bus_type will help to create a new iommu_group for child<br>device. Above vfio_add_group_dev creates iommu_group’s vfio_group and add this<br>vfio_group to vfio’s group list.</p>
<p>If user want to use above vfio_group together with a vfio container, it should<br>attach this vfio_group to related vfio container. When doing this attach<br>operation. vfio system will replay all mappings in vfio container to this<br>vfio group, which means this child’s DMA can see the address space managed by<br>vfio container.</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>vfio</tag>
      </tags>
  </entry>
  <entry>
    <title>pci设备直通qemu相关的RAS处理</title>
    <url>/pci%E8%AE%BE%E5%A4%87%E7%9B%B4%E9%80%9Aqemu%E7%9B%B8%E5%85%B3%E7%9A%84RAS%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p>v5.0-rc6里pci aer的处理逻辑是，当pf有aer时会扫描pf父总线下的所有function, 并<br>调用对应function的pci_driver-&gt;err_handler-&gt;err_detected/slot_reset函数<br>(如果有这些回调，这里不展开描述)。</p>
<p>基于上面的逻辑，如果正好有vf通过vfio驱动直通到qemu, 那么vfio-pci驱动里的<br>pci_driver-&gt;err_handler-&gt;err_detected(具体函数是: vfio_pci_aer_err_detected)<br>将被调用到。vfio中的err_detected函数向qemu发一个eventfd消息。qemu收到该消息的<br>时候会终止qemu进程(abort)</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>PCIe</tag>
        <tag>RAS</tag>
      </tags>
  </entry>
  <entry>
    <title>perf flame graph笔记</title>
    <url>/perf-flame-graph%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li><p>git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2JyZW5kYW5ncmVnZy9GbGFtZUdyYXBo">https://github.com/brendangregg/FlameGraph<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>perf record -a -g -F 100<br>这里-g是打点的时候函数调用都算，比如a函数调用了b，b里打的点也同时算在a里。<br>-F是1秒中的打点次数，我们可以提高这个值来提高采样次数，比如这里的系统如果是<br>16核，那么1s的采样数量就是16 × 100。</p>
</li>
<li><p>perf script &gt; out.perf</p>
</li>
<li><p>cp out.perf /path_to/FlameGraph/</p>
</li>
<li><p>/path_to/FlameGraph/stackcollapse-perf.pl out.perf &gt; out.folded</p>
</li>
<li><p>/path_to/FlameGraph/flamegraph.pl out.folded &gt; out.svg</p>
</li>
<li><p>然后可以在浏览器里打开out.svg</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>pstack使用笔记</title>
    <url>/pstack%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>使用方法：pstack pid, 就是说要先把程序跑起来，查见进程号，再使用pstack pid</p>
<ol>
<li><p>在ubunbu系统上，使用sudo apt-get install pstack可以直接安装pstack，注意这里<br>   安装上的是二进制的程序。之前对这样安装上的pstack作了测试，无法显示函数的调用<br>   关系。根据man pstack的说明，现在的pstack只支持32bit ELF binaries</p>
</li>
<li><p>网上的一些文章指出，pstack只是一个脚本程序，在相关网站上下载了pstack.sh脚本<br>   测试。使用时总是提示出错，但是表现看不出来错误，全部删去，就留下前几行，还是<br>   显示有错，新建一个脚本文件，照抄那几行过来（这时两个文件看起来一样），但是用<br>   diff ***.sh ***.sh测试一下，竟然不一样。用ghex查看二进制的文件，看出是字符行<br>   尾的时候的编码不一样。想到下载的脚本可以是在windows下的脚本，下载安装 dos2unix<br>编码转换工具，然后dos2unix pstack.sh, 工具可以使用。</p>
<p>pstack.sh脚本简单分析:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* pstack.sh */</span><br><span class="line">#! /bin/sh</span><br><span class="line"># 输入参数以及pid是否存在的判断</span><br><span class="line">if test $# -ne 1; then</span><br><span class="line">    echo &quot;Usage: `basename $0 .sh` &lt;process-id&gt;&quot; 1&gt;&amp;2</span><br><span class="line">    exit 1 </span><br><span class="line">fi</span><br><span class="line">if test ! -r /proc/$1; then</span><br><span class="line">    echo &quot;Process $1 not found.&quot; 1&gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># GDB doesn&#x27;t allow &quot;thread apply all bt&quot; when the process isn&#x27;t</span><br><span class="line"># threaded; need to peek at the process to determine if that or the</span><br><span class="line"># simpler &quot;bt&quot; should be used.</span><br><span class="line"># 先设置参数bt, 要是多线程的情况，那么设置为：thread apply all bt</span><br><span class="line">backtrace=&quot;bt&quot;</span><br><span class="line">if test -d /proc/$1/task ; then</span><br><span class="line">    # Newer kernel; has a task/ directory.</span><br><span class="line">    if test `/bin/ls /proc/$1/task | /usr/bin/wc -l` -gt 1 2&gt;/dev/null ; then</span><br><span class="line">        backtrace=&quot;thread apply all bt&quot;</span><br><span class="line">    fi</span><br><span class="line">elif test -f /proc/$1/maps ; then</span><br><span class="line">    # Older kernel; go by it loading libpthread.</span><br><span class="line">    if /bin/grep -e libpthread /proc/$1/maps &gt; /dev/null 2&gt;&amp;1 ; then</span><br><span class="line">        backtrace=&quot;thread apply all bt&quot;</span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line"> </span><br><span class="line">GDB=$&#123;GDB:-/usr/bin/gdb&#125;</span><br><span class="line"></span><br><span class="line"># echo $GDB -&gt; /usr/bin/gdb</span><br><span class="line"></span><br><span class="line"># -nx: Do not execute commands from any `.gdbinit&#x27; initialization files.  Normal</span><br><span class="line">   ly, the commands in these files are executed after all the command options  </span><br><span class="line">   and  argu‐ments have been processed.</span><br><span class="line"># --quiet: Do not print the introductory and copyright messages.  These message</span><br><span class="line">   s are also suppressed in batch mode</span><br><span class="line"># --batch: ...</span><br><span class="line"># --readnever: ...</span><br><span class="line">if $GDB -nx --quiet --batch --readnever &gt; /dev/null 2&gt;&amp;1; then</span><br><span class="line">    readnever=--readnever</span><br><span class="line">else</span><br><span class="line">    readnever=</span><br><span class="line">fi</span><br><span class="line"># 单步运行/usr/bin/gdb -nx --quiet --batch --readnever时,--readnever会出错</span><br><span class="line"> </span><br><span class="line"># Run GDB, strip out unwanted noise.</span><br><span class="line"># 原来用/proc/$1/exe找到pid对应的命令行，但是发现下面命令中的是/proc/$1/exe</span><br><span class="line"># 改成下面的&#x27;echo /proc/$1/exe&#x27;就可以了</span><br><span class="line">$GDB --quiet $readnever -nx &#x27;echo /proc/$1/exe&#x27; $1 &lt;&lt;EOF 2&gt;&amp;1 |</span><br><span class="line">$backtrace </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li>
</ol>
<p>这里，运行pstack [pid]一次会显示当时的函数调用，不能显示指定位置的堆栈情况。<br>可以先运行gdb –quiet -nx /proc/command/exe command, 待gdb起来后，在想要的地<br>方设立断点，程序停住之后，使用bt，显示出当时的堆栈函数调用情况<br> <br>整理显示输出格式<br>/bin/sed -n <br>    -e ‘s/^(gdb) //‘ <br>    -e ‘/^#/p’ <br>    -e ‘/^Thread/p’</p>
<p>附：测试程序和堆栈情况（单线程和多线程）</p>
<p>单线程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int func_called(int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">	int c;</span><br><span class="line">	c = a + b;</span><br><span class="line">	return c;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void func_calling(void)</span><br><span class="line">&#123;</span><br><span class="line">	int a = 1, b = 2;</span><br><span class="line">	int r;</span><br><span class="line">	r = func_called(a, b);</span><br><span class="line">	printf(&quot;r = %d\n&quot;, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	while(1) &#123;</span><br><span class="line">		func_calling();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">29788 pts/5    00:00:34 test2</span><br><span class="line">29819 pts/4    00:00:00 ps</span><br><span class="line">***@A101107831:/vm/***/notes$ ./pstack.sh 29788</span><br><span class="line">[sudo] password for ***: </span><br><span class="line">#0  0x00007f6d7d366910 in __write_nocancel ()</span><br><span class="line">#1  0x00007f6d7d2f9883 in _IO_new_file_write (f=0x7f6d7d639260, </span><br><span class="line">#2  0x00007f6d7d2f974a in new_do_write (fp=0x7f6d7d639260, </span><br><span class="line">#3  0x00007f6d7d2faeb5 in _IO_new_do_write (fp=&lt;optimized out&gt;, </span><br><span class="line">#4  0x00007f6d7d2fa025 in _IO_new_file_xsputn (n=1, data=&lt;optimized out&gt;, </span><br><span class="line">#5  _IO_new_file_xsputn (f=0x7f6d7d639260, data=&lt;optimized out&gt;, n=1)</span><br><span class="line">#6  0x00007f6d7d2ca4a7 in _IO_vfprintf_internal (s=&lt;optimized out&gt;, </span><br><span class="line">#7  0x00007f6d7d2d38d9 in __printf (format=&lt;optimized out&gt;) at printf.c:35</span><br><span class="line">#8  0x000000000040054d in func_calling () at test2.c:15</span><br><span class="line">#9  0x0000000000400563 in main () at test2.c:27</span><br></pre></td></tr></table></figure>
<p>注：使用pc上已经在运行的应用程序调试时，无法显示具体的函数名和相关参数，原因是pc<br>    上的应用程序没有加入调试信息</p>
<p>多线程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># include&lt;stdio.h&gt;</span><br><span class="line"># include&lt;pthread.h&gt;</span><br><span class="line"># include&lt;string.h&gt;</span><br><span class="line"></span><br><span class="line">pthread_t tid1;</span><br><span class="line">pthread_t tid2;</span><br><span class="line"></span><br><span class="line">void* thread1(void* arg)</span><br><span class="line">&#123;</span><br><span class="line">	while (1) &#123;</span><br><span class="line">		printf(&quot;thread_id = %d\n&quot;, (int)tid1);</span><br><span class="line">		sleep(1);</span><br><span class="line">	&#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void* thread2(void* arg)</span><br><span class="line">&#123;</span><br><span class="line">	while (1) &#123;</span><br><span class="line">		printf(&quot;thread_id = %d\n&quot;, (int)tid2);</span><br><span class="line">		sleep(1);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	int err;</span><br><span class="line">	</span><br><span class="line">	err = pthread_create(&amp;tid1, NULL, thread1, NULL);</span><br><span class="line">	if (err != 0)&#123;</span><br><span class="line">		fprintf(stderr, &quot;can&#x27;t create thread: %s\n&quot;, strerror(err));</span><br><span class="line">	&#125;</span><br><span class="line">	err = pthread_create(&amp;tid2, NULL, thread2, NULL);</span><br><span class="line">	if (err != 0)&#123;</span><br><span class="line">		fprintf(stderr, &quot;can&#x27;t create thread: %s\n&quot;, strerror(err));</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	while(1); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>测试输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*** 30280 22661 30280 99    3 21:51 pts/5    00:00:12 ./multi_threads</span><br><span class="line">*** 30280 22661 30281  0    3 21:51 pts/5    00:00:00 ./multi_threads</span><br><span class="line">*** 30280 22661 30282  0    3 21:51 pts/5    00:00:00 ./multi_threads</span><br><span class="line">*** 30284  3758 30284  0    1 21:51 pts/4    00:00:00 ps -eLf</span><br><span class="line">***@A101107831:/vm/***/notes$ ./pstack.sh 30280</span><br><span class="line">[sudo] password for ***: </span><br><span class="line">Thread 3 (Thread 0x7f2a351ae700 (LWP 30281)):</span><br><span class="line">#0  0x00007f2a3526e84d in nanosleep () at ../sysdeps/unix/syscall-template.S:82</span><br><span class="line">#1  0x00007f2a3526e6ec in __sleep (seconds=0)</span><br><span class="line">#2  0x00000000004006fc in thread1 (arg=0x0) at multi_threads.c:14</span><br><span class="line">#3  0x00007f2a35575e9a in start_thread (arg=0x7f2a351ae700)</span><br><span class="line">#4  0x00007f2a352a2ccd in clone ()</span><br><span class="line">#5  0x0000000000000000 in ?? ()</span><br><span class="line">Thread 2 (Thread 0x7f2a349ad700 (LWP 30282)):</span><br><span class="line">#0  0x00007f2a3526e84d in nanosleep () at ../sysdeps/unix/syscall-template.S:82</span><br><span class="line">#1  0x00007f2a3526e6ec in __sleep (seconds=0)</span><br><span class="line">#2  0x0000000000400736 in thread2 (arg=0x0) at multi_threads.c:22</span><br><span class="line">#3  0x00007f2a35575e9a in start_thread (arg=0x7f2a349ad700)</span><br><span class="line">#4  0x00007f2a352a2ccd in clone ()</span><br><span class="line">#5  0x0000000000000000 in ?? ()</span><br><span class="line">Thread 1 (Thread 0x7f2a35988700 (LWP 30280)):</span><br><span class="line">#0  main () at multi_threads.c:39</span><br></pre></td></tr></table></figure>
<p>注：按照下面gdb multi-thread debug方法，在pc上无法显示有关多线程的信息(被调试进<br>    程使用pc已经在运行的应用程序进程)</p>
<p>gdb多线程debug:<br><span class="exturl" data-url="aHR0cHM6Ly9zb3VyY2V3YXJlLm9yZy9nZGIvb25saW5lZG9jcy9nZGIvVGhyZWFkcy5odG1sI1RocmVhZHM=">https://sourceware.org/gdb/onlinedocs/gdb/Threads.html#Threads<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>软件调试</tag>
      </tags>
  </entry>
  <entry>
    <title>python学习笔记</title>
    <url>/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li>作用区域</li>
</ol>
<hr>
<blockquote>
<pre><code>#!/usr/bin/python

global y
y = 3

def func(x):
   x = 2
   print &quot;x is&quot;, x

def func_1():
   global y # if delete this line, &#39;y&#39; below is a local one
   y = 5
   print &quot;y is&quot;, y

x = 10
func(x)
print &quot;valuce of x is&quot;, x

func_1()
pRint &quot;valuce of y is&quot;, y
</code></pre>
</blockquote>
<p>函数内的x是local的，不改变x=10的值。在函数func_1内指示y是global的，函数内改变<br>y的值，函数外y=3变成y=5。</p>
<ol start="2">
<li>数据存储方式 </li>
</ol>
<hr>
<blockquote>
<pre><code>#!/usr/bin/python

x = 3
y = x

id_x = id(x)
id_y = id(y)
print &quot;x id is&quot;, id_x
print &quot;y id is&quot;, id_y

x = 5
id_x_new = id(x)
id_y_new = id(y)
print &quot;x_new id is&quot;, id_x_new
print &quot;y_new id is&quot;, id_y_new

# will appear error, as x has been deleted
#del(x)
#id_x_del = id(x)
#print &quot;x_del id is&quot;, id_x_del

# will not appear error
#del(x)
#id(y)

x_list = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]
y_list = x_list
id_x_list = id(x_list)
id_y_list = id(y_list)
print &quot;x_list id is&quot;, id_x_list
print &quot;y_list id is&quot;, id_y_list

x_list.append(&quot;d_added&quot;)
print &quot;new x_list is&quot;, x_list
id_new_x_list = id(x_list)
print &quot;new x_list id is&quot;, id_new_x_list

y_list.append(&quot;y&quot;)
print &quot;new y_list is&quot;, y_list
id_new_y_list = id(y_list)
print &quot;new y_list id is&quot;, id_new_y_list
</code></pre>
</blockquote>
<p>python中的数据都是类。python中的数据分为不可变变量和可变变量，其中数字，字符串<br>是不可变变量，其他的是可变变量。id(x)显示的是x变量的存储’地址’，根据id()可以了解<br>可变变量和不可变变量的性质。x的值不一样，id(x)的结果是不一样的，x是一个数字，是<br>不可变的，所以为x赋新值本质上是重新创建了一个变量，x_list是一个列表，是可变的，<br>所以改变x_list的值，就是改变它本身的值。y = x并没有新建了一个变量，而是为x变量<br>增加了一个叫y的索引，本质上是一个存储结构，所以id(x) = id(y); 所以改变x_list,<br>y_list也跟着变了。</p>
<ol start="3">
<li>典型数据结构</li>
</ol>
<hr>
<ul>
<li>列表</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

global shoplist
shoplist = [&#39;apple&#39;, &#39;mango&#39;, &#39;carrot&#39;, &#39;banana&#39;]

def print_shoplist():
   print shoplist


lenth = shoplist.__len__()
print &quot;len of shoplist is&quot;, lenth

shoplist.sort()
print_shoplist()

shoplist.append(&#39;pear&#39;)
print_shoplist()

shoplist.__delitem__(0)
print_shoplist()

print shoplist[0]
print shoplist[-1]

# use help() to check functions which list offered
#help(list)
</code></pre>
</blockquote>
<p>列表是python的内置数据结构, 存放一组数据，列表里面的值是可以改变的。用help(list)<br>可以查看列表类中所包含的方法。上面列出几个方法：<strong>len__返回列表的长度，sort对<br>列表的数据排序，append在列表的最后加入一个数据，__delitem</strong>(x)删去索引是x的数据.<br>列表中数据的索引和c语言中数组的下标一样，但是可以逆向索引，如shoplist[-1]得到最<br>后一个元素的值。</p>
<ul>
<li>元组</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

zoo = (&#39;wolf&#39;, &#39;elephant&#39;, &#39;penguin&#39;)
new_zoo = (&#39;monkey&#39;, &#39;dolphin&#39;, zoo)

print &quot;len of zoo is&quot;, len(zoo)
print &quot;len of new zoo is&quot;, len(new_zoo)
print &quot;2 of new zoo is&quot;, new_zoo[2]
print &quot;[2][2] of new zoo is&quot;, new_zoo[2][2]

print &quot;%s is %d years old&quot; %(&#39;John&#39;, 12)

def func_return_multi():
   return &#39;John&#39;, 12

print func_return_multi()
</code></pre>
</blockquote>
<p>元组也是python的内置数据结构，但是元组里的值是不可变的，当然如果元组里的元素是<br>一个列表，列表里的值是可以变的。元组的用途有很多，比如上面的格式话输出，当有多<br>个输出值时，它们的真值要用一个元组包含起来; python中的函数可以一次返回多个值，<br>返回的多个值被包含在一个元组中。</p>
<ul>
<li>字符串</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

string = &quot;0123456789&quot;
print &quot;string is&quot;, string

new_string = string[0:5]
print &quot;new_string is&quot;, new_string

print &#39;string 2 to end is&#39;, string[2:]
print &#39;string 1 to -1 is&#39;, string[1:-1]
print &#39;string start to end is&#39;, string[:]
print &#39;string start to end is&#39;, string[::4]

# list and tuple also have those kinds of operations
</code></pre>
</blockquote>
<p>上面是字符串的切片操作，列表和元组也有相同的操作。</p>
<ul>
<li>字典</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

b = &#123;
   &#39;Swaroop&#39;: &#39;swaroopch@byteofpython.info&#39;,
   &#39;Larry&#39;  : &#39;larry@wall.org&#39;,
   &#39;Matsumoto&#39; : &#39;matz@ruby-lang.org&#39;,
   &#39;Spammer&#39;   : &#39;spammer@hotmail.com&#39;
    &#125;

print b[&#39;Larry&#39;]

# add a key-&gt;value in dictionary
b[&#39;Sherlock&#39;] = &quot;Sherlock@gmail.com&quot;
print b

b.pop(&#39;Larry&#39;)
print b

if &#39;Larry&#39; in b:
   print &quot;Larry is in b&quot;
else:
   print &quot;Larry is not in b&quot;
</code></pre>
</blockquote>
<p>字典又一一对应的一组key-&gt;value组成，key需要是不可变变量。</p>
<ul>
<li>集合</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

s = set([1, 2, 3, 4])

print s

# add a key in set
s.add(10)
print s

s.remove(1)
print s

if 2 in s:
   print &quot;2 is in s&quot;
else:
   print &quot;2 is not in s&quot;
</code></pre>
</blockquote>
<p>集合是一组值的集合，用一个列表初始化。集合中的元素也需要是不可变变量。</p>
<ol start="4">
<li>函数</li>
</ol>
<hr>
<ul>
<li>函数基础</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

def test_add(a, b):
   return a + b

print &quot;1 + 2 =&quot;, test_add(1, 2)

add_test = test_add
print &quot;1 + 2 =&quot;, add_test(1, 2)

# need &quot;pass&quot; to fill this &quot;none content function&quot;
def nop():
   pass

# default input
def ball(r, color = &quot;red&quot;, vendor = &quot;A&quot;, llist = [1, 2, 3]):
   llist.append(4)
   print r
   print color
   print vendor
   print llist

print &quot;test_1&quot;
ball(5)

print &quot;test_2&quot;
ball(5, &quot;blue&quot;)

print &quot;test_3&quot;
ball(5, vendor = &quot;B&quot;)

print &quot;test_4&quot;
ball(5, llist = [4, 5, 6])
ball(5)

# variable input
print
print &quot;variable input test&quot;
def sum(*number):
   sum = 0
   for i in number:
       sum = sum + i
   return sum
print &quot;sum is&quot;, sum(1, 2, 3)

num = [1, 2, 3, 4]
print &quot;sum is&quot;, sum(*num)

# key word input
print
print &quot;key word input test&quot;
def key_test(a, b, **c):
   print a
   print b
   print c
print  &quot;key is&quot;, key_test(1, 2)
print
print  &quot;key is&quot;, key_test(1, 2, name = &quot;Sherlock&quot;)
print
dict_test = &#123;&quot;name&quot; : &quot;John&quot;, &quot;age&quot; : 12&#125;
print  &quot;key is&quot;, key_test(1, 2, **dict_test)
</code></pre>
</blockquote>
<p> **函数名是一个指向函数对象的引用，所以可以把一个函数名赋值给一个变量<br> **函数的参数可以是默认参数，可变参数，关键字参数等。<br>   当函数带默认参数时，默认参数需要是不可变参数。不然就像上面代码中显示的那样，<br>   如果函数中改变这个参数的值，以后这个参数的值相应的也都改变了。函数可以带可变<br>   参数, 可变参数可以直接传入函数，也可以先把所有参数组成一个列表，再通过*list<br>   传入。通过**dictionary的方式可以传入一个字典。</p>
<ol start="4">
<li>面向对象</li>
</ol>
<hr>
<blockquote>
<pre><code>#!/usr/bin/python

class ball(object):
   def __init__(self, r, color = &quot;green&quot;, vendor = &quot;A&quot;):
       # init r, but we need not to declare r
       self.r = r
       self.color = color
       # private element
       self.__vendor = vendor
       
   def run(self):
       print &quot;ball is running&quot;
   def show_color(self):
       print &quot;color of ball is&quot;, self.color

# init an instance
ball_test_1 = ball(5)
ball_test_1.show_color()

print
ball_test_2 = ball(5, &quot;red&quot;)
ball_test_2.show_color()

print
ball_test_1.name = &quot;John&quot;
print ball_test_1.name

# test private element
# will appear error when run below command
#print ball_test_1.__vendor

# ok when run below command, but could not write this code, we should write a
# function to show ball&#39;s vendor
print ball_test_1._ball__vendor

# inherit test
print
class football(ball):
   def run(self):
       print &quot;football is running&quot;

football_test_1 = football(10)
football_test_1.run()

# polymorphism test
print
def run(ball_t):
   ball_t.run()
   print &quot;**** ****&quot;
run(ball_test_1)
run(football_test_1)

print
print type(ball_test_1)
print type(football_test_1)
</code></pre>
</blockquote>
<ul>
<li>python中不需要在变量使用前先定义。</li>
<li>私有变量需要加__的前缀</li>
<li>note: raw_input()输入的变量是字符串的，要输入数字需要：int(raw_input())</li>
</ul>
<p> todo: 异常/标准库/I/O/进程/图形/网络/数据库/web…</p>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu PCIe设备增加pasid capability</title>
    <url>/qemu-PCIe%E8%AE%BE%E5%A4%87%E5%A2%9E%E5%8A%A0pasid-capability/</url>
    <content><![CDATA[<p>首先pasid cap是一个PCIe extended的cap，它的位置应该在PCIe配置空间0x100开始(包括)<br>往后的空间上。</p>
<p>在qemu的启动命令里直接加一个PCI设备，qemu把它看作的是一个PCI设备，用lspci看到的<br>配置空间只有0x0~0xff。qemu里对于PCI和PCIe设备是分开对待的，如果要接入一个PCIe设备，<br>需要先在根总线下接一个pcie_port，然后在pcie_port下在接入PCIe设备:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-device pcie-root-port,id=root_port,bus=pcie.0 \</span><br><span class="line">-device ghms_pci,bus=root_port</span><br></pre></td></tr></table></figure>

<p>为了使的接入的设备是一个PCIe设备，设备的TypeInfo中的接口应该定义成PCIe:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static InterfaceInfo ghms_pci_if[] = &#123;</span><br><span class="line">    &#123; INTERFACE_PCIE_DEVICE &#125;,</span><br><span class="line">    &#123; &#125;,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>下面就在设备class的realize函数里增加pasid cap的初始化代码，目前qemu代码(5.1.50)<br>里还没有直接可以调用的函数，我们仿照其他的cap，在hw/pci/pcie.c里给pasid加上一个<br>初始化函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void pcie_pasid_init(PCIDevice *dev, uint16_t offset)</span><br><span class="line">&#123;</span><br><span class="line">    pcie_add_capability(dev, PCI_EXT_CAP_ID_PASID, PCI_PASID_VER, offset,</span><br><span class="line">                        PCI_PASID_SIZEOF);</span><br><span class="line">    dev-&gt;exp.pasid_cap = offset;</span><br><span class="line"></span><br><span class="line">    /* 把pasid max bit配置成了2^4 - 1 = 15 */</span><br><span class="line">    pci_set_word(dev-&gt;config + offset + PCI_PASID_CAP, 4 &lt;&lt; 8 | 0x6);</span><br><span class="line">    pci_set_word(dev-&gt;wmask + offset + PCI_PASID_CTRL, 0);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在设备class的realize函数里调用如上函数加上pasid cap:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define GHMS_PCI_EXPRESS_CAP_OFFSET         0xe0</span><br><span class="line">#define GHMS_PCI_PASID_CAP_OFFSET           0x100</span><br><span class="line">pcie_endpoint_cap_init(pdev, GHMS_PCI_EXPRESS_CAP_OFFSET);</span><br><span class="line">pcie_pasid_init(pdev, GHMS_PCI_PASID_CAP_OFFSET);</span><br></pre></td></tr></table></figure>
<p>pcie_add_capability里会用PCI_EXPRESS_CAP检测是不是PCIe设备，所以要先加上PCI_EXPRESS_CAP，<br>在0x40(0x34是capabilities pointer)到0xff这段地址选一个位置加上，如上选的是0xe0(避开<br>之前的MSI cap)。如上的函数内部会找到cap list尾，然后把新加的cap挂上去，所以我们这里<br>不需要做额外处理。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu insn_start中间码分析</title>
    <url>/qemu-insn-start%E4%B8%AD%E9%97%B4%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>qemu在每一个guest指令的中间码实现之间都要插入一条insn_start的中间码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">translator_loop</span><br><span class="line">  +-&gt; while () &#123;</span><br><span class="line">         ops-&gt;insn_start(db, cpu);                                               </span><br><span class="line"></span><br><span class="line">         /* 循环翻译一个tb里的guest指令 */</span><br><span class="line">         [...]</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<p>插入insn_start的行为靠guest相关的一个回调函数完成，riscv上是riscv_tr_insn_start,<br>不通构架下基本都是插入一条insn_start指令，只不过insn_start指令带的参数个数有可能<br>是不一样的。riscv上带来pc和0两个参数。</p>
<p>qemu在后端翻译的时候解析insn_start并把guest pc等信息保存在TCGContext的gen_insn_end_off<br>和gen_insn_data表项里，其中前者保存的是每条guest指令对应的host指令的地址，后者保<br>存的是每条guest指令对应的guest pc以及其它信息。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_gen_code</span><br><span class="line">  +-&gt; case INDEX_op_insn_start:</span><br><span class="line"></span><br><span class="line">      /* 比如，如果是第一条guest指令，num_insns为0的位置记录的就是0 */</span><br><span class="line">      size_t off = tcg_current_code_size(s);                          </span><br><span class="line">      s-&gt;gen_insn_end_off[num_insns] = off;                           </span><br><span class="line"></span><br><span class="line">      /* guest pc以及可能的其它参数记录在如下表里 */</span><br><span class="line">      s-&gt;gen_insn_data[num_insns][i] = a;                             </span><br></pre></td></tr></table></figure>
<p>qemu在翻译完一个tb后，根据如上保存的信息，在生成代码的尾部生成对应的信息:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tb_gen_code</span><br><span class="line">  +-&gt; tcg_gen_code</span><br><span class="line">  +-&gt; encode_search</span><br></pre></td></tr></table></figure>

<p>如上的准备是为了guest产生异常的时候可以精确的找到guest pc，还是用riscv举例，像ecall<br>这种主动产生的异常，在模拟触发异常时同步更新下guest pc就好:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">trans_ecall</span><br><span class="line">  +-&gt; generate_exception(ctx, RISCV_EXCP_U_ECALL)</span><br><span class="line">    +-&gt; tcg_gen_movi_tl(cpu_pc, ctx-&gt;base.pc_next)</span><br><span class="line">    +-&gt; gen_helper_raise_exception(cpu_env, tcg_constant_i32(excp))</span><br></pre></td></tr></table></figure>
<p>注意qemu在模拟的时候为了性能不会每条guest指令都更新cpu_env的guest cpu，只是在需要<br>的时候才更新。</p>
<p>但是除了ecall指令触发的U_ECALL这种异常，CPU上其它的异常都是被动产生的，比如下面的<br>图上，guest insn0/insn1就可能触发各种被动的异常，比如guest insn1是存储指令，那么<br>在执行它对应的host指令时就可能跑到模拟触发缺页异常，qemu会跳出当前的翻译执行大循环，<br>跳到异常模拟的地方，异常模拟的地方会把异常上下文报告给软件，其中就包括guest insn1<br>的pc。被动异常可能发生在各种guest指令上，所以qemu就记录了如上guest pc和host pc的<br>对照表，被动异常发生时通过host pc反查到guest pc并更新到guest cpu_env上。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pc_0:   guest insn0       IR0       host insn0</span><br><span class="line">                          IR1       host insn1</span><br><span class="line">                                    host insn2</span><br><span class="line"></span><br><span class="line">pc_1:   guest insn1       IR3       host insn3</span><br><span class="line">                          IR4       host insn4</span><br><span class="line">                          IR5       host insn5</span><br><span class="line">                                    host insn6</span><br></pre></td></tr></table></figure>
<p>如下是相关的代码逻辑：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cpu_loop_exit_restore(CPUState *cpu, uintptr_t pc)                         </span><br><span class="line">  +-&gt; cpu_restore_state(cpu, pc, true);                                       </span><br><span class="line">    +-&gt; cpu_restore_state_from_tb</span><br><span class="line">      /* 反查guest pc */</span><br><span class="line">      [...]</span><br><span class="line">      /* 更新guest pc */</span><br><span class="line">      restore_state_to_opc()</span><br><span class="line">  +-&gt; cpu_loop_exit(cpu);                                                         </span><br></pre></td></tr></table></figure>

<p>注意，load/store操纵的guest va不要和load/store指令的地址搞混，load/store指令本身<br>的地址还是要靠如上的方式获得。</p>
<p>qemu里host和target的概念比较绕，这里再次明确下，qemu/tcg/README其实有澄清。对于<br>qemu这个大的范围target指的就是被模拟的CPU构架，host就是qemu进程运行的CPU，但是<br>TCG里，target表示生成代码的CPU构架，就是qemu里的host。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu iommu模拟思路分析</title>
    <url>/qemu-iommu%E6%A8%A1%E6%8B%9F%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>smmuv3的父设备的代码里有：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">smmu_base_realize</span><br><span class="line">  +-&gt; pci_setup_iommu</span><br><span class="line">    +-&gt; bus-&gt;iommu_fn = smmu_find_add_as;</span><br><span class="line">    +-&gt; bus-&gt;iommu_opaque = SMMUState;</span><br></pre></td></tr></table></figure>
<p>把一个获取as的方法放到了PCIBus里，一般的这个方法是有IOMMU驱动定义，IOMMU驱动为<br>IOMMU管理的设备在IOMMU驱动里建立一个数据结构存储相关信息，这个信息里就有设备对应<br>的AS。通过这个方法，使用设备的bdf作为输入，后续设备可有拿到他自己对应的AS。<br>设备随后发dma的时候通过他自己的AS得到smmu里的translate函数然后做翻译。</p>
<p>我们可以看一个具体的intel e1000的虚拟网卡：qemu/hw/net/e1000.c<br>在这个设备注册的时候会调用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* qemu/hw/pci/pci.c */</span><br><span class="line">do_pci_register_device</span><br><span class="line">  +-&gt;pci_init_bus_master</span><br><span class="line">    +-&gt; AddressSpace *dma_as = pci_device_iommu_address_space</span><br><span class="line">          /* 这个iommu_fn就是上面的smmu_find_add_as */</span><br><span class="line">      +-&gt; iommu_bus-&gt;iommu_fn(PCIBus, SMMUState, devfn)</span><br><span class="line">      /* 在哪里把 dma_as存在dev-&gt;bus_master_as里的？*/</span><br></pre></td></tr></table></figure>

<p>设备模拟一个dma读写的实现：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_dma_read/write</span><br><span class="line">  ...</span><br><span class="line">    /* pci_get_address_space_space得到dev-&gt;bus_master_as */</span><br><span class="line">    +-&gt; dma_memory_rw(pci_get_address_space(dev), ...)</span><br></pre></td></tr></table></figure>
<p>可以看到这个函数后面的调用链里会最终调用到iommu里的translate函数，然后对翻译<br>都的地址读写。</p>
<p>对于普通的DMA，如上已经够了。但是，对于带PASID的翻译请求，现在还没有支持的逻辑。<br>我们查看具体的translate函数的入参：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IOMMUTLBEntry (*translate)(IOMMUMemoryRegion *iommu, hwaddr addr,</span><br><span class="line">                           IOMMUAccessFlags flag, int iommu_idx);</span><br></pre></td></tr></table></figure>
<p>我们可以用最后一个参数iommu_idx去传递pasid给具体的翻译函数，这个iommu_idx不能直接<br>使用，在使用前需要通过attrs_to_index这个函数把MemTxAttrs attrs翻译成iommu_idex。<br>所以，想要在当前的smmu驱动里支持pasid，就要在smmu驱动的imrc里实现attrs_to_index<br>的回调函数。如果我们的输入把attrs数值上相等映射成iommu_idx，attrs_to_index可以<br>是如下这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static int smmuv3_attrs_to_index(IOMMUMemoryRegion *iommu, MemTxAttrs attrs)</span><br><span class="line">&#123;</span><br><span class="line">    return attrs.pasid &amp; 0xffff;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应的我们给设备驱动可以提供这样的带PASID的DMA读写函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static inline int pci_dma_rw_pasid(PCIDevice *dev, dma_addr_t addr,</span><br><span class="line">                             void *buf, dma_addr_t len, DMADirection dir,</span><br><span class="line">                             uint32_t pasid)</span><br><span class="line">&#123;</span><br><span class="line">    MemTxAttrs attr = &#123; .pasid = 0xffff &amp; pasid &#125;;</span><br><span class="line"></span><br><span class="line">    return dma_memory_rw_attr(pci_get_address_space(dev), addr, buf, len,</span><br><span class="line">			      dir, attr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>iommu</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu QSIMPLEQ速记</title>
    <url>/qemu-QSIMPLEQ%E9%80%9F%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><p>核心的数据结构是：链表头和链表元素，链表头就两个数据：sqh_first和sqh_last，sqh_first<br>是第一个链表元素的指针，sqh_last是链表最后一个元素里指向下一个元素的指针的地址。</p>
<p>链表元素类型用户自定义，但是里面必须用QSIMPLEQ_ENTRY定义一个链表元素的指针。整体<br>示意图如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">list head              list elm                 list elm</span><br><span class="line">+------------+         +--------------+         +--------------+</span><br><span class="line">| sqh_first -+--------&gt;| ...          |    +---&gt;| ...          |</span><br><span class="line">|            |         |              |    |    |              |</span><br><span class="line">| sqh_last  -+---+     | entry/field: |    |    | entry/field: |</span><br><span class="line">+------------+   |     |              |    |    |              |</span><br><span class="line">                 |     |   sqe_next --+----+    |   sqe_next   |</span><br><span class="line">                 |     |              |         |      ^       |</span><br><span class="line">                 |     +--------------+         +------+-------+</span><br><span class="line">                 +-------------------------------------+        </span><br></pre></td></tr></table></figure>

<h2 id="基本操作示例"><a href="#基本操作示例" class="headerlink" title="基本操作示例"></a>基本操作示例</h2><p>有了上面的基本数据结构，我们就很容易理解关于这个链表的各种操作。</p>
<p>链表head动态初始化，sqh_last在初始情况下先指向head里的sqh_first。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define QSIMPLEQ_INIT(head) do &#123;                                        \       </span><br><span class="line">    (head)-&gt;sqh_first = NULL;                                           \       </span><br><span class="line">    (head)-&gt;sqh_last = &amp;(head)-&gt;sqh_first;                              \       </span><br><span class="line">&#125; while (/*CONSTCOND*/0)                                                        </span><br></pre></td></tr></table></figure>

<p>在链表结尾插入一个新节点。因为sqh_last已经保存了链表最后一个元素里的sqe_next的<br>地址，所以直接把sqe_next指向新的元素，再更新head里的sqh_last就好。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define QSIMPLEQ_INSERT_TAIL(head, elm, field) do &#123;                     \       </span><br><span class="line">    (elm)-&gt;field.sqe_next = NULL;                                       \       </span><br><span class="line">    *(head)-&gt;sqh_last = (elm);                                          \       </span><br><span class="line">    (head)-&gt;sqh_last = &amp;(elm)-&gt;field.sqe_next;                          \       </span><br><span class="line">&#125; while (/*CONSTCOND*/0)                                                        </span><br></pre></td></tr></table></figure>

<p>找链表最后一个元素。因为可以直接找到最后一个元素的sqe_next即field结构的地址，通过<br>类似Linux内核container_of的方式就可以反查到元素的地址。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define QSIMPLEQ_LAST(head, type, field)                                \       </span><br><span class="line">    (QSIMPLEQ_EMPTY((head)) ?                                           \       </span><br><span class="line">        NULL :                                                          \       </span><br><span class="line">            ((struct type *)(void *)                                    \       </span><br><span class="line">        ((char *)((head)-&gt;sqh_last) - offsetof(struct type, field))))           </span><br></pre></td></tr></table></figure>

<p>遍历链表元素。这个没有什么特殊的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define QSIMPLEQ_FOREACH(var, head, field)                              \       </span><br><span class="line">    for ((var) = ((head)-&gt;sqh_first);                                   \       </span><br><span class="line">        (var);                                                          \       </span><br><span class="line">        (var) = ((var)-&gt;field.sqe_next))                                        </span><br></pre></td></tr></table></figure>

<p>在链表头插入一个新节点。首先更新新节点的sqe_next指针，然后更新sqh_first使其指向<br>新插入的节点，这两个操作对所有情况都是一样的。对于空链表的情况，因为新插入的节点<br>也是末尾节点，所以还要更新下head里的sqh_last。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define QSIMPLEQ_INSERT_HEAD(head, elm, field) do &#123;                     \       </span><br><span class="line">    if (((elm)-&gt;field.sqe_next = (head)-&gt;sqh_first) == NULL)            \       </span><br><span class="line">        (head)-&gt;sqh_last = &amp;(elm)-&gt;field.sqe_next;                      \       </span><br><span class="line">    (head)-&gt;sqh_first = (elm);                                          \       </span><br><span class="line">&#125; while (/*CONSTCOND*/0)                                                        </span><br></pre></td></tr></table></figure>

<p>在一个元素后插入新节点。基本逻辑和在链表头插入一个新节点的逻辑一样，都要额外处理<br>下插入节点是末尾节点的情况。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define QSIMPLEQ_INSERT_AFTER(head, listelm, elm, field) do &#123;           \       </span><br><span class="line">    if (((elm)-&gt;field.sqe_next = (listelm)-&gt;field.sqe_next) == NULL)    \       </span><br><span class="line">        (head)-&gt;sqh_last = &amp;(elm)-&gt;field.sqe_next;                      \       </span><br><span class="line">    (listelm)-&gt;field.sqe_next = (elm);                                  \       </span><br><span class="line">&#125; while (/*CONSTCOND*/0)                                                        </span><br></pre></td></tr></table></figure>

<p>从链表头上删除一个节点。基本逻辑和在链表头插入一个新节点的逻辑一样，都要额外处理<br>下插入节点是末尾节点的情况。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define QSIMPLEQ_REMOVE_HEAD(head, field) do &#123;                          \       </span><br><span class="line">    typeof((head)-&gt;sqh_first) elm = (head)-&gt;sqh_first;                  \       </span><br><span class="line">    if (((head)-&gt;sqh_first = elm-&gt;field.sqe_next) == NULL)              \       </span><br><span class="line">        (head)-&gt;sqh_last = &amp;(head)-&gt;sqh_first;                          \       </span><br><span class="line">    elm-&gt;field.sqe_next = NULL;                                         \       </span><br><span class="line">&#125; while (/*CONSTCOND*/0)                                                        </span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu set_label中间码分析</title>
    <url>/qemu-set-label%E4%B8%AD%E9%97%B4%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="set-label功能介绍"><a href="#set-label功能介绍" class="headerlink" title="set_label功能介绍"></a>set_label功能介绍</h2><p>qemu tcg在使用中间码实现跳转时要使用label打一个跳转目的地址的标记，然后就可以使用<br>br或者brcond跳转到label标记的地方。</p>
<p>我们具体看一个例子，比如，riscv的qemu前端是这样支持jalr的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static bool trans_jalr(DisasContext *ctx, arg_jalr *a)</span><br><span class="line">&#123;</span><br><span class="line">    TCGLabel *misaligned = NULL;</span><br><span class="line"></span><br><span class="line">    tcg_gen_addi_tl(cpu_pc, get_gpr(ctx, a-&gt;rs1, EXT_NONE), a-&gt;imm);</span><br><span class="line">    tcg_gen_andi_tl(cpu_pc, cpu_pc, (target_ulong)-2);</span><br><span class="line"></span><br><span class="line">    gen_set_pc(ctx, cpu_pc);</span><br><span class="line">    if (!has_ext(ctx, RVC)) &#123;</span><br><span class="line">        TCGv t0 = tcg_temp_new();</span><br><span class="line"></span><br><span class="line">        misaligned = gen_new_label();</span><br><span class="line">        tcg_gen_andi_tl(t0, cpu_pc, 0x2);</span><br><span class="line">        tcg_gen_brcondi_tl(TCG_COND_NE, t0, 0x0, misaligned);</span><br><span class="line">        tcg_temp_free(t0);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    gen_set_gpri(ctx, a-&gt;rd, ctx-&gt;pc_succ_insn);</span><br><span class="line">    tcg_gen_lookup_and_goto_ptr();</span><br><span class="line"></span><br><span class="line">    if (misaligned) &#123;</span><br><span class="line">        gen_set_label(misaligned);</span><br><span class="line">        gen_exception_inst_addr_mis(ctx);</span><br><span class="line">    &#125;</span><br><span class="line">    ctx-&gt;base.is_jmp = DISAS_NORETURN;</span><br><span class="line"></span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如上，它先定义一个TCGLabel的变量的指针，使用之前创建一个label变量，并使用之前定义<br>的指针引用label变量，在需要跳转的地方使用gen_set_labe放置一个set_label中间码，使用<br>br或者brcond就可以跳到set_label的地址上。</p>
<h2 id="set-label-qemu后端实现"><a href="#set-label-qemu后端实现" class="headerlink" title="set_label qemu后端实现"></a>set_label qemu后端实现</h2><p>gen_new_label会创建label结构，并把它放入TCGContext的labels链表，注意label结构内<br>还包含一个relocs(重定位)链表。TCGContext的labels链表保存的是当前TB翻译上下文里的<br>所有label，而label里的relocs链表保存的是指向这个label的所有br/brcond对应host的跳转<br>指令的信息。</p>
<p>qemu在翻译如上jalr指令，当解析到brcondi_tl时，它只知道满足条件时要跳到misaligned<br>这个label标界的地址上，但是现在它并不知道实际要跳转的地址，所以在qemu后端翻译的时候<br>把brcondi_tl这个信息记录在misaligned label的relocs链表里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/riscv/tcg-target.c.inc */</span><br><span class="line">tcg_out_op</span><br><span class="line">  +-&gt; case INDEX_op_br:</span><br><span class="line">       /*</span><br><span class="line">        * 这个地方并不生成host指令，而是记录需要更新跳转地址的host跳转指令的信息，</span><br><span class="line">        * 比如，host跳转指令的地址、类型以及参数等信息。这样后面才能利用这些信息</span><br><span class="line">        * 更新跳转指令的目的地址。</span><br><span class="line">        */</span><br><span class="line">       tcg_out_reloc(s, s-&gt;code_ptr, R_RISCV_JAL, arg_label(a0), 0);</span><br><span class="line">       tcg_out_opc_jump(s, OPC_JAL, TCG_REG_ZERO, 0);</span><br></pre></td></tr></table></figure>

<p>qemu解析到set_label中间码时，才知道label标记的具体地址，显然set_label标记的是一个<br>host VA。set_label中间码的翻译把这个具体地址记录在label结构里:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/tcg.c */</span><br><span class="line">tcg_gen_code</span><br><span class="line">  +-&gt; case INDEX_op_set_label:</span><br><span class="line">       tcg_reg_alloc_bb_end(s, s-&gt;reserved_regs);</span><br><span class="line">       /* 把当前要填充的host指令的地址存入label结构 */</span><br><span class="line">       tcg_out_label(s, arg_label(op-&gt;args[0]));</span><br></pre></td></tr></table></figure>

<p>等到TB里的全部指令都翻译完成了，qemu在TB翻译的最后会扫描labels链表，为TB里每个label<br>更新它所对应的br/brcond翻译后的跳转指令的目的地址:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/tcg.c */</span><br><span class="line">tcg_gen_code</span><br><span class="line">  [...]</span><br><span class="line">  +-&gt; tcg_resolve_relocs(s)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu qom分析</title>
    <url>/qemu-qom%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="用c语言实现面向对象的模型"><a href="#用c语言实现面向对象的模型" class="headerlink" title="用c语言实现面向对象的模型"></a>用c语言实现面向对象的模型</h2><p> qemu里用c语言实现了面向对象的模型。我们先梳理用c实现面向对应的基本逻辑。面向对象<br> 的三个特征是：封装、继承和多态。</p>
<p> 封装可以用struct实现。</p>
<p> 继承可以用struct包含的方式实现，把父类的struct放到子类struct的最开始的位置，这样<br> 子类的指针可以直接强制转换成父类的指针，在子类的函数，比如子类的初始化函数里可以<br> 直接得到父类的指针，然后调用父类的初始化函数。但是继承层级大于两级的时候似乎是有<br> 问题的(fixme)</p>
<p> 多态可以用函数指针的方式实现。</p>
<p> qemu里的实现，多了TypeInfo这个概念，它是class的描述。</p>
<h2 id="类的定义"><a href="#类的定义" class="headerlink" title="类的定义"></a>类的定义</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">type_init(fn)</span><br><span class="line">      /* 用宏定义了一个动态库的初始化函数, qemu编译出的库有？*/</span><br><span class="line">  +-&gt; module_init</span><br><span class="line">   +-&gt; register_module_init</span><br><span class="line">         /* 如下都用edu设备举例(hw/misc/edu.c)，这里的fn就是pci_edu_register_types */</span><br><span class="line">     +-&gt; e-&gt;init = fn</span><br></pre></td></tr></table></figure>
<p> fn这个函数一般是TypeInfo的注册函数, 把TypeInfo挂到系统的链表里。class是随后解析<br> Typeinfo的内容动态生成的。</p>
<h2 id="对象的生成"><a href="#对象的生成" class="headerlink" title="对象的生成"></a>对象的生成</h2><p> 顺着qemu的main函数，看看class和对象是怎么生成的：qemu/softmmu/main.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">main</span><br><span class="line">  +-&gt; qemu_init</span><br><span class="line">    +-&gt; qemu_init_subsystems</span><br><span class="line">          /* 根据TypeInfo创建class */</span><br><span class="line">      +-&gt; module_call_init(MODULE_INIT_QOM)</span><br><span class="line">            /*</span><br><span class="line">             * init即为如上的fn, 这里的init只是把TypeInfo向qemu注册，类的</span><br><span class="line">             * 初始化还在后面。具体拿edu里的pci_edu_register_types函数看下。</span><br><span class="line">             */     </span><br><span class="line">        +-&gt; ModuleEntry-&gt;init</span><br></pre></td></tr></table></figure>
<p>  可以看见这个函数拿自己的初始化函数定义了TypeInfo数据结构，然后把他注册到系统<br>  TypeInfo的链表。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static void pci_edu_register_types(void)</span><br><span class="line">&#123;</span><br><span class="line">    static InterfaceInfo interfaces[] = &#123;</span><br><span class="line">        &#123; INTERFACE_CONVENTIONAL_PCI_DEVICE &#125;,</span><br><span class="line">        &#123; &#125;,</span><br><span class="line">    &#125;;</span><br><span class="line">    static const TypeInfo edu_info = &#123;</span><br><span class="line">        .name          = TYPE_PCI_EDU_DEVICE,</span><br><span class="line">        .parent        = TYPE_PCI_DEVICE,</span><br><span class="line">        .instance_size = sizeof(EduState),</span><br><span class="line">        .instance_init = edu_instance_init,</span><br><span class="line">        .class_init    = edu_class_init,</span><br><span class="line">        .interfaces = interfaces,</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    type_register_static(&amp;edu_info);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  顺着qemu_init函数继续往下看，找下class以及具体的设备是在哪里创建的。还是以edu<br>  这个设备为例。这个设备使用-device edu的qemu命令行参数启动，所有它创建的位置应该<br>  在：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu_opts_foreach(qemu_find_opts(&quot;device&quot;),</span><br><span class="line">                  device_init_func, NULL, &amp;error_fatal);</span><br></pre></td></tr></table></figure>
<p>   下面具体分析其中的qdev_device_add:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> qdev_device_add</span><br><span class="line">       /* driver就是模块里的device name, edu的driver就是edu */</span><br><span class="line">   +-&gt; driver = qemu_opt_get(opts, &quot;driver&quot;)</span><br><span class="line">     +-&gt; module_object_class_by_name(*driver)</span><br><span class="line">       +-&gt; oc = object_class_by_name(typename)</span><br><span class="line">             /*</span><br><span class="line">              * type_initialize是根据注册的Type创建class的函数。创建class的具体</span><br><span class="line">              * 实例的时候，如果class没有创建，就会创建class，被创建的class的指针</span><br><span class="line">              * 会放到注册Type的class域段。</span><br><span class="line">              *</span><br><span class="line">              * 可以看到这个函数为class分配了空间，递归初始化了父类，把父类空间</span><br><span class="line">              * 中的内容copy到了当前类最开始的空间。初始化class的interface和</span><br><span class="line">              * property，并在最后调用了class的init函数，把class的数据和操作函数</span><br><span class="line">              * 都添加上。</span><br><span class="line">              *</span><br><span class="line">              * 创建的interface class会挂到对应device class的链表上。</span><br><span class="line">              */</span><br><span class="line">         +-&gt; type_initialize(type)</span><br><span class="line">       /* 注意，这里返回的是DeviceClass */</span><br><span class="line">   +-&gt; dc = qdev_get_device_class(&amp;driver, errp)</span><br><span class="line">       /* 找见设备对应的bus */</span><br><span class="line">   +-&gt; bus = qbus_find(path, errp)</span><br><span class="line">       /* 创建设备, 可以看到如果没有class的话，在如下函数里会先创建class */</span><br><span class="line">   +-&gt; dev = qdev_new(driver)</span><br><span class="line">     +-&gt; object_new(typename)</span><br><span class="line">           /*</span><br><span class="line">            * 为设备对象分配了内存空间, 把设备里的class指针指向class，为设备</span><br><span class="line">            * 初始化class里定义的各个property。调用instance_init初始化设备。</span><br><span class="line">            * 注意这个时候设备还不是在可用的状态。</span><br><span class="line">            */</span><br><span class="line">       +-&gt; object_new_with_type(ti)</span><br><span class="line">/* 解析输入的设备属性并且保存到设备的属性hash表里 */</span><br><span class="line">   +-&gt; qemu_opt_foreach(opts, set_property, dev, errp)</span><br><span class="line">   +-&gt; qdev_realize(DEVICE(dev))</span><br><span class="line">         /* 调用到class里的realize函数激活设备, 具体的分析在下面一节 */</span><br><span class="line">     +-&gt; object_property_set_bool(OBJECT(dev), &quot;realized&quot;, true, errp)</span><br></pre></td></tr></table></figure>

<h2 id="properties是什么"><a href="#properties是什么" class="headerlink" title="properties是什么"></a>properties是什么</h2><p> 所谓属性，就是在一个对象里定义的一些功能，这些功能有名字，有对应的执行函数，还有<br> 添加和删除函数。当添加一个属性的时候，就是把这个属性已经对应的执行函数保存到对象<br> 专门用来存各种属性的一个hash table。当执行属性的操作时，就是执行对应属性附带的<br> 执行函数。</p>
<p> 我们还是拿edu这个设备为例。edu在实例初始化的时候挂给PCIDeviceClass的realize一个<br> 回调函数pci_edu_realize，这个函数就是PCI设备里realize属性的执行函数。我们需要明确<br> 这个realize属性在哪里添加和在哪里调用。</p>
<p> device class的初始化函数里增加了realized属性：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* hw/core/qdev.c */</span><br><span class="line">device_class_init</span><br><span class="line">     /*</span><br><span class="line">      * 把realized属性加到ObjectClass里。device_set_realized里会调用DeviceClass里的</span><br><span class="line">      * realize回调函数。DeviceClass里的realize回调在pci_device_class_init里挂成</span><br><span class="line">      * pci_qdev_realize。pci_qdev_realize调用PCIDeviceClass里的realize函数，这个</span><br><span class="line">      * 函数又是由具体设备的class init函数添加，比如edu的edu_class_init。</span><br><span class="line">      */</span><br><span class="line">  -&gt; object_class_property_add_bool(ObjectClass, &quot;realized&quot;, device_get_realized, device_set_realized)</span><br></pre></td></tr></table></figure>

<p> 在如上的qdev_device_add里，在创建了设备的实例后，后调用qdev_realize把设备realize，<br> 这个函数会从Device这一层，层层的调用realize函数:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* hw/core/qdev.c */</span><br><span class="line">qdev_realize</span><br><span class="line">  -&gt; object_property_set_bool(OBJECT(dev), &quot;realized&quot;, true, errp)</span><br><span class="line">    [...]</span><br><span class="line">    -&gt; object_property_set</span><br><span class="line">         /* 可以看到realized相关的add和find都是发生在Object、ObjectClass这个层次 */</span><br><span class="line">      -&gt; ObjectProperty *prop = object_property_find(obj, name, errp)</span><br><span class="line">           /* 先用obj找到ObjectClass，再在ObjectClass找property */</span><br><span class="line">        -&gt; object_class_property_find(klass, name, NULL)</span><br><span class="line">           /* 在Object里找property */</span><br><span class="line">        -&gt; g_hash_table_lookup(obj-&gt;properties, name)</span><br><span class="line"></span><br><span class="line">      -&gt; prop-&gt;set(obj, v, name, prop-&gt;opaque, &amp;err)</span><br></pre></td></tr></table></figure>
<p> 如上，分析qemu的qom重点关注如下的文件: hw/misc/edu.c, hw/pci/pci.c, hw/core/qdev.c,<br> qom/object.c。各个层级的Type定义分别对应的文件里(这里用pci设备为例)</p>
<p> 一个典型的使用属性的地方是在qemu启动的时候通过命令行参数给一个设备传递一个属性值。<br> 我们分析这里的代码流程，还是以edu为例，edu_instance_init里的object_property_add_uint64_ptr<br> 为edu设备加了dma_mask这样一个设备属性。在qemu的启动命令行里可以如下配置使用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--device edu,dma_mask=0xffffff</span><br></pre></td></tr></table></figure>
<p> 可以看到qemu_opt_foreach(opts, set_property, dev, errp)解析设备属性在instance_init<br> 之后，在realize函数调用之前。所以，edu驱动里在instance_init里把设备属性的定义加<br> 到设备对应的属性hash表里，如上的解析函数才能把命令行输入的属性和设备属性匹配。<br> edu需要在realize函数或者realize之后才能使用传入的设备属性。</p>
<p> link属性</p>
<p> 有了上面的分析，link属性的使用也可以想到，他同样可以使用qemu的启动命令行确定qemu<br> 部件之间的逻辑关系。</p>
<p> child属性</p>
<h2 id="interface"><a href="#interface" class="headerlink" title="interface"></a>interface</h2><p> 目前只看了PCI/PCIe设备里使用了interface这个东西，PCIe设备用INTERFACE_PCIE_DEVICE<br> PCI设备用INTERFACE_CONVENTIONAL_PCI_DEVICE。pci设备的realize函数里根据interface<br> 的情况决定是否要使能PCI_CAP_EXPRESS，这个只在PCIe的时候使能。</p>
<p> 注意，PCI设备只有0x0~0xff的配置空间。</p>
<h2 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h2><p> 这里写了一个dma engine的qemu设备作为<a href="https://wangzhou.github.io/%E5%A6%82%E4%BD%95%E5%9C%A8qemu%E9%87%8C%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA%E8%99%9A%E6%8B%9F%E8%AE%BE%E5%A4%87/">例子</a>。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tcg goto_tb分析</title>
    <url>/qemu-tcg-goto-tb%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="goto-tb分析"><a href="#goto-tb分析" class="headerlink" title="goto tb分析"></a>goto tb分析</h2><p> <a href="https://wangzhou.github.io/qemu-tcg%E8%B7%B3%E8%BD%AC%E7%9A%84%E5%A4%84%E7%90%86/">qemu tcg跳转的处理</a>中分析了qemu中模拟跳转的逻辑，其中提到了chained tb的概念，<br> chained tb中有一个限制，这个在它的代码实现里看出来：两个chained tb对应的guest<br> 指令需要在同一个guest的page里。下面分析这样限制的原因。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> VA1 ---&gt; +----------+ </span><br><span class="line">          |          |  map1</span><br><span class="line">          |          | ----------+</span><br><span class="line"> VA2 ---&gt; +----------+  map2     |     +----------+ &lt;--- PA1</span><br><span class="line">          |          | ------+   +---&gt; |          |</span><br><span class="line">          |          |  map3 |         |          |</span><br><span class="line">          +----------+ ----+ |         +----------+ &lt;--- PA2</span><br><span class="line">                           | |         |          |</span><br><span class="line">                           | +-------&gt; |          |</span><br><span class="line">                           |           +----------+ &lt;--- PA3</span><br><span class="line">                           |           |          |</span><br><span class="line">                           +---------&gt; |          |</span><br><span class="line">                                       +----------+</span><br><span class="line">tb1 key: pa1</span><br><span class="line">+-------+ &lt;---------------------+   &lt;--- htable</span><br><span class="line">|       |                       |</span><br><span class="line">+-------+                       |</span><br><span class="line">    |                           |</span><br><span class="line">    v tb2 key: pa2              |</span><br><span class="line">+-------+ &lt;-------------+       |</span><br><span class="line">|       |               |       |</span><br><span class="line">+-------+               |       |</span><br><span class="line">                        |       |</span><br><span class="line">tb_jmp_cache key: va    |       |</span><br><span class="line">         +----------------------------------------+</span><br><span class="line">         |             va2     va1                |</span><br><span class="line">         +----------------------------------------+</span><br></pre></td></tr></table></figure>
<p> qemu在翻译执行的时候，对于tb有两个cache，首先所有生成的tb都会放到htable这个哈希表，<br> 这个是我们这里说的第一个cache，这个哈希表的key是guest代码块的起始pa以及一些其它的<br> flag，另外一个cache叫tb_jmp_cache，这个是很简单的用guest代码块的起始va做key(这个<br> cache和本文分析的问题无关，对于它的分析，我们单独放到最后)。</p>
<p> htable用pa以及flag做key，在翻译执行的时候首先会用va查tlb得到pa，如果tlb里没有缓存，<br> 那就触发page table walk或者取指令异常，终归是要把pa找到并填到tlb里，因为是物理地址<br> 做key，所以有可能不同虚拟地址映射到相同物理地址上时，对应的tb也只有一个。</p>
<p> 当程序运行时，一段代码的位置，从va看是不变的，但是代码的物理存储位置可能是变化的，<br> 比如上图中从map2到map3映射的改变。从htable的设计上看，这种映射关系改变和tb之间并<br> 没有直接的联系，因为改变的只是va到pa的映射，而tb在htable上是用pa做key，退一步讲,<br> 如果多个不同的va映射到一个pa上，其中一个映射改变了，其它的映射还在，对应的tb应该<br> 继续保留。</p>
<p> 如上的情况是va映射不同pa上的代码是相同的情况，最常见的是页换出换入的情况。如果chain<br> tb跨越了这两个虚拟页，chain tb在语意上已经不对了，因为改变映射关系后，被指向的tb<br> 应该是以PA3页面上的PA作为key的，但是实际上因为改变映射的两个物理页上的指令是一样<br> 的，运行可能不会出问题。</p>
<p> 但是，如果改变映射前后的物理页上的指令不一样，chain tb跨越不同页就会出错，当然<br> 这种情况很少会有实际的应用场景。</p>
<p> chained tb还需要面对的一个问题是，如果被指向的tb对应的guest指令被修改了怎么办，<br> 比如，上面tb2对应的guest指令被修改了，之前得到tb2显然是不能用了。qemu使用了一个叫<br> PageDesc的软件结构管理guest的物理内存，管理的单位是guest的页大小，一个PageDesc<br> 对应一个guest页，PageDesc记录着相关guest页对应的所有tb，理论上看，如果一个guest<br> 页上的指令有修改，qemu只要找到修改的guest指令对应的tb块，把这个tb块从chained tb<br> 的链条里移除就好。</p>
<p> 这里具体上是靠tb里的jmp_reset_offset、jmp_list_head、jmp_list_next以及jmp_desc<br> 域段实现的，这个几个域段的语意分别是，jmp_list_head是指向tb的tb组成的链表，jmp_desc<br> 是本tb的两个可能的直接跳转的地址，jmp_list_next用于把tb链入jmp_list_head链表，因为<br> 一个tb可能指向两个tb，一个tb可能被链入两个jmp_list_head，所以jmp_list_next数组有<br> 两个元素，jmp_reset_offset是本tb两个可能的直接跳转地址的复位值。</p>
<p> 在两个tb相连的时候更新如上的信息，jmp_reset_offset在后端翻译里计算和更新。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tb_add_jump(tb, n, tb_next</span><br><span class="line">      /* 把跳转地址记录在jmp_dest[n] */</span><br><span class="line">  +-&gt; qatomic_cmpxchg(&amp;tb-&gt;jmp_dest[n], (uintptr_t)NULL, (uintptr_t)tb_next)</span><br><span class="line"></span><br><span class="line">      /* 更新host指令中的跳转地址 */</span><br><span class="line">  +-&gt; tb_set_jmp_target(tb, n, (uintptr_t)tb_next-&gt;tc.ptr)</span><br><span class="line"></span><br><span class="line">      /* 把指向tb_next的tb插入tb_next里的jmp_list_head链表 */ </span><br><span class="line">  +-&gt; tb-&gt;jmp_list_next[n] = tb_next-&gt;jmp_list_head</span><br><span class="line">  +-&gt; tb_next-&gt;jmp_list_head = (uintptr_t)tb | n</span><br></pre></td></tr></table></figure>

<p>chained tb之间的关系可以用如下的图来描述：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+----------------+   +----------------+   +----------------+</span><br><span class="line">|tb0             |   |tb1             |   |tb2             |</span><br><span class="line">|                |   |                |   |                |</span><br><span class="line">|jmp_list_next[0]|&lt;--|jmp_list_next[0]|&lt;+ |jmp_list_next[0]|&lt;-------------+</span><br><span class="line">|jmp_list_next[1]|   |jmp_list_next[1]| +-|jmp_list_next[1]|&lt;-----------+ |</span><br><span class="line">+----------------+   +----------------+   +----------------+            | |</span><br><span class="line">                \            |             /       \                    | |</span><br><span class="line">                 \           |            /         \ 0                 | |</span><br><span class="line">                0 \        0 |           / 1         v                  | |</span><br><span class="line">                   \         |          /           +----------------+  | |</span><br><span class="line">                    v        v         v            |tb4             |  | |</span><br><span class="line">                     +----------------+             |                |  | |</span><br><span class="line">                     |tb3             |             |...             |  | |</span><br><span class="line">                     |                |             |jmp_list_head --+--+-+</span><br><span class="line">                     |...             |             +----------------+  |</span><br><span class="line">                     |jmp_list_head --+---------------------------------+</span><br><span class="line">                     +----------------+</span><br></pre></td></tr></table></figure>

<p>从如上数据结构中去掉一个tb的逻辑如下, 比如我们这里要去掉tb3，具体的逻辑是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">do_tb_phys_invalidate</span><br><span class="line">      /*</span><br><span class="line">       * 找到tb3指向的节点，把tb3指向的tb中的jmp_list_head上的tb3节点去掉, 分别</span><br><span class="line">       * 对tb3可能指向的两个tb节点做这样的操作。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; tb_remove_from_jmp_list(tb, 0)</span><br><span class="line">  +-&gt; tb_remove_from_jmp_list(tb, 1)</span><br><span class="line"></span><br><span class="line">      /* 从tb3的jmp_list_head的到指向他的tb, 更新这些tb的指向 */</span><br><span class="line">  +-&gt; tb_jmp_unlink(tb)</span><br></pre></td></tr></table></figure>

<p> 如上已经介绍了goto tb相关的关键逻辑，下面进一步看下qemu PageDesc相关的整体实现。<br> PageDesc初始化的地方是page_init函数，可以看出相关的位置是各种tcg资源初始化的地方:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* accel/tcg/tcg-all.c */</span><br><span class="line">tcg_init_machine</span><br><span class="line">  +-&gt; page_init</span><br><span class="line">        /*</span><br><span class="line">         * 这个函数里初始化PageDesc的各级表项的配置，它和页表类似的用一个多级表</span><br><span class="line">         * 管理整个物理地址空间。</span><br><span class="line">         *</span><br><span class="line">         * 对于riscv 44bit的物理地址空间来说，如果一个PageDesc管理4KB的页面，</span><br><span class="line">         * 整个物理地址空间的管理分为两级，第二级一项管理1K个页面，那么第一级有</span><br><span class="line">         * 44 - 12 - 10 = 22，2 ^ 22项。</span><br><span class="line">         */</span><br><span class="line">    +-&gt; page_table_config_init</span><br><span class="line">  +-&gt; tb_htable_init</span><br><span class="line">      /* tcg后端翻译需要的资源 */</span><br><span class="line">  +-&gt; tcg_init</span><br></pre></td></tr></table></figure>
<p> 通过物理地址找见PageDesc的函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">page_find_alloc</span><br></pre></td></tr></table></figure>
<p> 把一个tb加入到一个PageDesc里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tb_page_add </span><br><span class="line">      /*</span><br><span class="line">       * 把tb插入first_tb指向的链表的第一个节点，n是1表示有指令跨域了一个page，</span><br><span class="line">       * 那么就要在两个page对应的PageDesc中都要做记录。</span><br><span class="line">       *</span><br><span class="line">       * 实际上在riscv的qemu tcg模型里，只有一种跨page边界的情况，我们把这个逻辑</span><br><span class="line">       * 独立放到下面。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; tb-&gt;page_next[n] = p-&gt;first_tb</span><br><span class="line">  +-&gt; p-&gt;first_tb = tb | n</span><br><span class="line">      /*</span><br><span class="line">       * 如果是第一次给这个PageDesc加tb，就是这个page上的指令第一次翻译出tb块，</span><br><span class="line">       * 把对应page的dirty bit清理(初始化)下。</span><br><span class="line">       *</span><br><span class="line">       * 如下的dirty bit和qemu里虚拟机的热迁移有关系。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; tlb_protect_code(page_addr)</span><br><span class="line">        /* softmmu/physmem.c */</span><br><span class="line">    +-&gt; cpu_physical_memory_test_and_clear_dirty</span><br></pre></td></tr></table></figure>
<p> 从PageDesc里去掉一个tb，直接从链表里去掉对应的tb即可：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tb_page_remove(p, tb)</span><br></pre></td></tr></table></figure>
<p> PageDesc里tb被去掉的调用路径：(to do test)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">store_helper</span><br><span class="line">  +--&gt; notdirty_write(env_cpu(env), addr, size, full, retaddr);</span><br><span class="line">    +-&gt; tb_invalidate_phys_page_fast(pages, ram_addr, size, retaddr);</span><br><span class="line">      +-&gt; tb_invalidate_phys_page_range__locked</span><br><span class="line">        +-&gt; tb_phys_invalidate__locked(tb);</span><br><span class="line">              /* 如上讲的guest代码被修改后，需要移除对应的tb，相关逻辑的入口是这里*/</span><br><span class="line">          +-&gt; do_tb_phys_invalidate(tb, true);</span><br><span class="line">                /* 移除htable里的tb */</span><br><span class="line">            +-&gt; qht_remove(&amp;tb_ctx.htable, tb, h)</span><br><span class="line">                /* 移除PageDesc里的tb */</span><br><span class="line">            +-&gt; tb_page_remove</span><br><span class="line">         /* 从tb链条里移除tb */</span><br><span class="line">            +-&gt; tb_remove_from_jmp_list(tb, 0);                                             </span><br><span class="line">            +-&gt; tb_remove_from_jmp_list(tb, 1);                                             </span><br><span class="line">            +-&gt; tb_jmp_unlink(tb);                                                          </span><br></pre></td></tr></table></figure>

<p> 我们可以把gdb的断点设在tb_invalidate_phys_page_range__locked上，然后运行一段自修改<br> 代码(<span class="exturl" data-url="aHR0cHM6Ly85dG81YW5zd2VyLmNvbS9ob3ctdG8td3JpdGUtc2VsZi1tb2RpZnlpbmctY29kZS1pbi1j">自修改代码可以参考这里<i class="fa fa-external-link-alt"></i></span>)，看看实际运行的效果。</p>
<h2 id="指令垮页分析"><a href="#指令垮页分析" class="headerlink" title="指令垮页分析"></a>指令垮页分析</h2><p> 实际上，在riscv 7.1.50的qemu tcg模拟中，只有一个4B指令跨越页边界这一种情况存在了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_tr_translate_insn</span><br><span class="line">  [...]</span><br><span class="line">  /* Only the first insn within a TB is allowed to cross a page boundary. */  </span><br><span class="line">  if (ctx-&gt;base.is_jmp == DISAS_NEXT) &#123;                                       </span><br><span class="line">      /*</span><br><span class="line">       *  pc_first       pc_next</span><br><span class="line">       *      |            |</span><br><span class="line">       *  |   v       |    v      |</span><br><span class="line">       *  +-----------+-----------+</span><br><span class="line">       *  |   Page    |   Page    |</span><br><span class="line">       *</span><br><span class="line">       */</span><br><span class="line">      if (!is_same_page(&amp;ctx-&gt;base, ctx-&gt;base.pc_next)) &#123;                     </span><br><span class="line">          ctx-&gt;base.is_jmp = DISAS_TOO_MANY;                                  </span><br><span class="line">      &#125; else &#123;                                                                </span><br><span class="line">          unsigned page_ofs = ctx-&gt;base.pc_next &amp; ~TARGET_PAGE_MASK;          </span><br><span class="line">          /*</span><br><span class="line">           *  pc_first pc_next</span><br><span class="line">           *      |    |</span><br><span class="line">           *  |   v    v  |           |</span><br><span class="line">           *  +-----------+-----------+</span><br><span class="line">           *  |   Page    |   Page    |</span><br><span class="line">           *</span><br><span class="line">           *  这里就是pc_next为页尾 - 2Byte的情况，这种情况下，如果pc_next指向</span><br><span class="line">           *  的指令是一个2B的压缩编码指令，最后这个指令还可以放到当前tb，如果是</span><br><span class="line">           *  一个4B的普通指令，那么pc_next + len就到了下一个页里，需要结束当前</span><br><span class="line">           *  tb。</span><br><span class="line">           *</span><br><span class="line">           *  对于下个4B编码的指令，下次翻译先取16bit，得到是一个32bit指令后再</span><br><span class="line">           *  再把后16bit取出来，前端翻译后走到上面第一个分支里，发现已经跨越了</span><br><span class="line">           *  页边界，于是停止当前tb的翻译，这个tb里只翻译了一个跨越页边界的32bit</span><br><span class="line">           *  指令。</span><br><span class="line">           */ </span><br><span class="line">          if (page_ofs &gt; TARGET_PAGE_SIZE - MAX_INSN_LEN) &#123;                   </span><br><span class="line">              uint16_t next_insn = cpu_lduw_code(env, ctx-&gt;base.pc_next);     </span><br><span class="line">              int len = insn_len(next_insn);                                  </span><br><span class="line">              if (!is_same_page(&amp;ctx-&gt;base, ctx-&gt;base.pc_next + len)) &#123;       </span><br><span class="line">                  ctx-&gt;base.is_jmp = DISAS_TOO_MANY;                          </span><br><span class="line">              &#125;                                                               </span><br><span class="line">          &#125;                                                                   </span><br><span class="line">      &#125;                                                                       </span><br><span class="line">   &#125;                                                                           </span><br></pre></td></tr></table></figure>

<h2 id="tb-jmp-cache分析"><a href="#tb-jmp-cache分析" class="headerlink" title="tb_jmp_cache分析"></a>tb_jmp_cache分析</h2><p> tb_jmp_cache用va做key是存在问题的，一个系统里可能有相同的va映射到不同的pa，比如，<br> 两个进程的va相同，映射的pa就可能不同。所以，要保证在tb_jmp_cache里，va只唯一映射<br> 一个pa，这个在user mode似乎没有问题，在system mode的完整逻辑还有待分析。(todo)</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tcg中断模拟</title>
    <url>/qemu-tcg%E4%B8%AD%E6%96%AD%E6%A8%A1%E6%8B%9F/</url>
    <content><![CDATA[<p>qemu的整个中断处理流程可以大概分为核内和核外，核内指的是CPU核在收到中断信号时的<br>处理逻辑，核外我们可以认为就是指各种中断控制器的处理逻辑。</p>
<p>qemu tcg整体的翻译执行流程大概如下，这里描述的是核内模拟的顶层逻辑。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">setjmp;</span><br><span class="line"></span><br><span class="line">while (检查并处理中断或异常) &#123;</span><br><span class="line">       前端翻译;</span><br><span class="line">       后端翻译;</span><br><span class="line">       执行host执行改变guest CPU状态; // 有异常时longjmp到setjmp处</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以，实际响应并处理中断总是在一个tb翻译执行完。qemu有chained tb，如果一直在chained<br>tb里跳来跳去，那不是一直响应不了中断，为此qemu里的中断可以触发跳过一个tb的执行，<br>这样，一旦有中断来了，就可以快速响应，下面会介绍这个地方。</p>
<p>riscv上核内和核外的中断信号接口用gpio管脚来模拟。CPU核通过如下函数注册一个向cpu<br>写入中断的接口。向一个gpio上写入信息，逻辑上应该触发gpio handler被调用一次。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* target/riscv/cpu.c */</span><br><span class="line">qdev_init_gpio_in(DEVICE(cpu), riscv_cpu_set_irq, 12)</span><br></pre></td></tr></table></figure>

<p>从逻辑上分析，应该是中断控制器调用这个接口把中断写入到cpu。riscv上有多个中断控制<br>器: plic, aclint以及AIA, qemu上它们的模拟都在/hw/intc/下，对应文件分别是：sifive_plic.c，<br>riscv_aclint.c，riscv_aplic.c和riscv_imsic.c，其实AIA被实现为最后两个文件。plic/aplic<br>和外部设备相关，aclint是核内部的中断控制器，主要产生时钟中断和SWI，imsic则是为了<br>支持MSI和虚拟化中断直通。</p>
<p>中断控制器里使用qdev_init_gpio_out创建一个输出的GPIO，然后使用qdev_connect_gpio_out<br>把输出GPIO和输入的GPIO连在一起，在中断控制器里使用qemu_set_irq/qemu_irq_raise向<br>CPU发中断，本质上是触发riscv_cpu_set_irq被调用到。至于gpio_in对应的qemu_irq.handler<br>如何被gpio_out看见，就是qemu_set_irq/qemu_irq_raise如何触发riscv_cpu_set_irq被调用，<br>其中的细节分析，我们放到本文的最后。</p>
<p>如上，我们已经把核内和核外中断的大概逻辑梳理了下，下面重点看CPU核接收到中断的逻辑。<br>riscv上中断控制器的qemu模拟需要在独立的文档中描述。</p>
<p>riscv_cpu_set_irq修改riscv系统寄存器mip，把中断类型写入到里面。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_cpu_set_irq</span><br><span class="line">     /* 可以看到这里是把中断无条件写入到mip里的 */</span><br><span class="line">  -&gt; riscv_cpu_update_mip</span><br><span class="line">       /*</span><br><span class="line">        * 写cpu-&gt;interrupt_request, 大循环里的cpu_handle_irq根据这个判断是否</span><br><span class="line">        * 要处理中断</span><br><span class="line">	*/</span><br><span class="line">    -&gt; cpu_interrupt</span><br><span class="line">      -&gt; cpu-&gt;interrupt_request |= mask;</span><br><span class="line">      -&gt; qatomic_set(&amp;cpu_neg(cpu)-&gt;icount_decr.u16.high, -1);</span><br></pre></td></tr></table></figure>
<p>注意写icount_decr.u16.high的这个逻辑。qemu在一个tb的前端翻译的前后会插入一段翻译<br>的代码，相关逻辑在gen_intermediate_code-&gt;translator_loop-&gt;gen_tb_start/gen_tb_end，<br>gen_tb_start中产生的逻辑会检测如上icount_decr.u16.high的值，如果比0小，就会直接跳到<br>tb最后的一个label处，这个label在gen_tb_end里配置，这样，中断一旦写了icount_decr.u16.high<br>这个标记，tb执行的时候就会跳过当前tb的业务代码，相当于空执行了一个tb。这个跳转直接<br>跳过了chained tb的地方，所以可以从chained tb里出来。</p>
<p>如上是中断上报CPU的模拟逻辑，报到CPU的中断需要在qemu翻译执行的大逻辑里处理，下面<br>描述这部分内容。上面一开始提到的“检查并处理中断或异常”处理中断的具体函数是cpu_handle_irq。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * cpu_handle_irq里调用cpu_exec_interrupt回调函数处理中断，riscv_cpu_exec_interrupt</span><br><span class="line"> * 从mip里拿到中断号, 最后调用riscv_cpu_do_interrupt处理中断。riscv_cpu_exec_interrupt在</span><br><span class="line"> * target/riscv/cpu_helper.c</span><br><span class="line"> */</span><br><span class="line">riscv_cpu_exec_interrupt</span><br><span class="line">     /* 拿到中断号, 具体的逻辑我们可以在下面打开看下 */</span><br><span class="line">  -&gt; interruptno = riscv_cpu_local_irq_pending(env);</span><br><span class="line">     /*</span><br><span class="line">      * 注意这里会配置exception_index这个值，这个值在异常处理</span><br><span class="line">      * cpu_handle_exception里也会读</span><br><span class="line">      */</span><br><span class="line">  -&gt; cs-&gt;exception_index = RISCV_EXCP_INT_FLAG | interruptno;</span><br><span class="line">     /* riscv在这个里一并处理异常和中断，改变机器的状态和pc，最后清exception_index */</span><br><span class="line">  -&gt; riscv_cpu_do_interrupt(cs);</span><br><span class="line">       /* RISCV_EXCP_NONE是-1 */</span><br><span class="line">    -&gt; cs-&gt;exception_index = RISCV_EXCP_NONE</span><br></pre></td></tr></table></figure>

<p>可以看见如下的逻辑先计算出各个特权级中断的enable状态，然后用enable状态和cpu中已<br>保存的pending状态得到实际要处理的中断(A处的pending表示)，注意这个是一个临时变量<br>CPU中保存的pending依然在。随后，根据中断优先级和中断委托逻辑得到要真实触发的中断。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static int riscv_cpu_local_irq_pending(CPURISCVState *env)</span><br><span class="line">&#123;</span><br><span class="line">    int virq;</span><br><span class="line">    uint64_t irqs, pending, mie, hsie, vsie;</span><br><span class="line"></span><br><span class="line">    /* Determine interrupt enable state of all privilege modes */</span><br><span class="line">    if (riscv_cpu_virt_enabled(env)) &#123;</span><br><span class="line">	/* n: cpu目前处于v状体，所以，高特权级m/hs的中断是打开的 */</span><br><span class="line">        mie = 1;</span><br><span class="line">        hsie = 1;</span><br><span class="line">	/*</span><br><span class="line">	 * n: 对于vs的中断，当cpu现在状态是vs，就看SIE的配置，当cpu现在是vu，</span><br><span class="line">	 *    高特权级中断一定是开着，所以vs的中断是开着。</span><br><span class="line">	 */</span><br><span class="line">        vsie = (env-&gt;priv &lt; PRV_S) ||</span><br><span class="line">               (env-&gt;priv == PRV_S &amp;&amp; get_field(env-&gt;mstatus, MSTATUS_SIE));</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">	/*</span><br><span class="line">	 * n: cpu不在v状体，所以，cpu可能在m，或者比m低，同样的逻辑，在m，就看</span><br><span class="line">	 *    MIE的配置，比m低，那m中断一定是开着。</span><br><span class="line">	 */</span><br><span class="line">        mie = (env-&gt;priv &lt; PRV_M) ||</span><br><span class="line">              (env-&gt;priv == PRV_M &amp;&amp; get_field(env-&gt;mstatus, MSTATUS_MIE));</span><br><span class="line">	/*</span><br><span class="line">	 * n: cpu不在v，cpu比S低，中断一定是开，cpu在S，就看SIE，cpu在M，</span><br><span class="line">	 *    hs的中断应该是关的啊，为啥这里没有涉及？</span><br><span class="line">	 */</span><br><span class="line">        hsie = (env-&gt;priv &lt; PRV_S) ||</span><br><span class="line">               (env-&gt;priv == PRV_S &amp;&amp; get_field(env-&gt;mstatus, MSTATUS_SIE));</span><br><span class="line">	/* n: 因为cpu不在v，那cpu的特权级一定比vs高，vs中断一定是关的 */</span><br><span class="line">        vsie = 0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /* Determine all pending interrupts */</span><br><span class="line">    pending = riscv_cpu_all_pending(env);     &lt;------- A</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * n: 中断来了，这里就有优先级的处理，大的分类上，M &gt; HS &gt; VS, 就是说有了M,</span><br><span class="line">     *    我们就先处理M的中断，依此类推。这里有一个问题，如果pending了多个中断，</span><br><span class="line">     *    高特权级的中断先处理了，低特权级的中断pending还在么？</span><br><span class="line">     *</span><br><span class="line">     *    从实现上，中断pending记录在env-&gt;mip里，暂时没有看见哪里会清mip。</span><br><span class="line">     */</span><br><span class="line">    /* n: 中断有pending, 并且没有被代理出去，并且中断enable是开的，我们才计算中断号 */</span><br><span class="line">    /* Check M-mode interrupts */</span><br><span class="line">    irqs = pending &amp; ~env-&gt;mideleg &amp; -mie;</span><br><span class="line">    if (irqs) &#123;</span><br><span class="line">        /* n: 计算一个特权级下最高优先级的中断 */</span><br><span class="line">        return riscv_cpu_pending_to_irq(env, IRQ_M_EXT, IPRIO_DEFAULT_M,</span><br><span class="line">                                        irqs, env-&gt;miprio);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /* Check HS-mode interrupts */</span><br><span class="line">    irqs = pending &amp; env-&gt;mideleg &amp; ~env-&gt;hideleg &amp; -hsie;</span><br><span class="line">    if (irqs) &#123;</span><br><span class="line">        return riscv_cpu_pending_to_irq(env, IRQ_S_EXT, IPRIO_DEFAULT_S,</span><br><span class="line">                                        irqs, env-&gt;siprio);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /* Check VS-mode interrupts */</span><br><span class="line">    irqs = pending &amp; env-&gt;mideleg &amp; env-&gt;hideleg &amp; -vsie;</span><br><span class="line">    if (irqs) &#123;</span><br><span class="line">        virq = riscv_cpu_pending_to_irq(env, IRQ_S_EXT, IPRIO_DEFAULT_S,</span><br><span class="line">                                        irqs &gt;&gt; 1, env-&gt;hviprio);</span><br><span class="line">        return (virq &lt;= 0) ? virq : virq + 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /* Indicate no pending interrupt */</span><br><span class="line">    return RISCV_EXCP_NONE;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>下面分析下，qemu_set_irq/qemu_irq_raise如何触发riscv_cpu_set_irq被调用。</p>
<p>qdev_init_gpio_in(DeviceState *dev, qemu_irq_handler handler, int n)给CPU核创建<br>n个中断gpio接口，每个中断gpio接口的中断处理函数都是handler，这里的dev就是CPU核的<br>父类，下面展开这个函数看下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qdev_init_gpio_in</span><br><span class="line">     /*</span><br><span class="line">      * dev里有gpios，它是一个元素为NameGPIOList的链表，每个NameGPIOList是一组gpio，</span><br><span class="line">      * NameGPIOList里直接嵌入了qemu_irq，每一个gpio都有一个，这样每个gpio都可以</span><br><span class="line">      * 当一个中断管脚使用。</span><br><span class="line">      *</span><br><span class="line">      * 这个函数就是创建dev里n个这样的gpio, 这个n个gpio被一个NameGPIOList所管理。</span><br><span class="line">      */</span><br><span class="line">  +-&gt;qdev_init_gpio_in_named_with_opaque</span><br></pre></td></tr></table></figure>

<p>qdev_init_gpio_out(DeviceState *dev, qemu_irq *pins, int n)给dev创建n个中断输出<br>接口，这里可以把dev理解为中断控制器。需要注意的是，qemu_irq本来已经是指针了，这里<br>把指针的地址送进来是为了后续配置qemu_irq这个指针的值。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qdev_init_gpio_out</span><br><span class="line">  +-&gt; qdev_init_gpio_out_named</span><br><span class="line">        /*</span><br><span class="line">	 * 给dev增加link属性，每个qemu_irq都增加了一个，link属性的名字是：unnamed-gpio-out[num],</span><br><span class="line">	 */</span><br><span class="line">    +-&gt; object_property_add_link</span><br></pre></td></tr></table></figure>

<p>qdev_connect_gpio_out_named(DeviceState *dev, const char *name, int n, qemu_irq input_pin)<br>这个函数在中断控制器驱动里被调用，dev是中断控制器，input_pin是CPU核上的中断gpio,<br>通过name得到上面中断控制器增加的link属性的名字。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qdev_connect_gpio_out_named</span><br><span class="line">      /*</span><br><span class="line">       * 这个函数把dev里的名字是propname的link和input_pin建立一个link。这里的关键</span><br><span class="line">       * 就是要搞懂set_link的本质。这个函数内部会调用object一层的set函数，这个函数</span><br><span class="line">       * 把link属性所对应的变量的地址直接设置成被指向目标的地址，具体就是把上面</span><br><span class="line">       * gpio_out的qemu_irq换成了gpio_in的qemu_irq。这样，使用中断控制器的qemu_irq</span><br><span class="line">       * 就可以直接触发CPU核的中断回调函数，因为经过这么link过后，中断控制器的qemu_irq</span><br><span class="line">       * 已经就是CPU核的qemu_irq。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; object_property_set_link(OBJECT(dev), propname, OBJECT(input_pin), ...)</span><br></pre></td></tr></table></figure>

<p>中断控制器里在处理外设上报来的中断后，一般用qemu_set_irq/qemu_irq_raise把中断继续<br>上CPU核上报。需要注意的是，这里的qemu_irq是中断控制器上的，而上面的逻辑中，中断控制<br>器上的qemu_irq只是和CPU核的qemu_irq建立了link，但是，这里入参qemu_irq就是CPU核一侧<br>qemu_irq的指针，这样qemu_irq_raise中直接调用qemu_irq的handler发起中断的逻辑就都通了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu_irq_raise(qemu_irq irq)</span><br><span class="line">  +-&gt; qemu_set_irq</span><br><span class="line">    +-&gt; irq-&gt;handler</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>中断</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tcg中riscv page table walk分析</title>
    <url>/qemu-tcg%E4%B8%ADriscv-page-table-walk%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> 我们先看只有一级地址翻译的情况，地址翻译的基本逻辑是，CPU硬件使用VA去自动的查<br> 存放在内存里的页表，最终得到物理地址。没有虚拟化情况下，只需要做一级地址翻译就好，<br> 这种情况下，页表是放在物理内存上的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">block size          1G          2M          4K</span><br><span class="line">+---------+------------+----------+----------+-----------+</span><br><span class="line">|63     39|38        30|29      21|20      12|11        0|</span><br><span class="line">+---------+------------+----------+----------+-----------+</span><br><span class="line">            |              |              |        |</span><br><span class="line">            |              |              |        v</span><br><span class="line">            |              |              |    [11:0]  in-page offset</span><br><span class="line">            |              |              +--&gt; [20:12] L3 index</span><br><span class="line">            |              +--------------+--&gt; [29:21] L2 index</span><br><span class="line">            +--------------+--------------+--&gt; [38:30] L1 index</span><br><span class="line">            |              |              |</span><br><span class="line">        +---+--------------+--------------+--------------+</span><br><span class="line">        |   |              |              |              |</span><br><span class="line">SATP----+---+-&gt;+--------+  |              |              |</span><br><span class="line">        |   |  |...     |  |              |              |</span><br><span class="line">        |   |  +--------+  |              |              |</span><br><span class="line">        |   +-&gt;|        |--+-&gt;+--------+  |              |</span><br><span class="line">        |      +--------+  |  |...     |  |              |</span><br><span class="line">        |      |...     |  |  +--------+  |              |</span><br><span class="line">        |      +--------+  +-&gt;|        |--+-&gt;+--------+  |</span><br><span class="line">        |                     +--------+  |  |...     |  |</span><br><span class="line">        |                     |...     |  |  +--------+  |</span><br><span class="line">        |                     +--------+  +-&gt;|        |  |</span><br><span class="line">        |                                    +--------+  |</span><br><span class="line">        |                                    |...     |  |</span><br><span class="line">        | memory                             +--------+  |</span><br><span class="line">        +------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p> 在虚拟化两级地址翻译的场景下，要先把GVA翻译成GPA，再把GPA翻译成HPA，理解整个翻译<br> 逻辑的关键是明白第一级翻译用的页表是“放在”GPA上的。如下是整个两级地址翻译的流程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">block size          1G          2M          4K</span><br><span class="line">+---------+------------+----------+----------+-----------+</span><br><span class="line">|63     39|38        30|29      21|20      12|11        0|   VA</span><br><span class="line">+---------+------------+----------+----------+-----------+</span><br><span class="line">            |              |              |        |</span><br><span class="line">            |              |              |        v</span><br><span class="line">            |              |              |    [11:0]  in-page offset</span><br><span class="line">            |              |              +--&gt; [20:12] L3 index</span><br><span class="line">            |              +--------------+--&gt; [29:21] L2 index</span><br><span class="line">            +--------------+--------------+--&gt; [38:30] L1 index</span><br><span class="line">            |              |              |</span><br><span class="line">        +---+--------------+--------------+--------------+</span><br><span class="line">        |   |              |              |              |</span><br><span class="line">SATP----+---+-&gt;+--------+  |              |              |</span><br><span class="line">        |   |  |...     |  |              |              |</span><br><span class="line">        |   |  +--------+  |              |              |</span><br><span class="line">        | a +-&gt;|   | ^  |--+-&gt;+--------+  |              |</span><br><span class="line">        |      +---+-+--+  |  |...     |  |              |</span><br><span class="line">        |      |...| |  |  |  +--------+  |              |</span><br><span class="line">        |      +---+-+--+  +-&gt;|  |  ^  |--+-&gt;+--------+  |</span><br><span class="line">        |          | |    b   +--+--+--+  |  |...     |  |</span><br><span class="line">        |          | |        |..|  |  |  |  +--------+  |</span><br><span class="line">        |          | |        +--+--+--+  +-&gt;|  |  ^  |  |</span><br><span class="line">        |        1 | |           |  |    c   +--+--+--+  |</span><br><span class="line">        |          | |           |  |        |..|  |  |  |</span><br><span class="line">        | GPA      | |           |  |        +--+--+--+  |</span><br><span class="line">        +----------+-+-----------+--+-----------+--+-----+</span><br><span class="line">                   | |           |  |           |  |</span><br><span class="line">                   +-+-----------+  |           |  |</span><br><span class="line">                   | |              |           |  |</span><br><span class="line">                   +-+--------------+-----------+  |</span><br><span class="line">                   | | 2            | 3            | 4</span><br><span class="line">                   v +--------------+--------------+-----------------+</span><br><span class="line">               +---+--------------+--------------+--------------+    |</span><br><span class="line">               |   |              |              |              |    |</span><br><span class="line">HGATP----------+---+-&gt;+--------+  |              |              |    |</span><br><span class="line">               |   |  |...     |  |              |              |    |</span><br><span class="line">               |   |  +--------+  |              |              |    |</span><br><span class="line">               |   +-&gt;|        |--+-&gt;+--------+  |              |    |</span><br><span class="line">               |      +--------+  |  |...     |  |              |    |</span><br><span class="line">               |      |...     |  |  +--------+  |              |    |</span><br><span class="line">               |      +--------+  +-&gt;|        |--+-&gt;+--------+  |    |</span><br><span class="line">               |                     +--------+  |  |...     |  |    |</span><br><span class="line">               |                     |...     |  |  +--------+  |    |</span><br><span class="line">               |                     +--------+  +-&gt;|        |  |    |</span><br><span class="line">               |                                    +--------+  |    |</span><br><span class="line">               |                                    |...     |  |  load到页表里的地址</span><br><span class="line">               | HPA                                +--------+  |</span><br><span class="line">               +------------------------------------------------+</span><br></pre></td></tr></table></figure>
<p> 两级翻译的目的是找见如上VA对应的HPA，这个过程逻辑上分成两部分，第一部分是通过第一<br> 级翻译得到GPA，这个值存在如上c的地址上，我们只要完成第一级翻译的page table walk<br> 就可以得到这个值，第二级翻译是针对GPA再做一次第二级地址翻译得到HPA，其中因为第一<br> 部分的翻译使用的页表在GPA上，得到第一部分翻译要用的页表里的内容时就必须做第二级<br> 翻译，这会导致第一部分的翻译比第二部分的翻译复杂的多。</p>
<p> 整个第一部分翻译的流程是，硬件根据SATP中保存的页表基地址和VA的L1 index得到第一级<br> 页表的第一级页表项的地址，因为这个地址是GPA，硬件还要通过一次第二级地址翻译得到GPA<br> a对应的HPA，硬件继续访问这个HPA得到a处存储的数值，这个第二级地址翻译的过程，我们<br> 在上图中标记为1，可以看见这个过程需要进行4次内存访问，其中三次是访问页表项的内存，<br> 第四次是load到翻译得到的a地址对应的HPA上的值。得到的a地址上的值，是第一级地址翻译<br> 的第二级页表的基地址，它依然是一个GPA，这个基地址和VA的L2 index相加得到第一级地址<br> 翻译的第二级页表项的地址，要得到这个地址上的值，我们依然要像之前一样做一次第二级<br> 地址翻译，同样要做4次地址访问。依此类推，我们得到c地址上保存的值，这个过程又做了<br> 4次地址访问，从c地址上的值，可以得到VA对应的GPA。</p>
<p> 第二部分的地址翻译就是用上面得到GPA做一次第二级地址翻译得到对应的HPA，这个过程<br> 需要做3次地址访问。</p>
<p> 从整个分析可以看出，当做两级地址翻译时，三级页表在最差情况下要做15次内存访问才能<br> 完成整个翻译过程。</p>
<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p> 具体代码分析直接用注释的形式写到代码里了，可以参考<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3FlbXUvYmxvYi9iNWMzY2Y2N2NmNDM2N2U3M2U5ZDVmZDJiOGJiNzk3YWE3MGI4ZjZmL3RhcmdldC9yaXNjdi9jcHVfaGVscGVyLmMjTDEzMDU=">这里<i class="fa fa-external-link-alt"></i></span>。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>计算机体系结构</tag>
        <tag>页表</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tcg取指令逻辑分析</title>
    <url>/qemu-tcg%E5%8F%96%E6%8C%87%E4%BB%A4%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> qemu的基本的执行模型是翻译guest代码到tb和在host上执行tb两个动作交替进行，所以vCPU<br> 取指令这个动作可能发生在很多地方。它可以发生在翻译guest代码这个步骤，这时guest<br> 指令还没有翻译到tb里；它也可以发生在qemu搜tb哈希表这个步骤，qemu可能把相关的guest<br> 代码已经翻译到tb里了，所以也可以从这里“取指令”; qemu可以把tb chain起来，从一个tb<br> 直接跳到一个tb，我们也可以把这个tb跳转理解成一种取指令的方式。</p>
<p> 翻译guest代码要通过guest VA得到guest PA，进而的到host VA，然后就可以得到host VA<br> 上的guest指令，其中guest VA到guest PA可能TLB hit，也可能要经过page table walk，<br> 还有可能触发异常，借助软件把页补齐，guest PA和hostVA通常就是一个固定的偏移。<br> 指令翻译是一条一条进行的，但是终归在一个page内，所以在开始翻译一段指令之前可以<br> 先计算得到对应的host VA，这样后面的翻译，只要还是在相同page内，host VA就不变。</p>
<p> qemu搜tb哈希表时，要先做地址翻译得到guest VA对应的guest PA，然后用guest PA以及<br> 其它参数作为key去tb哈希表里搜索。</p>
<p> chain tb之间来回跳转，但是qemu规定chain tb只能在一个page的范围内，这样一旦guest<br> 代码在不同的page里，qemu取指令就不能直接跳过去，它必须要么做guest代码翻译要么做<br> tb哈希表搜索，而这两者都做guest VA到guest PA的地址翻译。</p>
<p> 那么我们能否去掉chain tb只能在一页内这个限制？相关的逻辑可以参考<a href="https://wangzhou.github.io/qemu-tcg-goto-tb%E5%88%86%E6%9E%90/">这里</a>。</p>
<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p> 如下是qemu翻译执行时取指令的代码逻辑：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tb_gen_code</span><br><span class="line">      /*</span><br><span class="line">       * phys_pc是guest PA，pc是guest VA，host_pc是host VA。在tb翻译开始时，先</span><br><span class="line">       * 得到各种地址。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; phys_pc = get_page_addr_code_hostp(env, pc, &amp;host_pc);                      </span><br><span class="line">        /*</span><br><span class="line">         * 先看TLB是否命中，TLB不命中会触发page table walk，tlb_fill里的page</span><br><span class="line">         * table walk失败后会直接触发异常。</span><br><span class="line">         *</span><br><span class="line">         * TLB相关的分析可以参考[这里](todo)。</span><br><span class="line">         */</span><br><span class="line">    +-&gt; probe_access_internal</span><br><span class="line"></span><br><span class="line">  +-&gt; gen_intermediate_code(cpu, tb, max_insns, pc, host_pc);                     </span><br><span class="line">    +-&gt; translator_loop(cs, tb, max_insns, pc, host_pc, &amp;riscv_tr_ops, &amp;ctx.base);  </span><br><span class="line"></span><br><span class="line">      +-&gt; db-&gt;host_addr[0] = host_pc;                                                 </span><br><span class="line">          /* 反复执行单条指令的翻译 */</span><br><span class="line">      +-&gt; riscv_tr_translate_insn(DisasContextBase *dcbase, CPUState *cpu);</span><br><span class="line">            /* 得到当前指令的编码 */</span><br><span class="line">        +-&gt; opcode16 = translator_lduw(env, &amp;ctx-&gt;base, ctx-&gt;base.pc_next);    </span><br><span class="line">              /*</span><br><span class="line">               * 这里会直接用上面db-&gt;host_addr[0]里的值的到host VA，但是也可能</span><br><span class="line">               * guest的指令会跨过页边界，这个时候就有可能要再走一下get_page_addr_code_hostp</span><br><span class="line">               * 的流程，得到下一页的host VA。</span><br><span class="line">               */</span><br><span class="line">          +-&gt; void *p = translator_access(env, db, pc, sizeof(ret));                      </span><br><span class="line">              /* 如果可以得到host VA，那么直接memcpy就可以得到指令编码 */</span><br><span class="line">          +-&gt; lduw_p(p);</span><br><span class="line">              /* 不明白什么逻辑会走到这里? */</span><br><span class="line">          +-&gt; cpu_lduw_code(env, pc);</span><br><span class="line">            +-&gt; load_helper</span><br></pre></td></tr></table></figure>

<p> 如下tb搜索时的逻辑：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tb_lookup</span><br><span class="line">  +-&gt; tb_htable_lookup(cpu, pc, cs_base, flags, cflags);                     </span><br><span class="line">    +-&gt;phys_pc = get_page_addr_code(desc.env, pc);                                 </span><br><span class="line">      +-&gt; get_page_addr_code_hostp(env, addr, NULL);                           </span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tcg后端翻译基本模型</title>
    <url>/qemu-tcg%E5%90%8E%E7%AB%AF%E7%BF%BB%E8%AF%91%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="qemu后端翻译干的是件什么事"><a href="#qemu后端翻译干的是件什么事" class="headerlink" title="qemu后端翻译干的是件什么事"></a>qemu后端翻译干的是件什么事</h2><p>qemu用tcg模拟guest指令执行，qemu把guest指令先翻译成中间码，然后再把中间码翻译成<br>host指令，host指令可以最终在host cpu上执行，这样就完成了翻译。</p>
<p>本文我们关注的是后端翻译模型，也就是中间码翻译成host指令的过程。中间码是一套完整<br>的指令集定义，使用中间码可以完整的表述guest指令的行为，看一个小例子对这种描述会<br>又更直观的感受。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">addi            sp,sp,-32                  &lt;-- guest汇编</span><br><span class="line">sd              s0,24(sp)</span><br><span class="line"></span><br><span class="line">add_i64 x2/sp,x2/sp,$0xffffffffffffffe0    &lt;-- 中间码</span><br><span class="line">add_i64 tmp4,x2/sp,$0x18</span><br><span class="line">qemu_st_i64 x8/s0,tmp4,leq,0</span><br></pre></td></tr></table></figure>
<p>我们看如上guest那条store指令，它被翻译成了两条中间码，第一条add_i64是用来计算sd<br>要store的地址，计算出的地址保存在tmp4这个虚拟寄存器里，第二条中间码把s0的值store<br>到tmp4描述的内存上，qemu用中间码和虚拟寄存器完整的表述guest的逻辑。这里qemu_st_i64<br>这个中间码表示一个store操作，store的数据和地址都用虚拟寄存器描述，所以在qemu_st_i64<br>之前要用add_i64先计算出store的地址，并保存在虚拟寄存器里。</p>
<p>qemu中，其它的guest指令也是这样先翻译成中间码和虚拟寄存器的表示，后端翻译基于中间<br>码和虚拟寄存器进行。上面的中间码表述中，x2/sp和x8/s0还是guest上寄存器的名字，但是<br>逻辑上guest上的寄存器都已经映射到qemu虚拟寄存器，所以中间码指令中的所有寄存器都是<br>qemu的虚拟寄存器。</p>
<p>现在假设我们已经得到一串中间码，我们再翻过头来看看要怎么完成模拟。qemu模拟的guest<br>cpu系统说到底就是host内存里表示的guest cpu的软件结构体的状态以及guest内存的状态，<br>qemu中间码已经完整的描述了guest状态改变的激励，拿上面addi和sd guest指令的模拟为例，<br>模拟addi的中间码是addi_64 x2/sp,x2/sp,$0xffffffffffffffe0，表示要把guest的sp加上<br>-32，sd的中间码表示要把guest sp + 24指向的地址上的值改成s0的值。我们拿到如上中间码<br>或者guest指令，甚至可以直接写c代码去完成模拟。qemu为了追求效率把中间码翻译成host<br>指令来完成模拟。</p>
<p>我们把上面的几条中间码写在一起：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add_i64 x2/sp,x2/sp,$0xffffffffffffffe0</span><br><span class="line">add_i64 tmp4,x2/sp,$0x18</span><br><span class="line">qemu_st_i64 x8/s0,tmp4,leq,0</span><br></pre></td></tr></table></figure>
<p>这几条中间码只是表意，实际真正更新guest cpu的数据结构和guest地址还需要host指令完成，<br>所以实际翻译后的host指令可能是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ldr      x20, [x19, #0x10]    把guest cpu中的sp load到host的x20寄存器</span><br><span class="line">sub      x20, x20, #0x20      使用host sub指令完成guest sp的计算</span><br><span class="line">str      x20, [x19, #0x10]    更新guest cpu中sp的值</span><br><span class="line">add      x21, x20, #0x18      使用host add指令计算store的地址，并保存到host的x21寄存器</span><br><span class="line">ldr      x22, [x19, #0x40]    把guest cpu中的s0 load到host的x22寄存器</span><br><span class="line">str      x22, [x21, xzr]      使用host str指令更新guest地址上的值</span><br></pre></td></tr></table></figure>
<p>qemu的后端翻译就是完成如上功能，总结起来就是：1. 分配host物理寄存器；2. 生成host<br>指令；3. host和guest之间的状态同步。</p>
<h2 id="分配host物理寄存器"><a href="#分配host物理寄存器" class="headerlink" title="分配host物理寄存器"></a>分配host物理寄存器</h2><p>先看下分配host物理寄存器会遇到什么问题。首先，虚拟寄存器和host物理寄存器是两个独<br>立的概念，虚拟寄存器可能会很多，而物理寄存器的个数是有限的，虚拟寄存器有自己的生<br>命周期，虚拟寄存器生命周期结束后，它所使用的物理寄存器就可以给其它虚拟寄存器使用。<br>因为host物理寄存器数目有限，就有可能出现host物理寄存器不够分的情况，这时候就需要<br>把已经分配但是目前还没有用到的host物理寄存器的值保存到内存，这样就可以腾出host物<br>理寄存器来使用。</p>
<p>qemu在处理host物理寄存器分配的时候，分了两步处理，第一步先确定虚拟寄存器的生命周<br>期，一般叫做寄存器活性分析，第二步根据虚拟寄存器活性分析的结果具体分配物理寄存器。</p>
<p>针对一段中间码，qemu对其做逆序遍历，依此确定虚拟寄存器的生命周期。如果一个虚拟寄<br>存器后续还中间码使用，那它还是live的，如果后面没有中间码用了，它就dead了。</p>
<p>所以，一个虚拟寄存器dead与否是和具体中间码一起看的，一个虚拟寄存器可能在前几个中<br>间码中是live的(虽然这几个中间码并没有使用这个虚拟寄存器)，最后一个使用它的中间码<br>后这个虚拟寄存器就dead了。qemu里只要记录虚拟寄存器被引用时的状态就好。</p>
<p>我们随便截取一段中间码看下，每条中间码后面数字表示出哪个虚拟寄存器dead了，其中0、<br>1、2等等表示这条中间码的第0个第1个第2个虚拟寄存器dead了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add_i64 x2/sp,x2/sp,$0xffffffffffffffe0  sync: 0  dead: 1 2  pref=0x3ff80000</span><br><span class="line">add_i64 tmp4,x2/sp,$0x18                 pref=0xffffffff</span><br><span class="line">qemu_st_i64 x8/s0,tmp4,leq,0             dead: 0 1</span><br><span class="line">add_i64 x8/s0,x2/sp,$0x20                sync: 0  pref=0x3ff80000</span><br><span class="line">mov_i64 x15/a5,$0x1                      sync: 0  dead: 0  pref=0xffffffff   &lt;- 8</span><br><span class="line">add_i64 tmp4,x8/s0,$0xffffffffffffffe4   pref=0xffffffff</span><br><span class="line">qemu_st_i64 $0x1,tmp4,leul,0             dead: 0 1</span><br><span class="line">mov_i64 x15/a5,$0x2                      sync: 0  dead: 0  pref=0xffffffff   &lt;- 7</span><br><span class="line">add_i64 tmp4,x8/s0,$0xffffffffffffffe8   pref=0xffffffff</span><br><span class="line">qemu_st_i64 $0x2,tmp4,leul,0             dead: 0 1</span><br><span class="line">add_i64 tmp4,x8/s0,$0xffffffffffffffec   pref=0xffffffff</span><br><span class="line">qemu_st_i64 $0x0,tmp4,leul,0             dead: 0 1</span><br><span class="line">add_i64 tmp4,x8/s0,$0xffffffffffffffe4   dead: 2  pref=0xffffffff</span><br><span class="line">qemu_ld_i64 x14/a4,tmp4,lesl,0           sync: 0  dead: 1  pref=0x3ff80000</span><br><span class="line">add_i64 tmp4,x8/s0,$0xffffffffffffffe8   dead: 2  pref=0xffffffff</span><br><span class="line">qemu_ld_i64 x15/a5,tmp4,lesl,0           dead: 1  pref=0xffffffff            &lt;- 6 ---</span><br><span class="line">add_i64 tmp4,x15/a5,x14/a4               dead: 1 2  pref=0xffffffff          &lt;- 5</span><br><span class="line">ext32s_i64 x15/a5,tmp4                   sync: 0  dead: 1  pref=0xffffffff   &lt;- 4 ---</span><br><span class="line">add_i64 tmp4,x8/s0,$0xffffffffffffffec   pref=0xffffffff</span><br><span class="line">qemu_st_i64 x15/a5,tmp4,leul,0           dead: 0 1                           &lt;- 3</span><br><span class="line">add_i64 tmp4,x8/s0,$0xffffffffffffffec   dead: 1 2  pref=0xffffffff</span><br><span class="line">qemu_ld_i64 x15/a5,tmp4,lesl,0           sync: 0  dead: 1  pref=0xffffffff   &lt;- 2 ---</span><br><span class="line">mov_i64 x10/a0,x15/a5                    sync: 0  dead: 0 1  pref=0xffffffff &lt;- 1</span><br></pre></td></tr></table></figure>
<p>我们拿x15寄存器为例，逆序遍历的时，位置1的中间码中的x15(下面叫p1的x15)后面没有中<br>间码用了，所以p1的x15是dead。p2的x15因为在p1要用，所以p2的x15是live。p3的x15是dead，<br>因为p2直接直接更新了x15，就是p3以下没有中间码使用p3的x15。p4的x15是live，因为p3<br>要使用p4的x15。p5的x15是dead，因为p4刷新了x15的值，p5以下没有中间码引用p5的x15。<br>p6/p7/p8的x15同样分析。</p>
<p>qemu在寄存器活性分析阶段，把每个中间码里的虚拟寄存器的活性做好标记，依次存到每个<br>中间码的life域段。然后再顺序遍历每个中间码分配物理寄存器。还是用上面这段中间码举<br>例，比如为p8的x15分配了host物理寄存器x20，因为p8的x15是dead，那么host这个x20寄存<br>器就可以在p8这条中间码后继续参与host物理寄存器分配，但是对于p4的x15，比如给p4的<br>x15分配了host物理寄存器x21，这个x21就必须一直被占用着，因为p3要用p4的x15，p3之后<br>x15 dead，对应的物理寄存器x21就可以再次参与host物理寄存器分配。</p>
<h2 id="生成host指令以及状态同步"><a href="#生成host指令以及状态同步" class="headerlink" title="生成host指令以及状态同步"></a>生成host指令以及状态同步</h2><p>我们把状态同步和host指令生成放到一起看，因为所谓状态同步也要生成host指令进行。</p>
<p>对于中间码的输入虚拟寄存器，需要先判断这个输入寄存器的值是保存在内存上，还是已经<br>保存在host物理寄存器上了，如果还在内存上，qemu就要分配host物理寄存器，然后插入host<br>上的load指令把内存上的值load到host物理寄存器上，如果虚拟寄存器的值已经在host物理<br>寄存器上，那么它直接就可以参与计算。对于中间码的输出虚拟寄存器，qemu需要为它分配<br>host物理寄存器。</p>
<p>中间码的输入和输出寄存器都有着落了，qemu就可以尝试把中间码翻译成host指令。这个翻<br>译可能直接就可以翻译成一条host指令，也可能需要再插入几条host指令调整下。</p>
<p>guest指令对应的中间码执行完后，需要把guest指令的输出同步回guest CPU数据结构，所以<br>qemu在这里还需要插入host store指令把数据刷回guest CPU。qemu在寄存器活性分析的时候<br>会把需要做同步的虚拟寄存器打上sync的标记，生成host指令的时候遇见sync标记就可以直接<br>插入host指令做同步。</p>
<p>并不需要每个guest指令执行完都要把信息刷回guest CPU数据结构，虽然guest CPU的信息<br>是定义在guest CPU数据结构中的，但是我们是模拟guest CPU，只要不破坏模拟的逻辑，host<br>物理寄存器上的值就可以先不刷回guest CPU数据结构。那什么时候需要刷回guest CPU，整个<br>TB执行完时，虚拟寄存器需要被同步回guest CPU，当中间码可能导致guest CPU异常时，需要<br>做同步，因为触发异常后，guest CPU跳转到异常处理地址，并且向软件报告异常处理的上下<br>文，其中guest CPU的通用寄存器就都是从guest CPU数据结构获取。</p>
<h2 id="加入BB的概念"><a href="#加入BB的概念" class="headerlink" title="加入BB的概念"></a>加入BB的概念</h2><p>上面讲的寄存器分配和状态同步其实还不完整，如上的中间码里是没有跳转的(br/brcond)，<br>qemu的一个翻译块(TB)里是可以存在跳转中间码的，在有跳转中间码的情况下，上面逆序遍<br>历确定虚拟寄存器活性的办法就会有问题。为此qemu中在TB的基础上又引入了Basic Block(BB)<br>的概念，简单讲在一个BB内中间码都是顺序执行的，这样如上的逻辑在BB内还是成立的。所以，<br>在BB的结尾就要dead全部虚拟寄存器，并且把guest CPU对应的虚拟寄存器向guest CPU数据<br>结构做同步。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tcg访存指令模拟</title>
    <url>/qemu-tcg%E8%AE%BF%E5%AD%98%E6%8C%87%E4%BB%A4%E6%A8%A1%E6%8B%9F/</url>
    <content><![CDATA[<h2 id="qemu-load-store基本流程模拟"><a href="#qemu-load-store基本流程模拟" class="headerlink" title="qemu load/store基本流程模拟"></a>qemu load/store基本流程模拟</h2><p>qemu里load/store分两种，一种是load/store guest地址，对应的中间码是qemu_ld/st_xxx，<br>另一种是load/store的是host地址，对应的中间码是ld/st_xxx，这一节中我们讨论的是前者。<br>qemu里load/store在system mode和user mode下的模拟又是不一样的，到了具体地方，我们<br>会具体说明下。</p>
<p>我们从一个普通的load指令的模拟代码入手分析。<br>/* qemu/target/riscv/insn_trans/trans_rvi.c.inc */</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">trans_lw</span><br><span class="line">  -&gt; gen_load</span><br><span class="line">    -&gt; tcg_gen_qemu_ld_tl</span><br><span class="line">      -&gt; gen_ldst_i64 INDEX_op_qemu_ld_i64</span><br></pre></td></tr></table></figure>
<p>如上，load操作的中间码是用gen_ldst_i64生成的，中间码的op code是INDEX_op_qemu_ld_i64。</p>
<p>正常的流程，翻译成中间码后，还要有中间码翻译成host指令的过程，qemu把load/store里<br>处理tlb访问、页表page walk、发起缺页异常的这些操作插到了把中间码翻译成host指令的过程中。<br>接着上面INDEX_op_qemu_ld_i64往下分析。</p>
<p>中间码翻译成host指令在tcg_gen_code函数里，tcg_gen_code一路调用下去，会到<br>tcg/riscv/tcg-target.c.inc里的tcg_out_qemu_ld_slow_path。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_gen_code</span><br><span class="line">     /* tcg/riscv/tcg-target.c.inc, 这里我们假设host平台也是riscv */</span><br><span class="line">  -&gt; tcg_out_op</span><br><span class="line">       /* 可以看到这里分了有MMU和没有MMU的情况，一般user mode是没有MMU的 */</span><br><span class="line">    -&gt; tcg_out_qemu_ld</span><br><span class="line">         /*</span><br><span class="line">          * 有MMU的情况，tlb_load这个函数检测是否tlb可以直接命中，如果命中，直接</span><br><span class="line">          * 就load数据了。没有MMU的情况, 计算地址后直接用tcg_out_qemu_ld_direct</span><br><span class="line">          * 生成的host指令load数据，需要注意的是没有MMU的时候，必然也没有tlb了,</span><br><span class="line">          * 另一个需要注意的点是，没有MMU的时候，qemu会直接load/store guest PA，</span><br><span class="line">          * 但是guest PA和host VA逻辑上并不相等，两者之差保存在全局变量guest_base</span><br><span class="line">          * 中，并在tcg_target_qemu_prologue里传递给TCG_GUEST_BASE_REG寄存器，</span><br><span class="line">          * 所以可以看到这里会结合TCG_GUEST_BASE_REG计算出对应的host VA，然后</span><br><span class="line">          * 针对host VA做load。</span><br><span class="line">          *</span><br><span class="line">          * tlb查找直接用tb里生成的host汇编指令做了，得到pa放到TMP0寄存器里,</span><br><span class="line">          * 如下生成的host汇编里插入了跳转指令，如果tlb有命中就继续执行</span><br><span class="line">          * tcg_out_qemu_ld_direct里生成的汇编指令，如果tlb没有命中就跳转到慢速</span><br><span class="line">          * 路径的代码上执行，后端翻译到这里的时候还不能确定慢速路径的跳转目的</span><br><span class="line">          * 地址是什么，所以可以看到如下函数里生成的跳转指令的目标先配置成了0。</span><br><span class="line">          */</span><br><span class="line">      -&gt; tcg_out_tlb_load(s, addr_regl, addr_regh, oi, label_ptr, 1)</span><br><span class="line">         /*</span><br><span class="line">          * tlb hit时，直接做load, 上一步得到的guest物理地址，计算得到host虚拟</span><br><span class="line">          * 地址，通过寄存器传递给下面生成的指令。guest物理地址和host虚拟地址</span><br><span class="line">          * 只相差一个addend。</span><br><span class="line">          */</span><br><span class="line">      -&gt; tcg_out_qemu_ld_direct(s, data_regl, data_regh, base, opc, is_64)</span><br><span class="line">         /*</span><br><span class="line">          * 创建一个lable的描述信息，放到后端解码的上下文里，这里没有生成后端指令。</span><br><span class="line">          *</span><br><span class="line">          * 慢速路径的返回地址: label-&gt;raddr = tcg_splitwx_to_rx(code_ptr)</span><br><span class="line">          * 跳转指令的地址记录在: label-&gt;label_ptr[0]</span><br><span class="line">          */</span><br><span class="line">      -&gt; add_qemu_ldst_label(s, 1, oi, (is_64 ? TCG_TYPE_I64 : TCG_TYPE_I32),</span><br><span class="line">                             data_regl, data_regh, addr_regl, addr_regh,</span><br><span class="line">                             s-&gt;code_ptr, label_ptr)</span><br><span class="line">         /*                                                      </span><br><span class="line">          *    注意这里code_ptr已经是上面ld_direct生成指令的后面 </span><br><span class="line">          *    一个地址了，而label_ptr里的内容还是bne指令对应的  </span><br><span class="line">          *    地址。                                            </span><br><span class="line">          *                tb code buff                          </span><br><span class="line">          *                  ---+---                             </span><br><span class="line">          *    find tlb ---&gt;    |                                </span><br><span class="line">          *                     |                                </span><br><span class="line">          *                     |                                </span><br><span class="line">          *                     |                                </span><br><span class="line">          *                    bne &lt;-------- label_ptr[0]     --------+</span><br><span class="line">          *                     |                                     |</span><br><span class="line">          *    direct ld ---&gt;   |                                     |</span><br><span class="line">          *                     |                                     |</span><br><span class="line">          *                    -+-                                    |</span><br><span class="line">          *    code_ptr ------&gt; |  &lt;------ other code  &lt;---------+    |</span><br><span class="line">          *                     |                                |    |</span><br><span class="line">          *                    -+-                               |    |</span><br><span class="line">          *                     |  &lt;------ ld slow path code  &lt;--+----+</span><br><span class="line">          *                     |                                |</span><br><span class="line">          *                    goto  ----------------------------+</span><br><span class="line">          *</span><br><span class="line">          * 所以，bne需要跳到慢速路径上，慢速路径的执行结果有两种:</span><br><span class="line">          * 1. 找见了pa，填充了tlb，这种情况应该回到快速路径上执行，</span><br><span class="line">          *    可以看到慢速路径后面是返回到了code_ptr这里继续执行。</span><br><span class="line">          * 2. 有异常产生，软件处理异常后，重新从ld指令开始翻译执行。</span><br><span class="line">          */</span><br><span class="line">     /*</span><br><span class="line">      * tcg/tcg-ldst.c.inc, 解析ldst label, 生成慢速路径的代码，补上上面bne的</span><br><span class="line">      * 跳转目的地址。</span><br><span class="line">      */</span><br><span class="line">  -&gt; tcg_out_ldst_finalize</span><br><span class="line">       /* tcg/riscv/tcg-target.c.inc */</span><br><span class="line">    -&gt; tcg_out_qemu_ld_slow_path</span><br><span class="line">         /*</span><br><span class="line">          * label_ptr[0]保存的是bne的地址, 但是注意这里的code_ptr是慢速路径指令</span><br><span class="line">          * 的起始地址，如下函数找见上面bne指令的地址，然后把慢速路径的跳转地址</span><br><span class="line">          * 补起来。</span><br><span class="line">          */                                         </span><br><span class="line">      -&gt; reloc_sbimm12(l-&gt;label_ptr[0], tcg_splitwx_to_rx(s-&gt;code_ptr)))</span><br><span class="line"></span><br><span class="line">         /* 各种不同数据宽度的load函数被定义到qemu_ld_helpers这个表里 */</span><br><span class="line">      -&gt; tcg_out_call(s, qemu_ld_helpers[opc &amp; MO_SSIZE])</span><br><span class="line">            /* 取ldul为例子，这是一个公共函数，定义在accel/tcg/cputlb */</span><br><span class="line">         -&gt; helper_le_ldul_mmu</span><br><span class="line">              /* accel/tcg/cputlb.c */</span><br><span class="line">           -&gt; load_helper</span><br><span class="line"></span><br><span class="line">         /* 如上的函数要把数据放到a0里，所以慢速路径是要把整个load执行完 */</span><br><span class="line">      -&gt; tcg_out_mov(s, (opc &amp; MO_SIZE) == MO_64, l-&gt;datalo_reg, a0)</span><br><span class="line">         /* 跳到direct ld结束的地址 */</span><br><span class="line">      -&gt; tcg_out_goto(s, l-&gt;raddr)</span><br></pre></td></tr></table></figure>

<p>在load_helper里做如上提到的load的各种硬件模拟。load_helper里对于没有找到tlb的情况<br>会调用体系构架相关的回调函数做tlb fill：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tlb_fill</span><br><span class="line">     /* target/riscv/cpu.c */</span><br><span class="line">  -&gt; cc-&gt;tcg_ops_tlb_fill risv下是riscv_cpu_tlb_fill</span><br><span class="line">       /*</span><br><span class="line">        * 先做page walk，翻译成功自然可以，翻译失败报缺页异常，我们把page walk</span><br><span class="line">        * 的逻辑分析独立放到下面。</span><br><span class="line">        */</span><br><span class="line">    -&gt; riscv_raise_exception</span><br></pre></td></tr></table></figure>

<h2 id="riscv-page-walk分析"><a href="#riscv-page-walk分析" class="headerlink" title="riscv page walk分析"></a>riscv page walk分析</h2><p> qemu riscv page walk基本逻辑可以参考<a href="https://wangzhou.github.io/qemu-tcg%E4%B8%ADriscv-page-table-walk%E5%88%86%E6%9E%90/">这里</a>。</p>
<h2 id="load-store-host地址"><a href="#load-store-host地址" class="headerlink" title="load/store host地址"></a>load/store host地址</h2><p> 我们分析riscv上的ld_i64的实现，它对应的中间码是INDEX_op_ld_i64。可以看见qemu后端<br> 会直接把中间码翻译成host load/store指令，当中间码里的offset参数和host load/store<br> 指令里的offset立即数的位宽不匹配的时候，qemu后端会做必要的调整。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/riscv/tcg-target.c.inc */</span><br><span class="line">tcg_out_ldst</span><br><span class="line">  /*</span><br><span class="line">   * 根据load/store的基地址寄存器的分配情况决定host load/store offset立即数位宽</span><br><span class="line">   * 不够的时候该怎么办: 基地址寄存器分配了0寄存器时要加入auipc计算地址，基地址</span><br><span class="line">   * 寄存器可用时，只要重新调整下基地址寄存器里的数值就好。</span><br><span class="line">   */</span><br><span class="line">  [...]</span><br><span class="line">  /* 直接生成host load/store指令 */</span><br><span class="line">  tcg_out_opc_store()/tcg_out_opc_imm()</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tcg模拟原子指令</title>
    <url>/qemu-tcg%E6%A8%A1%E6%8B%9F%E5%8E%9F%E5%AD%90%E6%8C%87%E4%BB%A4/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* target/riscv/insn_trans/trans_rva.c.inc */</span><br><span class="line">trans_amoadd_w</span><br><span class="line">  -&gt; gen_amo</span><br><span class="line">    -&gt; tcg_gen_atomic_fetch_add_tl</span><br><span class="line">      -&gt; tcg_gen_atomic_fetch_add_i64</span><br></pre></td></tr></table></figure>
<p>如上最后一个函数的定义在：tcg/tcg-op.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define GEN_ATOMIC_HELPER(NAME, OP, NEW)                                \</span><br><span class="line">static void * const table_##NAME[(MO_SIZE | MO_BSWAP) + 1] = &#123;          \</span><br><span class="line">    [MO_8] = gen_helper_atomic_##NAME##b,                               \</span><br><span class="line">    [MO_16 | MO_LE] = gen_helper_atomic_##NAME##w_le,                   \</span><br><span class="line">    [MO_16 | MO_BE] = gen_helper_atomic_##NAME##w_be,                   \</span><br><span class="line">    [MO_32 | MO_LE] = gen_helper_atomic_##NAME##l_le,                   \</span><br><span class="line">    [MO_32 | MO_BE] = gen_helper_atomic_##NAME##l_be,                   \</span><br><span class="line">    WITH_ATOMIC64([MO_64 | MO_LE] = gen_helper_atomic_##NAME##q_le)     \</span><br><span class="line">    WITH_ATOMIC64([MO_64 | MO_BE] = gen_helper_atomic_##NAME##q_be)     \</span><br><span class="line">&#125;;                                                                      \</span><br><span class="line">void tcg_gen_atomic_##NAME##_i32                                        \</span><br><span class="line">    (TCGv_i32 ret, TCGv addr, TCGv_i32 val, TCGArg idx, MemOp memop)    \</span><br><span class="line">&#123;                                                                       \</span><br><span class="line">    if (tcg_ctx-&gt;tb_cflags &amp; CF_PARALLEL) &#123;                             \</span><br><span class="line">        do_atomic_op_i32(ret, addr, val, idx, memop, table_##NAME);     \</span><br><span class="line">    &#125; else &#123;                                                            \</span><br><span class="line">        do_nonatomic_op_i32(ret, addr, val, idx, memop, NEW,            \</span><br><span class="line">                            tcg_gen_##OP##_i32);                        \</span><br><span class="line">    &#125;                                                                   \</span><br><span class="line">&#125;                                                                       \</span><br><span class="line">void tcg_gen_atomic_##NAME##_i64                                        \</span><br><span class="line">    (TCGv_i64 ret, TCGv addr, TCGv_i64 val, TCGArg idx, MemOp memop)    \</span><br><span class="line">&#123;                                                                       \</span><br><span class="line">    if (tcg_ctx-&gt;tb_cflags &amp; CF_PARALLEL) &#123;                             \</span><br><span class="line">        do_atomic_op_i64(ret, addr, val, idx, memop, table_##NAME);     \</span><br><span class="line">    &#125; else &#123;                                                            \</span><br><span class="line">        do_nonatomic_op_i64(ret, addr, val, idx, memop, NEW,            \</span><br><span class="line">                            tcg_gen_##OP##_i64);                        \</span><br><span class="line">    &#125;                                                                   \</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>do_atomic_op_i64里会调用gen_helper_atomic_add_xxx，这个函数的定义在:<br>accel/tcg/atomic_template.h</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define GEN_ATOMIC_HELPER(X)                                        \</span><br><span class="line">ABI_TYPE ATOMIC_NAME(X)(CPUArchState *env, target_ulong addr,       \</span><br><span class="line">                        ABI_TYPE val, MemOpIdx oi, uintptr_t retaddr) \</span><br><span class="line">&#123;                                                                   \</span><br><span class="line">    DATA_TYPE *haddr = atomic_mmu_lookup(env, addr, oi, DATA_SIZE,  \</span><br><span class="line">                                         PAGE_READ | PAGE_WRITE, retaddr); \</span><br><span class="line">    DATA_TYPE ret;                                                  \</span><br><span class="line">    atomic_trace_rmw_pre(env, addr, oi);                            \</span><br><span class="line">    ret = qatomic_##X(haddr, val);                                  \</span><br><span class="line">    ATOMIC_MMU_CLEANUP;                                             \</span><br><span class="line">    atomic_trace_rmw_post(env, addr, oi);                           \</span><br><span class="line">    return ret;                                                     \</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，里面还是使用的host平台上的基本的原子语义函数做的。</p>
<p>如果需要模拟多条指令拼起来的原子指令，我们就考虑用锁保护。要保护的对象是内存的状态。<br>之所以需要保护，是多CPU可能会去改相同的内存位置。qemu使用一个线程模拟一个CPU，<br>所以一个CPU对本CPU的寄存器的更新总是顺序的，所以CPU的寄存器状态是不需要做互斥的。</p>
<p>对于无法映射到host上原子指令的情况，其实qemu里已经做了处理，我们也可以直接使用<br>qemu中的方式处理。我们可以参考qemu对i386 cmpxchg16b指令的处理：qemu/target/i386/tcg/mem_helper.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helper_cmpxchg16b</span><br><span class="line">  +-&gt; cpu_loop_exit_atomic</span><br><span class="line">    +-&gt; cpu-&gt;exception_index = EXCP_ATOMIC;</span><br><span class="line">    +-&gt; cpu_loop_exit_restore(cpu, pc);</span><br><span class="line">      +-&gt; cpu_restore_state</span><br><span class="line">      +-&gt; cpu_loop_exit(cpu);</span><br></pre></td></tr></table></figure>
<p>如上，把CPU的状态设置为atomic异常，回退当前guest PC，这个使得下次再进来的时候可以<br>使指令再次执行。最后用长跳转跳出整个tb翻译执行的大循环。可以从<br>accel/tcg/tcg-accel-ops-mttcg.c中的CPU线程代码看相关调用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mttcg_cpu_thread_fn</span><br><span class="line">  +-&gt; tcg_cpus_exec</span><br><span class="line">  +-&gt; cpu_exec_step_atomic</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>
<p>可以看到cpu_exec_step_atomic里有tb的翻译执行的小循环。这里需要注意的地方有，tb<br>翻译执行是在一个互斥区里，执行tb翻译执行之前把这个tb配置成了只容许有一条guest指令，<br>这样做是为了使临界区尽量小。相应的cmpxchg16b翻译执行跑两遍，第一遍触发发原子异常，<br>第二遍跑到同样的位置会进入一个无锁的实现里执行一遍：target/i386/tcg/translate.c:<br>gen_helper_cmpxchg16b_unlocked，控制进哪个分支的逻辑是tb cflags的CF_PARALLEL，<br>在进入cpu_exec_step_atomic的时候会把这个标记为去掉，翻译的时候就会进入相应的代码，<br>产生相应的tb，如果是直接lookup执行这个tb，注意tb lookup的参数里也包含了<br>tb的cflags。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tcg翻译执行核心逻辑分析</title>
    <url>/qemu-tcg%E7%BF%BB%E8%AF%91%E6%89%A7%E8%A1%8C%E6%A0%B8%E5%BF%83%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="翻译执行循环基本逻辑"><a href="#翻译执行循环基本逻辑" class="headerlink" title="翻译执行循环基本逻辑"></a>翻译执行循环基本逻辑</h2><p> 整个模拟CPU执行指令的过程就是一个不断翻译执行的循环，当指令执行过程中有中断或者<br> 异常，整个翻译执行的循环被打断，处理中断或者异常。异常是执行指令的时候触发的，qemu<br> 在翻译执行的时候通过一个长跳转跳出循环，处理完异常，异常改变CPU状态和PC，qemu处理完<br> 异常后，从新PC位置继续翻译执行(这个新PC一般就是异常处理向量的入口)。中断是外设<br> 异步产生的，qemu在每次翻译执行的循环执行一次后，再次执行翻译执行之前检查下中断，<br> 如果有中断，qemu就处理中断，和异常一样，qemu改变CPU状态和PC后，再次进入翻译执行<br> 的循环。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">setjmp;</span><br><span class="line"></span><br><span class="line">while (检查并处理中断或异常) &#123;</span><br><span class="line">       前端翻译;</span><br><span class="line">       后端翻译;</span><br><span class="line">       执行host执行改变guest CPU状态; // 有异常时longjmp到setjmp处</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> qemu在翻译的时候不是逐条guest指令翻译的，而是把一堆guest指令翻译到一个translation<br> block(tb)里，执行也是以tb为单位。qemu针对tb做了一些优化，它把已经翻译的tb放到哈希表<br> 里，需要翻译的时候先查表，找到了就可以直接在host上运行tb里翻译好的指令，省去翻译<br> 的过程，在这个基础上，如果tb和tb之间有跳转关系，qemu也可以在前一个tb里加指针，直接<br> 指向下一个tb，一个tb执行完成，直接跳到下一个tb执行，这样连上面查表的过程也省去了，<br> 这样的tb叫chained tb，宏观上看，qemu执行时，如果都是chained tb，完全有可能翻译过<br> 一次后，再次执行的时候都在tb之间直接跳来跳去，没有翻译和查tb hash表的过程。</p>
<p> 整个翻译的逻辑都在tb_gen_code里。</p>
<p> 要理解具体的翻译执行的细节，需要了解整个机器是怎么起来的。qemu启动的时候时候，<br> 会在如下的流程里初始化所谓accelerator的东西，qemu把tcg和kvm看成是qemu翻译执行的<br> 两种加速器，如下就是相关初始化的配置，我们这里只关心tcg。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">main</span><br><span class="line">  +-&gt; qemu_init</span><br><span class="line">    +-&gt; configure_accelerators</span><br><span class="line">      +-&gt; do_configure_accelerator</span><br><span class="line">        +-&gt; accel_init_machine(accel, current_machine)</span><br><span class="line">              /*</span><br><span class="line">               * tcg的init_machine定义在accel/tcg/tcg-all.c, tcg_accel会被qemu</span><br><span class="line">               * 定义成一个类对象。tcg init_machine的回调函数是tcg_init_machine</span><br><span class="line">               */</span><br><span class="line">          +-&gt; acc-&gt;init_machine (tcg_init_machine)</span><br><span class="line">            +-&gt; tcg_init</span><br><span class="line">            | +-&gt; tcg_context_init</span><br><span class="line">            |       /*</span><br><span class="line">            |        * tcg/aarch64/tcg-target.c.inc，这个函数配置翻译时用到的host</span><br><span class="line">            |        * 寄存器的信息，tcg_target_call_clobber_regs表示需要调用者</span><br><span class="line">            |        * 保存的寄存器，这个函数把x19-x29(ARM64中需要被调用者保存)</span><br><span class="line">            |        * 从这个集合中去除，reserved_regs表示有固定用途的寄存器，qemu</span><br><span class="line">            |        * 后端翻译分配寄存器时，不能从其中分配，ARM64作为后端时，这样</span><br><span class="line">            |        * 的寄存器有：sp/fp(x29)/tmp(x30)/x18/vec_tmp。</span><br><span class="line">            |        *</span><br><span class="line">            |        * tcg_target_call_iarg_regs/tcg_target_call_oarg_regs表示ARM64</span><br><span class="line">            |        * host架构上函数入参和返回值可以用的寄存器，ARM64上直接静态</span><br><span class="line">            |        * 定义到了tcg-target.c.inc中。</span><br><span class="line">            |        */</span><br><span class="line">            |   +-&gt; tcg_target_init</span><br><span class="line">            |     /*</span><br><span class="line">            |      * tcg_ctx是TCGContext, 线程变量，是tb翻译执行的上下文. 每个</span><br><span class="line">            |      * tb里都有一个段前导代码，这个代码用来在真正执行tb里的host</span><br><span class="line">            |      * 指令的时候，做环境的准备。下面这个函数生成这段前导的指令。</span><br><span class="line">            |      * 从下面可见tb的结尾时的代码也在这里生成了，前后都在准备和</span><br><span class="line">            |      * 恢复执行tb的这个host函数的上下文，中间的br是跳掉tb的业务</span><br><span class="line">            |      * 逻辑里执行业务代码。</span><br><span class="line">            |      */</span><br><span class="line">            +-&gt; tcg_prologue_init(tcg_ctx)</span><br><span class="line">                  /* 我们这里假设host是arm64，tcg/aarch64/tcg-target.c.inc */</span><br><span class="line">              +-&gt; tcg_target_qemu_prologue</span><br><span class="line">                  /*</span><br><span class="line">                   * 如上的这个函数里，用代码生成了一段arm64的汇编，大概是：</span><br><span class="line">                   * (这个可以-d out_asm，通过输出host的反汇编得到)</span><br><span class="line">                   *  stp      x29, x30, [sp, #-0x60]!</span><br><span class="line">                   *  mov      x29, sp</span><br><span class="line">                   *  stp      x19, x20, [sp, #0x10]</span><br><span class="line">                   *  stp      x21, x22, [sp, #0x20]</span><br><span class="line">                   *  stp      x23, x24, [sp, #0x30]</span><br><span class="line">                   *  stp      x25, x26, [sp, #0x40]</span><br><span class="line">                   *  stp      x27, x28, [sp, #0x50]</span><br><span class="line">                   *  sub      sp, sp, #0x480</span><br><span class="line">                   *  mov      x19, x0        &lt;------ 第一个入参保存cpu结构体地址</span><br><span class="line">                   *  br       x1             &lt;------ 第二个入参保存的是生成指令地址</span><br><span class="line">                   *  movz     w0, #0         &lt;------ 这个地址保存到TCGContext的code_gen_epilogue</span><br><span class="line">                   *  add      sp, sp, #0x480</span><br><span class="line">                   *  ldp      x19, x20, [sp, #0x10]</span><br><span class="line">                   *  ldp      x21, x22, [sp, #0x20]</span><br><span class="line">                   *  ldp      x23, x24, [sp, #0x30]</span><br><span class="line">                   *  ldp      x25, x26, [sp, #0x40]</span><br><span class="line">                   *  ldp      x27, x28, [sp, #0x50]</span><br><span class="line">                   *  ldp      x29, x30, [sp], #0x60</span><br><span class="line">                   *  ret      </span><br><span class="line">                   *</span><br><span class="line">                   *  这些生成的指令被放到TCGContext的code_ptr, code_gen_prologue</span><br><span class="line">                   *  也指向相同的一片buf。</span><br><span class="line">                   */</span><br></pre></td></tr></table></figure>
<p> 各个CPU线程的初始化流程是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* target/riscv/cpu.c */</span><br><span class="line">riscv_cpu_realize</span><br><span class="line">      /* softmmu/cpus.c */</span><br><span class="line">  +-&gt; qemu_init_vcpu</span><br><span class="line">        /* 拉起guest cpu的线程, tcg的回调定义在accel/tcg/tcg-cpus.c */</span><br><span class="line">    +-&gt; cpus_accel-&gt;create_vcpu_thread  // tcg_start_vcpu_thread</span><br><span class="line">      +-&gt; qemu_thread_create拉起线程: tcg_cpu_thread_fn</span><br><span class="line">          /* 如上线程的主体就是上面翻译执行的主循环 */</span><br></pre></td></tr></table></figure>

<h2 id="前端翻译"><a href="#前端翻译" class="headerlink" title="前端翻译"></a>前端翻译</h2><p> 前端翻译在gen_intermediate_code里完成, 翻译成的中间码都挂到了tcg_ctx的ops链表里。<br> 这里有几个相关的数据结构：TranslationBlock tb, TCGContext tcg_ctx, DisasContextBase dcbase。<br> tb是指一个具体翻译块，tcg_ctx是一个CPU的翻译上下文，对于每个具体的翻译块，进入和<br> 出来翻译翻译块的host二进制都是相同的，就是上面prologue中的二进制。tb中翻译的业务<br> 代码的host二进制在一个翻译上下文中产生，并添加到tb的各种缓存结构中。(todo：还没有<br> 找见tcg_ctx的到一个tb时，新建tb中host二进制存储空间的地方)。dcbase用于前端翻译，<br> 前端翻译可以看作是guest二进制反汇编成qemu中间指令的过程，想必disas context的命名<br> 也来自这里。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tb_gen_code</span><br><span class="line">  +-&gt; gen_intermediate_code</span><br><span class="line">        /* 这里翻译就是把一个个的guest指令得到的中间码连同操作数挂到ops链表里 */</span><br><span class="line">    +-&gt; translator_loop</span><br><span class="line">  +-&gt; tcg_gen_code</span><br></pre></td></tr></table></figure>

<p> 前端翻译中，我们会涉及TCGv以及helper函数的概念。TCGv从概念的角度可以看成是中间码<br> 使用的寄存器，前端模拟实现一个指令的时候，要用到临时变量的时候，都要申请一个这样<br> 的寄存器。比如，我们看下riscv的add指令的前端翻译的实现：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static bool trans_add(DisasContext *ctx, arg_add *a)</span><br><span class="line">&#123;</span><br><span class="line">    return gen_arith(ctx, a, &amp;tcg_gen_add_tl);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static bool gen_arith(DisasContext *ctx, arg_r *a,</span><br><span class="line">                      void(*func)(TCGv, TCGv, TCGv))</span><br><span class="line">&#123;</span><br><span class="line">    TCGv source1, source2;</span><br><span class="line">    source1 = tcg_temp_new();        &lt;------ A</span><br><span class="line">    source2 = tcg_temp_new();</span><br><span class="line"></span><br><span class="line">    gen_get_gpr(source1, a-&gt;rs1);    &lt;------ B</span><br><span class="line">    gen_get_gpr(source2, a-&gt;rs2);</span><br><span class="line"></span><br><span class="line">    (*func)(source1, source1, source2);</span><br><span class="line"></span><br><span class="line">    gen_set_gpr(a-&gt;rd, source1);</span><br><span class="line">    tcg_temp_free(source1);</span><br><span class="line">    tcg_temp_free(source2);</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static inline void gen_get_gpr(TCGv t, int reg_num)</span><br><span class="line">&#123;</span><br><span class="line">    if (reg_num == 0) &#123;</span><br><span class="line">        tcg_gen_movi_tl(t, 0);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        tcg_gen_mov_tl(t, cpu_gpr[reg_num]);     &lt;------ C</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 在A行，我们申请了一个source1 TCGv, 在C行，我们把add指令的rs1寄存器上的值传递给<br> source1，后续继续使用source1参与计算。source1和guest寄存器实际上都是保存在host<br> 的内存上的，实际运行的时候，host上的程序其实做的就是内存数据搬移的操作。后续add<br> 指令的模拟，在中间码的层面看是把source1/source2相加，host实际计算的时候要把数据<br> 移动到寄存器上计算，所以直接翻译可能是这样的：reg1 = load(rs1的内存), store(reg1, source1的内存),<br> reg1 = load(source1的内存), 同样的方式把rs2保存的值加载到reg2，reg3 = add(reg1, reg2),<br> store(reg3, rd的内存), 最后可能就优化成了reg1 = load(rs1的内存), reg2 = load(rs2的内存),<br> reg3 = add(reg1, reg2), store(reg3, rd的内存)。</p>
<p> 我们实际看个模拟执行的例子，使用-d in_asm,op,out_asm得到guest汇编、中间码和host汇编：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IN: test_add</span><br><span class="line">0x0000000000010430:  1101              addi            sp,sp,-32</span><br><span class="line">0x0000000000010432:  ec22              sd              s0,24(sp)</span><br><span class="line">0x0000000000010434:  1000              addi            s0,sp,32</span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">OP:</span><br><span class="line"> ld_i32 tmp0,env,$0xfffffffffffffff0</span><br><span class="line"> movi_i32 tmp1,$0x0</span><br><span class="line"> brcond_i32 tmp0,tmp1,lt,$L0</span><br><span class="line"></span><br><span class="line"> ---- 0000000000010430</span><br><span class="line"> mov_i64 tmp2,x2/sp</span><br><span class="line"> movi_i64 tmp3,$0xffffffffffffffe0</span><br><span class="line"> add_i64 tmp2,tmp2,tmp3</span><br><span class="line"> mov_i64 x2/sp,tmp2</span><br><span class="line"> [...]</span><br><span class="line"></span><br><span class="line">OUT: [size=192]</span><br><span class="line">0xffff7c025c00:  b85f0274  ldur     w20, [x19, #0xfffffffffffffff0]		[tb header &amp; initial instruction]</span><br><span class="line">0xffff7c025c04:  7100029f  cmp      w20, #0</span><br><span class="line">0xffff7c025c08:  5400052b  b.lt     #0xffff7c025cac</span><br><span class="line">0xffff7c025c0c:  f9400a74  ldr      x20, [x19, #0x10]    &lt;----- 10430</span><br><span class="line">0xffff7c025c10:  d1008294  sub      x20, x20, #0x20</span><br><span class="line">0xffff7c025c14:  f9000a74  str      x20, [x19, #0x10]</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p> 就只看第一条addi指令的模拟过程，这是test_add这个函数一进来开栈的指令，把sp向低地址<br> 移动32。可以看到中间码是和前端翻译过程相对应的，先把寄存器值和-32保存到TCGv变量上，<br> 然后对TCGv做add运算，然后把运算结果保存回sp。最终翻译得到的host代码，把sp的值load<br> 到x20，计算后再存回cpu结构体的对应位置，x19保存就是cpu结构体的地址。</p>
<p> 具体代码实现上，TCGv这个变量的数值其实就是对应变量相对于存储空间基地址的偏移。<br> 这个存储空间不只是有描述cpu的结构体(riscv上是CPURISCVState)，还有TCGContext，CPU<br> 的寄存器都是存在cpu结构体里的，上面这个例子的sp就是这样。TCGContext保存变量的逻辑<br> 还没有搞清。前端翻译只是把这些信息都挂到中间码的链表里，得到host指令在后端翻译里。</p>
<p> qemu可以用生成的host指令模拟guest，也可以直接调用host上的函数改变guest CPU的状态，<br> 后者在qemu里叫helper函数。理论上，所有的模拟都可以用helper函数，但是，显然helper<br> 函数会降低模拟的速度。</p>
<p> 以riscv为例，增加一个helper函数的一般套路是: 1. 在target/riscv/op_helper.c里增加<br> 函数的定义；2. 在target/riscv/helper.h增加对应的宏，宏的参数分别是：helper函数名字、<br> 函数的返回值、函数的入参；3. 在中间码里用gen_helper_xxx直接调用helper函数，返回值<br> 保存在gen_helper_xxx的第一个参数里，常数入参需要用tcg_const_i32/i64生成下常数TCGv，<br> 实际上是为这个常数分配TCG寄存器存储空间。</p>
<p> helper函数的实现逻辑是生成函数调用的上下文，然后跳转到函数的地址执行指令，也就是<br> 先把函数的入参放到寄存器上，然后调用跳转指令跳到函数地址执行。</p>
<h2 id="后端翻译"><a href="#后端翻译" class="headerlink" title="后端翻译"></a>后端翻译</h2><p> 后端翻译在tcg_gen_code里，核心是在一个循环里处理前端翻译的中间码，把中间码翻译成<br> host上的汇编，具体分析可以参考<a href="https://wangzhou.github.io/qemu-tcg%E4%B8%AD%E9%97%B4%E7%A0%81%E4%BC%98%E5%8C%96%E5%92%8C%E5%90%8E%E7%AB%AF%E7%BF%BB%E8%AF%91/">这里</a>。</p>
<h2 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h2><p> 执行翻译好的host指令是在大循环的cpu_loop_exec_tb里。翻译好的host汇编的整体逻辑<br> 如上面prologue的样子，tb里的业务代码对应的host汇编通过中间的br指令调用。tb对应的<br> 业务代码对应的host汇编，也就是前端翻译、后端翻译一起得到的host二进制放在tb-&gt;tc.ptr<br> 指向的地址，prologue的二进制放在tcg_ctx-&gt;code_gen_prologue指向的地址。tb-&gt;tc.ptr<br> 在函数调用的时候被放到了x1寄存器，这个和br x1也是相对应的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cpu_loop_exec_tb</span><br><span class="line">  +-&gt; cpu_tb_exec</span><br><span class="line">    +-&gt; tcg_qemu_tb_exec(env, tb-&gt;tc.ptr)</span><br><span class="line">      +-&gt; tcg_ctx-&gt;code_gen_prologue(env, tb-&gt;tc.ptr)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tcg跳转的处理</title>
    <url>/qemu-tcg%E8%B7%B3%E8%BD%AC%E7%9A%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p>以beq为例:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* qemu/target/riscv/insn_trans/trans_rvi.c.inc */</span><br><span class="line">trans_beq</span><br><span class="line">  -&gt; gen_branch</span><br><span class="line">    -&gt; gen_new_lable</span><br><span class="line">    -&gt; tcg_gen_brcond_tl</span><br><span class="line">    -&gt; gen_goto_tb</span><br><span class="line">    -&gt; gen_set_label</span><br><span class="line">    -&gt; gen_goto_tb</span><br><span class="line">    -&gt; ctx-&gt;base.is_jmp = DISAS_NORETURN;</span><br></pre></td></tr></table></figure>
<p>如上的逻辑是生成中间码的，生成的代码还要执行，我们不用关注host上执行的细节，只要<br>中间码这边的逻辑通就好。那就比较好理解上面的代码，结合tcg/README里的介绍。</p>
<p>gen_new_lable是创建了一个lable，brcond_t是brcond_i32/i64 t0, t1, cond, label，根据<br>t0/t1的计算决定是否要跳到lable处执行，gen_set_lable是set_label $label，相当于在<br>当前位置设置lable。所以上面的代码生成的代码伪代码表示大概是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lable l;</span><br><span class="line"></span><br><span class="line">if (t0 cond t1) &#123;</span><br><span class="line">    goto l;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">goto_tb(pc顺移);</span><br><span class="line"></span><br><span class="line">l:</span><br><span class="line">    goto_tb(计算新pc);</span><br></pre></td></tr></table></figure>
<p>其中gen_goto_tb的逻辑是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gen_goto_tb</span><br><span class="line">    if (translator_use_goto_tb(&amp;ctx-&gt;base, dest)) &#123;</span><br><span class="line">        tcg_gen_goto_tb(n);</span><br><span class="line">        tcg_gen_movi_tl(cpu_pc, dest);</span><br><span class="line">        tcg_gen_exit_tb(ctx-&gt;base.tb, n);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        tcg_gen_movi_tl(cpu_pc, dest);</span><br><span class="line">        tcg_gen_lookup_and_goto_ptr();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>可见这里有两种实现方式：如果goto_tb在tcg后端有实现就用goto_tb来跳转，否则就用<br>goto_ptr来实现。goto_ptr的方式相对简单，先设置跳转的PC，然后会调用到<br>lookup_tb_ptr(在accel/tcg/cpu-exec.c)，找对应的tb执行，如果没有找见就退出当前tb。</p>
<p>goto_tb的方式比较绕一点，我们那riscv的后端实现具体看下。tcg_gen_goto_tb对应的<br>中间码是INDEX_op_goto_tb，riscv的后端实现是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/riscv/tcg-target.c.inc */</span><br><span class="line">tcg_out_op</span><br><span class="line">  -&gt; tcg_out_ld(s, TCG_TYPE_PTR, TCG_REG_TMP0, TCG_REG_ZERO,</span><br><span class="line">                   (uintptr_t)(s-&gt;tb_jmp_target_addr + a0));</span><br><span class="line">  -&gt; tcg_out_opc_imm(s, OPC_JALR, TCG_REG_ZERO, TCG_REG_TMP0, 0);</span><br><span class="line">  -&gt; set_jmp_reset_offset(s, a0);</span><br><span class="line">       /*</span><br><span class="line">        * 注意a0就是tcg_gen_goto_tb的入参，就是n。注意，这个是理解goto_tb的关键，</span><br><span class="line">	* tcg_out_opc_imm在当前的tb里产生一条指令，这个地方把这个指令在tb里的位置</span><br><span class="line">	* 写在了tb_jmp_target_addr[n]这个地方。这个动作为后面链接下一个tb留出了</span><br><span class="line">	* 一个指令的位置。</span><br><span class="line">        */</span><br><span class="line">    -&gt; s-&gt;tb_jmp_reset_offset[which] = tcg_current_code_size(s);</span><br></pre></td></tr></table></figure>
<p>riscv exit_tb的后端实现：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_out_op</span><br><span class="line">   if (a0 == 0) &#123;</span><br><span class="line">       tcg_out_call_int(s, tcg_code_gen_epilogue, true);</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">       tcg_out_movi(s, TCG_TYPE_PTR, TCG_REG_A0, a0);</span><br><span class="line">       tcg_out_call_int(s, tb_ret_addr, true);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>注意这里的a0和n相关，n这个变量从这里被传入后端执行，然后从exit_tb里带出来，给到<br>下个tb。我们回到主循环。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cpu_exec</span><br><span class="line">  -&gt; cpu_loop_exec_tb(cpu, tb, &amp;last_tb, &amp;tb_exit);</span><br><span class="line">  -&gt; tb_add_jump(last_tb, tb_exit, tb);</span><br><span class="line">    -&gt; tb_set_jmp_target(tb, n, (uintptr_t)tb_next-&gt;tc.ptr);</span><br><span class="line">      -&gt; uintptr_t offset = tb-&gt;jmp_target_arg[n];</span><br><span class="line">      -&gt; tb_target_set_jmp_target(tc_ptr, jmp_rx, jmp_rw, addr);</span><br></pre></td></tr></table></figure>
<p>如上，在下一次tb翻译执行循环里会把新tb里指令的地址直接覆盖上次tb里保留的位置。<br>所以，使用go_tb，第一次执行的时候会退出tb，执行下一个tb，用新tb指令地址覆盖之前tb<br>里的跳转预留位置，当再次执行前一个tb时，会直接跳转到新tb，就不会退出当前tb。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tlb实现分析</title>
    <url>/qemu-tlb%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="TLB相关数据结构"><a href="#TLB相关数据结构" class="headerlink" title="TLB相关数据结构"></a>TLB相关数据结构</h2><p> 每个vCPU都有一个TLB相关的数据结构，riscv上这个结构在RISCVCPU neg域段的CPUTLB tlb结构里。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">typedef struct CPUTLB &#123;</span><br><span class="line">    CPUTLBCommon c;</span><br><span class="line">    CPUTLBDesc d[NB_MMU_MODES];</span><br><span class="line">    CPUTLBDescFast f[NB_MMU_MODES];</span><br><span class="line">&#125; CPUTLB;</span><br></pre></td></tr></table></figure>
<p> 如上是CPUTLB的结构，CPUTLBCommon存放TLB的公有信息，目前是dirty标记、锁和一些统计<br> 变量，CPUTLBDescFast和CPUTLBDesc存放的都是TLB的内容，两者组成一个两级TLB，其中<br> CPUTLBDescFast是第一级，CPUTLBDesc是第二级，搜索的时候会先查第一级然后查第二级。</p>
<p> NB_MMU_MODES表示TLB的种类，目前riscv上的定义是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">U mode 0b000                                                              </span><br><span class="line">S mode 0b001                                                              </span><br><span class="line">M mode 0b011                                                              </span><br><span class="line">U mode HLV/HLVX/HSV 0b100                                                 </span><br><span class="line">S mode HLV/HLVX/HSV 0b101                                                 </span><br><span class="line">M mode HLV/HLVX/HSV 0b111                                                 </span><br></pre></td></tr></table></figure>

<p> 每个MMU mode下的CPUTLBDesc和CPUTLBDescFast都有若干个TLB entry组成的TLB表, 相关<br> 的TLB表的大小是可以动态调整的。其中一个TLB entry的定义是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">typedef struct CPUTLBEntry &#123;</span><br><span class="line">    union &#123;</span><br><span class="line">        struct &#123;</span><br><span class="line">            target_ulong addr_read;</span><br><span class="line">            target_ulong addr_write;</span><br><span class="line">            target_ulong addr_code;</span><br><span class="line">            uintptr_t addend;</span><br><span class="line">        &#125;;</span><br><span class="line">        uint8_t dummy[1 &lt;&lt; CPU_TLB_ENTRY_BITS];</span><br><span class="line">    &#125;;</span><br><span class="line">&#125; CPUTLBEntry;</span><br></pre></td></tr></table></figure>
<p> TLB entry对读、写以及代码是分开做缓存的。</p>
<p> 第二级TLB的数据结构，相关的数据可以大概分成三部分：1. 和大页相关；2. 和TLB table<br> 动态调整大小有关系；3. TLB entry内容相关。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">typedef struct CPUTLBDesc &#123;                                                     </span><br><span class="line">    target_ulong large_page_addr;                                               </span><br><span class="line">    target_ulong large_page_mask;                                               </span><br><span class="line"></span><br><span class="line">    int64_t window_begin_ns;                                                    </span><br><span class="line">    size_t window_max_entries;                                                  </span><br><span class="line">    size_t n_used_entries;                                                      </span><br><span class="line">    size_t vindex;                                                              </span><br><span class="line"></span><br><span class="line">    CPUTLBEntry vtable[CPU_VTLB_SIZE];                                          </span><br><span class="line">    CPUTLBEntryFull vfulltlb[CPU_VTLB_SIZE];                                    </span><br><span class="line">    CPUTLBEntryFull *fulltlb;                                                   </span><br><span class="line">&#125; CPUTLBDesc;                                                                   </span><br></pre></td></tr></table></figure>

<p> qemu里的TLB模拟并不是对真实硬件的模拟，而是针对所有构架做的一个通用的TLB实现，<br> 它的目的是加速地址翻译。</p>
<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> TLB的作用是加速地址访问时的地址翻译，地址访问一般分为显示地址访问和隐式地址访问，<br> 显示访问就是通过显示的load/store指令完成地址访问，隐式的访问是CPU在运行时不通过<br> 访存指令做的内存访问，比如访问页表以及取指令。不考虑虚拟化时，页表放在物理地址上，<br> 所以，我们这里先只考虑load/store以及取指令中涉及的TLB逻辑。</p>
<p> TLB无效化是TLB相关的重要操作，一般也是软件和TLB打交道的唯一接口，有专门的TLB无效<br> 化指令触发相关的逻辑。当虚拟地址到物理地址的映射改变时，就需要做TLB的无效化操作，<br> 相关指令可以有不同的参数，定义TLB无效化的范围。</p>
<p> qemu取指令的基本逻辑可以参考<a href="https://wangzhou.github.io/qemu-tcg%E5%8F%96%E6%8C%87%E4%BB%A4%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">这里</a>。qemu load/store的基本逻辑可以参考<a href="https://wangzhou.github.io/qemu-tcg%E8%AE%BF%E5%AD%98%E6%8C%87%E4%BB%A4%E6%A8%A1%E6%8B%9F/">这里</a>。</p>
<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p> TLB创建的相关代码分析：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* 对于取指令和load/store操作都是在page walk成功后创建对应的TLB */</span><br><span class="line">riscv_cpu_tlb_fill</span><br><span class="line">  +-&gt; tlb_set_page</span><br><span class="line">    [...]</span><br><span class="line">      +-&gt; tlb_set_page_full</span><br><span class="line">            /*</span><br><span class="line">             * 如果页属性是可写，会在TLB上打一个还没有写过的标记。因为代码页面</span><br><span class="line">             * 的权限在创建的时候一般不会有可写，所以，这里TLB_MOTDIRTY这个标记</span><br><span class="line">             * 针对的是数据相关的可写页面。</span><br><span class="line">             */</span><br><span class="line">        +-&gt; write_address |= TLB_MOTDIRTY;</span><br></pre></td></tr></table></figure>
<p> (todo: 补充大页和iommu的逻辑)</p>
<p>创建tb时也会配置代码所在page对应TLB的TLB_MOTDIRTY标记，这里TLB_MOTDIRTY是专门针对<br>指令页面的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cpu_exec</span><br><span class="line">  +-&gt; tb_gen_code</span><br><span class="line">    +-&gt; tb_link_page</span><br><span class="line">      +-&gt; tb_page_add</span><br><span class="line">        +-&gt; tlb_protect_code</span><br><span class="line">          [...]</span><br><span class="line">          +-&gt; tlb_reset_dirty</span><br><span class="line">                /* 这里会把两级TLB里的TLB_MOTDIRTY都配置上 */</span><br><span class="line">            +-&gt; tlb_reset_dirty_range_locked</span><br></pre></td></tr></table></figure>
<p> 数据的load/store访问，总是要进过TLB的，相关的逻辑可以参考<a href="https://wangzhou.github.io/qemu-tcg%E8%AE%BF%E5%AD%98%E6%8C%87%E4%BB%A4%E6%A8%A1%E6%8B%9F/">这里</a>。load/store<br> 以及取指令中的TLB搜索逻辑基本一致，我们在如下分析中统一说明。当TLB的flag区域里有<br> 标记时会强制进入load/store的慢速路径，在慢速路径里处理各种TLB flag，慢速路径里有<br> 专门对TLB_MOTDIRTY的处理，所以，对于代码页面，当程序把页面改成可写，然后改动代码，<br> 继续执行改动过的代码，就会出问题，因为guest代码可能已经被翻译到tb里，guest代码被<br> 改动后，曾经翻译得到tb就应该被删掉，如果这个tb在chain tb的链条里，同时应该从tb链<br> 条里把这个tb删除。相关的代码分析如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load_helper</span><br><span class="line">  +-&gt; index = tlb_index(env, mmu_idx, addr);                                      </span><br><span class="line">      /*</span><br><span class="line">       * 得到addr在第一级TLB也就是CPUTLBDescFast f中的entry，CPUTLBDescFast中的</span><br><span class="line">       * mask保存CPUTLBDescFast里每种MMU mode下TLB table的size，第一级TLB的是按照</span><br><span class="line">       * 虚拟地址页号在TLB table中依次存放。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; entry = tlb_entry(env, mmu_idx, addr);                                      </span><br><span class="line">  +-&gt; tlb_addr = code_read ? entry-&gt;addr_code : entry-&gt;addr_read;                 </span><br><span class="line"></span><br><span class="line">      /* 判断第一级TLB是否命中 */</span><br><span class="line">  +-&gt; if (!tlb_hit(tlb_addr, addr))</span><br><span class="line">          /*</span><br><span class="line">           * 第一级TLB没有命中，继续找第二级TLB，第二级TLB命中后直接把TLB的值和</span><br><span class="line">           * 第一级TLB交换。</span><br><span class="line">           */</span><br><span class="line">          if (!victim_tlb_hit(env, mmu_idx, index, tlb_off,                       </span><br><span class="line">                              addr &amp; TARGET_PAGE_MASK)) &#123;                         </span><br><span class="line">              /* 第二级TLB没有命中，于是去做page table walk */</span><br><span class="line">              tlb_fill(env_cpu(env), addr, size,                                  </span><br><span class="line">                       access_type, mmu_idx, retaddr);                            </span><br><span class="line">              index = tlb_index(env, mmu_idx, addr);                              </span><br><span class="line">              entry = tlb_entry(env, mmu_idx, addr);                              </span><br><span class="line">          &#125;                                                                       </span><br><span class="line">          tlb_addr = code_read ? entry-&gt;addr_code : entry-&gt;addr_read;             </span><br><span class="line">          tlb_addr &amp;= ~TLB_INVALID_MASK;                                          </span><br><span class="line">      &#125;                                                                           </span><br><span class="line"></span><br><span class="line">      /* TLB entry中物理地址的低位保存一些属性bit */</span><br><span class="line">  +-&gt; if (unlikely(tlb_addr &amp; ~TARGET_PAGE_MASK)) &#123;                               </span><br><span class="line">           /*</span><br><span class="line">            * 处理的内容有：非对齐情况，TLB_WATCHPOINT，TLB_MMIO, TLB_BSWAP。</span><br><span class="line">            * 在store_helper里，处理的内容还包括：TLB_DISCARD_WRITE，TLB_NOTDIRTY?</span><br><span class="line">            */</span><br><span class="line">           [...]</span><br><span class="line">       </span><br><span class="line">           /* guest PA加一个偏移addend得到host VA */</span><br><span class="line">           haddr = (void *)((uintptr_t)addr + entry-&gt;addend);                      </span><br><span class="line">           return load_memop(haddr, op);                                           </span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      /* 也是处理非对齐的情况？*/</span><br><span class="line">  +-&gt; if (size &gt; 1 &amp;&amp; unlikely((addr &amp; ~TARGET_PAGE_MASK) + size - 1 &gt;= TARGET_PAGE_SIZE)) &#123;                                     </span><br><span class="line">          [...]</span><br><span class="line">      &#125;                                                                           </span><br><span class="line">                                                                            </span><br><span class="line">      /* guest PA加一个偏移addend得到host VA，这里是主路径上 */</span><br><span class="line">  +-&gt; haddr = (void *)((uintptr_t)addr + entry-&gt;addend);                          </span><br><span class="line">  +-&gt; return load_memop(haddr, op);                                               </span><br></pre></td></tr></table></figure>
<p> 但是，指令的访问不一定每次都要经过TLB，可以说大部分不经过TLB，因为翻译过成的TB<br> 块是可以chain在一起的，这样整个执行的过程可能全部在TB链条里跳来跳去。因为qemu约束<br> chain tb只能在一个page内，所以tb在一个page内跳来跳去是安全的。当guest的执行逻辑<br> 进入一个新page时，取指令的时候，必然要做TLB相关的操作。</p>
<p> qemu提供了tlb无效化的公共函数，相关的实现在accel/tcg/cputlb.c。对于riscv或者x86<br> 这种借助IPI做remote tlb无效化的构架，tlb无效化在qemu(机器)层面就是无效化本CPU上<br> 的TLB, 对于ARM这种支持TLB硬件广播的构架，qemu实现就需要无效化本CPU以及其它CPU上<br> 的TLB。下面分析TLB硬件广播的实现逻辑：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tlb_flush_by_mmuidx_all_cpus(CPUState *src_cpu, uint16_t idxmap)           </span><br><span class="line">  +-&gt; const run_on_cpu_func fn = tlb_flush_by_mmuidx_async_work;                  </span><br><span class="line">  +-&gt; flush_all_helper(src_cpu, fn, RUN_ON_CPU_HOST_INT(idxmap));                 </span><br><span class="line">      /* 这里是用什么同步的？*/</span><br><span class="line">  +-&gt; fn(src_cpu, RUN_ON_CPU_HOST_INT(idxmap));                                   </span><br><span class="line">    +-&gt; tlb_flush_one_mmuidx_locked</span><br><span class="line">          /* 动态调整TLB table的大小就在这里 */</span><br><span class="line">      +-&gt; tlb_mmu_resize_locked(desc, fast, now);                                     </span><br><span class="line">          /* TLB无效化在这里实施 */</span><br><span class="line">      +-&gt; tlb_mmu_flush_locked(desc, fast);                                           </span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu user mode速记</title>
    <url>/qemu-user-mode%E9%80%9F%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p>qemu有两种的使用方法，一种是system mode，一种是user mode。前者模拟整个machine，<br>其上可以运行一个完成的guest OS，后者可以在host上运行一个guest的程序，这个时候他<br>通过tcg用软件模拟guest CPU的状态，当guest程序里有系统调用的时候，user mode会直接<br>在host上模拟系统调用。</p>
<p>配置编译：configure –target-list=riscv64-linux-user; make<br>在build下就会编译生成相关的命令行工具：qemu-riscv64</p>
<p>用这个工具就可以直接运行riscv64的程序：qemu-riscv64 riscv64_app</p>
<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>user mode的代码在qemu/linux-user/*，相关tcg的代码在qemu/tcg/*、qemu/accel/*，<br>qemu/target/riscv/*。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* qemu/linux-user/main.c */</span><br><span class="line">main</span><br><span class="line">  -&gt; cpu_loop</span><br><span class="line">       /* 这个函数里就是tcg相关的翻译执行过程了 */</span><br><span class="line">    -&gt; cpu_exec</span><br><span class="line">       /* 指令执行遇到tcg处理不了的，就落到swith里执行 */</span><br><span class="line">    -&gt; swith (trapnr)</span><br><span class="line">         /* 具体看下遇到系统调用的处理办法 */</span><br><span class="line">      -&gt; RISCV_EXCP_U_ECALL</span><br><span class="line">           /* qemu/linux-user/syscall.c */</span><br><span class="line">        -&gt; do_syscall</span><br><span class="line">	     /* 可以看到这里把所有的系统调用都模拟了下 */</span><br><span class="line">	  -&gt; do_syscall1</span><br></pre></td></tr></table></figure>

<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>使用qemu-riscv –help可以查到全部参数设定，进一步用qemu-riscv -d –help可以看debug<br>log的配置。其中-d可以配置打印出中间码、guest和host反汇编、guest cpu的寄存器等值，<br>这些东西在调试tcg代码相关代码的时候比较有用，-singlestep可以对guest的汇编逐条打印<br>出调试信息。这里可以看一个 -d cpu,op,in_asm的输出log。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-riscv64 -singlestep -d cpu,op,in_asm ~/a.out &amp;&gt; ~/log</span><br><span class="line"></span><br><span class="line">[...]</span><br><span class="line">IN: main</span><br><span class="line">0x0000000000010430:  1101              addi            sp,sp,-32</span><br><span class="line"></span><br><span class="line">OP:</span><br><span class="line"> ld_i32 tmp0,env,$0xfffffffffffffff8</span><br><span class="line"> brcond_i32 tmp0,$0x0,lt,$L0</span><br><span class="line"></span><br><span class="line"> ---- 0000000000010430</span><br><span class="line"> mov_i64 tmp2,x2/sp</span><br><span class="line"> add_i64 tmp2,tmp2,$0xffffffffffffffe0</span><br><span class="line"> mov_i64 x2/sp,tmp2</span><br><span class="line"> mov_i64 pc,$0x10432</span><br><span class="line"> call lookup_tb_ptr,$0x6,$1,tmp2,env</span><br><span class="line"> goto_ptr tmp2</span><br><span class="line"> set_label $L0</span><br><span class="line"> exit_tb $0xffff980ab5c3</span><br><span class="line"></span><br><span class="line"> pc       0000000000010430</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 0000000000010606 x2/sp 0000004000800370 x3/gp 0000000000071028</span><br><span class="line"> x4/tp 0000000000072710 x5/t0 0000000000072000 x6/t1 2f2f2f2f2f2f2f2f x7/t2 0000000000072000</span><br><span class="line"> x8/s0 0000000000010940 x9/s1 00000000000109d0 x10/a0 0000000000000001 x11/a1 00000040008004b8</span><br><span class="line"> x12/a2 00000040008004c8 x13/a3 0000000000000000 x14/a4 0000004000800398 x15/a5 0000000000010430</span><br><span class="line"> x16/a6 0000000000071138 x17/a7 0112702f5b5a4001 x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 ffffffffffffffff x29/t4 000000000006ead0 x30/t5 0000000000000000 x31/t6 0000000000072000</span><br><span class="line">----------------</span><br><span class="line">IN: main</span><br><span class="line">0x0000000000010432:  ec06              sd              ra,24(sp)</span><br><span class="line"></span><br><span class="line">OP:</span><br><span class="line"> ld_i32 tmp0,env,$0xfffffffffffffff8</span><br><span class="line"> brcond_i32 tmp0,$0x0,lt,$L0</span><br><span class="line"></span><br><span class="line"> ---- 0000000000010432</span><br><span class="line"> mov_i64 tmp2,x2/sp</span><br><span class="line"> add_i64 tmp2,tmp2,$0x18</span><br><span class="line"> mov_i64 tmp3,x1/ra</span><br><span class="line"> qemu_st_i64 tmp3,tmp2,leq,0</span><br><span class="line"> mov_i64 pc,$0x10434</span><br><span class="line"> call lookup_tb_ptr,$0x6,$1,tmp2,env</span><br><span class="line"> goto_ptr tmp2</span><br><span class="line"> set_label $L0</span><br><span class="line"> exit_tb $0xffff980ab703</span><br><span class="line"></span><br><span class="line"> pc       0000000000010432</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 0000000000010606 x2/sp 0000004000800350 x3/gp 0000000000071028</span><br><span class="line"> x4/tp 0000000000072710 x5/t0 0000000000072000 x6/t1 2f2f2f2f2f2f2f2f x7/t2 0000000000072000</span><br><span class="line"> x8/s0 0000000000010940 x9/s1 00000000000109d0 x10/a0 0000000000000001 x11/a1 00000040008004b8</span><br><span class="line"> x12/a2 00000040008004c8 x13/a3 0000000000000000 x14/a4 0000004000800398 x15/a5 0000000000010430</span><br><span class="line"> x16/a6 0000000000071138 x17/a7 0112702f5b5a4001 x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 ffffffffffffffff x29/t4 000000000006ead0 x30/t5 0000000000000000 x31/t6 0000000000072000</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p>IN: main表示当前在guest app的main里，下面是guest汇编代码，OP是tcg中间码，pc是这个<br>指令执行后guest寄存器里的值。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu中basic block以及tcg中各种变量的基本逻辑</title>
    <url>/qemu%E4%B8%ADbasic-block%E4%BB%A5%E5%8F%8Atcg%E4%B8%AD%E5%90%84%E7%A7%8D%E5%8F%98%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<h2 id="指令前端翻译"><a href="#指令前端翻译" class="headerlink" title="指令前端翻译"></a>指令前端翻译</h2><p> QEMU的前端翻译使用中间码实现guest指令的逻辑，中间码其实已经是一组完备的类RISC<br> 汇编指令定义。我们做前端翻译，可以理解为使用中间码作为汇编语言实现guest指令的逻辑。<br> QEMU代码qemu/tcg/README有怎么使用中间码的完整描述，值得仔细学习下。</p>
<p> 中间码的定义里有各种指令，还有不同作用域的“寄存器”的定义。我们关注的是各种“寄存器”，<br> 也就是tcg变量的定义。</p>
<p> QEMU翻译基于TB，一个TB里又可能有多个basic block，我们叫BB，tcg变量的作用域和BB<br> 有关系。tcg有三种类型的变量：temporary, local temporary和global，global这个不用<br> 多说，一般系统寄存器是global变量，temporary变量的生命只在一个BB内，local temporary<br> 变量的生命在一个TB内，可以跨越BB。</p>
<p> BB从上一个BB的结尾或者一个set_label指令开始, BB以分支指令(brcond_xxx)、goto_tb<br> 以及exit_tb结束，</p>
<p> 举一个arm里的例子：target/arm/translate.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static bool trans_LE</span><br><span class="line">       [...]</span><br><span class="line">       TCGv_i32 decr = tcg_temp_local_new_i32();                               </span><br><span class="line">       TCGv_i32 ltpsize = load_cpu_field(v7m.ltpsize);                         </span><br><span class="line">       tcg_gen_sub_i32(decr, tcg_constant_i32(4), ltpsize);                    </span><br><span class="line">       tcg_gen_shl_i32(decr, tcg_constant_i32(1), decr);                       </span><br><span class="line">       tcg_temp_free_i32(ltpsize);                                             </span><br><span class="line">                                                                               </span><br><span class="line">       tcg_gen_brcond_i32(TCG_COND_LEU, cpu_R[14], decr, loopend);             </span><br><span class="line">                                                                               </span><br><span class="line">       tcg_gen_sub_i32(cpu_R[14], cpu_R[14], decr);                            </span><br><span class="line">       tcg_temp_free_i32(decr);                                                </span><br><span class="line">       [...]</span><br></pre></td></tr></table></figure>
<p> 这里的decr就要是一个local temporary，因为这个变量在brcon_i32的前后都要使用，已经<br> 跨越了两个BB，这里的decr换成temporary变量就会出错。</p>
<p> global变量又分为寄存器和memory两类。memory一般定义的是CPU里的寄存器，比如riscv<br> 里是这样定义pc寄存器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cpu_pc = tcg_global_mem_new(cpu_env, offsetof(CPURISCVState, pc), &quot;pc&quot;);    </span><br></pre></td></tr></table></figure>
<p> 这种TCGv在后端翻译时，会把guest CPU寄存器的值先load到host寄存器里，计算完后再store<br> 回guest CPU结构体里，模拟过程会有访存行为。</p>
<p> 寄存器这种TCGv在后端翻译时，会直接映射到host的寄存器上，每次就可以直接访问，一般<br> 用来存放guest CPU env的指针，比如riscv上env变量是这么定义的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/tcg.c */</span><br><span class="line">tcg_context_init(unsigned max_cpus)                                 </span><br><span class="line">  +-&gt; tcg_global_reg_new_internal(s, TCG_TYPE_PTR, TCG_AREG0, &quot;env&quot;);        </span><br></pre></td></tr></table></figure>

<h2 id="QEMU代码分析"><a href="#QEMU代码分析" class="headerlink" title="QEMU代码分析"></a>QEMU代码分析</h2><p> 代码分析需要涉及tcg后端翻译，具体代码分析可以参考<a href="https://wangzhou.github.io/qemu-tcg%E4%B8%AD%E9%97%B4%E7%A0%81%E4%BC%98%E5%8C%96%E5%92%8C%E5%90%8E%E7%AB%AF%E7%BF%BB%E8%AF%91/">这里</a>。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu模拟系统指令</title>
    <url>/qemu%E6%A8%A1%E6%8B%9F%E7%B3%BB%E7%BB%9F%E6%8C%87%E4%BB%A4/</url>
    <content><![CDATA[<p> ecall指令是riscv里系统调用的指令，这个指令改变CPU的模式，并且改变CPU PC到异常处理<br> 代码的入口。可以想象，qemu里把这些两条模拟了就可以。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cpu_exec</span><br><span class="line">  -&gt; while (!cpu_handle_exception()) &#123;</span><br><span class="line">       -&gt; while (!cpu_handle_interrupt()) &#123;</span><br><span class="line">	    -&gt; 生成host上的指令并模拟guest cpu的行为</span><br><span class="line">	  &#125;</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure>
<p> qemu的cpu_exec在一个循环中模拟guest cpu的行为，每次进入循环就查下有没有异常发生，<br> 所以，应该循环内在异常发生时设置一个flag，每次进入的时候就检测这个flag，flag置位<br> 了就做异常处理。</p>
<p> 大概的流程是这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* qemu/accel/tcg/cpu-exec.c */</span><br><span class="line">cpu_handle_exception</span><br><span class="line">     /* 检测cpu-&gt;exception_index，这个就是flag */</span><br><span class="line">  -&gt; cc-&gt;tcg_ops-&gt;do_interrupt</span><br><span class="line">     /* target/riscv/cpu.c */</span><br><span class="line">     (riscv_cpu_do_interrupt)</span><br><span class="line">       -&gt; 配置各种cpu状态，其中包括pc</span><br><span class="line">       -&gt; riscv_cpu_set_mode</span><br></pre></td></tr></table></figure>

<p> 再以ecall指令的模拟函数为例看下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* target/riscv/insn_trans/trans_privileged.c.inc */</span><br><span class="line">trans_ecall</span><br><span class="line">  -&gt; generate_exception</span><br><span class="line">       /*</span><br><span class="line">        * 这个函数是宏生成的，相关的定义在riscv目录的helper.h，顺着查看就可以，</span><br><span class="line">        * 有个注意的地方是其中有个名叫glue的宏，作用是字符串拼接。</span><br><span class="line">	*/</span><br><span class="line">    -&gt; gen_helper_raise_exception</span><br><span class="line">      -&gt; tcg_gen_callN</span><br><span class="line">        -&gt; 在哈希表里找注册的函数，这里名字是helper_raise_exception</span><br><span class="line">	   这个函数定义在riscv目录下的op_helper.c，可以看到其中配置了cpu里的</span><br><span class="line">	   exception_index，然后cpu_restore_state，cpu_loop_exit。</span><br></pre></td></tr></table></figure>
<p> 但是helper_raise_exception怎么注册的？看下helper_table这哈希表在哪里初始化的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/tcg.c */</span><br><span class="line">tcg_context_init</span><br><span class="line">  -&gt; g_hash_table_insert(helper_table, all_helpers[i].func, ...)</span><br></pre></td></tr></table></figure>
<p> 上面把all_helpers这个表里的元素一个一个加到哈希表里。all_helpers这个表是静态<br> include了一个头文件生成的，里面把helper.h也包含了进来，但是上面分析helper.h<br> 里生成的是gen_helper_raise_exception，这里qemu里针对DEF_HELP_FLAGS_2尽然定义<br> 了两个版本，helper-gen.h里的生成gen_helper_xxx函数，helper-tcg.h里生成哈希表里<br> 的注册entry，用undef控制宏的作用域。</p>
<p> 至此qemu tcg模拟异常处理的逻辑就都打通了。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu模拟ARM构架综合分析</title>
    <url>/qemu%E6%A8%A1%E6%8B%9FARM%E6%9E%84%E6%9E%B6%E7%BB%BC%E5%90%88%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="总体介绍"><a href="#总体介绍" class="headerlink" title="总体介绍"></a>总体介绍</h2><p> 一般我们看一个构架的模拟，可能涉及这几个方面：</p>
<ul>
<li><p>非特权态指令模拟</p>
<p>这个模拟相对直接，主要做的工作是写中间码去模拟一条一条的指令，这些都是一些和<br>特权态无关的指令，涉及的主要是计算、跳转和load/store指令，这些指令一般改变通用<br>寄存器、内存以及PC寄存器的状态。</p>
</li>
<li><p>特权态指令模拟</p>
<p>特权指令可能会改变CPU的状态，相关的模拟可能和CPU异常处理有关系。ARMv8上使用<br>MSR/MRS两条指令，用系统寄存器的id访问系统寄存器，具体逻辑在下面整理。</p>
</li>
<li><p>CPU内部中断异常模拟</p>
</li>
<li><p>machine模拟</p>
</li>
<li><p>关键外设模拟</p>
</li>
</ul>
<p> 一般我们只关心前端翻译就好，前端翻译的代码一般在hw/arm，target/arm这两个目录下,<br> hw/arm放machine相关的代码，arm的这个目录下放了一堆不同厂家的平台代码，一般我们<br> 只使用virt这个平台，smmu的代码也在这里，gic相关的代码在hw/intc，target/arm下放<br> 指令模拟和中断异常相关的代码，集中在cpu核的模拟。</p>
<h2 id="系统寄存器访问"><a href="#系统寄存器访问" class="headerlink" title="系统寄存器访问"></a>系统寄存器访问</h2><p> ARM64的前端翻译入口是aarch64_tr_translate_insn，ARM这里写的又点乱，直接解析指令<br> 的编码，根据编码的特定域段进入不同类指令的解码函数解码，比较起来RISCV写的就很清<br> 爽了，RISCV上把指令全部定义文件里，通过脚本自动生成一个decode函数，所有解码的行为<br> 直接调用decode函数就好。</p>
<p> 系统寄存器解码的调用路径是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* target/arm/translate-a64.c */</span><br><span class="line">aarch64_tr_translate_insn</span><br><span class="line">  +-&gt; disas_b_exc_sys</span><br><span class="line">    +-&gt; disas_system</span><br><span class="line">      +-&gt; handle_sys(s, insn, l, op0, op1, op2, crn, crm, rt)</span><br><span class="line">        +-&gt; get_arm_cp_reginfo</span><br></pre></td></tr></table></figure>
<p> 从get_arm_cp_reginfo可以看出，系统寄存器被保存在名为cp_regs的一个哈希表里，这个<br> 函数就是通过指令的各个域段作为key找到相关系统寄存器的描述结构体，寄存器的相关<br> 操作函数都定义在这个结构体里，在系统初始化的时候插入到cp_regs哈希表里:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* target/arm/cpu.c */</span><br><span class="line">arm_cpu_realizefn</span><br><span class="line">  +-&gt; register_cp_regs_for_features</span><br><span class="line">        /* 在V8这个分支定义相关和注册的寄存器 */</span><br><span class="line">    +-&gt; if (arm_feature(env, ARM_FEATURE_V8))</span><br><span class="line">      [...]</span><br><span class="line">          /* 底层就是把定义的寄存器插入到cp_regs哈希表里 */</span><br><span class="line">      +-&gt; define_arm_cp_regs</span><br></pre></td></tr></table></figure>

<h2 id="CPU内部中断异常模拟"><a href="#CPU内部中断异常模拟" class="headerlink" title="CPU内部中断异常模拟"></a>CPU内部中断异常模拟</h2><p> (todo: )</p>
<h2 id="machine模拟"><a href="#machine模拟" class="headerlink" title="machine模拟"></a>machine模拟</h2><p> 我们只看virt平台的模拟逻辑，机器实例的初始化函数是machvirt_init。</p>
<p> (todo: 启动、多核、NUMA)</p>
<h2 id="关键外设模拟"><a href="#关键外设模拟" class="headerlink" title="关键外设模拟"></a>关键外设模拟</h2><p> ARM64的关键外设有GIC中断控制器和SMMU。</p>
<p> (todo: GIC)</p>
<p> SMMU的qemu模拟逻辑可以参考<a href="https://wangzhou.github.io/qemu-iommu%E6%A8%A1%E6%8B%9F%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90/">这里</a>。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>ARM64</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu编译系统分析</title>
    <url>/qemu%E7%BC%96%E8%AF%91%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> 首先qemu编译系统中，描述整个构建依赖是使用meson来做的，这个是qemu编译系统的核心。<br> meson是和make类似的构建工具，它的官网提供很好的指导文档，也可以参考<a href="https://wangzhou.github.io/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8meson%E6%9E%84%E5%BB%BA%E7%A8%8B%E5%BA%8F/">这里</a>快速了解下。</p>
<p> 配置使用的configure其实是一个shell脚本，它的主体都在解析configure的输入以及做系统<br> 环境的监测，最后使用收集到的参数调用meson setup。注意configure的那些–disable/–enable<br> 的参数都是按照字符串匹配去做检测的，所以不是随便每个参数都可以用这种方式配置。<br> 使用configure –help可以直观的看到可以在configure命令行直接可以配置的参数。</p>
<p> qemu根目录下有meson.build和meson_options.txt，前者除了定义构建的依赖关系，还做了<br> 大量系统监测的工作，根据监测的结果，meson使用configure这个特性直接把相关配置写到<br> config文件里，注意qemu meson生成了很多配置文件，你用“configure_file”去搜就是可以<br> 看到所有动态生成的配置文件。meson_options.txt定义了很多可选的配置项，相关的option<br> 项的type是feature，一般可以直接把value域段配置成disabled，从而关闭相关配置。</p>
<p> 顶层meson.build里通过一个个subdirs把下层目录包含进来，以此类推，在最底层使用source_set<br> 方法得到source set对象，调用这个对象的add方法把源文件一个一个收集起来，写入顶层<br> meson.build中定义的存放代码的字典类型的数据结构里，比如hw_arch，target_arch，<br> target_user_arch，target_softmmu_arch等。</p>
<p> configure执行meson setup会把依赖写入build.ninja，qemu没有直接用meson compile或者<br> ninja来构建，而是在外面又套了一层Makefile，Mafile里判断各种依赖，追后还是调用ninja<br> 来做构建。</p>
<p> qemu里还定义了很多Kconfig配置文件，比如hw/riscv/Kconfig、target/riscv/Kconfig等，<br> 这些Kconfig使用select定义了配置之间的一些逻辑关系，但是这个是在哪里解析的？</p>
<h2 id="一个故事"><a href="#一个故事" class="headerlink" title="一个故事"></a>一个故事</h2><p> 基于以上的认识，我们看看如何关掉CONFIG_PTHREAD_SETNAME_NP_W_TID这个配置，我们先<br> 问下chatGPT，它的一本正经的说：./configure –enable-pthread-setname_np_w_tid<br> 根据以上的分析，它是在胡扯了，搜索可以发现这个选项定义在顶层meson.build里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">config_host_data.set(&#x27;CONFIG_PTHREAD_SETNAME_NP_W_TID&#x27;, cc.links(gnu_source_prefix + &#x27;&#x27;&#x27;</span><br><span class="line">  #include &lt;pthread.h&gt;</span><br><span class="line"></span><br><span class="line">  static void *f(void *p) &#123; return NULL; &#125;</span><br><span class="line">  int main(void)</span><br><span class="line">  &#123;</span><br><span class="line">    pthread_t thread;</span><br><span class="line">    pthread_create(&amp;thread, 0, f, 0);</span><br><span class="line">    pthread_setname_np(thread, &quot;QEMU&quot;);</span><br><span class="line">    return 0;</span><br><span class="line">  &#125;&#x27;&#x27;&#x27;, dependencies: threads))</span><br></pre></td></tr></table></figure>
<p> 如上是用meson compiler property的links方法测试下面这段代码是否可以link libthreads<br> 库，如果可以link，就把这个配置写入config_host_data这个对象，这个对象后面会写到<br> 配置文件config-host.h里。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">genh += configure_file(output: &#x27;config-host.h&#x27;, configuration: config_host_data)</span><br></pre></td></tr></table></figure>
<p> 所以，如果临时hack的话，可以把config-host.h里的配置改下，然后直接去make，如果要<br> 改的彻底一点，都可以把这个检查直接去掉。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>编译链接</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu虚拟机通过tun/tap上网</title>
    <url>/qemu%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%80%9A%E8%BF%87tun-tap%E4%B8%8A%E7%BD%91/</url>
    <content><![CDATA[<p>参考: <span class="exturl" data-url="aHR0cDovL3d3dy4zNjBkb2MuY29tL2NvbnRlbnQvMTIvMDYxMS8xNC83OTgyMzAyXzIxNzQzODg1Ny5zaHRtbA==">http://www.360doc.com/content/12/0611/14/7982302_217438857.shtml<i class="fa fa-external-link-alt"></i></span></p>
<p>考虑用tun/tap的方式，那么需要宿主机(本人的是ubuntu12.04)的内核支持tun/tap的功能，<br>宿主机内核是支持这样的功能的，如果您用的操作系统内核不支持tun/tap，需要下载源码<br>然后编译对应的模块，然后插入对应的模块。</p>
<ol>
<li><p>主机已经有了/dev/net/tun, 需要修改可执行权限:<br>  sudo chmod o+x /dev/net/tun<br>  可以通过主机/boot/config-XXX文件查看是否配置了CONFIG_TUN, 2.6.X以后的内核<br>  已默认将TUN直接编译进内核，所以这里是CONFIG_TUN=y</p>
</li>
<li><p>下载tunctl工具的源码：<br>  <span class="exturl" data-url="aHR0cDovL3NvdXJjZWZvcmdlLm5ldC9wcmplY3RzL3R1bmN0bC8=">http://sourceforge.net/prjects/tunctl/<i class="fa fa-external-link-alt"></i></span><br>  解压之后直接make就好了。过程中有可能会出现make: docbook2man: Command not <br>  found, 这个是在生成对应的帮助文档的时候缺少了docbook2man的工具，现在ls一<br>  下发现tunctl已经编译好了，这个错误可以不用去管。同样的道理make install也<br>  会出错，不用make install了，直接用tunctl就可以了</p>
</li>
<li><p>参考[1]进行设置，要注意确定主机内核对TUN/TAP设备和虚拟网桥的支持，这里采用<br>  TUN/TAP的方式搭建网络。第一步已经确认了TUN/TAP，现在只需要确认网桥：<br>  思路是要先找到网桥的内核配置项, 然后去/boot/config-XXX下查看：<br>  随便一个内核源码，查看linux-src/net/bridge/Kconfig，其中第一项就是参考[1]<br>  中的网桥的说明，可见配置项就是CONFIG_BRIDGE; 在/boot/config-XXX中查看，发<br>  现CONFIG_BRIDGE=m, 于是用sudo modprobe bridge将其插入内核中</p>
</li>
<li><p>将参考[1]中的配置在这里重复一下：<br>  ifconfig eth0 down                # 关闭网口<br>  brctl addbr br0                   # 添加虚拟的网桥<br>  brctl addif br0 eth0              # 在网桥上添加网口<br>  brctl stp br0 off                 <br>  brctl setfd br0 1                 <br>  brctl sethello br0 1              <br>  ifconfig br0 0.0.0.0 promisc up       # 释放br0的ip地址<br>  ifconfig eth0 0.0.0.0 promisc up      # 释放eth0的ip地址<br>  dhclient br0# 自动获得br0的ip地址<br>  brctl show br0<br>  brctl showstp br0</p>
<p>  tunctl -t tap0 -u root        # 设定虚拟网卡上的端口tap0<br>  brctl addif br0 tap0# 在网桥上添加虚拟网卡的端口tap0<br>  ifconfig tap0 0.0.0.0 promisc up# 释放tap0的ip地址<br>  brctl showstp br0</p>
</li>
<li><p>启动qemu虚拟机：<br>  sudo qemu-system-x86_64 -m 1024 -net nic -net tap,ifname=tap0,script=no,downscript=no ./ubuntu_qemu.img<br>  这里假设虚拟机已经可以运转起来，关于ubuntu_qemu.img的制作可以查看qemu-img的用法</p>
</li>
<li><p>测试虚拟机网络：<br>  ping <span class="exturl" data-url="aHR0cDovL3d3dy5iYWlkdS5jb20v">www.baidu.com<i class="fa fa-external-link-alt"></i></span><br>  可以了</p>
</li>
</ol>
<p>附录：<br>   虚拟机自己的ip是：192.168.201.108/25<br>   br0的ip是：       192.168.201.23/25<br>   tap0, eth0没有ip, 但是eth0, br0的MAC地址相同<br>   主从之间通过上面的ip可以相互通信，另一台独立的pc通过虚拟机ip可以和虚拟机通信</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+----------+     </span><br><span class="line">|bridge:   |---&gt; br0 (ip:192.168.201.23/25 78:ac:c0:a8:fc:fe)</span><br><span class="line">|br0       |---&gt; eth0(78:ac:c0:a8:fc:fe)</span><br><span class="line">|          |</span><br><span class="line">+----------+</span><br><span class="line">     |</span><br><span class="line">     |tap0(ca:fd:08:a3:65:9d)</span><br><span class="line">     |</span><br><span class="line">     |eth0(192.168.201.108/25 52:54:00:12:34:56)</span><br><span class="line">+-----------+</span><br><span class="line">| guest os  |</span><br><span class="line">|           |</span><br><span class="line">+-----------+</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu里增加trace的方法</title>
    <url>/qemu%E9%87%8C%E5%A2%9E%E5%8A%A0trace%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<ol>
<li><p>在要加trace的模块对应的文件中加：#include “trace.h”</p>
</li>
<li><p>在trace.h文件里写 #include “trace/trace-xxx_xxx.h”</p>
<p>注意这里的连接符必须按照上面，其中的xxx是模块代码的路径，比如你的模块在qemu代码<br>的hw/arm下，如上应该写成#include “trace/trace-hw_arm.h”, 每一级路径都是下划线<br>连接。</p>
</li>
<li><p>在相同的目录创建trace-events文件，并在其中定义trace。定义的格式大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">example(uint8_t level, uint32_t offset, uint64_t pte) &quot;level: %u, pte offset: %u, pte value: 0x%lx&quot;</span><br></pre></td></tr></table></figure>
<p>“example”是trace点的名字，括号里是各个参数的类型，引号里是输出的内容。</p>
</li>
<li><p>在模块文件需要使用该trace点的地方使用 trace_example(xxx, xxx, xxx); 的方式调用。</p>
</li>
</ol>
<p>详细的说明可以参考qemu的开发手册：<span class="exturl" data-url="aHR0cHM6Ly9xZW11LXByb2plY3QuZ2l0bGFiLmlvL3FlbXUvZGV2ZWwvdHJhY2luZy5odG1s">https://qemu-project.gitlab.io/qemu/devel/tracing.html<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv AIA基本逻辑分析</title>
    <url>/riscv-AIA%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="AIA基本逻辑"><a href="#AIA基本逻辑" class="headerlink" title="AIA基本逻辑"></a>AIA基本逻辑</h2><p> 如下是AIA中APLIC和IMSIC的一个示意图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+------------+    +-------+    +-------------+    +---------------------------+</span><br><span class="line">|PCIe device |    | IOMMU |    | Bus network |    |   IMSIC                   |</span><br><span class="line">+------------+    |       |    |             |    |                           |</span><br><span class="line">        \         |       |    |             |    |  +---------------------+  |    +--------+</span><br><span class="line">         ---------+-------+-\  |             |    |  |M mode interrupt file|--+---&gt;| Hart 1 |</span><br><span class="line">                  |       |  \ |             |    |  +---------------------+  |    |        |</span><br><span class="line">                  +-------+   \|             |    |  +---------------------+  |    |        |</span><br><span class="line">                               +-------------+---&gt;|  |S mode interrupt file|--+---&gt;|        |</span><br><span class="line">+----------------+   +-----+  -+-------------+-/  |  +---------------------+  |    |        |</span><br><span class="line">|Platform device |--&gt;|APLIC| / |             |    |  +----------------------+ |    |        |</span><br><span class="line">+----------------+   |     |/  |             |    |  |Guest interrupt file 1|-+---&gt;|        |</span><br><span class="line">                     |     |   --------------+    |  +----------------------+ |    |        |</span><br><span class="line">+----------------+   |     |                      |  +----------------------+ |    |        |</span><br><span class="line">|Platform device |--&gt;|     |                      |  |Guest interrupt file 2|-+---&gt;|        |</span><br><span class="line">+----------------+   +-----+                      |  +----------------------+ |    |        |</span><br><span class="line">                                                  |  +----------------------+ |    |        |</span><br><span class="line">                                                  |  |Guest interrupt file N|-+---&gt;|        |</span><br><span class="line">                                                  |  +----------------------+ |    +--------+</span><br><span class="line">                                                  +---------------------------+</span><br></pre></td></tr></table></figure>
<p> 一个hart上M mode、S mode以及不同的vCPU都有不同的IMSIC interrupt file，每个IMSIC<br> interrupt file对下游设备提供一个MSI doorbell接口。PCIe设备写这个MSI doorbell接口<br> 触发MSI中断，APLIC写这个MSI doorbell接口也可以触发MSI中断。APLIC作为次一级的中断<br> 控制器可以把下游设备的线中断汇集到一个MSI中断上。</p>
<p> 标识一个MSI中断需要两个信息，一个CPU的外部中断，比如S mode external interrupt,<br> 另外一个是写入MSI doorbell的MSI message，对应的中断编号，前者叫major identity，<br> 后者叫minor identity。所谓interrupt file就是minor identity的线性表，里面保存着<br> 对应中断的配置情况，比如，enable/pending等状态。各个minor identity标识的中断的<br> 优先级随编号增大而降低。</p>
<p> 具体上看，每个interrupt file包含一个enable表和一个pending表，表中每个bit表示每个<br> MSI中的enable和pending状态。一个interrupt file支持的MSI中断个数，最小是63，最大<br> 是2047，从下面eip/eie寄存器的定义也可以得到这里的最大最小值，当eip/eie是32bit时，<br> 64个eip/eie寄存器可以表示的最大值是2048，当eip/eie是64bit时，协议定义奇数eip/eie<br> 是不存在的，这样可以表示的最大值也是2048。从下面qemu仿真中可以看到interrupt file<br> 在AIA硬件内部。</p>
<p> IMSIC通过一组CSR寄存器向外暴露信息或者接收CPU的配置。拿S mode的对应寄存器举例，<br> 相关的寄存器有：</p>
<ul>
<li><p>siselect/sireg</p>
<p>AIA使用siselect控制把如下寄存器映射到sireg上，这样通过两个CSR就可以访问一堆寄<br>存器，通过这种间接方式访问的寄存器有: eidelivery/eithreshold/eip0-eip63/eie0-eie63。</p>
<p>eidelivery控制imsic是否可以报中断给hart，其中有一个可选配置项是可以控制是否把<br>来自PLIC的中断直接报给hart。eithreshold可以设置优先级，比这个优先级高的中断才<br>能报给hart。eip0-eip63/eie0-eie63就是相关中断的pending/enable状态，一个bit表示<br>一个中断的相关状态。</p>
</li>
<li><p>stopi(S mode top interrupt)</p>
<p>读这个寄存器可以得到S mode下当前要处理的最高优先级的中断，包括major中断号和中断<br>优先级编号。(todo: 需要新增这个CSR的逻辑)</p>
</li>
<li><p>sseteipnum/sclreipnum/sseteienum/sclreienum</p>
<p>如上寄存器名字里，第一个s表示是S mode，set表示置1，clr表示清0，ei表示是外部中断，<br>p表示pending bit，num之前的e表示是enable bit，num表示操作的对象是中断的minor<br>identity编号。所以，这几个寄存器直接操作interrupt file里具体中断的pending和enable<br>状态。</p>
</li>
<li><p>stopei(S mode top external interrupt)</p>
<p>读这个寄存器可以得到S mode下当前要处理的最高优先级的外部中断的minor interrupt号。</p>
</li>
<li><p>seteipnum_le/seteipnum_be</p>
<p>这两个寄存器是MSI doorbell寄存器，在对应MSI doorbell page的最开始，一个是小端<br>格式，一个是大端格式，根据系统大小端配置，使用对应的寄存器。</p>
</li>
</ul>
<p> 可以看到riscv的MSI支持和ARM的GICv3(ITS)很不一样，imsic用一个表(逻辑上我们把pend/enable<br> 看成一个表)表示所有支持的MSI中断，这样PCI设备发出的MSI message其实对应的minor<br> interrupt identity，imsic收到minor interrupt identity后，直接配置对应的bit并且<br> 根据相关逻辑配置stopei, sseteipnum/sclreipnum/sseteienum/sclreienum也可以直接配置<br> interrupt file里的对应bit。而GICv3 ITS使用PCI设备相关的表格保存设备MSI中断对应的<br> 中断号，而且这些表格保存在内存里，可以想象GICv3在收到MSI message(ARM系统上一般<br> 一个PCI设备的MSI message从0开始依此递增)后应该从硬件报文里把设备信息(BDF)提取出来，<br> 然后再用设备信息去查找相关的表格得到MSI中断的硬件中断号，为了把这样的信息配置给<br> ITS，GICv3里就还需要设计各种command以及附带的command queue。从如上的分析中，我们<br> 可以看出为啥AIA设计比GICv3简单很多但是基本功能都支持的一些原因。</p>
<h2 id="APLIC的基本逻辑"><a href="#APLIC的基本逻辑" class="headerlink" title="APLIC的基本逻辑"></a>APLIC的基本逻辑</h2><p> (todo: …)</p>
<h2 id="IMSIC-DTS节点定义"><a href="#IMSIC-DTS节点定义" class="headerlink" title="IMSIC DTS节点定义"></a>IMSIC DTS节点定义</h2><p> IMSIC DTS节点各个域段的描述可以参考:<br> Linux/Documentation/devicetree/bindings/interrupt-controller/riscv.imsics.yaml。</p>
<p> 整个系统(包括NUMA系统)为M mode和S mode各创建一个imsic节点，如下是S mode的节点:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">imsics@28000000 &#123;                                               </span><br><span class="line">        phandle = &lt;0x12&gt;;                                       </span><br><span class="line">        riscv,group-index-shift = &lt;0x18&gt;;                       </span><br><span class="line">        riscv,group-index-bits = &lt;0x01&gt;;                        </span><br><span class="line">        riscv,hart-index-bits = &lt;0x02&gt;;                         </span><br><span class="line">        riscv,num-ids = &lt;0xff&gt;;                                 </span><br><span class="line">        reg = &lt;0x00 0x28000000 0x00 0x4000 0x00 0x29000000 0x00 0x4000&gt;;</span><br><span class="line">        interrupts-extended = &lt;0x10 0x09 0x0e 0x09 0x0c 0x09 0x0a 0x09 0x08 0x09 0x06 0x09 0x04 0x09 0x02 0x09&gt;;</span><br><span class="line">        msi-controller;                                         </span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        #interrupt-cells = &lt;0x00&gt;;                              </span><br><span class="line">        compatible = &quot;riscv,imsics&quot;;                            </span><br><span class="line">&#125;;                                                              </span><br></pre></td></tr></table></figure>
<p> 其中一堆group/hart-index等信息都是为了描述这个系统上各个cpu(vcpu)对应的MSI doorbell<br> 页面所在的位置。如上binding文件中描述了MSI doorbell page地址的计算方式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">XLEN-1           &gt;=24                                 12    0</span><br><span class="line">|                  |                                  |     |</span><br><span class="line">-------------------------------------------------------------</span><br><span class="line">|xxxxxx|Group Index|xxxxxxxxxxx|HART Index|Guest Index|  0  |</span><br><span class="line">-------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<p> group是NUMA node的概念，系统中不同NUMA节点上的MSI doorbell page所用的基地址不同，<br> 如上的系统有两个NUMA节点，所以reg域段有0x28000000和0x29000000两个基地址，每个NUMA<br> 节点上的MSI doorbell page按照如上的格式计算，格式中Guest Index/HART index的偏移和<br> 位宽在在DTS节点中定义在，没有定义的话就取binding文件中定义的默认值。</p>
<p> 所以，按照上面的DTS，我们可以得到有两个NUMA节点，每个NUMA节点里有4个CPU的场景下，<br> 这个系统上每个CPU的S mode MSI doorbell page的地址是：</p>
<p> 0x28000000 0x28001000 0x28002000 0x28003000<br> 0x29000000 0x29001000 0x29002000 0x29003000</p>
<h2 id="AIA-qemu模拟"><a href="#AIA-qemu模拟" class="headerlink" title="AIA qemu模拟"></a>AIA qemu模拟</h2><p> qemu tcg模拟imsic设备的驱动在：qemu/hw/int/riscv_imsic.c, riscv_aplic.c</p>
<ul>
<li>imsic基本逻辑</li>
</ul>
<p> imsic实例创建的接口是riscv_imsic_create，具体平台可以调用这个函数创建imsic设备,<br> imsic设备对外暴露一些规格相关的属性，比如mmode/hartid/num-pages/num-irqs，平台<br> 初始化的时候先根据对应的配置生成imsic dts节点，然后根据对应的配置模拟imsic设备。</p>
<p> imsic会暴露一些CSR寄存器给hart，在imsic的realize函数里调用hart侧的注册接口把访问<br> 具体寄存器的回调函数提供给hart的CSR访问框架:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_imsic_realize</span><br><span class="line">  +-&gt; riscv_cpu_set_aia_ireg_rmw_fn(..., riscv_imsic_rmw, ...)</span><br></pre></td></tr></table></figure>
<p> imsic创建一组MMIO模拟MSI doorbell寄存器seteipnum_le/seteipnum_be。</p>
<p> imsic的内部模拟逻辑也很直白，内部创建interrupt file以及相关寄存器的内存，来自设备<br> 的MSI message触发interrupt file的变化以及中断上报，来自CPU的CSR寄存器访问获得或者<br> 改变interrupt file以及imsic的配置状态。</p>
<p> (todo: aplic逻辑分析)</p>
<h2 id="AIA-Linux内核驱动"><a href="#AIA-Linux内核驱动" class="headerlink" title="AIA Linux内核驱动"></a>AIA Linux内核驱动</h2><p> Linux内核imsic的驱动在：Linux/drivers/irqchip/irq-riscv-imsic.c, irq-riscv-aplic.c</p>
<ul>
<li>imsic基本逻辑</li>
</ul>
<p> imsic在Linux内核里抽象为一个独立的中断控制器，代码构架和PLIC的逻辑基本上是一致的,<br> PLIC的内核驱动可以参考<a href="https://wangzhou.github.io/riscv-plic%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">这里</a></p>
<p> (todo: aplic逻辑分析)</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>中断</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu里pci设备的热插拔</title>
    <url>/qemu%E9%87%8Cpci%E8%AE%BE%E5%A4%87%E7%9A%84%E7%83%AD%E6%8F%92%E6%8B%94/</url>
    <content><![CDATA[<p>场景是一个pcie设备的vf通过vfio直通给qemu使用，这时如果我们在host上通过sysfs把<br>对应的vf disable掉。</p>
<p>正常来讲，qemu里的vf pci设备会表现为一个pci设备热拔出的行为。与之相对应的设置为：</p>
<ol>
<li><p>guest kernel的配置里要打开pci hotplug: CONFIG_HOTPLUG_PCI_PCIE.</p>
</li>
<li><p>guest kernel的启动cmdline里要是能pci native hotplug, 加上pcie_port=native</p>
</li>
<li><p>启动qemu的时候，需要把直通上来的pci vf挂到一个支持pci热插拔的pci桥下面:<br>比如在qemu里挂接一个ioh3420的pci桥，然后再把直通的vf挂在这个桥下。</p>
</li>
<li><p>本文的测试是在主线linux v5.0-rc6上做的，这个版本有一个pci hotplug的bug，这个<br>bug会导致虚拟机里vf无法被热拔。相关的fix补丁已经被pci maintainer ack, 会合<br>入v5.1主线版本。如果是在v5.0, 以及之前的内核的版本上测试，需要确认这个补丁<br>是否合入:</p>
<pre><code>[PATCH RESEND] PCI: pciehp: Assign ctrl-&gt;slot_ctrl before writing it to hardware
</code></pre>
</li>
</ol>
<p> 综合以上，如下的qemu启动命令，配合正确的kernel，可以支持qemu里直通vf的pci热拔<br> 操作:(这里已ARM64平台为例)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 -machine virt,gic_version=3 -enable-kvm -cpu host \</span><br><span class="line">-m 1024 -kernel ./Image -initrd ./minifs.cpio.gz -nographic -append \</span><br><span class="line">&quot;rdinit=init console=ttyAMA0 earlycon=pl011,0x9000000 pcie_ports=native&quot; \</span><br><span class="line">-device ioh3420,id=root_port \</span><br><span class="line">-device vfio-pci,host=0000:75:00.1,bus=root_port</span><br></pre></td></tr></table></figure>

<p>具体可以这样测试:(已HiSilicon D06 zip engine为例)</p>
<ol>
<li><p>在host上把vf和vfio驱动绑定:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo 1 &gt; /sys/devices/pci0000:74/0000:74:00.0/0000:75:00.0/sriov_numvfs</span><br><span class="line">echo 0000:75:00.1 &gt; /sys/bus/pci/drivers/hisi_zip/unbind</span><br><span class="line">echo vfio-pci &gt; /sys/devices/pci0000:74/0000:74:00.0/0000:75:00.1/driver_override</span><br><span class="line">echo 0000:75:00.1 &gt; /sys/bus/pci/drivers_probe</span><br></pre></td></tr></table></figure></li>
<li><p>启动qemu: 同上面的命令</p>
</li>
<li><p>在host上disable vf:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo 0 &gt; /sys/devices/pci0000:74/0000:74:00.0/0000:75:00.0/sriov_numvfs</span><br></pre></td></tr></table></figure></li>
</ol>
<p> 可以看到在qemu里，vf表现为一个pci热拔的动作:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(fix me: add log, 目前看上面的命令会在host上挂住)</span><br></pre></td></tr></table></figure>

<p>为了使得这篇介绍完整，对于qemu里pci设备的热插，可以这样来做:</p>
<ol>
<li><p>启动qemu后按ctrl a + c 进入qemu monitor(启动qemu的时候带ioh3420但是不带VF设备)</p>
</li>
<li><p>在qemu monitor里: device_add vfio-pci,host=0000:75:00.1,bus=root_port</p>
</li>
</ol>
<p> 这样可以把已经和vfio驱动绑定的VF PCI热插到qemu<br> (fix me: lspci看不到新设备，但是在qemu monitor里info pci可以看到新插入的设备)</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv AIA逻辑分析</title>
    <url>/riscv-AIA%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> 先看看要完成虚拟机中断，我们可以怎么做。一个直白的考虑是，所有的虚拟机中断都由<br> hypvisor也就是kvm来注入，这样注入中断需要qemu发kvm的ioctl，虚拟机里收到中断也处理<br> 不了，因为没有给虚拟机模拟guest内核可以看见的中断控制器，还要退出到qemu里处理中断。<br> 整个逻辑可以参考<a href="https://wangzhou.github.io/riscv-kvm%E4%B8%AD%E6%96%AD%E8%99%9A%E6%8B%9F%E5%8C%96%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/">https://wangzhou.github.io/riscv-kvm中断虚拟化的基本逻辑/</a></p>
<p> 如上的方式，中断注入和处理都需要qemu的参与，性能比较低。我们考虑怎么可以直接把<br> 中断送到虚拟机里，并且在虚拟机里就可以处理相关的中断。</p>
<p> 先看第二个问题，只需要叫guest内核可以直接访问到中断控制器的接口就好，直观的理解，<br> 就是在hypvisor(kvm)里给guest机器模拟一个中断控制器就好，实现上, 一方面要把中断<br> 控制器的信息在dtb里描述，这样guest内核才能获取中断控制器的信息，一方面要在第二层<br> 的地址翻译里加上中断控制器MMIO到实际物理MMIO的映射，这样guest内核里的中断控制器<br> 驱动才能物理上使用中断控制器，具体做法，就是在qemu里通过kvm的ioctl把上面的动作<br> 落实在硬件上。如果guest里有些控制是通过csr寄存器的，那么还要考虑csr的支持，这个<br> 要么硬件上就直接支持，否则需要trap进hypvisor去处理。 </p>
<p> 再看第一个问题：怎么把具体的中断送到虚拟机上。riscv的AIA imsic的拓扑大概是这样的：(AIA可以支持中断直通)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+------------+    +-------+    +-------------+    +---------------------------+</span><br><span class="line">|PCIe device |    | IOMMU |    | Bus network |    |   IMSIC                   |</span><br><span class="line">+------------+    |       |    |             |    |                           |</span><br><span class="line">        \         |       |    |             |    |  +---------------------+  |    +--------+</span><br><span class="line">         ---------+-------+-\  |             |    |  |M mode interrupt file|--+---&gt;| Hart 1 |</span><br><span class="line">                  |       |  \ |             |    |  +---------------------+  |    |        |</span><br><span class="line">                  +-------+   \|             |    |  +---------------------+  |    |        |</span><br><span class="line">                               \             |    |  |S mode interrupt file|--+---&gt;|        |</span><br><span class="line">                               |\            |    |  +---------------------+  |    |        |</span><br><span class="line">                               | ---------\  |    |  +----------------------+ |    |        |</span><br><span class="line">                               |           \ |    |  |Guest interrupt file 1|-+---&gt;|        |</span><br><span class="line">                               |            \|    |  +----------------------+ |    |        |</span><br><span class="line">                               |             \    |  +----------------------+ |    |        |</span><br><span class="line">                               |             |\   |  |Guest interrupt file 2|-+---&gt;|        |</span><br><span class="line">                               |             | \  |  +----------------------+ |    |        |</span><br><span class="line">                               |             |  \ |  +----------------------+ |    |        |</span><br><span class="line">                               |             |   &gt;|  |Guest interrupt file N|-+---&gt;|        |</span><br><span class="line">                               +-------------+    |  +----------------------+ |    +--------+</span><br><span class="line">                                                  +---------------------------+</span><br></pre></td></tr></table></figure>
<p> 从上图可以看出来，物理的core上，对于每个可以支持虚拟机，是存在物理的连接的。PCIe<br> 设备发出一个MSI中断(实际上是对一个地址的写操作)，进过IOMMU翻译得到物理地址，如果<br> 写在Guest interrupt file对应的地址上，中断信号就会送到Hart1(假设没有受IMSIC上配置<br> 的影响)。到了这里，后面的逻辑就比较有意思了，Hart1现在可能运行在不同的实例，比如，<br> 现在是Guest N的中断来了，但是Hart1可能跑Guest 1的实例，也可以跑host系统。如果，<br> Hart1跑的是Guest N的实例，那么直接中断现在CPU的运行就好，也就是说，硬件需要知道<br> 两个信息，一个是Hart1上跑的是哪个实例，一个是相关中断是发给哪个实例的，只有知道<br> 这两个信息，硬件才知道当前中断是不是发给当前实例的，具体上只要给中断和Hart1上都<br> 加上VMID这个信息就好。如果，中断和当前CPU上运行的实例不匹配，直白的做法是把这个<br> 中断记录在虚拟机管理器(也就是hypvisor里，hypvisor管理这虚拟机，必然要维护虚拟机<br> 的状态)，等到对应虚拟机投入Hart1上运行的时候，就可以响应这个中断。如果这样做，<br> 虚拟机上中断的响应完全依赖于hypvisor里虚拟机的调度，中断响应可能会不及时，一个<br> 可以想到的做法是，硬件识别到不是给当前实例的中断时，就把这个信息报到hypvisor上，<br> hypvisor可以调度对应的guest实例运行，具体实现上，可以用VMID去做这个识别。</p>
<p> 到此为止一切都好，但是你去看riscv协议，就会发现里面VMID这个概念只局限在第二层地址<br> 翻译常，并没有用VMID识别虚拟机。那riscv是怎么搞定上面的问题的。</p>
<p> S_GEXT被硬件直接配置mideleg代理到了HS，所以一旦有这个中断就在HS中做中断处理(虚拟<br> 机拉起之前并没有做继续委托)。看起来riscv的逻辑是这样的，hstatus.VGEIN可以实现类似<br> 过滤器的功能，当hstatus.VGEIN域段的数值和hgeip表示的vCPU相等时，mip.VSEIP才能被<br> 配置上，这样当一个特定的vCPU被调度运行时，hypvisor在投入vCPU运行之前把vCPU对应的<br> VGENIN打开，这样这个vCPU上的VS中断就可以直通到vCPU。但是，依照之前的分析，S_GEXT<br> 中断也会上报到hypvisor, 那就需要有机制可以做到，当VS中断对应的vCPU不在位的时候，<br> 中断投递到hypvisor，当VS中断对应的vCPU在位的时候，中断只直通到vCPU。前者可以通过<br> VGENIN过滤掉，针对后者，riscv上定义了hgeie，这个寄存器决定哪个vCPU的S_GEXT是有效<br> 的。所以，在一个vCPU投入运行之前，hypvisor可以配置VGENIN的值是这个vCPU的编号，<br> 配置hgeie对于这个vCPU无效，在这样的配置下，当这个vCPU对应的VS中断到来时，中断<br> 被直通到guest，当来的不是这个vCPU的VS中断时，在HS触发S_GEXT中断。</p>
<h2 id="硬件逻辑"><a href="#硬件逻辑" class="headerlink" title="硬件逻辑"></a>硬件逻辑</h2><p> 整个中断虚拟化需要riscv的H扩展和AIA中断控制器的配合完成，但是H扩展和AIA的逻辑是<br> 独立的，各自的逻辑都可以自圆其说。</p>
<p> H扩展的介绍可以参考<a href="https://wangzhou.github.io/riscv-KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E5%88%86%E6%9E%90/">https://wangzhou.github.io/riscv-KVM虚拟化分析/</a><br> 最核心的地方是，一个物理Hart上，定义了hgeie/hgeip寄存器，这个寄存器上的每个bit<br> 都对应这个物理Hart上一个虚拟机上的外部中断，hgeip表示对应虚拟机上有没有外部中断<br> 上报，hgeie表示对应的虚拟机外部中断会不会触发SGEI中断，H扩展的定义不关心外部的<br> 中断控制器。H扩展增加了SGEI这个中断类型，当接收到虚拟机外部中断时，硬件通过SGEI<br> 中断把这个信息报给hypvisor, hypvisor就可以去调度虚拟机投入运行，hgeie可以控制针对<br> 具体虚拟机的外部中断，是否上报SGEI。hstatus里的VGEIN控制一个具体虚拟机的外部中断<br> 是否可以直通到虚拟机，所谓直通到虚拟机，就是这个中断会触发CPU直接进入vCPU的中断<br> 上下文里。整个逻辑怎么串起来，在上面的章节里已经说明。</p>
<p> 下面看AIA的逻辑，riscv的aclint和plic是不支持PCI的MSI中断的，也不支持虚拟化中断<br> 直通，AIA主要是补齐了相关功能。具体看，AIA新增了IMSIC(incoming MSI controller),<br> APLIC(Advanced plic)，以及MSI中断直通对于IOMMU的要求。</p>
<p> IMSIC是一个可以独立使用的可以接收MSI的中断控制器，从上面章节中的示意图上可以看到，<br> 每个物理的HART都有一个独立的IMSIC，这个IMSCI在M mode、S mode以及对于虚拟化都有<br> 独立的资源，针对虚拟机的资源是每个虚拟机都有一份的，所谓资源，IMSIC上叫做interrupt<br> file，每个interrupt file有一个物理的MSI doorbell接口，而一个interrupt file被用来<br> 记录所有通过它上报的中断。我们从单个中断的视角再走一遍，也就是说中断写了，比如，<br> guest 1 interrupt file的MSI doorbell，那么hgeip的对应bit就会置1，如果这时hgeie<br> 对应bit置1，SGEI中断就会被触发，SGEI会被硬件代理到HS，那么就会进入hypvisor处理这个<br> 中断，如果hgeie对应bit是0，那么SGEI中断不会被触发，如果这时VGEIN配置成1，那么VS<br> 中断被触发，一般在拉起虚拟机之前，hypvisor已经把VS中断代理到VS，这时，这个中断<br> 就直接导致CPU进入vCPU的中断上下文。需要注意的是，我们看问题的时候，一般不要这样<br> 顺着状态变迁分析，但是这样看一遍会叫我们对这个问题有一个感性直观的问题。</p>
<p> APLIC可以单独使用，APLIC也可以配合IMSIC使用，如果APLIC配合IMSIC使用，那么APLIC<br> 的输出必须被连在IMSIC的输入上，这样一个线中断被转成一个MSI中断。APLIC单独使用的<br> 时候，不支持虚拟化中断直通。本文先不去分析APLIC的逻辑，这个需要在独立的文档中分析。</p>
<p> 理论上，我们对一个guest interrupt file的MSI doorbell写数据就可以触发对应的虚拟机<br> 机外部中断处于pending。但是，虚拟机里的直通设备并不能直接看到guest interrupt file<br> MSI doorbell的物理地址，所以，需要在guest的地址空间上为guest interrupt file的MSI<br> doorbell建立对应的映射，实际上就是在第二级页表里添加虚拟MSI doorbell到物理MSI<br> doorbell的映射。</p>
<h2 id="QEMU模拟逻辑"><a href="#QEMU模拟逻辑" class="headerlink" title="QEMU模拟逻辑"></a>QEMU模拟逻辑</h2><p> qemu里模拟中断的基本思路可以参考这里：<a href="https://wangzhou.github.io/qemu-tcg%E4%B8%AD%E6%96%AD%E6%A8%A1%E6%8B%9F/">https://wangzhou.github.io/qemu-tcg中断模拟/</a><br> qemu里的实现需要分两个方面去看，一个是AIA在qemu是怎么被模拟的，一个是AIA在qemu里<br> 是怎么被使用的，怎么被使用是说qemu里怎么调用KVM的接口在KVM的把AIA创建出来。<br> 如果用<a href="https://wangzhou.github.io/%E6%9E%84%E5%BB%BAriscv%E4%B8%A4%E5%B1%82qemu%E7%9A%84%E6%AD%A5%E9%AA%A4/">这里</a>提到的两层qemu<br> 的方法模拟整个系统，第一个方面是指第一层qemu的AIA的模拟，第二个方面指的是第二层<br> qemu的怎么创建KVM里的AIA设备。</p>
<p> 当前社区中的测试代码，第二层qemu是使用kvmtool代替的，kvmtool可以被简单理解为qemu<br> 里去掉tcg，只保留kvm的部分。</p>
<p> 如下是qemu中模拟AIA的基本逻辑:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* target/riscv/cpu.c */</span><br><span class="line">riscv_cpu_init</span><br><span class="line">      /* 这里是虚拟机外部中断的硬件信号的输入口 */</span><br><span class="line">  +-&gt; qdev_init_gpio_in(..., riscv_cpu_set_irq, IRQ_LOCAL_MAX+IRQ_LOCAL_GUEST_MAX)</span><br></pre></td></tr></table></figure>
<p> 这里定义CPU的中断输入接口，可以看到每个虚拟机的外部中断都会有一个实际的接口放出来。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_cpu_set_irq</span><br><span class="line">      /* 配置hgeip */</span><br><span class="line">  +-&gt; env-&gt;hgeip &amp;= ~((target_ulong)1 &lt;&lt; irq);</span><br><span class="line">  +-&gt; env-&gt;hgeip |= (target_ulong)1 &lt;&lt; irq;</span><br><span class="line">      /* 配置mip.SGEIP，并触发中断 */</span><br><span class="line">  +-&gt; riscv_cpu_update_mip(..., MIP_SGEIP, BOOL_TO_MASK(!!(env-&gt;hgeie &amp; env-&gt;hgeip))</span><br><span class="line">    +-&gt; gein = get_field(env-&gt;hstatus, HSTATUS_VGENIN);</span><br><span class="line">    +-&gt; vsgein = (env-&gt;hgeip &amp; (1ULL &lt;&lt; gein)) ? MIP_VSEIP : 0;</span><br></pre></td></tr></table></figure>
<p> 当上面的CPU中断输入接口上收到消息，就会调用这个函数处理，这个函数定义的是核内<br> 根据各种寄存器配置，对中断的处理逻辑。可以看到，这里的逻辑就是上面定义的实现。</p>
<p> 机器初始化的时候给每个core创建一个IMSIC：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* hw/riscv/virt.c */</span><br><span class="line">virt_create_aia</span><br><span class="line">      /* AIA中断控制器的出口和core的中断入口相接, 这个在AIA的qemu驱动里，hw/intc/riscv_imsic.c */</span><br><span class="line">  +-&gt; riscv_imsic_create</span><br><span class="line">    +-&gt; qdev_init_gpio_out_named(..., qdev_get_gpio_in(DEVICE(cpu), IRQ_LOCAL_MAX + i - 1))</span><br></pre></td></tr></table></figure>

<p> 如下是kvmtool里创建KVM里AIA的逻辑。首先是创建AIA设备，以及配置AIA设备:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* kvmtool/riscv/aia.c */</span><br><span class="line">aia__create</span><br><span class="line">  +-&gt; ioctl(..., KVM_CREATE_DEVICE, ...)</span><br><span class="line"></span><br><span class="line">/* kvmtool/riscv/aia.c */</span><br><span class="line">aia__init</span><br><span class="line">  +-&gt; 使用一组ioctl获取或者设置AIA device的属性。</span><br><span class="line">       /*</span><br><span class="line">        * 其中重要的一步是给AIA这个设备配置MMIO空间，可以想象，要叫guest的内核可以</span><br><span class="line">        * 直接访问这个MMIO空间，kvm里是需要给这个MMIO做stage 2的页表映射的。</span><br><span class="line">        *</span><br><span class="line">        * 我们从KVM_DEV_RISCV_AIA_GRP_ADDR这个kvm ioctl接口跟进去，会看到AIA的MMIO</span><br><span class="line">        * 地址被保存在了vcpu_aia-&gt;imsic_addr域段，查imsic_addr，可以发现它是在</span><br><span class="line">        * kvm_riscv_vcpu_aia_imsic_update里被更新到硬件，也就是把imsic_addr到实际</span><br><span class="line">        * 物理MMIO的映射加到stage2页表里。</span><br><span class="line">        * </span><br><span class="line">        * 可以看到这个映射是在vcpu投入运行之前加上的，调用逻辑是:</span><br><span class="line">        * kvm_arch_vcpu_ioctl_run</span><br><span class="line">        *   +-&gt; kvm_riscv_vcpu_aia_update</span><br><span class="line">        *     +-&gt; kvm_riscv_vcpu_aia_imsic_update</span><br><span class="line">        *</span><br><span class="line">        * 不过为啥要每次拉起虚拟机都做一次？</span><br><span class="line">        */</span><br><span class="line">  +-&gt; ioctl(aia_fd, KVM_SET_DEVICE_ATTR, &amp;aia_addr_attr)</span><br></pre></td></tr></table></figure>
<p>另外，kvmtool需要根据需要生成guest的dtb，其中就包括AIA的dtb，这个dtb里描述的AIA和上面<br>硬件定义的AIA匹配，guest内核用这个dtb的到AIA的信息，然后驱动上面配置好的AIA设备。</p>
<h2 id="Linux-KVM的相关逻辑"><a href="#Linux-KVM的相关逻辑" class="headerlink" title="Linux KVM的相关逻辑"></a>Linux KVM的相关逻辑</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* linux/arch/riscv/kvm/vcpu.c */</span><br><span class="line">kvm_arch_vcpu_create</span><br><span class="line">      /* 没有做什么 */</span><br><span class="line">  +-&gt; kvm_riscv_vcpu_aia_init </span><br><span class="line"></span><br><span class="line">/* linux/arch/riscv/kvm/main.c */</span><br><span class="line">kvm_arch_init</span><br><span class="line">      /* 初始化AIA以及中断虚拟化的一些全局参数 */</span><br><span class="line">  +-&gt; kvm_riscv_aia_init</span><br><span class="line">    +-&gt; csr_write CSR_HGEIE 的到hgeie的bit？</span><br><span class="line">        /*</span><br><span class="line">         * 每个物理CPU上维护一个aia_hgei_control的结构，在kvm这个层面管理这个物理</span><br><span class="line">         * CPU上vCPU的外部中断。</span><br><span class="line">         * </span><br><span class="line">         * 把IRQ_SEXT作为入参，调用irq_create_mapping得到一个hgei_parent_irq的中断号，</span><br><span class="line">         * 再给这个中断挂上中断处理函数。这里没有看懂?</span><br><span class="line">         * </span><br><span class="line">         * 似乎这个中断是直接报给kvm的，中断处理函数里通过CSR_HGEIE/CSR_HGEIP的到</span><br><span class="line">         * 中断发给哪个vCPU，对相应的vCPU做下kvm_vcpu_kick，这里没有看懂?</span><br><span class="line">         */</span><br><span class="line">    +-&gt; aia_hgei_init</span><br><span class="line">        /*</span><br><span class="line">         * 把AIA device注册一下，这样用户态下发ioctl创建AIA device直接在kvm公共</span><br><span class="line">         * 代码里调用AIA的回调函数就好。</span><br><span class="line">         */</span><br><span class="line">    +-&gt; kvm_register_device_ops(&amp;kvm_riscv_aia_device_ops, KVM_DEV_TYPE_RISCV_AIA)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>计算机体系结构</tag>
        <tag>中断</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv aclint逻辑分析</title>
    <url>/riscv-aclint%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> aclint是riscv上的核内中断本地中断控制器, 协议已经发布，内核(还没有到主线)和qemu<br> 中均已经做了支持。它的功能比较简单，就是提供了时钟中断和核间中断的功能。实现上，<br> 所有功能都是通过MMIO寄存器控制的，协议上抽象出了Machine-level Timer Device,<br> Machine-level Software Interrupt Device以及Supervisor-level Software Interrupt<br> Device三个设备。</p>
<p> 这里有几个概念要先分清楚，clint(core local interrupt)是和aclint(Advanced clint)<br> 独立的协议, clint发布在前，aclint兼容clint，发布在后。</p>
<p> qemu 7.1.50已经支持了aclint，但是如果想要打开，要在qemu启动命令行里加上-machine virt,aclint=on。<br> qemu启动加不加aclint=on，在qemu monitor里用info qtree查看设备, 会发现mtimer没有<br> 变化，但是swi在aclint=1的时候，多了一个，两个swi设备其中一个是clint的mswi, 一个是aclint<br> 的sswi。对应的在dts上，aclint没有开的时候，只是一个riscv,clint0节点，aclint打开之后<br> 变成了，mtimer/sswi/mswi三个节点, compatible分别是：riscv,aclint-mtimer/riscv,aclint-sswi/riscv,aclint-mswi。</p>
<p> 内核v5.19-rc8还没有支持aclint, 但是社区已经有patch在review。</p>
<h2 id="Machine-level-Timer-Device"><a href="#Machine-level-Timer-Device" class="headerlink" title="Machine-level Timer Device"></a>Machine-level Timer Device</h2><p> Timer的基本逻辑我们在<a href="https://wangzhou.github.io/riscv-timer%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/">riscv timer的基本逻辑</a>里已经介绍。</p>
<h2 id="Software-Interrupt"><a href="#Software-Interrupt" class="headerlink" title="Software Interrupt"></a>Software Interrupt</h2><p> 本文重点看看软件中断这部分。</p>
<p> clint下，只有M mode下的msip寄存器, 这个寄存器的最低bit读写被映射到CSR寄存器MIP的<br> MSIP bit，写这个bit可以触发其他核的M mode soft interrupt。</p>
<p> aclint下，兼容上面的msip寄存器，新增加了SETSSIP寄存器，并把msip寄存器和SETSSIP<br> 寄存器封装到了MSWI设备(Machine-level Software Interrupt Device)和<br> SSWI(Supervisor-level Software Interrupt Device)设备里。</p>
<p> 在开启aclint时，qemu会创建mswi和sswi设备，同时通过device tree上报两个设备节点。<br> qemu这块的代码在hw/intc/riscv_aclint.c, 用一套代码同时支持了mswi和sswi，虽然是<br> 一个读写接口，但是代码里用RISCVAclintSwiState里的sswi区分mswi和sswi。</p>
<p> 读写接口的实现和aclint协议是对应的，读接口上，是sswi时，返回0，是mswi时，返回MIP<br> 寄存器MIP_MSIP bit的值，写接口上，当写入值是1时，sswi和mswi都触发一个高电平中断<br> (协议上说是边沿中断)，当写入值是0时，mswi把中断线拉低，sswi没有动作。</p>
<p> 因为，aclint还没有在内核主线，内核目前使用msip的方式触发IPI，会看到内核会调用<br> S mode的ecall把触发IPI的请求交给BIOS处理，我们看到opensbi目前的实现，在处理IPT<br> 请求时是直接写了下MIP寄存器的MIP_SSIP。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>中断</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv call conversion速览</title>
    <url>/riscv-call-conversion%E9%80%9F%E8%A7%88/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p>riscv call conversion的协议在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3Jpc2N2LW5vbi1pc2EvcmlzY3YtZWxmLXBzYWJpLWRvYy9yZWxlYXNlcy9kb3dubG9hZC92MS4wL3Jpc2N2LWFiaS5wZGY=">这里<i class="fa fa-external-link-alt"></i></span>。所谓call conversion是指函数调用过程中二进制<br>接口的定义。如果一个程序都相同的编译器构建，自然二进制的接口是一样，但是，程序难免<br>要使用各种库，这样程序构建的时候就是直接链接库的二进制，双方的二进制接口就要对齐。</p>
<p>其实，只要是不同的二进制程序之间有相互关联都存在二进制接口对齐的问题，必须定义相关<br>的准则，比如一个内核模块和内核之间的ABI问题，在一个内核版本上编译好的内核模块在不<br>同的内核上可能就不能使用，这就需要发行版(这里讨论的是Linux的发行版)在一定版本内<br>ABI是要做到兼容的，比如redhat的发行版在大版本是一定会做到ABI兼容的。</p>
<p>call conversion定义的就是函数调用之间的准则，比如函数调用的时候怎么传递函数入参，<br>怎么保存寄存器等等。下面的章节逐一介绍下。</p>
<h2 id="寄存器相关的约定"><a href="#寄存器相关的约定" class="headerlink" title="寄存器相关的约定"></a>寄存器相关的约定</h2><p>函数调用到了新函数，要使用通用寄存器进行计算，但是这些寄存器上还有发起调用函数的<br>数据，必须把通用寄存器上的值先保存起来，等新函数使用完寄存器后，返回发起调用函数<br>执行后续代码的时候再恢复回来原先寄存器上的值。这就有两种办法，一种是发起调用的函<br>数负责恢复和保存寄存器，一种是在被调函数里使用寄存器之前先保存旧值，使用完再把把<br>寄存器的旧值恢复回去，一般CPU都会把整个通用寄存器(GPR)分成这两种寄存器，前者叫作<br>caller saved registers, 后者叫做callee saved registers。</p>
<p>我们先看caller saved registers，从被调函数看，它可以放心的使用caller saved registers，<br>因为调用函数已经做了保存，调用函数负责在调用被调函数之前保存caller saved registers，<br>调用函数并不需要保存所有的caller saved registers，它只要保存自己使用的caller saved<br>registers。</p>
<p>被调函数只需要保存以及恢复它使用的callee saved registers就好，对于callee saved<br>registers需要先保存再使用。</p>
<p>riscv定义的caller saved register有ra、t0-t6、a0-a7，定义的callee saved register<br>有sp、s0-s11，浮点也有对应的caller/callee saved registers。t0-t6、a0-a7和s0-s11<br>的属性是直接定义的，但是ra和sp的属性是天然自带的。ra保存函数的返回地址，涉及到的<br>指令是: jal rd, offset或者jalr rd, offset(rs1)，比如用jal ra, offset实现函数调用，<br>jal把pc跳到pc + offset，并把jal的下一条指令的地址保存到ra，子函数要靠ra返回到父<br>函数的调用点，同理父函数的ra保存的是父函数的返回地址，所以在父函数在调用子函数之<br>前要保存父函数自己的返回地址。sp是栈指针，进入函数首先就要开栈，为函数的临时变量<br>准备存储空间，离开函数之前退栈，撤销函数的临时变量存储空间。</p>
<p>riscv定义的函数参数传递方式是使用a0-a7作为入参，使用a0-a1传递返回值，如果函数入参<br>寄存器放不下，使用调用函数栈传递参数，多余的参数从栈顶到栈底依此排列。</p>
<p>写个小程序，然后反汇编后对照的看下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int add(int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">	return a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	int a = 1, b = 2, c = 5;</span><br><span class="line"></span><br><span class="line">	c += add(a, b);</span><br><span class="line"></span><br><span class="line">	return c;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">0000000000000612 &lt;main&gt;:</span><br><span class="line"> 612:	1101                	addi	sp,sp,-32    开栈</span><br><span class="line"> 614:	ec06                	sd	ra,24(sp)    ra是caller save，下面638行会覆盖ra，所以这里比如caller save</span><br><span class="line"> 616:	e822                	sd	s0,16(sp)    s0 callee save, 保存完就可以用</span><br><span class="line"> 618:	1000                	addi	s0,sp,32     main函数上下文使用s0作为帧指针</span><br><span class="line"> 61a:	4785                	li	a5,1</span><br><span class="line"> 61c:	fef42223          	sw	a5,-28(s0)</span><br><span class="line"> 620:	4789                	li	a5,2</span><br><span class="line"> 622:	fef42423          	sw	a5,-24(s0)</span><br><span class="line"> 626:	4795                	li	a5,5</span><br><span class="line"> 628:	fef42623          	sw	a5,-20(s0)   a/b/c逐个入栈，其实没有必要。还有很多没有必要的入栈</span><br><span class="line"> 62c:	fe842703          	lw	a4,-24(s0)</span><br><span class="line"> 630:	fe442783          	lw	a5,-28(s0)</span><br><span class="line"> 634:	85ba                	mv	a1,a4        准备add入参</span><br><span class="line"> 636:	853e                	mv	a0,a5</span><br><span class="line"> 638:	fb3ff0ef          	jal	ra,5ea &lt;add&gt; 函数调用</span><br><span class="line"> 63c:	87aa                	mv	a5,a0        函数返回值a0</span><br><span class="line"> 63e:	873e                	mv	a4,a5</span><br><span class="line"> 640:	fec42783          	lw	a5,-20(s0)   从栈上读到c</span><br><span class="line"> 644:	9fb9                	addw	a5,a5,a4     c和函数返回值做加法</span><br><span class="line"> 646:	fef42623          	sw	a5,-20(s0)</span><br><span class="line"> 64a:	fec42783          	lw	a5,-20(s0)</span><br><span class="line"> 64e:	853e                	mv	a0,a5        准备main的返回值</span><br><span class="line"> 650:	60e2                	ld	ra,24(sp)    恢复caller save的ra，为ret做准备</span><br><span class="line"> 652:	6442                	ld	s0,16(sp)    恢复callee save的s0</span><br><span class="line"> 654:	6105                	addi	sp,sp,32     退栈</span><br><span class="line"> 656:	8082                	ret                  函数返回</span><br><span class="line"></span><br><span class="line">00000000000005ea &lt;add&gt;:</span><br><span class="line"> 5ea:	1101                	addi	sp,sp,-32    开栈</span><br><span class="line"> 5ec:	ec22                	sd	s0,24(sp)    callee save, 然后在函数上下文才能用s0做帧指针</span><br><span class="line"> 5ee:	1000                	addi	s0,sp,32     上面保存了s0，所以这里把s0用做帧指针</span><br><span class="line"> 5f0:	87aa                	mv	a5,a0        </span><br><span class="line"> 5f2:	872e                	mv	a4,a1</span><br><span class="line"> 5f4:	fef42623          	sw	a5,-20(s0)</span><br><span class="line"> 5f8:	87ba                	mv	a5,a4</span><br><span class="line"> 5fa:	fef42423          	sw	a5,-24(s0)</span><br><span class="line"> 5fe:	fec42703          	lw	a4,-20(s0)</span><br><span class="line"> 602:	fe842783          	lw	a5,-24(s0)</span><br><span class="line"> 606:	9fb9                	addw	a5,a5,a4</span><br><span class="line"> 608:	2781                	sext.w	a5,a5</span><br><span class="line"> 60a:	853e                	mv	a0,a5</span><br><span class="line"> 60c:	6462                	ld	s0,24(sp)    恢复callee save寄存器</span><br><span class="line"> 60e:	6105                	addi	sp,sp,32     退栈</span><br><span class="line"> 610:	8082                	ret                  函数返回</span><br></pre></td></tr></table></figure>

<p>函数参数用栈传递的情况：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int add(int a, int b, int c, int d, int e, int f, int g, int h, int i, int j)</span><br><span class="line">&#123;</span><br><span class="line">	return a + b + c + d + e + f + g + h + i + j;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	int a = 1, b = 2, c = 5;</span><br><span class="line">	</span><br><span class="line">	c += add(a, b, 3, 4, 5, 6, 7, 8, 9, 10);</span><br><span class="line"></span><br><span class="line">	return c;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">0000000000000680 &lt;main&gt;:</span><br><span class="line"> 680:	7179                	addi	sp,sp,-48</span><br><span class="line"> 682:	f406                	sd	ra,40(sp)</span><br><span class="line"> 684:	f022                	sd	s0,32(sp)</span><br><span class="line"> 686:	1800                	addi	s0,sp,48</span><br><span class="line"> 688:	4785                	li	a5,1</span><br><span class="line"> 68a:	fef42223          	sw	a5,-28(s0)</span><br><span class="line"> 68e:	4789                	li	a5,2</span><br><span class="line"> 690:	fef42423          	sw	a5,-24(s0)</span><br><span class="line"> 694:	4795                	li	a5,5</span><br><span class="line"> 696:	fef42623          	sw	a5,-20(s0)</span><br><span class="line"> 69a:	fe842583          	lw	a1,-24(s0)</span><br><span class="line"> 69e:	fe442503          	lw	a0,-28(s0)</span><br><span class="line"> 6a2:	47a9                	li	a5,10</span><br><span class="line"> 6a4:	e43e                	sd	a5,8(sp)</span><br><span class="line"> 6a6:	47a5                	li	a5,9</span><br><span class="line"> 6a8:	e03e                	sd	a5,0(sp)   &lt;--- 可以看出超过入参寄存器的参数</span><br><span class="line"> 6aa:	48a1                	li	a7,8            放到调用函数(caller)栈一开始的位置</span><br><span class="line"> 6ac:	481d                	li	a6,7</span><br><span class="line"> 6ae:	4799                	li	a5,6</span><br><span class="line"> 6b0:	4715                	li	a4,5</span><br><span class="line"> 6b2:	4691                	li	a3,4</span><br><span class="line"> 6b4:	460d                	li	a2,3</span><br><span class="line"> 6b6:	f35ff0ef          	jal	ra,5ea &lt;add&gt;</span><br><span class="line"> 6ba:	87aa                	mv	a5,a0</span><br><span class="line"> 6bc:	873e                	mv	a4,a5</span><br><span class="line"> 6be:	fec42783          	lw	a5,-20(s0)</span><br><span class="line"> [...]</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv kvm中断虚拟化的基本逻辑</title>
    <url>/riscv-kvm%E4%B8%AD%E6%96%AD%E8%99%9A%E6%8B%9F%E5%8C%96%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> kvm虚拟化的时候，guest的代码直接运行在host上，怎么样触发虚拟机中断是一个问题。<br> 在非虚拟化的时候，中断的触发是一个物理过程，中断被触发后，跳到异常向量，异常向量<br> 先保存被中断的上下文，然后执行异常向量的业务代码。但是，kvm虚拟化的场景，所谓虚拟机<br> 只是运行的线程，我们假设硬件可以直接触发中断，但是触发中断的时候，物理CPU都可能<br> 运行的是其他的虚拟机，怎么把特定虚拟机上的中断投递正确，这是一个需要解决的基本问题。</p>
<p> 我们再看另一个场景，在kvm虚拟化的时候，系统里有一个完全用软件模拟的IO设备，比如，<br> 一个网卡，那这个网卡的中断怎么传递给正在运行的虚拟机。从上帝视角看，运行kvm虚拟机<br> 就是在kvm ioctl(KVM_RUN)里切到虚拟机的代码去执行，要打断它，一个自然的想法就是从<br> qemu里再通过一定的方法“注入”一个中断，可以想象，所谓的“注入”中断，就是写一个虚拟机<br> 对应的寄存器，触发这个虚拟机上的执行流跳转到异常向量。</p>
<p> 对于核内的中断，比如时钟中断，还可以在host kvm里就直接做注入。比如，可以在kvm里<br> 起一个定时器，当定时到了的时候给虚拟机注入一个时钟中断。</p>
<p> 下面具体看下当前riscv的具体实现是怎么样的。</p>
<h2 id="时钟中断"><a href="#时钟中断" class="headerlink" title="时钟中断"></a>时钟中断</h2><p> 在kvm vcpu创建的时候，为vcpu创建timer，实现上就是创建一个高精度定时器，定时到了<br> 的时候给vcpu注入一个时钟中断。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* linux/arch/riscv/kvm/vcpu.c */</span><br><span class="line">kvm_arch_vcpu_create</span><br><span class="line">  +-&gt; kvm_riscv_vcpu_timer_init</span><br><span class="line">        /* 中断注入的接口被封装到这个函数里，中断注入接口是kvm_riscv_vcpu_set_interrupt */</span><br><span class="line">    +-&gt; t-&gt;hrt.function = kvm_riscv_vcpu_hrtimer_expired</span><br></pre></td></tr></table></figure>
<p> 可以看到注入中断实际上是给kvm管理的vcpu软件结构体的irqs_pending/irqs_pending_mask<br> bit置一，后面这个vcpu实际换到物理cpu上执行的时候，再写相应的寄存器触发cpu中断。</p>
<p> kvm同时通过ioctl接口向qemu提供一组timer寄存器的读写接口，看起来，qemu会在vm stop<br> 的时候把timer的这组寄存器读上来，在vm resume的时候把这组寄存器写下去，这样在vm<br> 停止的时候，vm的timer状态就是没有变化的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* linux/virt/kvm/kvm_main.c */</span><br><span class="line">kvm_vcpu_ioctl</span><br><span class="line">      /* linux/arch/riscv/kvm/vcpu.c, riscv里这个ioctl用来配置和读取寄存器，其中就包括timer相关寄存器的操作 */</span><br><span class="line">  +-&gt; kvm_arch_vcpu_ioctl</span><br><span class="line">    [...]</span><br><span class="line">          /*</span><br><span class="line">           * 这里只看下timer寄存器的配置接口, 如下的函数里就涵盖了</span><br><span class="line">           * frequency/time/compare/timer启动的配置，其中timer启动的实现就是启动</span><br><span class="line">           * 上面提到的高精度定时器</span><br><span class="line">           */</span><br><span class="line">      +-&gt; kvm_riscv_vcpu_set_reg_timer</span><br></pre></td></tr></table></figure>
<p> qemu里在target/riscv/kvm.c里把timer寄存器配置的函数注册到了qemu里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kvm_arch_init_vcpu</span><br><span class="line">  +-&gt; qemu_add_vm_change_state_handler(kvm_riscv_state_change, cs)</span><br></pre></td></tr></table></figure>
<p> 在vm_state_notify里调用，看起来vm_state_notify是在vm启动停止的时候才会使用，用来<br> 做kvm和qemu里的信息的同步。</p>
<p> 我们再考虑一个问题，怎么做到虚拟机的时间和实际时间相等。可以想象，只要让模拟vcpu<br> timer的那个定时器一直跑就可以，每次都把时间更新到vcpu的数据结构里就好。在每次停止<br> 和启动vm的时候，把时间在kvm和qemu之间做同步，vm停止的时候，vm的timer就也是停下来<br> 的。在qemu中date下获取当前时间，然后在qemu monitor里停止vm，过一会后启动vm，可以<br> 看见，时间基本是没有改变的。</p>
<h2 id="外设中断注入"><a href="#外设中断注入" class="headerlink" title="外设中断注入"></a>外设中断注入</h2><p> 完全虚拟的设备向kvm注入中断时，虚拟设备这一层发起中断的流程还和tcg下的一样，到了<br> 向vcpu触发中断这一步，没有向tcg那样写到vcpu在qemu的结构体里，因为写到这个结构体<br> 里丝毫不会对kvm运行的指令有影响，这一步使用kvm_riscv_set_irq向kvm里注入中断，<br> 这个函数的封装函数在target/riscv/cpu.c里注册：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* qemu/target/riscv/cpu.c */</span><br><span class="line">riscv_cpu_init</span><br><span class="line">      /* riscv_cpu_set_irq里会调用kvm_riscv_set_irq */</span><br><span class="line">  +-&gt; qdev_init_gpio_in(DEVICE(cpu), riscv_cpu_set_irq, ...)</span><br></pre></td></tr></table></figure>

<p> kvm_riscv_set_irq使用ioctl VM_INTERRUPT注入中断，基本的kvm里的调用流程如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* linux/virt/kvm/kvm_main.c */</span><br><span class="line">kvm_vcpu_ioctl</span><br><span class="line">      /* linux/arch/riscv/kvm/vcpu.c */</span><br><span class="line">  +-&gt; kvm_arch_vcpu_async_ioctl</span><br><span class="line">    +-&gt; kvm_riscv_vcpu_set_interrupt</span><br></pre></td></tr></table></figure>

<h2 id="中断直通"><a href="#中断直通" class="headerlink" title="中断直通"></a>中断直通</h2><p> riscv现在还没有中断直通，ARM的GICv4支持了这个功能，太复杂了，再独立的文章中分析吧。</p>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>中断</tag>
        <tag>riscv</tag>
        <tag>kvm</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv qemu virt平台CPU拓扑分析</title>
    <url>/riscv-qemu-virt%E5%B9%B3%E5%8F%B0CPU%E6%8B%93%E6%89%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>riscv的qemu virt平台描述一个完整的机器，这个机器包括CPU、内存、以及各种外设,<br>在qemu的代码里，这个virt机器用virt-machine类描述，它的实例是RISCVVirtState。</p>
<p>一个RISCVVirtState包含多个RISCVHartArrayState，这个数据结构描述一组CPU core, 一个<br>RISCVVirtState里会预留一定数量的RISCVHartArrayState，但是机器实际的RISCVHartArrayState<br>个数是根据qemu启动时配置的numa个数决定的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* hw/riscv/virt.c */</span><br><span class="line">virt_machine_init</span><br><span class="line">  +-&gt; riscv_socket_count</span><br></pre></td></tr></table></figure>
<p>所以在qemu上RISCVHartArrayState描述的是一个NUMA节点上的CPU core。</p>
<p>RISCVHartArrayState里用RISCVCPU来描述一个具体的CPU core，里面存放CPU的各种参数。<br>需要注意的是，虽然软件上看CPU似乎是分层的，但是在硬件(qemu)的角度看，CPU上的各种<br>寄存器只是CPU上的一个变量。RISCVHartArrayState、RISCVCPU都分别对应有自己的类和实例。</p>
<p>用图表示一下，virt平台上各个对象的实例大概就是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------------------------------------------------------------------------------+</span><br><span class="line">| RISCVVirtState                                                                  |</span><br><span class="line">|                                                                                 |</span><br><span class="line">| +-----------------------------------+     +-----------------------------------+ |</span><br><span class="line">| |  RISCVHartArrayState              |     |  RISCVHartArrayState              | |</span><br><span class="line">| |                                   |     |                                   | |</span><br><span class="line">| |+----------+----------+----------+ |     |+----------+----------+----------+ | |</span><br><span class="line">| || RISCVCPU | RISCVCPU | RISCVCPU | |     || RISCVCPU | RISCVCPU | RISCVCPU | | |</span><br><span class="line">| |+----------+----------+----------+ |     |+----------+----------+----------+ | |</span><br><span class="line">| +-----------------------------------+     +-----------------------------------+ |</span><br><span class="line">|                                                                                 |</span><br><span class="line">+---------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>下面描述如上各个对象的初始化逻辑，整个逻辑从virt平台初始化开始。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">virt_machine_init</span><br><span class="line">      /* 初始化hart_array, 被初始化的hart_array的指针通过&amp;s-&gt;soc[i]传入 */</span><br><span class="line">  +-&gt; object_initialize_child(OBJECT(machine), soc_name, &amp;s-&gt;soc[i], TYPE_RISCV_HART_ARRAY);</span><br><span class="line">      /* 触发hart_array的realize函数被调用 */</span><br><span class="line">  +-&gt; sysbus_realize(SYS_BUS_DEVICE(&amp;s-&gt;soc[i]), &amp;error_fatal);</span><br><span class="line">        /*</span><br><span class="line">	 * hart_array的realize函数, 其中会初始化和realize各个hart，因为是在hart_array</span><br><span class="line">	 * 的上下文，这里就可以针对不同的hart做一些差异化的配置，比如, 下面配置</span><br><span class="line">	 * 每个hart的hart id。</span><br><span class="line">	 */</span><br><span class="line">    +-&gt; riscv_harts_realize</span><br><span class="line">          /* RISCVCPU的内存是在hart_array的realize函数里申请的 */</span><br><span class="line">      +-&gt; s-&gt;harts = g_new0(RISCVCPU, s-&gt;num_harts);</span><br><span class="line">          /* 针对每个hart都调用一个下 */</span><br><span class="line">      +-&gt; riscv_hart_realize</span><br><span class="line">            /* RISCVCPU初始化, 对应的初始化函数在这里被调用(riscv_cpu_init) */</span><br><span class="line">        +-&gt; object_initialize_child(OBJECT(s), &quot;harts[*]&quot;, &amp;s-&gt;harts[idx], cpu_type);</span><br><span class="line">	  +-&gt; riscv_cpu_init</span><br><span class="line">	    /* 为每个hart配置复位pc地址 */</span><br><span class="line">        +-&gt; qdev_prop_set_uint64(DEVICE(&amp;s-&gt;harts[idx]), &quot;resetvec&quot;, s-&gt;resetvec);</span><br><span class="line">	    /* 为每个hart配置hart id */</span><br><span class="line">        +-&gt; s-&gt;harts[idx].env.mhartid = s-&gt;hartid_base + idx;</span><br><span class="line">        +-&gt; qemu_register_reset(riscv_harts_cpu_reset, &amp;s-&gt;harts[idx]);</span><br><span class="line">	    /*</span><br><span class="line">	     * 触发RISCVCPU的realize函数被调用，其中的细节分析，可以参考下面的链接。</span><br><span class="line">	     */</span><br><span class="line">        +-&gt; qdev_realize(DEVICE(&amp;s-&gt;harts[idx]), NULL, errp); (riscv_cpu_realize)</span><br><span class="line">	  +-&gt; [...]</span><br><span class="line">          +-&gt; qemu_init_vcpu(cs);</span><br><span class="line">          +-&gt; cpu_reset(cs);</span><br><span class="line">          +-&gt; mcc-&gt;parent_realize(dev, errp);</span><br></pre></td></tr></table></figure>
<p><a href="https://wangzhou.github.io/%E5%A4%9A%E6%A0%B8%E5%90%AF%E5%8A%A8%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">RISCVCPU realize函数细节分析</a></p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>riscv</tag>
        <tag>CPU</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv plic基本逻辑分析</title>
    <url>/riscv-plic%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="plic基本设计"><a href="#plic基本设计" class="headerlink" title="plic基本设计"></a>plic基本设计</h2><p> plic(platform-level interrupt controller)是riscv上一个核外中断控制器，这个中断<br> 控制器的spec不到20页，设计的很简单，只支持线中断。</p>
<p> riscv上有一堆中断控制器，clint/aclint/plic/aplic/aia。clint/aclint是核内的中断<br> 控制器，plic是一个简易的核外中断控制器，aplic是advanced plic，但是实际上aplic是<br> AIA定义的一部分，协议上aplic和plic没有兼容性上的联系。本文我们只分析plic的逻辑。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+------+     +--------+    M-mode irq   +--------------+</span><br><span class="line">| uart |----&gt;|  plic  |----------------&gt;| riscv hart0  |</span><br><span class="line">+------+     |        |                 |              |</span><br><span class="line">             |        |    S-mode irq   |              |</span><br><span class="line">        ----&gt;|        |----------------&gt;|              |</span><br><span class="line">             |        |----+            +--------------+</span><br><span class="line">        ----&gt;|        |---+| M-mode irq +--------------+</span><br><span class="line">             |        |--+|+-----------&gt;| riscv hart1  |</span><br><span class="line">        ----&gt;|        |-+||  S-mode irq |              |</span><br><span class="line">             +--------+ ||+------------&gt;|              |</span><br><span class="line">                        ||              |              |</span><br><span class="line">                        ||              +--------------+</span><br><span class="line">                        ||  M-mode irq  +--------------+</span><br><span class="line">                        |+-------------&gt;| riscv hartN  |</span><br><span class="line">                        |   S-mode irq  |              |</span><br><span class="line">                        +--------------&gt;|              |</span><br><span class="line">                                        |              |</span><br><span class="line">                                        +--------------+</span><br></pre></td></tr></table></figure>
<p> 如上是plic以及相关外设和cpu hart的拓扑结构，我们用一个uart示意下外设, 其它外设<br> 接入的逻辑和uart是一样的。对于一个riscv hart，plic有两个输出，分别接hart的M mode<br> 外部中断和S mode外部中断。外设的中断线固定的接入plic上的一个输入管脚。</p>
<p> 外设的中断信号通过plic中的各种逻辑后上报到riscv hart，plic的协议里甚至给了一个plic的硬件逻辑原理图：<br> <img src="/riscv-plic%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/plic_hardware.png" alt="PLIC硬件实现框图"></p>
<p> 结合如上的硬件实现框图和plic spec里的描述，我们很容易了解到plic内部的控制逻辑是<br> 怎么样的。简单的讲，plic的逻辑分gateway和core两大部分，gateway控制着外设的中断<br> 信号能否进来plic，core是plic具体的处理逻辑，大的逻辑上看，外设的中断信息可能被<br> 送到任意hart的外部中断输入管脚上，但是具体会送到哪个hart的哪个外部中断输入管脚上<br> 就要看plic的配置。</p>
<p> plic上的几个关键的定义有，中断ID(Interrupt Identifier)，中断通知(Interrupt Notification)，<br> 中断ID区分不同外设的中断，不同外设的中断会固定接到不同的中断ID上，这个定义会在机器<br> 硬件描述信息中体现，比如，描述在DTS里。plic为它支持的每个中断设置一个pending bit,<br> 用这个pending bit表示对应外设触发了相关的中断，plic spec里叫这个是Interrupt Notification,<br> 注意，每个外设的中断在plic里也只能pending一个，这个设计就有点简单了。plic有target<br> context的概念，简单看就是一个core上，M mode外部中断是一个target context，S mode<br> 外部中断是一个target context，前面说过，理论上每个输入plic的中断都可以输出到任何<br> 一个core的任何一个target context上，plic可以通过下面的enable register使能和关闭<br> 相关的输出路径，可见enable register是一个中断源和target context相乘之积大小的配置表。</p>
<p> plic中的配置全部通过MMIO接口进行，所支持的中断的pending和enable也都全部在MMIO配置，<br> 这种设计真是简单粗暴啊。我们下面把每个寄存器的定义展开，进一步说明plic：</p>
<ul>
<li><p>priority register</p>
<p>针对每个中断源的优先级配置，每个中断源都可以配置一个优先级。</p>
</li>
<li><p>pending bits register</p>
<p>针对每个中断源的配置。</p>
</li>
<li><p>enable register</p>
<p>如上提到的，每个中断源在每个target context下都有一个enable bit去配置。</p>
</li>
<li><p>threadhold register</p>
<p>针对每个target context的配置，当中断源的priority数值大于target context的数值时，<br>这个中断源的中断才能报给对应的target context。</p>
</li>
<li><p>claim register</p>
<p>针对每个target context的配置，target context从claim寄存器中读到中断ID号。</p>
</li>
<li><p>complete register</p>
<p>针对每个target context的配置，target context通过这个寄存器告诉plic中断已经处理，<br>plic在这之后才会打开gateway，叫后面的中断进来。</p>
</li>
</ul>
<p> plic spec上给了中断流程:<br> <img src="/riscv-plic%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/plic_flow.png" alt="PLIC中断处理流程"></p>
<h2 id="qemu-virt平台的定义"><a href="#qemu-virt平台的定义" class="headerlink" title="qemu virt平台的定义"></a>qemu virt平台的定义</h2><p> plic的qemu代码在qemu/hw/intc/sifive_plic.c，在qemu virt平台里有调用plic的创建函数<br> sifive_plic_create。</p>
<p> virt平台在创建串口的时候，把串口中断接到了plic上，中断ID是10:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">serial_mm_init(system_memory, memmap[VIRT_UART0].base,</span><br><span class="line">    0, qdev_get_gpio_in(DEVICE(mmio_irqchip), UART0_IRQ), 399193,</span><br><span class="line">    serial_hd(0), DEVICE_LITTLE_ENDIAN);</span><br></pre></td></tr></table></figure>
<p> 这个和uart的DTS节点是相匹配的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create_fdt_uart</span><br><span class="line">      /* 先看不是AIA的情况 */</span><br><span class="line">  +-&gt; qemu_fdt_setprop_cells(mc-&gt;fdt, name, &quot;interrupts&quot;, UART0_IRQ, 0x4);</span><br></pre></td></tr></table></figure>
<p> 启动qemu后可以得到这样的中断信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># cat /proc/interrupts </span><br><span class="line">           CPU0       </span><br><span class="line">  1:          0  SiFive PLIC  33 Edge      virtio0</span><br><span class="line">  2:         86  SiFive PLIC  10 Edge      ttyS0</span><br><span class="line">  3:          0  SiFive PLIC  11 Edge      101000.rtc</span><br><span class="line">  5:      13798  RISC-V INTC   5 Edge      riscv-timer</span><br><span class="line">IPI0:         0  Rescheduling interrupts</span><br><span class="line">IPI1:         0  Function call interrupts</span><br><span class="line">IPI2:         0  CPU stop interrupts</span><br><span class="line">IPI3:         0  IRQ work interrupts</span><br><span class="line">IPI4:         0  Timer broadcast interrupts</span><br></pre></td></tr></table></figure>

<h2 id="内核实现"><a href="#内核实现" class="headerlink" title="内核实现"></a>内核实现</h2><p> plic的Linux内核驱动在: linux/drivers/irqchip/irq-sifive-plic.c，要理解plic的代码<br> 还需要理解intc的逻辑，intc是CPU core本地”中断控制器”，是CPU core内部处理中断逻辑<br> 的一个抽象，可以简单的理解intc的输入是各个CPU中断类型，输出是中断CPU。</p>
<p> 本地”中断控制器”的初始化触发路径是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start_kernel</span><br><span class="line">      /* arch/riscv/kernel/irq.c */</span><br><span class="line">  +-&gt; init_IRQ</span><br><span class="line">    +-&gt; irqchip_init</span><br><span class="line">          /*</span><br><span class="line">           * 扫描__irqchip_of_table这个段里的所有中断控制器，然后调用它们的初始</span><br><span class="line">           * 化函数。</span><br><span class="line">           */</span><br><span class="line">      +-&gt; of_irq_init(__irqchip_of_table)</span><br></pre></td></tr></table></figure>
<p> 这个中断控制器的驱动在drivers/irqchip/irq-riscv-intc.c, 这个驱动会把中断控制器<br> 相关的structure of_device_id放到__irqchip_of_table段里。of_irq_init会扫面dts里<br> 所有的interrupt controller节点，并且按照最顶端的中断控制器依此向下遍历每个中断<br> 控制器节点，调用其中的初始化函数。具体到riscv就是依此调用riscv_intc_init、plic_init<br> 等，我们这里先考虑最简单的系统，即系统中只有plic。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_intc_init</span><br><span class="line">      /*</span><br><span class="line">       * 初始化intc对应的irq domain, 在riscv64上为这个irq domain分了64个中断输入，</span><br><span class="line">       * 这里的输入就是cpu core上对应的各种中断类型，比如M mode timer中断、S mode</span><br><span class="line">       * 外部中断等。</span><br><span class="line">       *</span><br><span class="line">       * 这里并没有为每个具体的中断类型建立irq_desc以及建立硬件中断号到virq的映射，</span><br><span class="line">       * 我们下面可以看到，直到有具体的用户时，才会建立对应的virq。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; irq_domain_add_linear</span><br><span class="line">  +-&gt; set_handle_irq</span><br></pre></td></tr></table></figure>
<p> set_handle_irq把riscv_intc_irq配置给handle_arch_irq，从entry.S的分析可以知道，<br> CPU被中断时会跳到handle_arch_irq执行，所以riscv_intc_irq就是riscv上中断处理的总<br> 入口。</p>
<p> plic会汇集外设的中断，然后接入CPU的M mode external irq和S mode external irq，所以<br> S mode external irq对应的irq_desc和virq是在plic初始化时做的，具体上，plic的初始化<br> 会找见plic的父中断控制器，就是intc，然后做相关操作:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">__plic_init</span><br><span class="line">      /*</span><br><span class="line">       * 通过plic dts中的interrupts-extended得到中断context的个数，简单情况下，</span><br><span class="line">       * 如果一个CPU core上有M mode external irq和S mode external irq，那么整个</span><br><span class="line">       * 系统上plic的context个数就是核数乘2。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; nr_contexts = of_irq_count(node)</span><br><span class="line">      /* 创建plic的irq domain */</span><br><span class="line">  +-&gt; irq_domain_add_linear</span><br><span class="line"></span><br><span class="line">  +-&gt; 针对每个context做如下迭代：</span><br><span class="line">        /*</span><br><span class="line">         * 找到plic的父节点，并调用irq_create_of_mapping，得到S mode external irq</span><br><span class="line">         * 的virq，这个过程会分配对应的irq_desc。</span><br><span class="line">         */</span><br><span class="line">    +-&gt; plic_parent_irq = irq_of_parse_and_map</span><br><span class="line">      +-&gt; irq_create_of_mapping</span><br><span class="line">        /* 配置intc S mode外部中断对应的处理函数 */</span><br><span class="line">    +-&gt; irq_set_chained_handler(plic_parent_irq, plic_handle_irq)</span><br></pre></td></tr></table></figure>

<p> 可以想象，一个中断连接到plic输入上的具体外设，应该是在外设对应的设备节点初始化<br> 的时候找见外设对应的plic节点，在plic irq domain上创建外设中断对应的irq_desc以及<br> virq。外设驱动里通过requst_irq接口把外设的中断处理函数注册给virq。</p>
]]></content>
      <tags>
        <tag>中断</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv qemu virt平台外设分析</title>
    <url>/riscv-qemu-virt%E5%B9%B3%E5%8F%B0%E5%A4%96%E8%AE%BE%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="分析线索"><a href="#分析线索" class="headerlink" title="分析线索"></a>分析线索</h2><p> riscv qemu virt上的各个设备的device tree节点生成和设备初始化在hw/riscv/virt.c里。<br> 基本逻辑是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * virt machine对象初始化函数, 这个函数的逻辑大概分三个部分：1. 创建相关外设；</span><br><span class="line"> * 2. 生成外设的device tree节点；3. 添加virt_machine_done函数，这个函数用来加载</span><br><span class="line"> * bios, 内核以及文件系统，准备开始运行bios之前的一段小程序。</span><br><span class="line"> */</span><br><span class="line">virt_machine_init</span><br><span class="line">      /* 创建核内中断控制器 */</span><br><span class="line">  +-&gt; riscv_aclint_swi_create/riscv_aclint_mtimer_create</span><br><span class="line">      /* 创建外设中断控制器 */</span><br><span class="line">  +-&gt; virt_create_plic</span><br><span class="line">      /* 创建内存和启动会用到的rom */</span><br><span class="line">  +-&gt; memory_region_add_subregion/memory_region_init_rom</span><br><span class="line">      /*</span><br><span class="line">       * 创建reboot和poweroff用的qemu设备，这个设备的逻辑比较简单，对外只呈现一个</span><br><span class="line">       * 写接口，poweroff请求就是exit qemu进程，reboot就是调用qemu reset相关函数</span><br><span class="line">       */</span><br><span class="line">  +-&gt; sifive_test_create</span><br><span class="line">      /* 创建virtio-mmio设备 */</span><br><span class="line">  +-&gt; sysbus_create_simple(&quot;virtio-mmio&quot;, ...)</span><br><span class="line">      /* 创建串口 */</span><br><span class="line">  +-&gt; serial_mm_init</span><br><span class="line">      /* 创建RTC设备 */</span><br><span class="line">  +-&gt; sysbus_create_simple(&quot;goldfish_rtc&quot;, ...)</span><br><span class="line">      /* 创建flash */</span><br><span class="line">  +-&gt; virt_flash_create/pflash_cfi01_legacy_drive/virt_flash_map</span><br><span class="line">      /* 创建外设的device tree节点 */</span><br><span class="line">  +-&gt; create_fdt</span><br><span class="line">      /* 这里只是注册一个回调函数 */</span><br><span class="line">  +-&gt; virt_machine_done</span><br></pre></td></tr></table></figure>
<p> 我们看看怎么快速找见上面外设的qemu实现和Linux内核驱动。由于device tree描述的平台<br> 设备驱动和设备绑定是通过compatible字段的，只要在内核里搜compatible字段的名字就好。<br> compatible的名字可以在create_fdt代码里得到，qemu支持只dump device tree，具体的<br> 命令是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-riscv64 -machine virt -machine dumpdtb=rv.dtb</span><br><span class="line">dtc rv.dtb -I dtb -o rv.dts</span><br></pre></td></tr></table></figure>
<p> 如果没有dtc这个工具，ubuntu上可以这样安装下：apt-get install device-tree-comiler</p>
<p> 在qemu monitor里使用info qtree可以列出虚拟机上有所得设备，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dev: fw_cfg_mem, id &quot;&quot;</span><br><span class="line">  data_width = 8 (0x8)</span><br><span class="line">  dma_enabled = true</span><br><span class="line">  x-file-slots = 32 (0x20)</span><br><span class="line">  acpi-mr-restore = true</span><br><span class="line">  mmio 0000000010100008/0000000000000002</span><br><span class="line">  mmio 0000000010100000/0000000000000008</span><br><span class="line">  mmio 0000000010100010/0000000000000008</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p> 其中dev这行的第一个域段是这个设备的名字，比如，如上fw_cfg_mem，我们就可以发现，<br> 它的qemu驱动在hw/nvram/fw_cfg.c, 这个设备的名字定义在：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* hw/nvram/fw_cfg.c */</span><br><span class="line">static const TypeInfo fw_cfg_mem_info = &#123;                                       </span><br><span class="line">    .name          = TYPE_FW_CFG_MEM,    &lt;--- fw_cfg_mem的宏定义</span><br><span class="line">    .parent        = TYPE_FW_CFG,                                               </span><br><span class="line">    .instance_size = sizeof(FWCfgMemState),                                     </span><br><span class="line">    .class_init    = fw_cfg_mem_class_init,                                     </span><br><span class="line">&#125;;                                                                              </span><br></pre></td></tr></table></figure>

<h2 id="逐个设备分析"><a href="#逐个设备分析" class="headerlink" title="逐个设备分析"></a>逐个设备分析</h2><p> qemu v7.1.50中，riscv virt平台上的设备有：fw_cfg_mem, cfi.pflash01, goldfish_rtc,<br> serial-mm, pci相关, virtio-mmio, riscv.sifive.test, riscv.sifive.plic, riscv.aclint.mtimer,<br> riscv.aclint.swi, riscv.hart_array。</p>
<p> 除了这些设备，还有PMU，因为是在核上，qemu的外设没有列出来。下面我们只是梳理下外设<br> 的大概逻辑，不过详细的分析。</p>
<ul>
<li><p>fw_cfg_mem</p>
<p>这个是qemu里定义的一个虚拟设备, 在qemu代码docs/specs/fw_cfg.rst有它的接口描述。<br>通过qemu启动命令可以把信息写入这个设备，系统上，通过内核驱动可以对这个设备做<br>读写以及配置操作。</p>
</li>
<li><p>cfi.pflash01</p>
<p>系统里有两个该类型的flash。</p>
</li>
<li><p>goldfish_rtc</p>
<p>一个提供墙上时钟的RTC设备。</p>
</li>
<li><p>serial-mm</p>
<p>经典的8250串口。</p>
</li>
<li><p>pci相关</p>
<p>提供PCI root port，其他的PCI/PCIe设备可以接入到这个root port下。</p>
</li>
<li><p>virtio-mmio</p>
<p>提供多个virtio-mmio设备，但是似乎这些设备没有具体的业务功能。</p>
</li>
<li><p>riscv.sifive.test</p>
<p>一个qemu中定义的设备，代码在hw/misc/sifive_test.c，这个设备很简单，就是提供了<br>一个MMIO写接口，软件通过这个写接口可以触发qemu虚拟机的poweroff和reset, 其中<br>poweroff就是exit掉qemu进程，reset是直接调用qemu_system_reset_request。</p>
<p>这个设备和syscon-reboot/syscon-poweroff共同支持虚拟机里linux系统的reset和poweroff。</p>
</li>
<li><p>riscv.sifive.plic</p>
<p>riscv系统上常用的核外外设中断控制器。 </p>
</li>
<li><p>riscv.aclint.mtimer</p>
<p>核内M mode timer，就是产生时钟中断的设备，一个这样的设备会有一组mtime, mtimcmp<br>寄存器，这两个寄存器是riscv特权级构架定义中的。</p>
</li>
<li><p>riscv.aclint.swi</p>
<p>核内software interrupt的设备，是一组核上的寄存器抽象出的设备，每个核都有一组<br>对应的寄存器，向相关寄存器写入数据触发其他核上的software interrupt，所以，Linux<br>内核里用这个接口触发IPI。</p>
<p>qemu 7.1.50上有两个这样的设备, 名字都叫riscv.aclint.swi，其中一个是sswi，另一个<br>是mswi。</p>
</li>
<li><p>riscv.hart_array </p>
<p>相关分析细节可以看<a href="https://wangzhou.github.io/riscv-qemu-virt%E5%B9%B3%E5%8F%B0CPU%E6%8B%93%E6%89%91%E5%88%86%E6%9E%90/">这里</a></p>
</li>
<li><p>riscv pmu</p>
<p>PMU设备，riscv核上用于做性能统计的寄存器组合起来抽象成这个设备。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv ebreak指令的使用</title>
    <url>/riscv-ebreak%E6%8C%87%E4%BB%A4%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="ebreak指令"><a href="#ebreak指令" class="headerlink" title="ebreak指令"></a>ebreak指令</h2><p> riscv上的ebreak指令和ecall指令的功能基本一致，都是用指令触发一个异常，ebreak使用<br> 指令出发一个ebreak异常。</p>
<h2 id="BUG-ON宏实现"><a href="#BUG-ON宏实现" class="headerlink" title="BUG_ON宏实现"></a>BUG_ON宏实现</h2><p> BUG_ON()是内核里常用的一个断言，当不满足BUG_ON的条件时，内核就会打印出当前的CPU<br> 寄存器上下文以及相关的内核调用栈。</p>
<p> riscv上BUG_ON内部实现是BUG()，它的定义在arch/riscv/include/asm/bug.h</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define BUG() do &#123;						\</span><br><span class="line">	__BUG_FLAGS(0);						\</span><br><span class="line">	unreachable();						\</span><br><span class="line">&#125; while (0)</span><br><span class="line"></span><br><span class="line">#define __BUG_FLAGS(flags)					\</span><br><span class="line">do &#123;								\</span><br><span class="line">	__asm__ __volatile__ (					\</span><br><span class="line">		&quot;1:\n\t&quot;					\</span><br><span class="line">			&quot;ebreak\n&quot;				\</span><br><span class="line">			&quot;.pushsection __bug_table,\&quot;aw\&quot;\n\t&quot;	\</span><br><span class="line">		&quot;2:\n\t&quot;					\</span><br><span class="line">			__BUG_ENTRY &quot;\n\t&quot;			\</span><br><span class="line">			&quot;.org 2b + %3\n\t&quot;                      \</span><br><span class="line">			&quot;.popsection&quot;				\</span><br><span class="line">		:						\</span><br><span class="line">		: &quot;i&quot; (__FILE__), &quot;i&quot; (__LINE__),		\</span><br><span class="line">		  &quot;i&quot; (flags),					\</span><br><span class="line">		  &quot;i&quot; (sizeof(struct bug_entry)));              \</span><br><span class="line">&#125; while (0)</span><br></pre></td></tr></table></figure>
<p>如上的.pushsection指示编译器把下面label2开头的一段二进制在链接的时候<br>放到名叫__bug_table的段里。</p>
<p>ebreak指令的执行最终会执行到ebreak的异常处理函数: do_trap_break，这个函数的定义在<br>linux/arch/riscv/kernel/traps.c，这个函数的实现包含了kprobe、uprobe、kgdb以及BUG<br>的逻辑。</p>
<p>BUG的逻辑实现在report_bug函数，核心的逻辑是使用异常PC在__bug_table这个段里查找对应<br>的信息，然后打印查找到的内容。</p>
<h2 id="uaccess实现"><a href="#uaccess实现" class="headerlink" title="uaccess实现"></a>uaccess实现</h2><p> riscv的对应实现在linux/arch/riscv/include/asm/uaccess.h，用户态内核态之间传递信息<br> 的逻辑基本上在uaccess里实现，比如，copy_from_user、copy_to_user等等。</p>
<p> 这里只看一个copy_to_user的实现:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* linux/include/linux/uaccess.h */</span><br><span class="line">copy_to_user</span><br><span class="line">      /* linux/lib/usercopy.c */</span><br><span class="line">  +-&gt; _copy_to_user</span><br><span class="line">        /* 下面的体系构架相关的函数在arch/riscv/include/asm/uaccess.h */</span><br><span class="line">    +-&gt; access_ok</span><br><span class="line">    +-&gt; raw_copy_to_user</span><br><span class="line">          /* arch/riscv/lib/uaccess.S */</span><br><span class="line">      +-&gt; __asm_copy_to_user(void __user *to, const void *from, unsigned long n)</span><br></pre></td></tr></table></figure>
<p>__asm_copy_to_user的代码分析如下，直接以注释的形式写在下面的代码之间:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ENTRY(__asm_copy_to_user)</span><br><span class="line">ENTRY(__asm_copy_from_user)</span><br><span class="line"></span><br><span class="line">	/* Enable access to user memory */</span><br><span class="line">	li t6, SR_SUM</span><br><span class="line">	csrs CSR_STATUS, t6</span><br><span class="line"></span><br><span class="line">	/* 如上，a0是用户态目的地址，a1是内核buffer的地址, a2是要copy的size，a3是源数据buffer的结尾 */</span><br><span class="line">	add a3, a1, a2</span><br><span class="line">	/* Use word-oriented copy only if low-order bits match */</span><br><span class="line">	/* 64bit的系统，SZREG是8，t0得到目的地址的低3bit，t1得到源地址的低3bit */</span><br><span class="line">	andi t0, a0, SZREG-1</span><br><span class="line">	andi t1, a1, SZREG-1</span><br><span class="line">	/* </span><br><span class="line">         * 目的地址和源地址低3bit不相等的时候，跳到label 2，有数据要拷贝的时候，</span><br><span class="line">         * 会直接跳到label 5, label 5本来是拷贝非8Byte数据的，但是，在源数据buffer</span><br><span class="line">         * 和目的数据buffer地址不对应的情况，直接在label 5用单Byte拷贝的方式复制</span><br><span class="line">         * 整段内存。</span><br><span class="line">         */</span><br><span class="line">	bne t0, t1, 2f</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * 对于源数据buffer，下面每一段表示8B，那么t0和t1的志向的位置如下：</span><br><span class="line">         *</span><br><span class="line">	 * |  8B   |       |       |       |       |</span><br><span class="line">	 * +-------+-------+-------+-------+-------+</span><br><span class="line">	 * |   ^   |       |       |    ^  |       |</span><br><span class="line">         *     |                        |</span><br><span class="line">         *     +------------------------+</span><br><span class="line">         *     |   ^               ^    | 源数据buffer</span><br><span class="line">         *         |               |</span><br><span class="line">         *     a1                       a3</span><br><span class="line">         *         t0              t1</span><br><span class="line">         *</span><br><span class="line">         * 如下这块代码的逻辑是找到源数据buffer中整8Byte的部分，t0指向起始地址，</span><br><span class="line">         * t1指向结尾地址。</span><br><span class="line">	 */</span><br><span class="line">	addi t0, a1, SZREG-1</span><br><span class="line">	andi t1, a3, ~(SZREG-1)</span><br><span class="line">	andi t0, t0, ~(SZREG-1)</span><br><span class="line">	/*</span><br><span class="line">	 * a3: terminal address of source region</span><br><span class="line">	 * t0: lowest XLEN-aligned address in source</span><br><span class="line">	 * t1: highest XLEN-aligned address in source</span><br><span class="line">	 */</span><br><span class="line">        /*</span><br><span class="line">         * t0 &gt;= t1是一些边角的情况，跳到label 2，有数据要拷贝的时候，再跳到label 5</span><br><span class="line">         * 做单Byte拷贝。过了这个点后，下面是拷贝算法的主题逻辑。</span><br><span class="line">         */</span><br><span class="line">	bgeu t0, t1, 2f</span><br><span class="line">        /* a1 &lt; t0说明有t0前面多出来的那一截，跳到label 4，单Byte拷贝结尾这段非对齐数据 */</span><br><span class="line">	bltu a1, t0, 4f</span><br><span class="line">1:</span><br><span class="line">	/*</span><br><span class="line">	 * fixup宏的定义是：</span><br><span class="line">	 *</span><br><span class="line">	 *      .macro fixup op reg addr lbl</span><br><span class="line">	 * 100:</span><br><span class="line">	 *	\op \reg, \addr</span><br><span class="line">	 *	.section __ex_table,&quot;a&quot;</span><br><span class="line">	 *	.balign RISCV_SZPTR</span><br><span class="line">	 *	RISCV_PTR 100b, \lbl</span><br><span class="line">	 *	.previous</span><br><span class="line">	 *	.endm</span><br><span class="line">	 *</span><br><span class="line">	 * 这个宏生成了一个指令，上下文看来，一般是生成load或者store指令，然后在</span><br><span class="line">	 * __ex_table这个段生成一个dword大小(8B)的数据，这个数据的格式是低4B是之前</span><br><span class="line">	 * 生成指令的地址，高4B是一个跳转的label。当load/store有异常发生的时候，</span><br><span class="line">         * 异常处理函数在__ex_table中找到对应的地址，高32bit保存就是要继续跳转的</span><br><span class="line">         * 地址，这里是继续跳转到label 10的地方，函数返回值配置成数据搬移的size，</span><br><span class="line">         * 然后函数返回。</span><br><span class="line">	 *</span><br><span class="line">	 * 相关的流程是：各种访存异常处理函数(e.g. do_trap_load_misaligned)-&gt;do_trap_error-&gt;fixup_exception</span><br><span class="line">	 * 在fixup_exception里配置regs-&gt;epc为fixup代码地址(label 10地址)，然后调用</span><br><span class="line">	 * sret跳到对应地址执行。这里怎么和异常恢复结合起来？还会有什么异常？</span><br><span class="line">	 */</span><br><span class="line">	fixup REG_L, t2, (a1), 10f</span><br><span class="line">	fixup REG_S, t2, (a0), 10f</span><br><span class="line">	addi a1, a1, SZREG</span><br><span class="line">	addi a0, a0, SZREG</span><br><span class="line">	bltu a1, t1, 1b</span><br><span class="line">2:</span><br><span class="line">	bltu a1, a3, 5f</span><br><span class="line"></span><br><span class="line">3:</span><br><span class="line">	/* Disable access to user memory */</span><br><span class="line">	csrc CSR_STATUS, t6</span><br><span class="line">	li a0, 0</span><br><span class="line">	ret</span><br><span class="line">4: /* Edge case: unalignment */</span><br><span class="line">	fixup lbu, t2, (a1), 10f</span><br><span class="line">	fixup sb, t2, (a0), 10f</span><br><span class="line">	addi a1, a1, 1</span><br><span class="line">	addi a0, a0, 1</span><br><span class="line">	bltu a1, t0, 4b</span><br><span class="line">        /* 拷贝完前面一截非8Byte对齐的数据，跳到label 1以8Byte为单位复制数据 */</span><br><span class="line">	j 1b</span><br><span class="line">5: /* Edge case: remainder */</span><br><span class="line">	fixup lbu, t2, (a1), 10f</span><br><span class="line">	fixup sb, t2, (a0), 10f</span><br><span class="line">	addi a1, a1, 1</span><br><span class="line">	addi a0, a0, 1</span><br><span class="line">	bltu a1, a3, 5b</span><br><span class="line">	j 3b</span><br><span class="line">ENDPROC(__asm_copy_to_user)</span><br><span class="line">ENDPROC(__asm_copy_from_user)</span><br><span class="line">EXPORT_SYMBOL(__asm_copy_to_user)</span><br><span class="line">EXPORT_SYMBOL(__asm_copy_from_user)</span><br><span class="line">[...]</span><br><span class="line">10:</span><br><span class="line">	/* Disable access to user memory */</span><br><span class="line">	csrs CSR_STATUS, t6</span><br><span class="line">	mv a0, a2</span><br><span class="line">	ret</span><br></pre></td></tr></table></figure>

<h2 id="kprobe实现"><a href="#kprobe实现" class="headerlink" title="kprobe实现"></a>kprobe实现</h2><p> 基本原理是插入一个ebreak指令，ebreak触发kprobe的调用。代码入口在do_trap_break。</p>
<h2 id="GDB场景"><a href="#GDB场景" class="headerlink" title="GDB场景"></a>GDB场景</h2><p> 执行到ebreak指令的时候，如果在用户态就向用户态发一个SIGTRAP的信号，如果在内核态<br> 并且使能KGDB，就处理相关逻辑。代码入口在do_trap_break。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>riscv</tag>
        <tag>ISA</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv timer的基本逻辑</title>
    <url>/riscv-timer%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<h2 id="基础逻辑"><a href="#基础逻辑" class="headerlink" title="基础逻辑"></a>基础逻辑</h2><p> riscv上有两个最基本的中断控制器aclint和plic，前者的全称是Advanced Core Local<br> Interruptor，是核内的中断控制器，主要是用来产生timer中断和software中断，后者的全称<br> 是Platform Level Interruptor Controller，主要用来收集外设的中断，plic通过外部中断<br> 向CPU报中断。</p>
<p> timer相关的寄存器以及寄存器域段有: mip/mie里和timer相关的域段，mtime以及mtimecmp。<br> mie里有控制timer中断使能的bit: MTIE/STIE，控制M mode和S mode timer interrupter是否<br> 使能，mip里有表示是否存在pending的timer中断的bit: MTIP/STIP。</p>
<p> mtime是一个可读可写的计数器，其中的数值以一定的时间间隔递增，计数器计满后会回绕，<br> mtimecmp寄存器里的数值用来和mtime做比较，当mtime的值大于等于mtimecmp的值，并且<br> MTIE使能时，M mode timer中断被触发。</p>
<p> 软件可以在timer中断处理函数里，更新mtimecmp的值，从而维持一个固定周期的时钟中断，<br> 一般这个中断就是Linux内核的时钟中断。软件可以写STIP触发一个S mode timer中断。</p>
<p> aclint上把mtime以及mtimecmp抽象成一个M mode timer这样的设备，mtime和mtimecmp是这个<br> 设备上的MMIO接口，一个M mode timer上有一个mtime，一个M mode timer为服务的每个hart<br> 设置一个mtimecmp。aclint协议上描述，一个系统可能会有多个M mode timer设备，这样做<br> 的目的是在CPU存在分层拓扑的时候，比如CPU cluster/node/socket时，一组CPU可以和一个<br> M mode timer做在一起，方便这一组CPU的功耗管理。在系统中有多个M mode timer时，需要<br> 做多个mtime数值上的同步，使得多个mtime之间的误差在一定范围之内。</p>
<p> alint只定义了M mode timer, riscv的sstc扩展定义了S mode下的timer。对于S mode timer，<br> sstc只在每个hart上增加了stimecmp寄存器，当time计数值大于等于stimecmp时触发S mode<br> timer中断，stimecmp是一个CSR寄存器。从timer整体定义上看，riscv这里定义的比较乱，<br> M mode timer抽象成一个外设，接口是MMIO，但是S mode timer却改成了CSR，而且sstc还<br> 修改了riscv特权级spec里的一些系统寄存器的定义，mip.STIP这个域段在stimecmp有无时，<br> 读写属性是不一样的，当支持S mode但是没有实现stimecmp时，mip.STIP是读写的，写这个<br> bit会触发S mode timer中断，当实现stimecmp时，mip.STIP是只读的。这样的实现意味着，<br> 在有S mode timer的系统上，将无法使用M mode timer从M mode通过mip.STIP触发S mode<br> timer中断，相关的软件方案需要随之变动。sstc里还有虚拟化相关的描述，这些需要在独立<br> 文档中描述。</p>
<p> riscv上还定义了一个用户态可以访问的计数器RDTIME，这个计数器从开机起就以一定的频率<br> 一直递增。</p>
<h2 id="qemu逻辑-M-mode-timer"><a href="#qemu逻辑-M-mode-timer" class="headerlink" title="qemu逻辑 - M mode timer"></a>qemu逻辑 - M mode timer</h2><p> qemu中的aclint和plic的代码路径分别在：hw/intc/riscv_aclint.c和hw/intc/sifive_plic.c。<br> 这里我们只关注和timer相关的部分，可以看到在riscv_aclint.c里只有M mode timer中断的<br> 触发代码。</p>
<p> 在qemu上跑内核的时候，发现总是一个M mode timer中断跟着一个S mode timer中断，然后<br> 再跟一个S mode ecall。qemu里并没有触发S mode timer的代码，可以猜测S mode timer中断<br> 是在opensbi里触发的。</p>
<p> 整个逻辑是：当mtime大于等于mtimecmp时触发一个M mode中断，opensbi里的中断处理逻辑<br> 会写STIP，由于S mode time中断已经被委托到S mode处理，在M mode返回S mode后，S mode<br> timer中断就会被触发。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* opensbi/lib/sbi/sbi_trap.c */</span><br><span class="line">sbi_trap_handler</span><br><span class="line">  +-&gt; sbi_trap_noaia/ais_irq</span><br><span class="line">    +-&gt; sbi_timer_process</span><br><span class="line">          /* 如果没有SSTC特性，才这样处理 */ </span><br><span class="line">      +-&gt; csr_set(CSR_MIP, MIP_STIP)</span><br></pre></td></tr></table></figure>
<p> S mode timer中断处理函数里通过S mode ecall写mtimecmp，为下一次M mode timer中断配置<br> 合理的数值。这里面可能有一个问题，mtime如果触发中断后不往前走，就会有时间上的误差，<br> 可以想象，如果mtime在中断触发后依然往前走，就不会有这个问题。</p>
<p> 查看qemu aclint的代码，其中使用QEMU_CLOCK_VIRTUAL这个时钟来计算mtime寄存器里的值,<br> 而QEMU_CLOCK_VIRTUAL是来自host上获取时间的函数clock_gettime/get_clock_realtime，<br> 获取的是一个不断流逝的时间值。</p>
<p> 说到虚拟机的timer，就有一个问题要问，虚拟机里的时间和真实世界里的时间数值上一样么？<br> 我们看下aclint驱动配置timecmp时，qemu里的timer中断定时是怎么处理里。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_aclint_mtimer_write</span><br><span class="line">  +-&gt; riscv_aclint_mtimer_write_timecmp</span><br><span class="line">    +-&gt; ns_diff = muldiv64(diff, NANOSECONDS_PER_SECOND, timebase_freq)</span><br></pre></td></tr></table></figure>
<p> 如上，使用传进来的timecmp值，计算出timer中断间隔的实际值，然后在host上启动一个<br> 定时器来做模拟，timebase_freq是mtime寄存器作为counter的频率，所以实际时间的计算<br> 就是：(mtimecmp - 上一个中断点的mtime值) * 1/timebase_freq * NANOSECONDS_PER_SECOND，<br> 单位是ns，就是上面muldiv64函数的计算结果。所以，虚拟机看到的时间和实际时间是一样的。</p>
<h2 id="qemu逻辑-S-mode-timer"><a href="#qemu逻辑-S-mode-timer" class="headerlink" title="qemu逻辑 - S mode timer"></a>qemu逻辑 - S mode timer</h2><p> qemu里S mode timer的代码在：target/riscv/time_helper.c，stimecmp是CSR寄存器，所以<br> stimecmp的访问代码在: target/riscv/csr.c。</p>
<p> 可以看到stimecmp的读写逻辑和timecmp的读写逻辑基本上是一致的，而时钟源都使用一样的<br> QEMU_CLOCK_VIRTUAL。具体的看，针对sstc，qemu在time_helper.c里实现了hw/intc/riscv_alint.c<br> 里很多一样的逻辑，通过CPU env里的rdtime_fn_arg/rdtime_fn拿到riscv_alint.c里定义<br> 的和时钟源相关的信息，比如，timebase_freq。</p>
<h2 id="qemu逻辑-RDTIME"><a href="#qemu逻辑-RDTIME" class="headerlink" title="qemu逻辑 - RDTIME"></a>qemu逻辑 - RDTIME</h2><p> 这个寄存器在U mode也可以访问，所以具体实现也是qemu user mode和system mode各自支持的。</p>
<p> qemu user mode下的实现，直接使用host上的特性tick执行实现，这样的实现感觉真心<br> 没有什么实用价值。</p>
<p> system mode下的实现，RDTIME的时钟源也是QEMU_CLOCK_VIRTUAL，读相关的CSR直接得到<br> 计算后的计数值，数值上和如上M/S mode timer计数器的值是一样，表示自系统启动以来<br> 的一个计数值。所以，这个值和墙上时钟还是不一样的，墙上时钟在系统关机后会使用主板<br> 上的纽扣电池继续运行。</p>
<h2 id="Linux内核逻辑"><a href="#Linux内核逻辑" class="headerlink" title="Linux内核逻辑"></a>Linux内核逻辑</h2><p> 内核timer初始化在：arch/riscv/kernel/time.c的time_init。内核相关驱动的位置在：<br> drivers/clocksource/timer-riscv.c。截止到目前为止，S mode timer的内核patch还在<br> 社区评审，相关的连接在lwn.net/Articles/886863/。</p>
<p> riscv timer的内核驱动把对应的of_device_id静态定义到__timer_of_table段里。time_init<br> 里的timer_probe会扫描__timer_of_table这个段里timer相关的of_device_id, 然后调用<br> 对应的初始化函数，这里就是调用riscv_timer_init_dt，这个函数会找见intc对应的domain，<br> 通过domain和S mode timer硬件中断号得到S mode timer对应中断的virq，最后向virq注册<br> S timer的中断处理函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_timer_init_dt</span><br><span class="line">  +-&gt; riscv_clock_event_irq = irq_create_mapping(domain, RV_IRQ_TIMER)</span><br><span class="line">  +-&gt; request_percpu_irq(riscv_clock_event_irq, riscv_timer_interrupt, ...)</span><br></pre></td></tr></table></figure>

<p> riscv timer驱动会注册clocksource和clock_event_device。</p>
<p> clocksource就是指不断递增的时钟源，比如riscv上的mtime寄存器以一定的频率增加计数，<br> 它就是一个clocksource. riscv_clocksource提供一个read接口，通过这个接口可以得到<br> 当前mtime计数器里的值。在riscv qemu virt平台上这个计数器的频率定义在cpus节点的<br> timebase-frequency字段，它的值是0x989680，就是十进制的10000000，也就是说这个计数<br> 器10ns计数一下。</p>
<p> clock_event_device指的是可以产生和时钟相关事件的device，比如riscv上当mtime的值<br> 大于等于mtimecmp的值时会上报一个M mode timer的中断，mtime和mtimecmp就可以被看作<br> 一个clock_event_device。riscv timer驱动里定义的struct clock_event_device riscv_clock_event<br> 是个per CPU变量，timer的中断处理函数就是调用riscv_clock_event里的event_handler。<br> 但是这个回调函数并不是在驱动里直接提供的。</p>
<p> event_handler的注册逻辑是通过这个驱动里注册的cpu hotplu回调函数riscv_timer_starting_cpu<br> 完成的，大概的调用逻辑如下，可以看见event_handler实际上是一个公共函数tick_handle_periodic。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start_kernel</span><br><span class="line">  +-&gt; time_init</span><br><span class="line">    +-&gt; timer_probe</span><br><span class="line">      +-&gt; cpuhp_setup_state</span><br><span class="line">        +-&gt; __cpuhp_setup_state</span><br><span class="line">          +-&gt; __cpuhp_setup_state_cpuslocked</span><br><span class="line">            +-&gt; cpuhp_issue_call</span><br><span class="line">              +-&gt; cpuhp_invoke_callback</span><br><span class="line">                +-&gt; riscv_timer_starting_cpu</span><br><span class="line">                  +-&gt; clockevents_config_and_register</span><br><span class="line">                    +-&gt; clockevents_register_device</span><br><span class="line">                      +-&gt; tick_check_new_device</span><br><span class="line">                        +-&gt; tick_setup_device</span><br><span class="line">                          +-&gt; tick_setup_periodic</span><br><span class="line">			        /* event_handler回调 */</span><br><span class="line">			    +-&gt; tick_handle_periodic</span><br></pre></td></tr></table></figure>
<p> tick_handle_periodic就是每次时钟中断时要运行的逻辑，相关逻辑已经和调度有关系，<br> 在另外讲调度的文章里独立分析吧。</p>
<p> 实际上，tick_handle_periodic只是在内核开始的时候用，内核随后会把event_handler切<br> 到高精度定时器的回调函数上hrtimer_interrupt，这个函数里只是处理时间相关的东西，<br> tick时发生的调度要回到entry.S里，也就是处理完timer中断再处理调度相关的逻辑。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>计算机体系结构</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv中断异常委托关系分析</title>
    <url>/riscv%E4%B8%AD%E6%96%AD%E5%BC%82%E5%B8%B8%E5%A7%94%E6%89%98%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> 在H扩展使能的情况下，我们要考虑M/HS/VS这三种模式下的中断异常委托关系。其中，medeleg/mideleg<br> 把中断异常委托到HS处理，hedeleg/hideleg把中断异常委托到VS处理。</p>
<p> medeleg/mideleg在opensbi里配置，代码在：opensbi/lib/sbi/sbi_hart.c: delegate_traps</p>
<p> hedeleg/hideleg在内核的KVM代码里配置，路径是：linux/arch/riscv/kvm/main.c: kvm_arch_hardware_enable<br> 在vcpu start的时候，会调用如上函数，配置hedeleg/hideleg，在vcpu disable的时候，<br> 把hedeleg/hideleg清0。</p>
<h2 id="medeleg-mideleg的逻辑"><a href="#medeleg-mideleg的逻辑" class="headerlink" title="medeleg/mideleg的逻辑"></a>medeleg/mideleg的逻辑</h2><p> 如果没有S mode，没有必要做委托相关的配置了，直接返回。如果S mode存在，S mode的<br> 几种中断被委托到S mode:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MIP_SSIP</span><br><span class="line">MIP_STIP</span><br><span class="line">MIP_SEIP</span><br><span class="line">MIP_LCOFIP (如果支持PMU中断)</span><br></pre></td></tr></table></figure>
<p> opensbi把如下异常委托到S mode处理: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MISALIGNED_FETCH</span><br><span class="line">BREAKPOINT</span><br><span class="line">USER_ECALL</span><br><span class="line">FETCH_PAGE_FAULT</span><br><span class="line">LOAD_PAGE_FAULT</span><br><span class="line">STORE_PAGE_FAULT</span><br></pre></td></tr></table></figure>
<p> 上面是没有H扩展的情况，如果平台支持H扩展，再把如下的异常委托到S mode处理:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">VIRTUAL_SUPERVISOR_ECALL</span><br><span class="line">FETCH_GUEST_PAGE_FAULT</span><br><span class="line">LOAD_GUEST_PAGE_FAULT</span><br><span class="line">VIRTUAL_INST_FAULT</span><br><span class="line">STORE_GUEST_PAGE_FAULT</span><br></pre></td></tr></table></figure>
<p> 为了方便描述，我们把上面第一组异常叫做A组，把上面第二组异常叫做B组。</p>
<p> 在没有H扩展的时候，把A组异常委托到S mode已经满足需求。</p>
<p> 我们考虑有H扩展的场景，这个时候又分虚拟化被使用和没有被使用的情况，没有使用虚拟化<br> 和没有H扩展的场景是一样的，无非是B组异常被委托到S mode，但是没有使用而已。</p>
<p> 在使用虚拟机的场景，虚拟机拉起之前，会进一步把A组异常委托到虚拟机的S mode，就是VS，<br> 这组异常在VS里处理，不必退出虚拟机。</p>
<p> 在使用虚拟机的场景里，B组异常的委托关系不变，依然在host S mode处理，就是HS。<br> 可以看出，B组异常都是CPU运行在VU/VS时的异常，而且在虚拟机里已经处理不了这些异常，<br> 比如，第二级地址翻译相关的异常以及从VS发起的ECALL，VS已经hold不住这些异常，这些<br> 异常需要在hypvisor里处理。比如，在只有host的情况，没有错误的情况下，访问一个物理<br> 地址是一定会成功的，但是，在虚拟机的内核里，访问虚拟机物理地址(IPA)的时候还要通过<br> IPA到PA的地址翻译，如果没有分配PA，就会陷入HS的缺页异常里处理缺页，同时VS模式下<br> 也可以发出ECALL，这个ECALL需要到HS下去处理。</p>
<p> 退出虚拟机会停止委托A组异常到VS，这时系统进入有H扩展但是没有使用虚拟机的场景，<br> 因为之前A组异常已经被委托到HS，这时系统也是可以正常工作的。</p>
<h2 id="hedeleg-hideleg的逻辑"><a href="#hedeleg-hideleg的逻辑" class="headerlink" title="hedeleg/hideleg的逻辑"></a>hedeleg/hideleg的逻辑</h2><p> vcpu开始运行之前，把如下异常和中断委托到VS模式下处理, vcpu退出运行的时候，需要把<br> hedeleg/hideleg清空，这个动作在：linux/arch/riscv/kvm/main.c: kvm_arch_hardware_disable。</p>
<p> 这个逻辑是很自然的，上面M mode下已经把需要在S mode处理的中断和异常委托在S mode下<br> 处理，如果没有H扩展，这些异常和中断就在host的内核态处理，如果有H扩展，那么在内核<br> KVM启动vcpu之前，继续把这些异常和中断委托到VS，这样CPU在运行虚拟机代码时，如果发生<br> 如下的异常和中断，就会使用VS模式定义的那些异常中断上下文寄存器在VS模式下处理这些<br> 异常和中断，当虚拟机退出时，把到VS模式的这些委托去掉，再发生这样的异常和中断，就<br> 在HS模式下处理。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">INST_MISALIGNED</span><br><span class="line">BREAKPOINT</span><br><span class="line">SYSCALL</span><br><span class="line">INST_PAGE_FAULT</span><br><span class="line">LOAD_PAGE_FAULT</span><br><span class="line">STORE_PAGE_FAULT</span><br><span class="line"></span><br><span class="line">VS_SOFT</span><br><span class="line">VS_TIMER</span><br><span class="line">VS_EXT</span><br></pre></td></tr></table></figure>
<p> 可以看到，如上的异常就是上面说的A组异常。</p>
<p> 我们可以从具体一个异常的角度再看看，比如一个user mode ECALL异常，当没有H扩展时，<br> 异常的处理使用S mode的那组CSR寄存器，当有H扩展时，在虚拟机里，用户态触发的ECALL<br> 在VS模式处理，使用VS模式的那组寄存器，VS模式CSR寄存器在vcpu启动之前已经都切到S<br> mode的CSR寄存器上，所以，直接使用S mode的寄存器就好。</p>
<h2 id="M-mode处理的异常和中断"><a href="#M-mode处理的异常和中断" class="headerlink" title="M mode处理的异常和中断"></a>M mode处理的异常和中断</h2><p> 除了如上委托到VS/HS模式的异常中断，其他没有委托的都要在M模式处理。我们把opensbi<br> 上定义的异常和中断类似都列出来，一个一个具体看下，如下用“&lt;—”标注的异常或者中断<br> 是在M mode处理的。可以看到在M mode处理的异常，要不就是没法挽救的异常，要不就是本来<br> 就应该在M mode处理的异常。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MISALIGNED_FETCH	0x0</span><br><span class="line">FETCH_ACCESS		0x1      &lt;--- 取指令异常，没有对齐</span><br><span class="line">ILLEGAL_INSTRUCTION	0x2	 &lt;--- 非法指令异常</span><br><span class="line">BREAKPOINT		0x3</span><br><span class="line">MISALIGNED_LOAD		0x4      &lt;--- 读地址不对齐</span><br><span class="line">LOAD_ACCESS		0x5      &lt;--- PMP读异常</span><br><span class="line">MISALIGNED_STORE	0x6      &lt;--- 写地址不对齐</span><br><span class="line">STORE_ACCESS		0x7      &lt;--- PMP写异常</span><br><span class="line">USER_ECALL		0x8</span><br><span class="line">SUPERVISOR_ECALL	0x9      &lt;--- S mode ECALL异常</span><br><span class="line">VIRTUAL_SUPERVISOR_ECALL0xa</span><br><span class="line">MACHINE_ECALL		0xb      &lt;--- M mode ECALL异常</span><br><span class="line">FETCH_PAGE_FAULT	0xc</span><br><span class="line">LOAD_PAGE_FAULT		0xd</span><br><span class="line">STORE_PAGE_FAULT	0xf</span><br><span class="line">FETCH_GUEST_PAGE_FAULT	0x14</span><br><span class="line">LOAD_GUEST_PAGE_FAULT	0x15</span><br><span class="line">VIRTUAL_INST_FAULT	0x16</span><br><span class="line">STORE_GUEST_PAGE_FAULT	0x17</span><br><span class="line"></span><br><span class="line">IRQ_S_SOFT			1</span><br><span class="line">IRQ_VS_SOFT			2</span><br><span class="line">IRQ_M_SOFT			3   &lt;---</span><br><span class="line">IRQ_S_TIMER			5</span><br><span class="line">IRQ_VS_TIMER			6</span><br><span class="line">IRQ_M_TIMER			7   &lt;---</span><br><span class="line">IRQ_S_EXT			9</span><br><span class="line">IRQ_VS_EXT			10</span><br><span class="line">IRQ_M_EXT			11  &lt;---</span><br><span class="line">IRQ_S_GEXT			12</span><br><span class="line">IRQ_PMU_OVF			13</span><br></pre></td></tr></table></figure>
<p> 需要特殊说明的是，上面的中断中的IRQ_VS_EXT, IRQ_VS_TIMER, IRQ_VS_SOFT在H扩展存在<br> 时，直接被硬件代理到HS，IRQ_S_GEXT在H扩展存在且GEILEN非0时同样被硬件代理到HS。</p>
]]></content>
      <tags>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv内存模型分析</title>
    <url>/riscv%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> 各个CPU构架的弱内存模型都会定义一些规则，这些规则约束各种内存序，在这些规则之外<br> 各种内存访问都是可以乱序的，所谓barrier指令只是其中的一些规则，这些规则用指令的<br> 方式显示约束了内存序，所以，只从barrier指令去看内存序，始终是只能看到其中的一部分，<br> 整体认识上还是模模糊糊。如果编程中会遇到可能的内存序问题，这些规则怎么说也的通读<br> 一遍，不然基本上就是一个“能跑就行”的程序。</p>
<p> riscv协议在Chapter 14定义了这些规则(ARMv8/v9协议在2.3章定义了相关规则)，在附录里<br> 专门做了这些规则的解释(RVWMO Explanatory Material, Version 0.1)，在附录里还有内存<br> 序相关的形式化验证模型(Formal Memory Model Specifications, Version 0.1)。</p>
<p> 注意内存序(consistency)和cache一致性(coherence)是两个不同的概念，前者是说的是硬件<br> 故意放松一些指令执行上的循序，依此来提高硬件性能，指令的这种乱序行为是程序员可以<br> 直接感知的，本文讨论的就是内存序的相关问题，而后者指的是，多核系统上，因为多核<br> 共享内存或者cache而引发的数据存储一致性上的问题，硬件需要用一定的cache一致性协议<br> (比如MESI协议)在硬件层面解决这个问题。</p>
<h2 id="riscv内存序规则"><a href="#riscv内存序规则" class="headerlink" title="riscv内存序规则"></a>riscv内存序规则</h2><p> riscv在Chapter 14定义了它的弱内存序，定义的逻辑是这样的，”Memory Model Primitives”<br> 这一节是一些术语的定义，”Syntactic Dependencies”这一节定义的是两个指令关系的名称，<br> 这一节并没有定义内存序的规则，后面定义规则时依赖这一节里定义的指令关系，”Preserved Program Order”<br> 这一节定义的是riscv弱内存序的规则，这一节是这一章的主体，一共定义了13条规则(rule)，<br> 随后在”Memory Model Axioms”这一节里定义了三条公理(Axioms)，这些约束大部分是符合<br> 我们的直观的认识，但是也有一些和直观的认知是不同的。学习这些规则时，我们要认识到<br> 这些规则之外的情况，这些情况都是可以乱序的，barrier指令正是对有可能出现的乱序情况<br> 的约束。下面逐条看下这些规则:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">• Overlapping-Address Orderings</span><br><span class="line"></span><br><span class="line">  1. b is a store, and a and b access overlapping memory addresses  </span><br><span class="line"></span><br><span class="line">  2. a and b are loads, x is a byte read by both a and b, there is no store to x</span><br><span class="line">     between a and b in program order, and a and b return values for x written by</span><br><span class="line">     different memory operations  </span><br><span class="line"></span><br><span class="line">  3. a is generated by an AMO or SC instruction, b is a load, and b returns a value</span><br><span class="line">     written by a  </span><br></pre></td></tr></table></figure>
<p> 访存地址有重合的情况，第一条规则是说后面的store不能跑到前面的指令之前执行。</p>
<p> 第二条规则就比较绕了，直接翻译是，a和b是load操作，x是a和b要读的数据，a和b之间没有<br> store修改x(在内存)的值，a和b load的值之前被不同的内存写操作修改。这一条规则进一步的<br> 解释是，两个load读同一个地址上值，新的load不能读到旧load读到的值，但是，还要进一步<br> 考虑两种情况，第一种是a和b之间有store修改x的值，这种情况下a和b不保序，第二种情况<br> 是，a和b读到的值是被不同写内存操作修改的值，这种情况a和b也不保序。</p>
<p> 第三条规则是说，后面的load不能超过之前由AMO/SC触发的写操作，直观的理解也是这样。</p>
<p> 注意，根据如上的定义，当a是一个普通的store，b是一个load，他们访问相同地址，load<br> 是可以跑到store之前的，不过load到的值是store缓存在store buff里的值，这个就是Intel<br> TSO内存序上唯一放松的内存序。</p>
<p> 再次总结下，load/store的四种可能情况是：1. load-load, 2, load-store, 3, store-store,<br> 4. store-load。规则1说的是2/3两种情况不能乱序，规则2说的是1的情况，规则3只说了<br> AMO/SC-load的情况，不能乱序，所以普通store-load的情况是可以乱序的，这种情况就是<br> 我们上面提到了TSO内存序上唯一的一个放松约束。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">• Explicit Synchronization </span><br><span class="line"></span><br><span class="line">  4. There is a FENCE instruction that orders a before b</span><br><span class="line"></span><br><span class="line">  5. a has an acquire annotation</span><br><span class="line"></span><br><span class="line">  6. b has a release annotation</span><br><span class="line"></span><br><span class="line">  7. a and b both have RCsc annotations </span><br><span class="line"></span><br><span class="line">  8. a is paired with b </span><br></pre></td></tr></table></figure>
<p> 用显示的barrier指令指定内存序的情况，这些去看barrier的定义就好，需要注意的点有：</p>
<p> acquire语意是说acquire后面的读写指令不能排到acquire之前(没有约束acquire之前读写<br> 指令的顺序)，逻辑示意如下。release的语意正好相反。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">         +-------------+</span><br><span class="line">         | load/store  |---------+</span><br><span class="line">         +-------------+         |</span><br><span class="line">                                 |</span><br><span class="line">----- instruction with acquire --+--</span><br><span class="line">  ^                              |</span><br><span class="line">  |      +-------------+         |</span><br><span class="line">  +------| load/store  |         v</span><br><span class="line">         +-------------+          </span><br></pre></td></tr></table></figure>
<p> riscv上acquire/release的作为AMO或者lr/sc指令的属性一起使用，ARM64上则有STLR/LDAR<br> 指令，这两条指令把acquire/release和普通load/store指令结合到了一起。</p>
<p> 如下图所示，如果要确保地址无关的两个store不乱序，可以在其中加一个写fence，但是<br> 这样会一把拦住fence前后的所有store，当把后一个store换成带release的store时(STLR)，<br> store和STLR保序的同时，STLR后面其它的地址无关的store/load也可以提前投机执行。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">         +-------------+                            +-------------+          </span><br><span class="line">         |    store    |---------+           ^      |    store    |---------+</span><br><span class="line">         +-------------+         |           |      +-------------+         |</span><br><span class="line">                                 v           |                              |</span><br><span class="line">------------- fence -------------+--       --+---------- STLR --------------+--</span><br><span class="line">  ^                                          |                              </span><br><span class="line">  |      +-------------+                     |      +------------------+</span><br><span class="line">  +------|    store    |                     +------| other store/load |         </span><br><span class="line">         +-------------+                            +------------------+</span><br></pre></td></tr></table></figure>

<p> 第七条规则中的RCsc指的是Release Consistency with sequentially-consistent synchronization<br> operation，对应的概念还有RCpc，这两者是acquire/release的两种分类，描述的是，比如<br> 上图中如果“other store/load“里如果有LDAR，那么这个LDAR是否可以和STLR乱序，如果<br> acquire/release的种类是RCsc，那么不能乱序，如果是RCpc，是可以乱序的。riscv的WMO<br> 中的acquire/release的种类是RCsc。</p>
<p> 第八条规则说的a和b是一个pair，是说lr/sc指令组成的pair。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">• Syntactic Dependencies </span><br><span class="line"></span><br><span class="line">  9. b has a syntactic address dependency on a</span><br><span class="line"></span><br><span class="line">  10. b has a syntactic data dependency on a</span><br><span class="line"></span><br><span class="line">  11. b is a store, and b has a syntactic control dependency on a </span><br></pre></td></tr></table></figure>
<p> 这几条规则说的是指令之间语意上的依赖，和最开始访存地址重合的依赖比较类似。</p>
<p> 需要注意的是规则11，这条规则提到了“控制依赖”，控制依赖的定义是，指令a和指令b，<br> 如果中间还有一条分支或者间接跳转指令(branch and indirect jump)m，如果a和m存在<br> Syntactic Dependencies，那么b和a之间就存在一个控制依赖，而规则11是说，在a和b之间<br> 存在一个控制依赖时，如果b是个store指令，那么a和b是不能乱序的。</p>
<p> riscv spec里在“Syntactic Dependencies”这一节定义各种依赖关系的定义，但是只是说<br> 当两条指令满足某种情况时，它们之间叫什么关系，并不是满足这种关系后就有序的约束，<br> riscv spec随后在“Preserved Program Order”这一节借助”Syntactic Dependencies”的定义，<br> 描述内存序的约束，规则9和规则10，直接就说满足地址依赖和数据依赖的两条指令有内存<br> 序上的约束，而规则11是说在满足控制依赖关系时，只对后面是store的情况有序上的约束。</p>
<p> 规则11的代码示意如下，左边两个是不会乱序的情况，其中一个是汇编实现，一个是对应的<br> C语言实现，最右边的是需要加读barrier的场景。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load a0, a1(0)         a = read(p1);         a = read(p1); &lt;--- 这个read后需要一个读barrier</span><br><span class="line">beqi a0, #1, lable     if (a == 1)           if (a == 1)</span><br><span class="line">store a2, a3(0)            write(p2, b);         c = read(p2);</span><br></pre></td></tr></table></figure>
<p> 这条规则是多么的反直觉，但是既然定义成了规则，它就是软硬件约定的编程接口！</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">• Pipeline Dependencies </span><br><span class="line"></span><br><span class="line">  12. b is a load, and there exists some store m between a and b in program order</span><br><span class="line">      such that  m has an address or data dependency on a, and b returns a value</span><br><span class="line">      written by m  </span><br><span class="line"></span><br><span class="line">  13. b is a store, and there exists some instruction m between a and b in program</span><br><span class="line">      order such that m has an address dependency on a  </span><br></pre></td></tr></table></figure>
<p> 这两条规则是说，依赖之间是可以前后构成依赖链条的。</p>
<p> 下面是三条Axioms的定义。</p>
<p> Load Value Axiom: 每个load指令得到值是global memory order上最近一次store的值，这个<br>                   store是global memory order或者是program order上在loadz之前。</p>
<p> Atomicity Axiom: 这个已经体现在lr/sc指令定义里。</p>
<p> Progress Axiom: 貌似是说乱序时提前执行总有一定的限制的。</p>
<h2 id="riscv-barrier指令"><a href="#riscv-barrier指令" class="headerlink" title="riscv barrier指令"></a>riscv barrier指令</h2><p> riscv上barrier有fence和fence.i，相关的指令可以参考<a href="https://wangzhou.github.io/riscv%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4%E6%95%B4%E7%90%86/">这里</a>。</p>
<h2 id="ARM内存序以及barrier指令"><a href="#ARM内存序以及barrier指令" class="headerlink" title="ARM内存序以及barrier指令"></a>ARM内存序以及barrier指令</h2><p> (todo)</p>
<h2 id="Linux内核内存序介绍"><a href="#Linux内核内存序介绍" class="headerlink" title="Linux内核内存序介绍"></a>Linux内核内存序介绍</h2><p> Linux内核有详细介绍内存序的文档：Documentation/memory-barriers.txt。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1Ra2JXZ0NTQUVvbw==">https://www.youtube.com/watch?v=QkbWgCSAEoo<i class="fa fa-external-link-alt"></i></span></li>
</ul>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>内存管理</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv内存管理相关指令整理</title>
    <url>/riscv%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h2 id="涉及的指令"><a href="#涉及的指令" class="headerlink" title="涉及的指令"></a>涉及的指令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-------------+-------------------+----------------------------+</span><br><span class="line">|memory order:| fence fence.i     |                            |</span><br><span class="line">|	      | sfence.w.inval    |                            |</span><br><span class="line">|             | sfence.inval.ir   |                            |</span><br><span class="line">+-------------+-------------------+----------------------------+</span><br><span class="line">|TLB invalid: | sfence.vma        |    hfence.vvma hfence.gvma |</span><br><span class="line">|             | sinval.vma        |                            |</span><br><span class="line">+-------------+-------------------+----------------------------+</span><br><span class="line">|cache      : | CMO extension                                  |</span><br><span class="line">+-------------+-------------------+----------------------------+</span><br></pre></td></tr></table></figure>
<p>如上是rv里和内存管理相关的指令一个总结，虚拟化相关的放在了右边，cache无效化相关<br>的指令在rv上在一个独立的扩展里介绍的：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3Jpc2N2L3Jpc2N2LUNNT3M=">https://github.com/riscv/riscv-CMOs<i class="fa fa-external-link-alt"></i></span></p>
<p>下面我们逐条指令分析下：(先不考虑虚拟化相关的指令)</p>
<h2 id="内存序指令"><a href="#内存序指令" class="headerlink" title="内存序指令"></a>内存序指令</h2><ul>
<li><p>fence</p>
<p>fence指令是RV的基本指令，定义在非特权ISA spec里。fence是数据访问的fence指令，<br>通过参数可以控制fence指令制造出的约束。</p>
</li>
<li><p>fence.i</p>
<p>RV协议在Zifencei扩展里定义fence.i, Zifencei在RV的非特权级ISA定义里。fence.i只是<br>对单核起作用，它保证是本核指令改动和CPU取指令之间的顺序，比如，软件改动了指令<br>序列，希望CPU可以fetch改动后的指令，就必须加一个fence.i指令，这个指令确保CPU<br>之前对指令的改动已经生效。</p>
<p>协议里对多核的描述是，改动指令的核需要先执行一个fence指令，然后在其他核上均执行<br>一个fence.i指令，才能保证如上的语意。软件上可以看到的一个场景是进程迁移的场景，<br>一个进程开始在核A上，修改了自己的将要执行的指令，然后迁移到核B上去继续执行修改<br>后的指令，在迁移之前就需要执行fence指令，再在核B上执行fence.i。</p>
<p>内核里主要是在arch/riscv/mm/cacheflush.c里使用fence.i，里面的使用方式叫remote<br>fence.i，就是多核的时候，触发其他核上执行fence.i，目前是用IPI实现的。单核调用<br>fence.i的场景没有发现。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">flush_icache_all</span><br><span class="line">      /*</span><br><span class="line">       * 如果有SBI的只是，那么走这个分支，其中会发S mode ecall把remote fence.i</span><br><span class="line">       * 发到opensbi处理。下面的remote sfence.vma也是一样的处理方式，我们在下面</span><br><span class="line">       * 展开说明opensbi里的处理方式。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; sbi_remote_fence_i</span><br><span class="line">      /*</span><br><span class="line">       * 如果不支持SBI，在kernel(S mode)就可以直接发IPI，触发其它核执行fence.i,</span><br><span class="line">       * 内核的IPI机制细节待分析。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; on_each_cpu(ipi_remote_fence_i, NULL, 1)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="TLB无效化指令"><a href="#TLB无效化指令" class="headerlink" title="TLB无效化指令"></a>TLB无效化指令</h2><ul>
<li><p>sfence.vma</p>
<p>sfence.vma是定义在RV特权级ISA里的指令，我们可以简单把它理解成带barrier的tlb无效<br>化指令。</p>
<p>所谓barrier，保证的是修改页表和CPU做page walk的顺序，这两个操作之间需要加sfence.vma，<br>保证CPU后面做page walk的时候拿到的时候修改之后的页表。</p>
<p>这个指令支队单核起作用，多核之间做页表和tlb的同步在RV上需要多条指令完成，显然<br>目前的做法，效率是不高的。多核之间做页表和tlb同步的常见是很常见的，比如，多线程<br>共用一个虚拟地址空间，而且多个线程跑到多个核上，那么一个核修改了页表，其他核上<br>的tlb就需要做无效化。RV协议上给出的方案是，修改页表的核修改页表后执行fence，这个<br>保证叫所有核看见页表的修改，然后发IPI给其它核，其他核上的IPI处理函数执行sfence.vma，<br>做完后通知修改页表的核。RV协议上叫这个过程是：模拟TLB shutdown。</p>
</li>
<li><p>sinval.vma</p>
<p>sinval.vma和sfence.vmao功能类似，区别在于sinval.vma不带barrier。</p>
<p>sfence.vma的使用集中在内核arch/riscv/mm/tlbflush.c，分为local和remote的使用方式，<br>local就是在单核上使用，remote就是多核之间使用，做多核的页表和TLB同步。我们这里<br>只看下remote sfence.vma的实现。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">flush_tlb_all</span><br><span class="line">  +-&gt; sbi_remote_sfence_vma</span><br><span class="line">    +-&gt; __sbi_rfence(SBI_EXT_RFENCE_REMOTE_SFENCE_VMA, cpu_mask, start, size, 0, 0)</span><br><span class="line">          /*</span><br><span class="line">           * sbi的协议这里改过，前一个是v0.1的版本，最新的基于v0.2的版本，这里就是</span><br><span class="line">           * 内核和BIOS的接口，接口表现为一个S mode的ecall请求。</span><br><span class="line">           */</span><br><span class="line">      +-&gt; __sbi_rfence_v02</span><br></pre></td></tr></table></figure>
<p>下面是opensbi里的实现：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* opensbi的C语言部分的入口，核启动时会调用sbi_init做初始化，其中包括ipi和tlb的初始化 */</span><br><span class="line">sbi_init</span><br><span class="line">  +-&gt; init_coldboot</span><br><span class="line">        /* lib/sbi/sbi_tlb.c */</span><br><span class="line">    +-&gt; sbi_ipi_init</span><br><span class="line">        /*</span><br><span class="line">         * 初始化一个核上处理tlb的相关资源，主要包括一个fifo队列，以及发送、处理</span><br><span class="line">         * 和同步要用的回调函数。</span><br><span class="line">         */</span><br><span class="line">    +-&gt; sbi_tlb_init</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * 如上内核里来的S mode ecall情况对应opensbi里的处理流程如下, _trap_handler是</span><br><span class="line"> * opensbi里异常处理的入口。</span><br><span class="line"> */</span><br><span class="line">_trap_handler </span><br><span class="line">      /* lib/sbi/sbi_trap.c */</span><br><span class="line">  +-&gt; sbi_trap_handler</span><br><span class="line">        /*</span><br><span class="line">         * M mode中断处理入口，这里是处理remote sfence.vma的入口，发送remote</span><br><span class="line">         * sfence.vma的核走的是下面sbi_ecall_handler的流程。最下面我们用一个图</span><br><span class="line">         * 说明下整个逻辑。</span><br><span class="line">         */</span><br><span class="line">    +-&gt; sbi_trap_noaia_irq</span><br><span class="line">      +-&gt; sbi_ipi_process</span><br><span class="line">        +-&gt; ipi_ops-&gt;process          &lt;---- tlb_process</span><br><span class="line">        /* lib/sbi/sbi_ecall.c */</span><br><span class="line">    +-&gt; sbi_ecall_handler</span><br><span class="line">          /* 对应的rfence handler在sbi_ecall_replace.c里注册 */</span><br><span class="line">      +-&gt; ext-&gt;handler  &lt;---- sbi_ecall_rfence_handler</span><br><span class="line">            /* 如上的函数处理SBI里定义的全部RFENCE请求，我们这里只看sfence.vma */</span><br><span class="line">        +-&gt; sbi_tlb_request</span><br><span class="line">          +-&gt; sbi_ipi_send_many</span><br><span class="line">            +-&gt; sbi_ipi_send</span><br><span class="line">                  /* 使用上面sbi_init流程里注册的回调函数 */</span><br><span class="line">              +-&gt; ipi_ops-&gt;update      &lt;---- tlb_update</span><br><span class="line">              +-&gt; ipi_ops-&gt;send	&lt;---- mswi_ipi_send </span><br><span class="line">              +-&gt; ipi_ops-&gt;sync	&lt;---- tlb_sync</span><br></pre></td></tr></table></figure>
<p>我们画一个示意图说明问题：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kernel:</span><br><span class="line"></span><br><span class="line">           |  core 0                     core 1           core 2</span><br><span class="line">           |</span><br><span class="line">           |  remote sfence.vma</span><br><span class="line">           v</span><br><span class="line">opensbi:  ----------------------------------------------------------------------</span><br><span class="line">           ^</span><br><span class="line">           |  _trap_handler</span><br><span class="line">           |  |              触发核间中断</span><br><span class="line">           |  v  update/send  --------&gt;  _trap_handler</span><br><span class="line">           |                             |</span><br><span class="line">           |                             v</span><br><span class="line">           |     sync         &lt;--------     process(sfenc.vma and update flag)</span><br><span class="line">           |      |</span><br><span class="line">           +------+</span><br></pre></td></tr></table></figure>
<p>总结下，opensbi完全在M mode模拟了TLB的多核广播，从内核视角看，core0发起remote<br>sfence.vma，这个调用返回时，已经完成多核之间的TLB同步。</p>
</li>
</ul>
<h2 id="cache相关指令"><a href="#cache相关指令" class="headerlink" title="cache相关指令"></a>cache相关指令</h2><p>   todo</p>
]]></content>
      <tags>
        <tag>内存管理</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>rpm yum使用笔记</title>
    <url>/rpm-yum%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="rpm"><a href="#rpm" class="headerlink" title="rpm"></a>rpm</h2><ol>
<li><p>install a rpm package: rpm -ivh *.rpm</p>
</li>
<li><p>query something:</p>
<p>  rpm -q kernel  –&gt; query a package name kernel.</p>
<p>  rpm -qf file   –&gt; query a file to find which package it belogs to</p>
<pre><code>                 so we can use this to find the package of one shell
                 command, like: rpm -qf /usr/sbin/lspci, it will get:
                 pciutils-3.5.1-2.el7.aarch64.
</code></pre>
<p>  rpm -ql package -&gt; list all files in this package.</p>
<p>  rpm -qa        –&gt; query all installed package in system.</p>
<p>  rpm -qi package -&gt; list related info of this package.</p>
<p>  rpm -qc package -&gt; find config file of this package, e.g.</p>
<pre><code>                 rpm -qc openssh-clients-xxxx, will get
                 /etc/ssh/ssh_config.
</code></pre>
<p>  rpm -qd package -&gt; find related document.</p>
<p>  rpm -qR package -&gt; find related required libs.</p>
</li>
</ol>
<h2 id="yum"><a href="#yum" class="headerlink" title="yum"></a>yum</h2><pre><code>   yum search kernel      --&gt; search in yum datebase.

   yum provides software  --&gt; find which package contains this software in
                              yum database. similar with rpm -qf file, but
                              rpm searchs packages locally.
                              yum provides fio, we will get:

Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
fio-3.1-1.el7.aarch64 : Multithreaded IO generation tool
Repo        : epel

   yum info package       --&gt; show information of one package
                     e.g. yum info kernel will show all kernel
                   packages with different versions

   yum list package       --&gt; list all version of this package
</code></pre>
<h2 id="rpmbuild"><a href="#rpmbuild" class="headerlink" title="rpmbuild"></a>rpmbuild</h2><pre><code>1. from source to build rpm package

    ...

P.S. For linux kernel, you can directly run: &quot;make rpm&quot; to do so,
     it will create kernel/kernel-dev/kernel-header rpm packages.

2. downlaod a rpm, modify it, and re-build a new rpm package

    ...
</code></pre>
<h2 id="download-rpm"><a href="#download-rpm" class="headerlink" title="download rpm"></a>download rpm</h2><pre><code>This document shares the way to download rpm package using yum tools:
https://www.ostechnix.com/download-rpm-package-dependencies-centos/

&quot;yum install --downloadonly&quot; will also install package.

&quot;yumdownloader&quot; just downloads the package.
</code></pre>
<h2 id="download-source"><a href="#download-source" class="headerlink" title="download source"></a>download source</h2><pre><code>   1. yum install yum-utils

      /* should be the name of package, just showed when yum search XXX */
   2. yumdownloader --source xorg-x11-drv-ati

   3. yum install mock
      useradd -s /sbin/nologin mockbuild

   4. rpm -hiv xorg-x11-drv-ati-7.7.1-3.20160928git3fc839ff.el7.src.rpm
      it will create ~/rpmbuild/SOURCE and install above package there.

   5. cd /root/rpmbuild/SOURCES

   6. xz -d *
      tar -xf *
</code></pre>
]]></content>
      <tags>
        <tag>包管理</tag>
        <tag>RPM/YUM</tag>
      </tags>
  </entry>
  <entry>
    <title>ssh笔记</title>
    <url>/ssh%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>我们在日常工作中常常使用ssh连接远程服务器。</p>
<p>一般我们这样使用: ssh user_name@remote_host<br>其中remote_host可以是远程服务器的域名或者是IP. 之后输入密码就可以登录远程服务器</p>
<p>利用ssh通信的其他命令(e.g. scp)的格式也和上面的相似。</p>
<p>对于想不每次手动输入密码登录, 我们可以把ssh-keygen生成的公钥放到远程服务器对应<br>home目录下的.ssh/authorized_keys这个文件里。这样直接输入上面的命令就可以登录远程<br>服务器。</p>
<p>如何把公钥里的内容放到远程服务器的authorized_keys里, 我们可以使用命令:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh user_name@remote_host &quot;cat &gt;&gt; ~/.ssh/authorized_keys&quot; &lt; ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure>
<p>如果服务端的ssh server不在常用的22端口上。我们可以用-p指定所用的端口:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh -p port_number user_name@remote_host</span><br></pre></td></tr></table></figure>
<p>如果远程服务器没有图形化界面，可以使用 ssh -X user_name@remote_host “command”<br>(e.g. ssh -X user_name@remote_host “thunderbird”), 在远程服务器上运行命令command,<br>而在本地机器上显示command的图形化输出。</p>
]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv原子指令分析</title>
    <url>/riscv%E5%8E%9F%E5%AD%90%E6%8C%87%E4%BB%A4%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="riscv原子指令"><a href="#riscv原子指令" class="headerlink" title="riscv原子指令"></a>riscv原子指令</h2><p> riscv的原子指令定义了load reserved/store conditional和直接更新内存的原子指令两大<br> 类。前者需要几条指令配合实现原子操作，后者一条指令就可以对内存做更新。riscv的这个<br> 版本里还没有提供CAS的单指令实现，协议推荐用lr/sc的方式实现CAS，并给出了参考代码。</p>
<p> lr/sc的方式在其他的构架里也都有，其基本的使用逻辑是，lr的时候同时针对load的地址<br> 设置一个硬件flag，sc操作的地址是之前设置flag的地址，而且flag是被设置的，而且没有<br> 其他的sc对之前的地址操作，那么这个sc指令才执行成功，否则执行失败。sc指令使用寄存器<br> 存放sc成功或失败的信息。这里的store/load是在同一个core上。lr/sc基础上还可以叠加上<br> order的语义，有acquire和release, acquire表示之后的指令不能在本条指令前投机执行<br> 完成，release表示之前的指令不能在本条指令之后执行完成。</p>
<p> 如上只是初步看lr/sc，详细的指令定义还需要各种限制堵住各种出问题的情况。这里更多<br> 的细节就是lr可以有多个，但是store只和相同core上的上一个匹配，sc只要有就会消除之<br> 前的flag，不管这个sc是成功还是失败，sc成功的情况只有上面一种，但是失败的情况就<br> 很多了，sc和上一个lr的地址不匹配会失败，sc之前已经有来自其他core或者设备做完相<br> 同地址的sc，本core上这个sc也是失败，同一个core上，sc和lr中间有sc，那么后一个sc<br> 也是失败。</p>
<p> 假设代码里lr/sc的操作地址都是一样，那么用lr/sc实现原子操作的基本逻辑就是循环的<br> 执行一段代码，在最后sc写入的时候，如果成功，这段代码做的原子操作就成功，如果sc<br> 写入失败，就重复执行整段代码。准备了半天都是为了sc的写内存，如果写入失败(另一个<br> core的sc写入成功)，就返回去重新尝试。下面的是一个伪代码写的原子加1实现，lr把addr<br> 地址上的数据读入rd，add把rd里的数据加1写入rs1，sc指令把rs1里的数据写到addr这个<br> 地址上，rx存储sc是否成功，rx为1，表示没有写成功，跳到lr重新做整套操作。如果，在<br> 弱内存序的机器上，还要加上必要的barrier，以保证sc和lr的顺序。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">core 1                   core 2</span><br><span class="line"></span><br><span class="line">     lable A:                 lable A:</span><br><span class="line">lr rd, [addr]            lr rd, [addr]</span><br><span class="line">add rs1, rd, #1          add rs1, rd, #1</span><br><span class="line">sc rs1, [addr], rx       sc rs1, [addr], rx</span><br><span class="line">beq rx, 1, lable A       beq rx, 1, lable A</span><br></pre></td></tr></table></figure>

<p> riscv上，lr/sc主要是用来实现CAS原子操作。Linux内核里riscv cmpxchg是这样实现的，<br> 和spec里提供的代码类似，多个一个fence。实际上最后更新内存的只是sc.w.rl这个指令，<br> 多核之间对于同一内存位置，sc之间，一个成功，另外一个就必须失败，这个是硬件语义<br> 保证的。相关的代码在arch/riscv/include/asm/cmpxchg.h</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define __cmpxchg(ptr, old, new, size)					\</span><br><span class="line">(&#123;									\</span><br><span class="line">	__typeof__(ptr) __ptr = (ptr);					\</span><br><span class="line">	__typeof__(*(ptr)) __old = (old);				\</span><br><span class="line">	__typeof__(*(ptr)) __new = (new);				\</span><br><span class="line">	__typeof__(*(ptr)) __ret;					\</span><br><span class="line">	register unsigned int __rc;					\</span><br><span class="line">	switch (size) &#123;							\</span><br><span class="line">	[...]</span><br><span class="line">	case 8:								\</span><br><span class="line">		__asm__ __volatile__ (					\</span><br><span class="line">			&quot;0:	lr.d %0, %2\n&quot;				\</span><br><span class="line">			&quot;	bne %0, %z3, 1f\n&quot;			\</span><br><span class="line">			&quot;	sc.d.rl %1, %z4, %2\n&quot;			\</span><br><span class="line">			&quot;	bnez %1, 0b\n&quot;				\</span><br><span class="line">			&quot;	fence rw, rw\n&quot;				\</span><br><span class="line">			&quot;1:\n&quot;						\</span><br><span class="line">			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span><br><span class="line">			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new)			\</span><br><span class="line">			: &quot;memory&quot;);					\</span><br><span class="line">		break;							\</span><br><span class="line">	[...]</span><br><span class="line">	__ret;								\</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p> riscv里的AMO指令是单原子指令对内存做更新，有swap/add/and/xor/or/max/min操作。</p>
<h2 id="qemu实现"><a href="#qemu实现" class="headerlink" title="qemu实现"></a>qemu实现</h2><p> qemu实现lr/sc的代码在：qemu/target/riscv/insn_trans/trans_rva.c.inc。cpu的结构体<br> 里会增加load_res和load_val。lr指令会把load的地址保存到load_res，把load的数据保存<br> 到load_val，sc先判断store的地址是否和load_res相同，如果不相同，store失败，写1到<br> rd寄存器，并清掉load_res，如果相等，使用一个host上cmpxchg原子指令做store操作。</p>
<p> sc的实现使用了原子CAS，这个操作的语义和上面内核里的是一样的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_gen_atomic_cmpxchg_i64(retv, addr, cmpv, newv, idx, memop)</span><br></pre></td></tr></table></figure>
<p> 如果是addr上的数据和cmpv相等，就把newv写入addr的位置，retv用来存储addr地址上原来<br> 的值。qemu里sc的这个代码的意思就是，如果load_res这个地址上的值和load_val相等，就<br> 把src2这个寄存器上的数据写入load_res地址，dest保存load_res地址上原来的值。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_gen_atomic_cmpxchg_tl(dest, load_res, load_val, src2,</span><br><span class="line">                          ctx-&gt;mem_idx, mop);</span><br></pre></td></tr></table></figure>

<p> lr把地址和数据都保存起来，是为了实现多个sc执行时可以互斥，sc的实现里使用了原子<br> CAS指令，如果保存的load_val和内存上的不一样，说明之前有sc改了内存上的数据，当前<br> 的sc里的CAS就不会往内存写数据，后续的逻辑会返回当前sc失败。如果保存的load_val和<br> 内存上的一样, 说明之前没有sc写这个地址，或者之前有sc对这个地址写了一个相同的值。<br> 如果是前者，sc正常写入，如果是后者，sc的写入可能会观测到两种可能的结果。</p>
<p> 我们具体考虑下:</p>
<p> 当core1刚执行完tcg_gen_atomic_cmpxchg_tl, 写入一个相同的值，还没有执行tcg_gen_movi_tl(load_res, -1)，<br> core2执行tcg_gen_atomic_cmpxchg_tl写入一个新值，由于load_res没有被更新，core2是<br> 可以写入这个新值的。如果，core1执行了tcg_gen_atomic_cmpxchg_tl, 写入一个相同的值，<br> 并且也执行了tcg_gen_movi_tl(load_res, -1)，core2如果这个时候执行tcg_gen_atomic_cmpxchg_tl，<br> 会执行失败。这是不是qemu这个地方的一个bug？</p>
<p> 不管sc成功还是失败都跳到l2 lable，这个地方会把load_res清掉。</p>
<p> qemu里对AMO的实现比较简单，就是直接映射到host上的对应原子指令实现。</p>
<h2 id="原子指令里的内存序"><a href="#原子指令里的内存序" class="headerlink" title="原子指令里的内存序"></a>原子指令里的内存序</h2><p> lr/sc和AMO的指令都支持在上叠加内存序的约束。aq表示本条执行之后的访存指令不能越<br> 过本条指令完成，rl表示本条指令不能越过之前的访存指令完成。</p>
<p> qemu的实现中，对于AMO的指令都没有对aq和rl做处理，对于lr/sc指令，是加了相关的<br> memory barrier，aq是TCG_BAR_LDAQ，rl是TCG_BAR_STRL，但是对于sc的正常处理分支，<br> 由于使用了host的atomic_cmpxchg指令，也是没有对aq/rl做显示的处理。</p>
<p> 内存序还有待深入分析。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>riscv</tag>
        <tag>原子指令</tag>
      </tags>
  </entry>
  <entry>
    <title>sed笔记</title>
    <url>/sed%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>sed是一个面向数据流的编辑器。和vim这种交互式编辑器不用，sed按照定义的处理模式处理<br>每一行文本。比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* 把test_file文件中每行的第一个&#x27;haha&#x27;换成&#x27;taoge&#x27; */</span><br><span class="line">sed &#x27;s/haha/taoge/&#x27; test_file</span><br></pre></td></tr></table></figure>
<p>其中s/haha/taoge就是定义的处理模式，test_files是要处理的文件。sed把处理后的文本<br>向标准输出，不会更改原文件。</p>
<p>一个sed命令还可以在sed和处理模式之间加入命令选项。比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sed -f sed_script test_file</span><br></pre></td></tr></table></figure>
<p>表示用sed_script中的处理模式（每行一个处理模式）处理test_file中的所有行。<br>又比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sed -e &#x27;s/this/a/; s/that/b/&#x27; test</span><br></pre></td></tr></table></figure>
<p>-e后面是分号隔开的多个匹配模式。</p>
<p>sed编辑器的处理模式中常用的命令有s, d, i, a, c, w, p, r, 寻址…</p>
<p>s命令比较常见，即像上面展示的：s/old/new/. vim的修改命令也是这样的，一般的，<br>linux邮件列表里指示修改的评论有时候也写成sed命令的样子了。都是过来人，行业”黑话”<br>一看都懂。s命令的最后面还可以跟一些flag, 比如s/old/new/g, 表示整行所有的old都替换<br>成new; s/old/new/2, 表示替换第二个old成new; s/old/new/p, 表示输出发生替换的行，<br>一般常常是：sed -n s/old/new/p，-n表示结果不输出，于是最终结果是只输出了发生替换<br>的行; s/old/new/w file, 表示把输出的结果保存在file文件中。</p>
<p>想指定具体行的修改，可以寻址方式修改, 比如：2s/old/new/, 2,3s/old/new, 1,$s/old/new<br>分别是修改第二行，第二行和第三行，第一行到最后一行。vim里格式是一样的。</p>
<p>更加一般的，这里的寻址方式也可以换成匹配字符, 比如：/wang/s/old/new/<br>找见有字符串’wang’的一行, 在这一行用s命令把第一个old改成new. 这个匹配特定行，再<br>处理的方式也可以在除了s命令的其他命令上使用。</p>
<p>d是删除命令。上面熟悉的格式可以套用到d命令上：sed ‘1d’ test, sed ‘1,3d’ test,<br>sed ‘/wang/d’ test, sed -e “/wang/d; 1d” test, 分别是删除第一行，删除1~3行，删除<br>含有’wang’的一行，删除含有’wang’的一行和第一行。</p>
<p>i, a, c是插入, 附加操作和更改操作, 比如：sed ‘2i new_line’ test,<br>sed ‘2a new_line’ test, sed ‘2a new_line’ test. 以上各自是在test文件的第二行之前<br>插入new_line这行，在test文件第二行之后插入new_line这行，直接把test文件的第二行改成<br>new_line这行。</p>
<p>y是变换命令。可以完成类似文本加密的工作, 当然这里说的加密只是一对一的做字符替换,<br>比如：sed ‘y/123/579/‘ test, 可以把test中所有的字符1,2,3依次替换成5,7,9</p>
<p>之前提到flag p, p用于输出指定的值，一般都结合-n使用，-n用于禁止输出，这样就可以<br>控制输出想要输出的东西了，比如：sed -n  ‘/wang/p’ test, sed -n ‘2,3p’ test<br>输出有’wang’字符的行，输出第2行和第三行。</p>
<p>之前也提到了flag w, w用于把改动写入到文件, 比如：sed ‘1,3w new_file’ test,<br>sed ‘/wang/w new_file’ test, 把test文件的1~3行写入到new_file中, 把test文件中<br>含有’wang’的一行写入到new_file文件中。</p>
<p>和w相对的还有读命令，比如： sed ‘3r new’ file, 读出new文件中的内容然后加到file<br>文件的第三行之后。当然也可以 sed ‘/wang/r new’ file, 读出new文件中的内容，加到file<br>文件含有’wang’这一行之后。</p>
]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>tmux tips</title>
    <url>/tmux-tips/</url>
    <content><![CDATA[<p>There are three basic concepts in tmux: pane, window, session.<br>After you run tmux in shell, you open a pane in a window in a session.</p>
<p>Let’s first consider the pane operation in one window:</p>
<pre><code>ctl + B + %: splite current pane into two, left and right
ctl + B + &quot;: splite current pane into two, up and down
ctl + B + z: full screen for current pane, or return back.
</code></pre>
<p>If you want multiple window:</p>
<pre><code>ctl + B + c: create a window
</code></pre>
<p>If you login your linux system, you can:</p>
<pre><code>tmux ls: to see the sessions    
tmux a -t session_name(or session number): to attach to related session
</code></pre>
<p>If you want to copy log in tmux to a file:</p>
<pre><code>ctl + b + :
capture-pane -S -3000 + return  this copied 3000 lines into buffer
ctl + b + :
save-buffer /path/to/your_file  this copied content in buffer to file,
                                in my environment, path should be a full path.
</code></pre>
<p>tmux will use .bash_profile by default, so if you want to use the configures in<br>.bashrc，you should “source ~/.bashrc” in .bash_profile. One problem I met is tmux<br>fails to output colors for some commands such as ls, “source ~/.bashrc” in .bash_profile<br>can solve this problem.</p>
]]></content>
      <tags>
        <tag>开发工具</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv跳转指令整理</title>
    <url>/riscv%E8%B7%B3%E8%BD%AC%E6%8C%87%E4%BB%A4%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h2 id="riscv跳转指令"><a href="#riscv跳转指令" class="headerlink" title="riscv跳转指令"></a>riscv跳转指令</h2><p>跳转指令可以分为: 直接跳转、寄存器跳转以及条件跳转，这几个概念并不是在一个层面上的。</p>
<h3 id="直接跳转"><a href="#直接跳转" class="headerlink" title="直接跳转"></a>直接跳转</h3><p>直接跳转中，跳转的偏移直接编码在指令里，所以跳转地址是固定的。因为指令编码只有<br>32bit，除去指令op code，用于编码偏移的位数是有限的，所以跳转的距离也是有限的。</p>
<p>比如，jal rd, offset指令的格式是:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------+-----------+---------+------------+----+--------+</span><br><span class="line">| imm[20] | imm[10:1] | imm[11] | imm[19:12] | rd | opcode |</span><br><span class="line">+---------+-----------+---------+------------+----+--------+</span><br></pre></td></tr></table></figure>
<p>它的偏移编码是20bit，所有支持跳转的范围是+/-1MB。那么如果跳转距离超过范围，就需要<br>把跳转的目的地址编码到寄存器里，使用如下寄存器跳转的方式完成跳转。</p>
<h3 id="寄存器跳转"><a href="#寄存器跳转" class="headerlink" title="寄存器跳转"></a>寄存器跳转</h3><p>寄存器跳转中，跳转指令从gpr里得到跳转的目标地址，这时跳转的目的地址是动态的，跳<br>转的范围也足够覆盖64bit的地址空间。</p>
<p>riscv里使用jalr rd, offset(rs1)指令完成寄存器跳转，和上面jal一样，jalr也是带rd，<br>rd用来保存jalr/jal的下一条指令的地址，函数调用时，利用rd保存函数返回的地址。</p>
<p>使用jalr之前需要先把跳转的目的地址加载到rs1里(先认为offset是0)，也就是加载一个64<br>bit数到一个gpr里。显然riscv 32bit的指令编码一条指令是无法搞定这个操作的。这里一般<br>使用auipc + addi/ld?</p>
<p>auipc rd, imm的意思是Add Upper Immediate to PC</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+------------+----+--------+</span><br><span class="line">| imm[31:12] | rd | opcode |</span><br><span class="line">+------------+----+--------+</span><br></pre></td></tr></table></figure>
<p>imm做符号扩展并左移12bit后和PC相加，结果保存到rd。可以见imm是0，可以用auipc得到PC。<br>如果一个符号和auipc的距离在32bit描述范围之内，编译器就可以把这个offset的高20bit<br>编码到auipc的imm，再用一条addi指令加上offset的低12bit，大概的示意图如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                       +------------+----+--------+</span><br><span class="line">auipc rd, imm_h        | imm[31:12] | rd | opcode |</span><br><span class="line">                       +------------+----+--------+</span><br><span class="line">                         ^    +-----------+----+-----+----+--------+</span><br><span class="line">addi rd, rd, imm_l       |    | imm[11:0] | rs | 000 | rd | opcode |</span><br><span class="line">                         |    +-----------+----+-----+----+--------+</span><br><span class="line">                         |            ^</span><br><span class="line">jalr ra, offset(rd)      |            |</span><br><span class="line"> ^                       |            |</span><br><span class="line"> |                     +------------+-----------+</span><br><span class="line"> |        addr offset  | imm[31:12] | imm[11:0] |</span><br><span class="line"> |                     +------------+-----------+</span><br><span class="line"> v</span><br><span class="line">label: xxx</span><br></pre></td></tr></table></figure>

<h3 id="条件跳转"><a href="#条件跳转" class="headerlink" title="条件跳转"></a>条件跳转</h3><p>条件跳转根据两个输入寄存器和判断条件决定是否跳转，可见条件跳转可以实现高级语言里<br>的分支语句。riscv里条件跳转的跳转偏移被直接编码到指令里，所以除去指令的op code以<br>及两个输入寄存器，留给offset的编码已经比较小。</p>
<p>比如，bge rs1, rs2, offset指令的格式是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------+-----------+-----+-----+--------+----------+---------+--------+</span><br><span class="line">| imm[12] | imm[10:5] | rs2 | rs1 | funct3 | imm[4:1] | imm[11] | opcode |</span><br><span class="line">+---------+-----------+-----+-----+--------+----------+---------+--------+</span><br></pre></td></tr></table></figure>
<p>它的偏移编码是12bit，所有支持跳转的范围是+/-4KB。</p>
<p>riscv里的条件跳转指令还有如下，可见有些比较的模式是相互之间做等价的，比如，大于<br>和小于的比较用一个指令就好，只要交换一下两个比较寄存器就好。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">beq rs1, rs2, offset</span><br><span class="line">bne rs1, rs2, offset</span><br><span class="line"></span><br><span class="line">bge rs1, rs2, offset</span><br><span class="line">bgt rs1, rs2, offset</span><br><span class="line">bgeu rs1, rs2, offset</span><br><span class="line">bgtu rs1, rs2, offset</span><br><span class="line"></span><br><span class="line">blt rs1, rs2, offset</span><br><span class="line">bltu rs1, rs2, offset</span><br><span class="line"></span><br><span class="line">beqz rs2, offset</span><br><span class="line">bltz rs2, offset</span><br><span class="line">blez rs2, offset</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>strace使用笔记</title>
    <url>/strace%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="基本功能："><a href="#基本功能：" class="headerlink" title="基本功能："></a>基本功能：</h1><ol>
<li><p>strace ./a.out 依次显示各个系统调用</p>
</li>
<li><p>strace -c ./a.out 可以打印出a.out中各个系统调用的使用次数和时间等信息</p>
</li>
<li><p>strace -ff -o strace_log ./a.out 可以把输出的结果存在strace_log.<pid>中</pid></p>
</li>
<li><p>strace -f 可以在fork之后跟踪各进程内的系统调用</p>
</li>
<li><p>strace -i ls 显示系统调用时的instruction pointer</p>
</li>
<li><p>strace -r|-t|-tt|-T 现实相对时间戳、绝对时间、微秒时间和系统调用消耗时间</p>
</li>
<li><p>strace -e 可以跟踪指定的事件(strace -e open ./a.out)</p>
</li>
<li><p>strace -p <pid> 可以对指定进程进行跟踪</pid></p>
</li>
<li><p>strace -O &lt;<strong>ms&gt; 可以把</strong>ms的时间认为是strace的开销，strace在统计时间的时<br>  候会减去**ms时间，这个时间一般是由程序运行时间和strace跟踪情况下程序的运<br>  行时间相减得到, 是个经验值</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>软件调试</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu使用LDAP做用户管理</title>
    <url>/ubuntu%E4%BD%BF%E7%94%A8LDAP%E5%81%9A%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<p>The basic view about centralized authentication using LDAP are as below:</p>
<ol>
<li>We need a server(A) to install LDAP server which has the accounts’<br>information and deals with authentication requests.</li>
<li>We need to install LDAP client in another server(B) which you want to login.</li>
<li>You can login server B after adding relative account in LDAP server. Or if<br>you already have some accounts information, you can login from another<br>server using LDAP server.</li>
</ol>
<h2 id="Configure-LDAP-server"><a href="#Configure-LDAP-server" class="headerlink" title="Configure LDAP server"></a>Configure LDAP server</h2><ol>
<li><p>sudo apt-get install slapd ldap-utils<br>During the install process, it needs to fill in some basic configurations.<br>You can also use command below to change them.</p>
</li>
<li><p>sudo dpkg-reconfigure slapd<br>Refer to [1] to see how to configure.</p>
</li>
<li><p>Add groups, users.<br>You can use ldif file to do this, another way is to use ldapscripts.<br>(1) ldif way</p>
<pre><code>a. Edit ldif file as below: vi add_content.ldif
...
b. Add users, groups, users&#39; directory and groups&#39; directory as below;
   ldapadd -x -D cn=admin, dc=company, dc=com -W -f add_content.ldif

it will appear:

Enter LDAP Password:
adding new entry &quot;ou=User,dc=company,dc=com&quot;
...

NOTE: if failed there, mostly you should check basic slapd configure.

c. ldapdelete -x -D &quot;cn=admin,dc=company,dc=com&quot; -W &quot;uid=test,ou=User,dc=company&quot;
   Using above command to delete one entry.
</code></pre>
<p>   ldapsearch -x -LLL -b dc=company,dc=com</p>
<pre><code>   Using above command to search information of all entries.
</code></pre>
<p>(2) ldapscripts way</p>
<pre><code>a. sudo apt-get install ldapscripts

b. Configure /etc/ldapscripts/ldapscripts.conf:
   SERVER=localhost
</code></pre>
<p>   SUFFIX=”dc=company,dc=com”<br>   GSUFFIX=”ou=Groups”<br>   USUFFIX=”ou=Users”<br>   BINDDN=”cn=admin,dc=company,dc=com”</p>
<pre><code>   BINDPWDFILE=&quot;/etc/ldapscripts/ldapscripts.passwd&quot;

c. echo -n &quot;your_root_passwd_for_ldap&quot; &gt; /etc/ldapscripts/ldapscripts.passwd

d. User relative command to add group, user: ldapadduser, ldapaddgroup

NOTE: We can add an account by ldapadduser account_name group_name.
      e.g. ldapadduser test User
  Use c step above to write password to ldapscripts.passwd. This
  will *NOT* write &quot;\n&quot; at the end of password line.
</code></pre>
</li>
</ol>
<h2 id="Configure-LDAP-client-2"><a href="#Configure-LDAP-client-2" class="headerlink" title="Configure LDAP client[2]"></a>Configure LDAP client[2]</h2><ol>
<li><p>sudo apt-get install libpam-ldap nscd<br>When installing libpam-ldap, it will ask you to configure LDAP client during<br>the install process. You can also change the configuration as below.</p>
</li>
<li><p>sudo apt-get install ldap-auth-config<br>sudo dpkg-reconfigure ldap-auth-config[2]</p>
</li>
<li><p>Modify /etc/nsswitch.conf to choose how to make authentication:</p>
<pre><code>passwd: files ldap
group: files ldap
shadow: files ldap
</code></pre>
<p>NOTE: You’d better put “filles” befort “ldap”</p>
</li>
<li><p>Build home directory automatically in LDAP client[3]<br>add line: session required pam_mkhomedir.so skel=/etc/skel/ umask=0022<br>to /etc/pam.d/common-account</p>
</li>
</ol>
<h2 id="Migrate-accounts-information"><a href="#Migrate-accounts-information" class="headerlink" title="Migrate accounts information"></a>Migrate accounts information</h2><p>If you already have users information in /etc/passwd, /etc/group, /etc/shadow,<br>and you want to use LDAP manage users information, you can do as below:</p>
<ol>
<li><p>sudo apt-get install migrationtools</p>
</li>
<li><p>Modify /etc/migrationtools/migrate_common.ph:<br>   $DEFAULT_BASE = “dc=company,dc=com”;</p>
</li>
<li><p>/usr/share/migrationtools/migrate_passwd.pl /etc/passwd add_people.ldif</p>
</li>
<li><p>Modify add_people.ldif:<br>Change the information about group for every user.</p>
<p>Copy the encrypted pass word in /etc/shadow to replace “x” in “userPassword: {crypt}x”<br>attribution in add_people.ldif</p>
<p>FIXME…</p>
</li>
<li><p>ldapadd -x -D cn=admin,dc=company,dc=com -W -f add_people.ldif</p>
</li>
</ol>
<p>NOTE: I just need the password information here. If you want login using LDAP,<br>      you should also consider to migrate group information in /etc/group</p>
<h2 id="Backup-existed-LDAP-date"><a href="#Backup-existed-LDAP-date" class="headerlink" title="Backup existed LDAP date"></a>Backup existed LDAP date</h2><p>…</p>
<p>Reference:</p>
<ul>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZGlnaXRhbG9jZWFuLmNvbS9jb21tdW5pdHkvdHV0b3JpYWxzL2hvdy10by1pbnN0YWxsLWFuZC1jb25maWd1cmUtYS1iYXNpYy1sZGFwLXNlcnZlci1vbi1hbi11YnVudHUtMTItMDQtdnBz">https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-a-basic-ldap-server-on-an-ubuntu-12-04-vps<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuZGlnaXRhbG9jZWFuLmNvbS9jb21tdW5pdHkvdHV0b3JpYWxzL2hvdy10by1hdXRoZW50aWNhdGUtY2xpZW50LWNvbXB1dGVycy11c2luZy1sZGFwLW9uLWFuLXVidW50dS0xMi0wNC12cHM=">https://www.digitalocean.com/community/tutorials/how-to-authenticate-client-computers-using-ldap-on-an-ubuntu-12-04-vps<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cDovL3d3dy5kZWJpYW4tYWRtaW5pc3RyYXRpb24ub3JnL2FydGljbGUvNDAzL0dpdmluZ191c2Vyc19hX2hvbWVfZGlyZWN0b3J5X2F1dG9tYXRpY2FsbHk=">http://www.debian-administration.org/article/403/Giving_users_a_home_directory_automatically<i class="fa fa-external-link-alt"></i></span></li>
</ul>
]]></content>
      <tags>
        <tag>运维</tag>
        <tag>LDAP</tag>
      </tags>
  </entry>
  <entry>
    <title>vSVA逻辑分析</title>
    <url>/vSVA%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="qemu基础认识"><a href="#qemu基础认识" class="headerlink" title="qemu基础认识"></a>qemu基础认识</h2><p> qemu里linux系统的用户态代码跑在cpu EL0, qemu里linux系统的内核态代码跑在cpu EL1。</p>
<p> qemu创建虚拟机的时候ioctl(CREATE_VM，VCPU，MEMORY)会到cpu EL2创建VM的记录信息。<br> ioctl(VM_RUN)会把PC指向虚拟机起始地址，然后退出到EL1。EL2只起到虚拟机管理的作用，<br> 虚拟机上的代码正常运行时，代码占据真实的CPU，并且如果是用户态代码，跑在物理CPU<br> 的EL0, 如果是内核态代码就直接跑到物理CPU的EL1。当CPU访问物理内存的时候，VA-&gt;IPA<br> 的转换由MMU S1直接支持, 当IPA的地址落在之前ioctl注册的虚拟机地址空间时，硬件自动<br> 完成MMU S2的转换。可见，虚拟机里的进程页表是直接放到虚拟机内核地址空间的。</p>
<p> 虚拟机里的代码运行在CPU EL0/EL1。当有IO访问的时候, 因为之前创建虚拟机的时候<br> 已经把IO地址空间配置给虚拟机，这里有IO访问的时候会触发CPU异常，虚拟机退出，CPU<br> 进入EL2, CPU在EL2处理后退出到虚拟机qemu里，qemu可以具体去处理这个IO，比如是一个<br> 网络IO，那qemu可以直接起socket，把报文发出去。注意，这里的虚拟机退出是指CPU不再<br> 运行虚拟机里执行的代码，因为CPU并不知道如果控制IO。</p>
<h2 id="vSVA"><a href="#vSVA" class="headerlink" title="vSVA"></a>vSVA</h2><p> vSVA的目标是在虚拟机里(qemu)，使的IO设备可以直接使用进程VA。所以，我们这里的<br> 假设是物理IO设备已经通过host上vfio驱动直通给虚拟机。</p>
<p> 要实现vSVA的目标，我们需要同时使能SMMU的S1,S2地址翻译，S1进行VA-&gt;IPA翻译，S2<br> 进行IPA-&gt;PA翻译，如果是host vfio使能，我们认为S2的翻译已经通过vfio配置在SMMU里。</p>
<p> 所以，vSVA的方案需要把虚拟机系统里的进程页表同步到host SMMU上。因为是vSVA，就<br> 有可能出现设备发起内存访问的时候，host SMMU上虚拟机里的进程页表项不存在的情况，<br> 所以，host上的SMMU要可以支持S1缺页。因为，S2用vfio支持，vfio采用pin内存的方式，<br> 暂时我们不需要S2的缺页。这里说的host上SMMU支持S1缺页，并不是在host系统上做S1缺页，<br> 我们这里讨论的是nested SMMU, 所以在host SMMU硬件检测到S1缺页的时候，应该把这个<br> 信息上报给guest里的SMMU，guest里使用和host一样的SMMU驱动处理缺页，当guest处理完<br> 这个缺页后，应该把对应的页表信息同步到SMMU的物理硬件上(SMMU.CD.TT0里)。因为，<br> guest里的进程页表和SMMU CD上的页表物理上不是一个，很明显这里有一个设备和vcpu页<br> 表的同步问题，在host SVA上这个问题不存在，因为host SVA上cpu和SMMU是物理上共用<br> 相同页表。因此，在需要在vcpu无效化页表的时候，需要把信息同步到host的SMMU上，<br> 这个信息包括页表项和TLB。host SVA上也有这个问题，但是如果用SMMU stall mode, 可<br> 以配置DVM，把CPU侧TLB invalidate广播到SMMU，这样就不需要软件同步。</p>
<p> 在guest里多进程使用一个设备的资源，就需要支持PASID。这里的逻辑和上面的是一样的，<br> 只不过扩展到多进程。</p>
<h2 id="软件框架"><a href="#软件框架" class="headerlink" title="软件框架"></a>软件框架</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+----------------------+</span><br><span class="line">| guest           user |</span><br><span class="line">|                      |</span><br><span class="line">|                      |</span><br><span class="line">|                      |</span><br><span class="line">|----------------------|   --------------------    VA</span><br><span class="line">|               kernel |                                 +------------+</span><br><span class="line">|                      |                                 | page table |</span><br><span class="line">|                      |                                 +------------+</span><br><span class="line">|                      |                                         ^</span><br><span class="line">+----------------------+                                         |</span><br><span class="line">+----------------------+         --------------------  IPA       |</span><br><span class="line">| host                 |                                         |</span><br><span class="line">|                      |                                         |</span><br><span class="line">|                      |             +---------+                 |</span><br><span class="line">|                      |             | DDR     |        PA       |</span><br><span class="line">|                      |             +---------+                 |</span><br><span class="line">|                      |                                         |</span><br><span class="line">|                      |                                         |</span><br><span class="line">|                      |                                         |</span><br><span class="line">|                      |                                         |</span><br><span class="line">+----------------------+                                         |</span><br><span class="line">        |                                                        |</span><br><span class="line">        |                   +-----+                              |</span><br><span class="line">        |          +------&gt; | S1  |  VA-&gt;IPA  &lt;------------------+</span><br><span class="line">     +--+---+ -----+        +-----+                  </span><br><span class="line">     | SMMU |</span><br><span class="line">     +------+ -----+        +-----+</span><br><span class="line">        ^          +------&gt; | S2  |  IPA-&gt;PA</span><br><span class="line">        |                   +-----+</span><br><span class="line">     +-----+</span><br><span class="line">     | dev |</span><br><span class="line">     +-----+</span><br></pre></td></tr></table></figure>
<p> 我们顺着具体的数据流看看需要的接口，在dev的控制寄存器被map到guest的用户态后，<br> 用户态可以直接给guest VA配置给dev，启动dev从VA处读写数据。dev发出的访问到达<br> SMMU后首先要进过S1的翻译，得到IPA，所以S1需要guest里的进程的页表。</p>
<p> 目前Redhat的Eric在做ARM nested SMMU的支持，他把相关的补丁集合到了他的分支里，<br> 你可以在这个地方看到完整的内核补丁：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2VhdWdlci9saW51eA==">https://github.com/eauger/linux<i class="fa fa-external-link-alt"></i></span> branch:<br> v5.6-2stage-v11_10.1。这组补丁里给vfio加了一个ioctl(VFIO_IOMMU_SET_PASID_TABLE)，<br> 用这个ioctl把虚拟机里的SMMU的CD地址(IPA)直接传给host，并且配置给物理SMMU的CD<br> 基地址。对于预先在vcpu一侧有缺页的情况，这里S1可以查页表翻译，SMMU硬件在nested<br> 模式下，会对CD基地址做S2翻译的到CD的真正物理地址，然后找见页表做翻译。可见qemu<br> 里的SMMU驱动使用和host SMMU相同的驱动，初始化qemu里SMMU的CD.TT0, 然后把CD直接<br> 通过系统调用配置到物理SMMU上。需要注意，这里CD里的页表基地址是IPA，SMMU硬件<br> 会先根据S2页表翻译IPA到PA得到页表物理基地址。</p>
<p> 对于dev传给SMMU的VA没有页表的情况, S1要做缺页处理。这里的缺页处理在逻辑上应该<br> 上报给guest，因为要做vSVA，是要给虚拟机里的进程的页表加页表项。Eric这组补丁里，<br> 在vfio里加了一个event queue的队列，mmap到host用户态，用来传递这个信息。逻辑上看，<br> qemu应该处理并上报这个缺页请求，qemu里的SMMU驱动做缺页处理。在qemu的SMMU驱动做<br> 缺页处理的时候，来自dev的请求是stall在SMMU里的，所以，SMMU缺页处理完毕后，应该<br> 有通知机制通知到host SMMU，使能stall的请求继续。</p>
<p> 可以看到当页表有变动的时候，在guest和物理SMMU上同步页表的开销是很大的。</p>
<p> 当guest里的进程有退出或者内存有释放时，需要更新guest里进程的页表，vcpu tlb，<br> host SMMU上相关进程页表和tlb。Eric补丁里vfio里提供了ioctl(VFIO_IOMMU_CACHE_INVALIDATE)<br> 用来更新host SMMU上的相关tlb。这里vcpu可以做带VMID/ASID的DVM, 直接无效化相关的tlb。</p>
<h2 id="virtio-iommu"><a href="#virtio-iommu" class="headerlink" title="virtio iommu"></a>virtio iommu</h2><p> 以上的分析都是基于nested IOMMU/SMMU的方案。目前Jean在做virtio iommu的方案。<br> 这个方案在qemu里实现一个virtio iommu的虚拟设备qemu/hw/virtio/virtio-iommu.c,<br> 虚拟机内核里的drivers/iommu/virtio-iommu.c驱动这个虚拟设备，现在看来这个是<br> 用纯软件实现VA-&gt;IPA的映射。</p>
<p> 基于以上的分析，可以基于vfio接口在virtio iommu里实现有物理SMMU支持的virtio-iommu。<br> 但是，这个需要virtio-iommu协议的支持。目前，Jean在搞virt-iommu的协议<br> jpbrucker.net/virtio-iommu/spec, 目前看virtio iommu spec中PASID/fault的支持<br> 还不完善。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>《大学》阅读速记</title>
    <url>/%E3%80%8A%E5%A4%A7%E5%AD%A6%E3%80%8B%E9%98%85%E8%AF%BB%E9%80%9F%E8%AE%B0/</url>
    <content><![CDATA[<p>《大学》全文比较短，只有1500个字左右，主要讲的是个人修为和管理好国家的关系，这里<br>个人修为我们可以理解为个人对自身和家庭的建设，管理好国家似乎对国家治理者有用，我们<br>理解的时候可以把它泛化成做成一件具体的事情或者是事业成功。</p>
<p>文中的逻辑是个人修为和管理好国家有着因果关系，但是不是直接的因果关系，文中建立起<br>一个因果的链条用来说明其中的关系。然后，具体解释了因果链条里的每一种状态的含义。</p>
<p>文中给出的因果关系是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">物格而后知至；知至而后意诚；意诚而后心正；心正而后身修；身修而后家齐；家齐而后国治；</span><br><span class="line">国治而后天下平。</span><br></pre></td></tr></table></figure>
<p>格物就是研究事物的道理，知至就是通晓其中的道理，意诚文中专门写了一段解释，就是要<br>实事求是看问题，不要装，心正是要避免情绪化，因为一旦情绪化后可能丧失对事物的判断，<br>身修文中没有解释，但是文章中引用其他的文学作品比如《诗经》，描述了身修之后的君子<br>的样子，大概具有如下的几个品德：实事求是、进取、识别贤能和小人、谨慎、私德高。</p>
<p>文章中还在一开始引入了一个概念，叫“知止”。这个概念比较有意思：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">大学之道，在明明德，在亲民，在止于至善。知止而后有定；定而后能静；静而后能安；</span><br><span class="line">安而后能虑；虑而后能得。</span><br></pre></td></tr></table></figure>
<p>知止是说要明白目标是什么，有了目标的牵引才能有定、能静、能安、能虑和能得。所以<br>当你不能定静安虑得的时候，想想你的目标，或许有一点帮助。</p>
<p>文中还给出了修身和齐家的关系，简单说就是，要分的请好坏才能把家搞好，因为，人们有时<br>很难看到事物的有点和缺点，特别是对于自己喜好或者是厌恶的人或事。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">人之其所亲爱而辟焉，之其所贱恶而辟焉，之其所畏敬而辟焉，之其所哀矜而辟焉，之其所敖惰</span><br><span class="line">而辟焉。故好而知其恶，恶而知其美者，天下鲜矣。</span><br></pre></td></tr></table></figure>

<p>文中还给出了齐家和治国的关系，简单说就是国家治理者对人民有着很大的表率作用。这个<br>可以推导出一些基本结论，文中认为，管理者要严格要求自己，给人民作出榜样，国家不好<br>管理，管理者要谨慎，国家强大(经济实力强)是管理好的体现。</p>
<p>比较有意思的是，文中给出了用人时，品德和才能的权衡考虑，可以想象，根据文中的逻辑，<br>《大学》把一个人的品德放到了才能的前面。同样对于一个国家，文中认为应该把道义放到<br>利益的前面。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">仁者以财发身，不仁者以身发财。未有上好仁，而下不好义者也；未有好义，其事不终者也；</span><br><span class="line">未有府库财，非其财者也。孟献子曰：“畜马乘，不察于鸡豚；伐冰之家，不畜牛羊；百乘之家，</span><br><span class="line">不畜聚敛之臣；与其有聚敛之臣，宁有盗臣。”此谓国不以利为利，以义为利也。</span><br></pre></td></tr></table></figure>

<p>文中还强调了落地的重要性：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">唯仁人放流之，迸诸四夷，不与同中国。此谓惟仁人为能爱人，能恶人。见贤而不能举，</span><br><span class="line">举而不能先，命也；见不善而不能退，退而不能远，过也。</span><br></pre></td></tr></table></figure>
<p>就是要真正的把贤者放到做事的位置上，不善者要及时罢免。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>大学</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu包管理笔记</title>
    <url>/ubuntu%E5%8C%85%E7%AE%A1%E7%90%86%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p> ubuntu是linux系统的一个发行版，其实在debian上修改而来。最开始debian的软<br> 件包管理命令是dpkg, 这个命令可以安装单个软件包，也可以对现有的软件包做<br> 一些查找之类的工作。但是dpkg命令只能处理单个deb包的情况，对于安装的软件<br> 要依赖其他包的情况debian开发了apt工具。ubuntu的包管理也是使用apt, ubuntu<br> 在全球范围内维护很多个软件仓库，这些仓库中的软件包都是一样的，使用apt命令<br> 的时候就是从这些软件仓库的其中之一去下载软件包并安装，这些软件仓库的URL保<br> 存在本地ubuntu的/etc/apt/source.list中</p>
<p>下面以例子的形式介绍具体的用法：</p>
<ol>
<li><p>下载自己ubuntu上的ls的源代码:<br>which ls 得到ls命令对应的二进制文件的路径: /bin/ls<br>dpkg -S /bin/ls 查找是什么deb包包含/bin/ls, 若只用ls会有很多无用的查找结果<br>该命令的到的结果: coreutils: /bin/ls<br>sudo apt-get search coreutils 下载coreutils包的源代码</p>
</li>
<li><p>sudo apt-get update 更新软件仓库</p>
</li>
<li><p>sudo apt-get upgrade 更新已经安装的软件到最新的版本</p>
</li>
<li><p>dpkg -i ***.deb 安装一个deb包到系统</p>
</li>
<li><p>sudo apt-get install *** 安装软件***，如果有依赖的包没有，其会自动下载需要依<br>赖的包，然后安装软件</p>
</li>
<li><p>sudo apt-get remove *** 卸载软件***</p>
</li>
<li><p>升级ubuntu内核：ubuntu的内核差最新的内核比较远，怎么更新到最新的内核?<br><span class="exturl" data-url="aHR0cDovL2tlcm5lbC51YnVudHUuY29tL35rZXJuZWwtcHBhL21haW5saW5lLw==">http://kernel.ubuntu.com/~kernel-ppa/mainline/<i class="fa fa-external-link-alt"></i></span> 有各个kernel版本的deb包, 选择<br>喜欢的一个进去，作为例子这里选v3.13-rc1-trusty<br>如果pc是64位的，下载linux-image-***amd64.deb, linux-headers-***amd64.deb<br>linux-header-***all.deb三个deb包<br>dpkg -i ***.deb分别安装这三个包，然后sudo update_grub更新grub<br>重启电脑后会发现kernel已经更新到3.13</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>包管理</tag>
        <tag>apt/dpkg</tag>
      </tags>
  </entry>
  <entry>
    <title>wireshark and tcpdump</title>
    <url>/wireshark-and-tcpdump/</url>
    <content><![CDATA[<p>tcpdump可以捕获某个网络端口的所有的网络报文。可以用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump -i eth0 -w dump_file</span><br></pre></td></tr></table></figure>
<p>捕获从eth0经过的网络报文，然后把他们的具体信息存在dump_file指定的文件中。</p>
<p>用wireshark可以打开dump_file文件解析其中的报文。wireshark是一个图形化的网络报文<br>分析工具。在windows和linux下都有wireshark的版本。</p>
<p>一般的，看报文的统计结果的时候，比如看有多少重传的TCP报文，可以在wireshark中<br>打开Analyze -&gt; Export Info -&gt; Notes 查看。</p>
]]></content>
      <tags>
        <tag>网络</tag>
        <tag>wireshark</tag>
      </tags>
  </entry>
  <entry>
    <title>vim使用笔记</title>
    <url>/vim%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li><p>水平创建一个terminal:</p>
<p>vert term</p>
</li>
<li><p>term</p>
</li>
<li><p>ctrl + w + N<br>打开的terminal进入vim的normal模式，可以使用vim的复制命令复制信息到原来的窗口里。<br>注意这里是大写的N。</p>
</li>
<li><p>在插入模式的时候，当前的一个单词输入错了，可以用ctrl + w删去当前的单词，<br>ctrl + u删除到首行。注意这个在shell里也是可以使用的。</p>
<p>这个可以在输入的时候不用退出插入模式来删除单词，当然是针对英文的。</p>
</li>
<li><p>切换到普通模式可以用ctrl + [, 用手指整体移开去按esc。</p>
</li>
<li><p>普通模式下，z + enter把当前行拉到最顶行。</p>
</li>
<li><p>普通模式下，按R进入替换模式，这时直接输入，输入的字符直接覆盖之前的内容。<br>这个功能和word下的替换是一样的。gR是虚拟替换模式，按实际的占位替换。</p>
</li>
<li><p>注意多用.命令，这个命令重复之前得到操作。我们要先定义一个操作的意思。<br>普通模式下，一个操作就是一个操作。进入和esc之间的整个插入模式算一个操作。</p>
</li>
<li><p>录制宏: q[a-z] 开始录制宏，q 停止录制宏，@[a-z] 使用宏。</p>
</li>
<li><p>daw 普通模式下删除当前的单词，delete a word </p>
</li>
<li><p>cw 普通模式下，删除当前位置到单词结尾，change word<br>caw 在光标处于一个单词中间的时候，可以删除这个单词。a是around的意思。</p>
</li>
<li><p>f + char，跳到本行第一个字符; t + char, 光标移动到char的前一个字符</p>
</li>
<li><p>全局替换：%s/xxx/yyy/g<br>s代表替换，%表示1,$，是全局的意思。</p>
</li>
<li><p>转换成大写: 可视状态选要转换的部分，gU</p>
</li>
<li><p>快速移动光标可以使用叫easymotion的vim插件。</p>
<p>在~/.vim/bundle下git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Vhc3ltb3Rpb24vdmltLWVhc3ltb3Rpb24uZ2l0">https://github.com/easymotion/vim-easymotion.git<i class="fa fa-external-link-alt"></i></span><br>在再~/.vim/bundle下git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1Z1bmRsZVZpbS9WdW5kbGUudmltLmdpdA==">https://github.com/VundleVim/Vundle.vim.git<i class="fa fa-external-link-alt"></i></span><br>后者是一个vim插件的管理器，可以用来安装easymotion。之后，按照Vundle.vim里<br>README.md的提示, 把“3. Configure Plugins”的一段配置copy到.vimrc里，把其中<br>的Plugin xxx都删了，改成Plugin ‘easymotion/vim-easymotion’。打开vim run:<br>PluginInstall, 会有easymotion安装提示出来，并显示已经装好了。打开vim run:<br>help easymotion.txt, 可以看到easymotion的help文档。</p>
<p>具体使用的时候在.vimrc里加入一行快捷键的映射，可以是：<br>nmap ss &lt;Plug&gt;(easymotion-s2)  这样的效果是当你按ss的后会进入easymotion的<br>搜索输入，这个时候输入想要调到的地方的两个连续字符，之后整个屏幕凡是有这<br>两个连续字符的地方都会高亮，并且出现一个标记的字符，直接按这个字符就可以把<br>光标跳到对应的位置。</p>
<p>太太强大了！</p>
</li>
<li><p>使用vim画ASCII图</p>
<p>Linux kernel中的文档也有很多包含ASCII图。介绍两个在vim下画ASCII图的工具。<br>使用工具画图, 效率更高。</p>
<ol>
<li><p>boxe </p>
<p>可以插入一些这个软件中自带的图形，效果如下：<br>输入命令：echo “example” | boxes -d dog</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">          __   _,--=&quot;=--,_   __</span><br><span class="line">         /  \.&quot;    .-.    &quot;./  \</span><br><span class="line">        /  ,/  _   : :   _  \/` \</span><br><span class="line">        \  `| /o\  :_:  /o\ |\__/</span><br><span class="line">         `-&#x27;| :=&quot;~` _ `~&quot;=: |</span><br><span class="line">            \`     (_)     `/</span><br><span class="line">     .-&quot;-.   \      |      /   .-&quot;-.</span><br><span class="line">.---&#123;     &#125;--|  /,.-&#x27;-.,\  |--&#123;     &#125;---.</span><br><span class="line"> )  (_)_)_)  \_/`~-===-~`\_/  (_(_(_)  (</span><br><span class="line">(  example                              )</span><br><span class="line"> )                                     (</span><br><span class="line">&#x27;---------------------------------------&#x27;</span><br></pre></td></tr></table></figure>
<p>在ubuntu下直接 sudo apt-get install boxes 安装即可。<br>可以在/etc/boxes/boxes-config中查看其支持的图形。</p>
</li>
<li><p>vim插件drawIt</p>
<p>若手工去画ASCCI图，需要不断的调整。这个插件的功能简单的说，即他能使我们先把<br>字符摆好，然后运用该插件加上线条：</p>
<p>step 1: 输入表格中的字符</p>
<p>  AAAA     BBBB     BBBB      CCCC</p>
<p>  AAAA     BBBB     BBBB      CCCC</p>
<p>  AAAA     BBBB     BBBB      CCCC</p>
<p>step 2: 打开drawIt功能加上表格的框框</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+--------+--------+---------+--------+</span><br><span class="line">|  AAAA  |  BBBB  |  BBBB   |  CCCC  |</span><br><span class="line">+--------+--------+---------+--------+</span><br><span class="line">|  AAAA  |  BBBB  |  BBBB   |  CCCC  |</span><br><span class="line">+--------+--------+---------+--------+</span><br><span class="line">|  AAAA  |  BBBB  |  BBBB   |  CCCC  |</span><br><span class="line">+--------+--------+---------+--------+</span><br></pre></td></tr></table></figure>
<p>具体的安装和使用方法见：<br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY2hpbmF1bml4Lm5ldC91aWQtMjMxMDUyNjEtaWQtMTA5NTA4Lmh0bWw=">http://blog.chinaunix.net/uid-23105261-id-109508.html<i class="fa fa-external-link-alt"></i></span></p>
</li>
</ol>
</li>
<li><p>set cc=80 //第80列是一道红色的线，一般代码在红线以内（只占79列）<br>统一缩进：shift + &gt; or shift + &lt; </p>
</li>
<li><p>ts *** 查看定义, 在行末模式下输入:ts 待查类型/函数, 将得到他的定义</p>
</li>
</ol>
<ol start="19">
<li><p>XX, XXs/a/b/g 第XX行到第XX行中所有的a换成b。<br>**,**s/a/b/gc 可以选择要改变的“a”, 如果有些“a”不想变成“b”, 这个选项还是比较有用的</p>
<p>加注释的方式可以是：<strong>，</strong>s/^////g  或 <strong>，</strong>s/^/#/g</p>
<p>当然加注释的方法还可以，control+v, 向下选中要加的行，I 表示插入，光标会跳回<br>第一行，然后输入//作为注释最后按exit键，这时会发现之前选中的行之前都加上了//注释</p>
</li>
<li><p>代码补全 ctrl+p
 </p>
</li>
<li><p>多个tags文件在.vimrc中用逗号隔开即可, 搜索的时候可以在多个tags中:<br>set tags=/path_1/tags,/path_2/tags</p>
<p>set tags=tags;  注意要加“；”，这个配置可以逐级向上查找tag文件，找见后就set tag。<br>这个对于有多个代码库的情况比较方便，不用在.vimrc里把tag的路径写死，只要把tag<br>文件放在代码的根目录下，不管在哪里打开, 总可以找到tag,并且set tag</p>
</li>
<li><p>跳到函数的开始、结尾：<br>    [[, ]], [], ][<br>    1. 两个符号相同，则跳到函数的开头。[[跳到前一个的开头，]]跳到后一个的开头<br>    2. 两个符号不同，则跳到函数的结尾。[]跳到前一个的结尾，][跳到后一个的结尾</p>
</li>
<li><p>ctrl+d ctrl+u: 上下翻半页。ctrl+e ctrl+y: 光标不动，上下滚屏</p>
</li>
<li><p>ctrl+v块选择, shift+v行选择</p>
</li>
<li><p>set tabstop=8, tab按键一次缩进8个字符的宽度<br>    set softtabtop, linux下最好不要设置，若softtabtop=4, 第一次输入4个空格，第二次<br>    按输入4个空格，然后把8个空格转变成一个tab. kernel中的缩进是8个字符哦！</p>
</li>
<li><p>映射F4，F2，F3按键到对应的插件程序，第一个列出文件中所有的变量名、函数名和宏<br>    第二个列出目录结构，第三个查找光标所在处的字符串，可以改变该字符串的查找路径！</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">noremap &lt;F4&gt; :TlistToggle&lt;CR&gt;</span><br><span class="line">noremap &lt;F2&gt; :NERDTreeToggle&lt;CR&gt;</span><br><span class="line">nnoremap &lt;silent&gt; &lt;F3&gt; :Grep&lt;CR&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>set spell可以为你检查文件中有没有拼错的词，但貌似只支持英文. </p>
</li>
<li><p>jump between two files or two functions<br>    向前跳到前几次光标的位置：ctrl + i<br>    向后跳到后几次光标的位置：ctrl + o 这样可以在函数定义和调用处来回跳动</p>
</li>
<li><p>尽量在vim内完成所有操作，包括:make, vimgrep(缩写成vim)<br>vimgrep的格式是：vim /search_patten/ **/*.[ch] **是当前目录以及以下目录的通配符，<br>*是当前目录的通配符。vimgrep搜索得到的各个条目会保存到一个叫quickfix的表里，<br>这个是vim的一个基本的功能，copen可以开一个窗口，然后在新开窗口中打开quickfix表，<br>cclose关闭打开的quickfix表。</p>
</li>
<li><p>tj命名，tags jump 符号，可以搜索一个符号的位置，并且跳过去。stj split一个新<br>的窗口显示。有很多t开头的命令，最常用的要数ts了，tags select。</p>
</li>
<li><p>ab命令，用一个缩写来代替一组字符。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>一个优先级队列的实现</title>
    <url>/%E4%B8%80%E4%B8%AA%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="优先级队列概念"><a href="#优先级队列概念" class="headerlink" title="优先级队列概念"></a>优先级队列概念</h2><p> 从用户的角度看，优先级队列支持，队列元素出队列时，总是输出当前队列中的最大或者最小值。</p>
<p> 我们要实现的接口有：</p>
<ul>
<li><p>创建队列的接口</p>
</li>
<li><p>销毁队列的接口</p>
</li>
<li><p>把一个元素入队列的接口</p>
</li>
<li><p>输出队列中的最大或者是最小值的接口</p>
</li>
</ul>
<p> 优先级队列的实现，一般采用大顶堆或者小顶堆的数据结构。我们以大顶堆为例说明问题。<br> 一个大顶堆是一个完全二叉树，这个二叉树的每个节点的子节点都比父节点小。</p>
<p> 我们只要搞清入队和出队的逻辑，就搞清了全部逻辑。先看入队的逻辑。我们面对的问题是<br> 现在队列已经是一个大顶堆了，如何调整数据结构，使得入队后的队列还是一个大顶堆。<br> 我们把要入队的元素放在这个完全二叉树的最末尾，入队的元素如果比他的父节点小，那么<br> 现在就是一个大顶堆，如果入队元素比父节点大，那么交换入队元素和父节点，在入队元素<br> 和父节点组成的子树中，满足大顶堆的逻辑，因为入队元素如果比父节点大，那么一定大于<br> 可能的父节点的左节点。交换后的整个树，只可能在父节点的位置(数值已经是入队元素)和<br> 父节点的父节点处不满足大顶堆的逻辑，如果不满足，也就是父节点比他的父节点还大，迭代<br> 一开始的逻辑即可。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    B             B             A</span><br><span class="line">   / \           / \           / \</span><br><span class="line">  C   O         A             B</span><br><span class="line"> / \           / \           / \</span><br><span class="line">O   A         O   C         O   C</span><br><span class="line"></span><br><span class="line">     if A &gt; C        if A &gt; B</span><br><span class="line">     -------&gt;        -------&gt;</span><br></pre></td></tr></table></figure>

<p> 如上，入队后，我们就立马把队列调整成大顶堆逻辑。出队的时候，直接取根节点就是最大值。<br> 但是，根节点输出后，我们还要把这个队列调整成大顶堆逻辑。一般，我们把整个树最末尾<br> 的节点填到根节点的位置，这个时候，一般整个树就不是大顶堆的结构。我们把根节点和他<br> 比较大的一个子节点做交换，这样保证交换后的根节点和他的子节点满足大顶堆逻辑，可能<br> 不满足大顶堆逻辑的地方是，交换数值的子节点位置和他的子节点们，同样的逻辑做迭代即可。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    A             C             B</span><br><span class="line">   / \           / \           / \</span><br><span class="line">  B   D         B   D         C   D</span><br><span class="line"> / \           / \           / \</span><br><span class="line">O   C         O             O</span><br><span class="line"></span><br><span class="line"> A出队，C换到堆顶     if C &lt; B 或者 C &lt; D, 且 B &gt; D</span><br><span class="line"> ---------------&gt;     ----------------------------&gt; </span><br></pre></td></tr></table></figure>

<p> 注意如上的分析都是逻辑层面的分析。</p>
<h2 id="C语言实现"><a href="#C语言实现" class="headerlink" title="C语言实现"></a>C语言实现</h2><p> 实现上要注意的地方有：</p>
<ul>
<li><p>可以用数组实现树，那么求一个节点的左节点、右节点和父节点可以是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define get_left(i)		(2 * (i) + 1)</span><br><span class="line">#define get_right(i)		(2 * (i) + 2)</span><br><span class="line">#define get_father(i)		((i) % 2 ? (i) / 2 : (i) / 2 - 1)</span><br></pre></td></tr></table></figure></li>
<li><p>树的元素可以用void *来做，这样可以支持不同的数据结构元素。</p>
</li>
<li><p>可以要求用户提供比较函数，一个是因为树的元素是void *，内部实现自己并不知道怎么比较，<br>另一个是因为，用户可以灵活的改变比较函数，按照不同的逻辑定义元素大小的规则。</p>
</li>
</ul>
<p> 具体实现的代码在：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL21hc3Rlci9wcmlvcml0eV9x">https://github.com/wangzhou/tests/master/priority_q<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>一个弹球小游戏</title>
    <url>/%E4%B8%80%E4%B8%AA%E5%BC%B9%E7%90%83%E5%B0%8F%E6%B8%B8%E6%88%8F/</url>
    <content><![CDATA[<p>这个小游戏来自Unix/Linu编程时间教程(Understanding Unix/Linux Programming: A<br>Guide to Theory and Practice)。编译调试环境为ubuntu 14.04.2, 需要安装ncurses库。</p>
<p>下载代码后，运行：make生成可执行文件pong, ./pong 即可运行。</p>
<p>git库地址在：<br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3BvbmcuZ2l0">https://github.com/wangzhou/pong.git<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>Linux用户态编程</tag>
      </tags>
  </entry>
  <entry>
    <title>uthash和glib hash</title>
    <url>/uthash%E5%92%8Cglib-hash/</url>
    <content><![CDATA[<h2 id="hash"><a href="#hash" class="headerlink" title="hash"></a>hash</h2><p> 哈希表是一种这样的数据结构，它以key-value的形式把value存储到哈希表里。用户可以<br> 通过一组接口做增删改查的操作。</p>
<h2 id="uthash"><a href="#uthash" class="headerlink" title="uthash"></a>uthash</h2><p> uthash的介绍在这里: <span class="exturl" data-url="aHR0cHM6Ly90cm95ZGhhbnNvbi5naXRodWIuaW8vdXRoYXNoLw==">https://troydhanson.github.io/uthash/<i class="fa fa-external-link-alt"></i></span><br> 它是一个用宏写的哈希表，使用的时候只要include uthash.h就好，所有信息都在这个文件<br> 里了。uthash的代码里附带了一个example.c的使用示例，我们简单看下这个文件，主要是<br> 注意它使用时候的一些坑。目前还没有发现在哪里有使用uthash。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;string.h&gt;</span><br><span class="line">#include &lt;assert.h&gt;</span><br><span class="line">#include &quot;./uthash.h&quot;</span><br><span class="line"></span><br><span class="line">struct my_struct &#123;</span><br><span class="line">    int id;                    /* 这个id后面我们用来做my_struct的索引 */</span><br><span class="line">    char name[10];</span><br><span class="line">    UT_hash_handle hh;         /* 要hash的数据结构里必须放一个这个句柄，必须写成hh */</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">/* 定义hash表的表头，需要初始化成NULL */</span><br><span class="line">struct my_struct *users = NULL;</span><br><span class="line"></span><br><span class="line">/* 所有的uthash操作都是HASH_xx的定义，我们这里封装一层函数 */</span><br><span class="line">void add_user(int user_id, char *name)</span><br><span class="line">&#123;</span><br><span class="line">    struct my_struct *s;</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * 查看user_id为key的数据是否存在，返回数据的指针，s为NULL，数据不存在。</span><br><span class="line">     * 注意，user_id就是key的值，这个接口实现的比较特别，单通过这个接口内部的</span><br><span class="line">     * 实现根本不知道内部的那个数据是key。</span><br><span class="line">     *</span><br><span class="line">     * 这个接口要配合下面的HASH_ADD_INT来用。这个接口的语义是：</span><br><span class="line">     * 用s里的id为key插入s到users，这里这个id表示的是struct my_struct里的id这个</span><br><span class="line">     * 参数名字，所以一定要写的和struct my_struct里的id一样，本质上是一个字符。</span><br><span class="line">     *</span><br><span class="line">     * HASH_FIND_INT也是根据HASH_ADD_INT里的id知道key是s里的哪个元素。</span><br><span class="line">     */</span><br><span class="line">    HASH_FIND_INT(users, &amp;user_id, s);</span><br><span class="line">    if (s == NULL) &#123;</span><br><span class="line">        s = (struct my_struct*)malloc(sizeof(struct my_struct));</span><br><span class="line">        s-&gt;id = user_id;</span><br><span class="line">        HASH_ADD_INT(users, id, s);  /* 用s里的id为key插入s到users */</span><br><span class="line">    &#125;</span><br><span class="line">    strcpy(s-&gt;name, name);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">struct my_struct *find_user(int user_id)</span><br><span class="line">&#123;</span><br><span class="line">    struct my_struct *s;</span><br><span class="line"></span><br><span class="line">    HASH_FIND_INT(users, &amp;user_id, s);</span><br><span class="line">    return s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void delete_user(struct my_struct *user)</span><br><span class="line">&#123;</span><br><span class="line">    HASH_DEL(users, user);  /* 直接指向数据的指针, 用这个作为删除的标记 */</span><br><span class="line">    free(user);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void delete_all()</span><br><span class="line">&#123;</span><br><span class="line">    struct my_struct *current_user, *tmp;</span><br><span class="line"></span><br><span class="line">    /* 遍历哈希表中的每个元素 */</span><br><span class="line">    HASH_ITER(hh, users, current_user, tmp) &#123;</span><br><span class="line">        HASH_DEL(users, current_user);</span><br><span class="line">        free(current_user);  /* 删除操作并不影响数据内存，需要用户显示释放数据内存 */</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void print_users()</span><br><span class="line">&#123;</span><br><span class="line">    struct my_struct *s;</span><br><span class="line"></span><br><span class="line">    /* 也可以用这种直白的方式遍历哈希表，但是这个相当于知道了hh的内部数据，最好不要这样 */</span><br><span class="line">    for(s=users; s != NULL; s=(struct my_struct*)(s-&gt;hh.next)) &#123;</span><br><span class="line">        printf(&quot;user id %d: name %s\n&quot;, s-&gt;id, s-&gt;name);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int name_sort(struct my_struct *a, struct my_struct *b)</span><br><span class="line">&#123;</span><br><span class="line">    return strcmp(a-&gt;name, b-&gt;name);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int id_sort(struct my_struct *a, struct my_struct *b)</span><br><span class="line">&#123;</span><br><span class="line">    return (a-&gt;id - b-&gt;id);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void sort_by_name()</span><br><span class="line">&#123;</span><br><span class="line">    /* 还支持对哈希表里数据排序 */</span><br><span class="line">    HASH_SORT(users, name_sort);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void sort_by_id()</span><br><span class="line">&#123;</span><br><span class="line">    HASH_SORT(users, id_sort);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	struct my_struct *tmp;</span><br><span class="line">	int num;</span><br><span class="line"></span><br><span class="line">	add_user(5, &quot;wang&quot;);</span><br><span class="line">	add_user(1, &quot;zheng&quot;);</span><br><span class="line">	add_user(4, &quot;xu&quot;);</span><br><span class="line">	add_user(3, &quot;fang&quot;);</span><br><span class="line">	add_user(2, &quot;huang&quot;);</span><br><span class="line"></span><br><span class="line">	print_users();</span><br><span class="line"></span><br><span class="line">	printf(&quot;\n&quot;);</span><br><span class="line">	sort_by_id();</span><br><span class="line">	print_users();</span><br><span class="line">	/*</span><br><span class="line">	 * sort_by_id()打印出来的结果是这样的：</span><br><span class="line">	 *</span><br><span class="line">	 * user id 1: name zheng</span><br><span class="line">	 * user id 2: name huang</span><br><span class="line">	 * user id 3: name fang</span><br><span class="line">	 * user id 4: name xu</span><br><span class="line">	 * user id 5: name wang</span><br><span class="line">	 */</span><br><span class="line"></span><br><span class="line">	printf(&quot;\n&quot;);</span><br><span class="line">	sort_by_name();</span><br><span class="line">	print_users();</span><br><span class="line">	/*</span><br><span class="line">	 * sort_by_name()打印出来的结果是这样的：</span><br><span class="line">	 *</span><br><span class="line">	 * user id 3: name fang</span><br><span class="line">	 * user id 2: name huang</span><br><span class="line">	 * user id 5: name wang</span><br><span class="line">	 * user id 4: name xu</span><br><span class="line">	 * user id 1: name zheng</span><br><span class="line">	 */</span><br><span class="line"></span><br><span class="line">	printf(&quot;\n&quot;);</span><br><span class="line">	tmp = find_user(4);</span><br><span class="line">	/*</span><br><span class="line">	 * 非常别扭的删除接口，尽然不能用key作为索引直接调删除接口，还要先用key找见元素</span><br><span class="line">	 * 的指针，然后再删除。</span><br><span class="line">	 */</span><br><span class="line">	delete_user(tmp);</span><br><span class="line">	print_users();</span><br><span class="line"></span><br><span class="line">	printf(&quot;\n&quot;);</span><br><span class="line">	/* 统计哈希表里有多少个元素 */</span><br><span class="line">	num = HASH_COUNT(users);</span><br><span class="line">	printf(&quot;there is %d elements\n&quot;, num);</span><br><span class="line"></span><br><span class="line">	printf(&quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">	/* 释放哈希表 */</span><br><span class="line">	HASH_CLEAR(hh, users);</span><br><span class="line">	assert(!users);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="glib-hash"><a href="#glib-hash" class="headerlink" title="glib hash"></a>glib hash</h2><p>  glib是GNOME lib, 这个库提供了各种基本的数据结构，使用比较广泛。我们这里主要看<br>  glib中提供的哈希表相关接口的使用。QEMU的代码使用了glib，我们这里的介绍也截取<br>  了部分QEMU里和哈希表有关的东西。</p>
<p>  下面的测试在ubuntu20.04(aarch64)上，安装glib库和编译测试代码的命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libglib2.0-dev</span><br><span class="line">gcc test.c `pkg-config --cflags --libs glib-2.0`</span><br></pre></td></tr></table></figure>
<p>  下面把介绍直接写到代码的注释说明里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;glib.h&gt;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdint.h&gt;</span><br><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">#include &lt;malloc.h&gt;</span><br><span class="line"></span><br><span class="line">/* copy from qemu code */</span><br><span class="line">#define PRIME32_1   2654435761U</span><br><span class="line">#define PRIME32_2   2246822519U</span><br><span class="line">#define PRIME32_3   3266489917U</span><br><span class="line">#define PRIME32_4    668265263U</span><br><span class="line">#define PRIME32_5    374761393U</span><br><span class="line">#define QEMU_XXHASH_SEED 1</span><br><span class="line"></span><br><span class="line">static inline uint32_t rol32(uint32_t word, unsigned int shift)</span><br><span class="line">&#123;</span><br><span class="line">    return (word &lt;&lt; shift) | (word &gt;&gt; ((32 - shift) &amp; 31));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static inline uint32_t</span><br><span class="line">qemu_xxhash7(uint64_t ab, uint64_t cd, uint32_t e, uint32_t f, uint32_t g)</span><br><span class="line">&#123;</span><br><span class="line">    uint32_t v1 = QEMU_XXHASH_SEED + PRIME32_1 + PRIME32_2;</span><br><span class="line">    uint32_t v2 = QEMU_XXHASH_SEED + PRIME32_2;</span><br><span class="line">    uint32_t v3 = QEMU_XXHASH_SEED + 0;</span><br><span class="line">    uint32_t v4 = QEMU_XXHASH_SEED - PRIME32_1;</span><br><span class="line">    uint32_t a = ab;</span><br><span class="line">    uint32_t b = ab &gt;&gt; 32;</span><br><span class="line">    uint32_t c = cd;</span><br><span class="line">    uint32_t d = cd &gt;&gt; 32;</span><br><span class="line">    uint32_t h32;</span><br><span class="line"></span><br><span class="line">    v1 += a * PRIME32_2;</span><br><span class="line">    v1 = rol32(v1, 13);</span><br><span class="line">    v1 *= PRIME32_1;</span><br><span class="line"></span><br><span class="line">    v2 += b * PRIME32_2;</span><br><span class="line">    v2 = rol32(v2, 13);</span><br><span class="line">    v2 *= PRIME32_1;</span><br><span class="line"></span><br><span class="line">    v3 += c * PRIME32_2;</span><br><span class="line">    v3 = rol32(v3, 13);</span><br><span class="line">    v3 *= PRIME32_1;</span><br><span class="line"></span><br><span class="line">    v4 += d * PRIME32_2;</span><br><span class="line">    v4 = rol32(v4, 13);</span><br><span class="line">    v4 *= PRIME32_1;</span><br><span class="line"></span><br><span class="line">    h32 = rol32(v1, 1) + rol32(v2, 7) + rol32(v3, 12) + rol32(v4, 18);</span><br><span class="line">    h32 += 28;</span><br><span class="line"></span><br><span class="line">    h32 += e * PRIME32_3;</span><br><span class="line">    h32  = rol32(h32, 17) * PRIME32_4;</span><br><span class="line"></span><br><span class="line">    h32 += f * PRIME32_3;</span><br><span class="line">    h32  = rol32(h32, 17) * PRIME32_4;</span><br><span class="line"></span><br><span class="line">    h32 += g * PRIME32_3;</span><br><span class="line">    h32  = rol32(h32, 17) * PRIME32_4;</span><br><span class="line"></span><br><span class="line">    h32 ^= h32 &gt;&gt; 15;</span><br><span class="line">    h32 *= PRIME32_2;</span><br><span class="line">    h32 ^= h32 &gt;&gt; 13;</span><br><span class="line">    h32 *= PRIME32_3;</span><br><span class="line">    h32 ^= h32 &gt;&gt; 16;</span><br><span class="line"></span><br><span class="line">    return h32;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">typedef struct key &#123;</span><br><span class="line">    int bus;</span><br><span class="line">    int devfn;</span><br><span class="line">&#125; key;</span><br><span class="line"></span><br><span class="line">typedef struct value &#123;</span><br><span class="line">    int base;</span><br><span class="line">    int bar;</span><br><span class="line">&#125; value;</span><br><span class="line"></span><br><span class="line">static guint key_hash(gconstpointer v)</span><br><span class="line">&#123;</span><br><span class="line">    key *k = (key *)v;</span><br><span class="line"></span><br><span class="line">    return qemu_xxhash7((uint64_t)k-&gt;bus, k-&gt;devfn, 0, 0, 0);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static gboolean key_equal(gconstpointer v1, gconstpointer v2)</span><br><span class="line">&#123;</span><br><span class="line">    key *k1 = (key *)v1;</span><br><span class="line">    key *k2 = (key *)v2;</span><br><span class="line"></span><br><span class="line">    return (k1-&gt;bus == k2-&gt;bus) &amp;&amp; (k1-&gt;devfn == k2-&gt;devfn);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	/* 表示一个哈希表的具柄 */</span><br><span class="line">    	GHashTable *hash_table;</span><br><span class="line">	/* 哈希表的key是可以自定义的, 可以把用到的参数封装到一个struct里，把这个struct作为key */</span><br><span class="line">	value v, *p_v;</span><br><span class="line">	key k, *p_k;</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * 初始化哈希表具柄, 函数的定义在gnome的官网都可以查询：</span><br><span class="line">	 * https://developer.gnome.org/glib/stable/glib-Hash-Tables.html</span><br><span class="line">	 *</span><br><span class="line">	 * ghash是要建立一个key struct到一个value struct的映射，所以下面的key_hash,</span><br><span class="line">	 * key_equal这两个函数就比较容易理解。</span><br><span class="line">	 *</span><br><span class="line">	 * key_hash的输入是用户自定义的key struct，输出是一个hash值，ghash真正用</span><br><span class="line">	 * 计算得到的这个hash值作为key。可以看到这个计算是一个数学问题，QEMU里直接</span><br><span class="line">	 * 把Jenkins hash, xxhash的代码放到了QEMU的代码里，我们这里也照搬过来，</span><br><span class="line">	 * 上面直接copy的是xxhash的部分代码，他完成的功能比较直白，就是输入一组</span><br><span class="line">	 * 数，然后按照一定的算法输出一个哈希值。这里的key_hash就是直接调用xxhash</span><br><span class="line">	 * 的函数。</span><br><span class="line">	 *</span><br><span class="line">	 * key_equal是判断两个key相等的函数，一般就是key里面的每一个元素都相等就</span><br><span class="line">	 * 认为两个key相等。</span><br><span class="line">	 *</span><br><span class="line">	 * 后面的两个函数是key和value的销毁函数，一般是g_free。注意，如果不是动态</span><br><span class="line">	 * 创建的结构就不需要配置这里的销毁函数。</span><br><span class="line">	 */</span><br><span class="line">    	hash_table = g_hash_table_new_full(key_hash, key_equal, g_free, g_free);</span><br><span class="line"></span><br><span class="line">	p_v = malloc(sizeof(v)); p_v-&gt;base = 1; p_v-&gt;bar = 3;</span><br><span class="line">	p_k = malloc(sizeof(k)); p_k-&gt;bus = 0x10; p_k-&gt;devfn = 0x75;</span><br><span class="line"></span><br><span class="line">	/* 把一个key-value的map插入到哈希表里，如上，key, value这里是需要动态分配的结构 */</span><br><span class="line">	g_hash_table_insert(hash_table, p_k, p_v);</span><br><span class="line"></span><br><span class="line">	k.bus = 0x10; k.devfn = 0x75;</span><br><span class="line">	/* 找一个key对应的value, 这时key可以是静态的结构 */</span><br><span class="line">	p_v = g_hash_table_lookup(hash_table, &amp;k);</span><br><span class="line">	if (p_v)</span><br><span class="line">		printf(&quot;found!\n&quot;);</span><br><span class="line">	else</span><br><span class="line">		printf(&quot;not found!\n&quot;);</span><br><span class="line"></span><br><span class="line">	/* 销毁哈希表 */</span><br><span class="line">	g_hash_table_remove_all(hash_table);</span><br><span class="line">	</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  简单起见也可以用glib提供的key_hash和key_equal函数。比如，如果用一个int值作为key,<br>  就可以用g_direct_hash/g_direct_equal:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">h = g_hash_table_new(g_direct_hash, g_direct_equal);</span><br><span class="line">g_hash_table_lookup(h, GINT_TO_POINTER(int_key));</span><br><span class="line">g_hash_table_insert(h, GINT_TO_POINTER(int_key), value);</span><br></pre></td></tr></table></figure>
<p>  如上direct的方式是直接用key为形参做索引的。g_direct_hash的实现是把输入强转成int<br>  作为key。但是使用g_int_hash/g_int_equal时，q_int_hash把输入转成int指针然后取其中<br>  的内容，可见这个时候lookup/insert的输入应该是int型key的地址。</p>
]]></content>
      <tags>
        <tag>Linux用户态编程</tag>
        <tag>glib</tag>
        <tag>hash</tag>
      </tags>
  </entry>
  <entry>
    <title>一个服务器的配置过程</title>
    <url>/%E4%B8%80%E4%B8%AA%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h2><p>服务器相关参数：HUAWEI RH2285 server, 300G x 12, memory:4G x 12,<br>LSI SAS 1068E, ip: ***, gateway: ***, netmask: ***,<br>DNS1: ***, DNS2: ***</p>
<p>配置一个开发用的服务器需要考虑的有：</p>
<ol>
<li>硬盘怎么分区, 2) RAID怎么设置，3) 网络怎么配置，4) 装什么软件, 5) 每个用<br>户的磁盘容量限制. 下面逐个说明</li>
</ol>
<h2 id="配置步骤"><a href="#配置步骤" class="headerlink" title="配置步骤"></a>配置步骤</h2><p>(1) 磁盘分区：</p>
<p>服务器原有12块硬盘，每个300G。一般安装系统的分区要做RAID1，故用硬盘0，1<br>做一个RAID1, 容量300G, 作系统分区等；硬盘2-11做一个RAID1E, 容量1.5T,<br>挂载在/home上。在安装操作系统的时候看到的是sda和sdb两个硬盘，这里把300G<br>的划分成和系统相关的分区，把1.5T独立作为一个分区挂载在/home上。具体分区如下：</p>
<p>|—&gt;/home 1.5T<br>|<br>|—&gt;/usr 40G<br>|<br>|—&gt;/var 40G<br>|<br>|—&gt;/ 10G<br>|<br>|—&gt;/tmp 5G<br>|<br>|—&gt;/boot 1G<br>|<br>|—&gt;none 200G</p>
<p>分区的思路为：/boot存放系统启动的内容，包括内核压缩文件和grub; /usr为系统<br>以后安装软件的目录，所以可分的大些； /var为系统存放日志文件<br>的目录且重启之后不会删除，所以也要大点；/tmp为系统存放临时<br>文件的地方并且要是挂载tmpfs文件系统后，/tmp的内容会存放在<br>内存中，所以不需要太大；根文件系统的其他部分以后不会有大的<br>变动，所以都放在/下</p>
<p>(2) RAID设置</p>
<p>本服务器支持了硬RAID，采用的是LSI的RAID卡1068E, 配置过程如下:</p>
<ol>
<li><p>开机, 一直等到initial界面，按contrl+c进入硬RAID配置界面。<br>华为RH2285服务器采用的是LSI 1068ＥRAID卡，可以配置1-2个RAID0，1-2个<br>RAID1，1-2个RAID1E</p>
</li>
<li><p>RAID设置是：硬盘0，1做一个RAID1，容量300G，用于除了home的分区；<br>硬盘2-11做一个RAID1E，容量1.5T，只用于home分区。</p>
</li>
<li><p>做RAID之前首先要删除之前的RAID:<br>C1068E—&gt;RAID Properties—&gt;Manage Array—&gt;Delete Array</p>
</li>
<li><p>做RAID1：<br>C1068E—&gt;RAID Properties—&gt;Create IM Volume—&gt;选择硬盘0、1，按c创<br>建RAID1</p>
</li>
<li><p>做RAID1E：<br>C1068E—&gt;RAID Properties—&gt;Create IME Volume—&gt;选择硬盘2-11，按c创<br>建RAID1E。系统的12块硬盘做成的一个RAID1和一个RAID1E，安装操作系统时将<br>只会看到sda和sdb两个硬盘</p>
</li>
</ol>
<p>(3) 网络配置</p>
<p>服务器的ip要配置成静态ip, 需要了解配置的有：ip, DNS, 网关, 一般在文件<br>/etc/network/interfaces, /etc/resolv.conf中配置</p>
<p>(4) 软件安装</p>
<p>配置好网络后，使用sudo apt-get install安装(这里安装的系统是ubuntu)需要的<br>软件。这里安装gcc, qemu, make, git, vim, gdb, ctags, cscope, openjdk, ftp,<br>tftp, scp, perf用户态程序等开发工具</p>
<p>windows和linux通信需要samba服务器, 其配置过程如下:</p>
<p>samba服务器要配置成为：<br>a. 用户名和大服务器用户名相同<br>b. 各用户分别共享自己的home目录，用户之间互不可见<br>c. /home/share向每个团队成员共享，对非团队成员不可见</p>
<ol>
<li><p>samba服务器的配置文件在/etc/samba/下，有smb.conf, smbpasswd等。<br>samba服务器可以设定不同的安全等级，如share级别无需密码就可以访问，这<br>里采用user级别，访问时需要用户名和密码。samba服务器的可用三种方式鉴<br>权：smbpasswd, tdbsam, ldapsam, 这里采用smbpasswd</p>
</li>
<li><p>sudo vi /etc/samba/smb.conf<br>设定安全级别：security = user<br>设定鉴权方式：passdb backend = smbpasswd<br>设定存放密码的文件：<br>smb passwd file = /etc/samba/smbpasswd<br>若是没有smbpasswd, 手动建一个, 以后创建的samba用户的密码会保存在其中<br>添加用户并设定初始密码：sudo smbpasswd -a username</p>
</li>
<li><p>建立共享的share目录：mkdir /home/share<br>在smb.conf中增加配置：<br>[share]<br>path = /home/share<br>writable = yes<br>broweable = yes<br>available = yes<br>valid users = @user<br>这里之前把需要有共享权限的用户加入了新建的用户组user, 也可以把用户名<br>用逗号隔开加入。加入组中的用户在windows—&gt;“运行”中输入//server-ip/<br>然后输入用户名和密码即可登陆共享目录</p>
</li>
<li><p>建立自己共享的目录：<br>在smb.conf的[global]段中加入：config file = /etc/samba/smb.conf.%U<br>这表示到对应的子配置文件去找配置项。<br>拷贝一份配置文件：cp smb.conf smb.conf.username1, 在其中加入：<br>[username1]<br>path = /home/username1<br>writable = yes<br>broweable = no<br>valid users = username1<br>其他用用户都这样配置。username1用户在windows—&gt;“运行”中输入//server-<br>ip/username1, 然后输入用户名和密码即可登陆/home/username1</p>
</li>
<li><p>重启samba服务器:<br>sudo service smbd restart<br>sudo service nmbd restart</p>
</li>
</ol>
<p>注：连接samba服务器，第一次输入密码后，再次连接不需要输入密码。这是因为<br>关闭窗口后连接并没有真正断开，可以在dos命令行下用net use查看相应的<br>连接，再用net usr *** /del 关闭该连接</p>
<p>参考：<br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY2hpbmF1bml4Lm5ldC91aWQtMjA1MzcwODQtaWQtMjk3Nzg1MC5odG1s">http://blog.chinaunix.net/uid-20537084-id-2977850.html<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jicy5jaGluYXVuaXgubmV0L3RocmVhZC05MjIwMjktMS0xLmh0bWw=">http://bbs.chinaunix.net/thread-922029-1-1.html<i class="fa fa-external-link-alt"></i></span></p>
<p>(5) quota配置</p>
<p>quota可以限制每个用户磁盘的使用量。这里总磁盘可用容量为1.3T，设计使用人<br>数为20，每个用户的home目录的限额为: 60G(soft), 65G(hard), 宽限时间为14天。<br>具体注意见:</p>
<p>配置参照<span class="exturl" data-url="aHR0cDovL2xpbnV4LnZiaXJkLm9yZy9saW51eF9iYXNpYy8wNDIwcXVvdGEucGhw">http://linux.vbird.org/linux_basic/0420quota.php<i class="fa fa-external-link-alt"></i></span> 即可。需注意：</p>
<ol>
<li><p>查看内核是否支持quota:<br>CONFIG_QUOTA = y<br>CONFIG_XFS_QUOTA = y</p>
</li>
<li><p>umount /home的时候可能出现device busy的情况。解决办法是:<br>以root进入系统，使用lsof | grep /home 找到相应的进程，杀死相应的进程<br>再执行 umoung /home</p>
</li>
<li><p>可能系统中没有quotacheck命令，sudo apt-get install quota 安装即可</p>
</li>
</ol>
<p>(6) 安装系统中需注意：</p>
<ol>
<li><p>安装系统前查一下服务器的系统兼容性，ubuntu-server-12.04在RH2285上会<br>安装出错，上面安装的是untuntu-server-10.04</p>
</li>
<li><p>安装grub的时候应该选择RAID1(300G)中对应的分区，这里选择/dev/sdb1，挂<br>载的目录是/boot</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>一种硬件队列的驱动设计</title>
    <url>/%E4%B8%80%E7%A7%8D%E7%A1%AC%E4%BB%B6%E9%98%9F%E5%88%97%E7%9A%84%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/</url>
    <content><![CDATA[<p>首先描述这个硬件的队列模型：</p>
<p>他有N个发送队列(sq)，N个完成队列(cq), 这N个sq和cq一一对应组成N个队列对(qp)。这<br>N个qp对应一个事件队列(eq)。</p>
<p>用户可以依次在sq里填入请求，然后发通知(doorbell)给硬件，要求硬件执行这个请求。<br>可以执行请求的部件我们把他独立开来(core)。core执行完请求后会依次在cq里填入请求<br>完成(描述符)。为了异步的通知给用户，当cq里有请求完成描述符的时候，并且没有屏蔽<br>cq写eq的通道的时候，cq会依次在eq里填入一个事件完成描述符(eqe)。对于eq，在eq里有事件<br>完成描述符，并且没有屏蔽中断的时候，向CPU报一个中断。</p>
<p>软件可以操作doorbell更新sq尾指针，更新cq头指针，更新eq头指针。其他硬件指针硬件<br>自己会更新(比如，core在取走一个sq请求的时候更新sq头指针，cq、eq在被写入的时候，<br>硬件更新cq tail和eq tail)。</p>
<p>cq在满足条件向eq填一个eqe的同时自动屏蔽cq写eq的通道。软件可以再下发一个特定的<br>doorbell打开这个通道。</p>
<p>eq满足条件上报中断的同时自动屏蔽中断上报。软件下发doorbell更新eq head时，除了<br>更新硬件eq head，还同时打开了中断屏蔽。</p>
<p>整体的硬件框图类似是这样的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">              cq/sq/eq doorbell</span><br><span class="line">          |       |                    |       ^ irq</span><br><span class="line">+---------+-------+--------------------+-------+----------------------+</span><br><span class="line">|         |       |                    |       |                      |</span><br><span class="line">|         |       v                    |       |            		|</span><br><span class="line">|   +---+-+-+---+---+----              |       |                      |</span><br><span class="line">|   |sqe|s|e|sqe|sqe|                  |       |                      |</span><br><span class="line">|   +---+-+-+---+---+----              |       |                      |</span><br><span class="line">|   +---+-v-+---+---+-\--              |       |                      |</span><br><span class="line">|   |cqe|cqe|cqe|cqe|  \  \            |       |                      |</span><br><span class="line">|   +---+---+---+---+---\  \           |       |                      |</span><br><span class="line">|         ^       ^ ^    \  \          |       |                      |</span><br><span class="line">|         |       |  \    \  \         |       |                      |</span><br><span class="line">|        head    tail \    \  \        |       |                      |</span><br><span class="line">|                      \    \  v       v       |                      |</span><br><span class="line">|                       \    \   +---+---+---+---+-----               |</span><br><span class="line">|   N个                  \    \  |eqe|eqe|eqe|eqe|...                 |</span><br><span class="line">|                         \    X +---+---+---+---+-----               |</span><br><span class="line">|                          \  / \      ^       ^                      |</span><br><span class="line">|   +---+---+---+---+----   \/   \     |       |                      |</span><br><span class="line">|   |sqe|sqe|sqe|sqe|       /\    \   head    tail                    |  </span><br><span class="line">|   +---+---+---+---+----  /  \    \                                  |</span><br><span class="line">|   +---+---+---+---+---- /    \    \                                 |</span><br><span class="line">|   |cqe|cqe|cqe|cqe|    /      \    \                                |</span><br><span class="line">|   +---+---+---+---+----        \    \                               |</span><br><span class="line">|                                 \    v                              |</span><br><span class="line">|                                  \+--------+                        |</span><br><span class="line">|                                   |        |                        |</span><br><span class="line">|                                   | core   |                        |</span><br><span class="line">|                                   |        |                        |</span><br><span class="line">|                                   +--------+                        |</span><br><span class="line">|  cq/sq depth = M   eq depth = K                                     |</span><br><span class="line">+---------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>对于sq，软件需要维护软件sq_head, sq_tail。保证发到sq的请求不会导致sq溢出。<br>对于cq，其input来自core, output来自中断处理程序中更新cq head。<br>对于eq，其input来自cq，output来自中断处理程序中更新eq head。</p>
<p>软件设计的重点在于，在保证cq，eq不发生溢出的情况下，做到高性能。我们先找到保证<br>cq、eq不发生溢出的条件。然后，提高中断的汇聚程度提高性能(同时不破坏之前的约束<br>条件)。</p>
<p>cq不溢出的条件是(考虑中断很久才到，这时候sq, cq已被填满, 注意因为上面我们已经<br>限制sq不溢出，所以这里即使没有中断更新cq head，cq也不会溢出)。中断中我们通过<br>eqe中的索引找到对应的cq进行处理，必须先跟新cq head，才能更新软件的sq head。否则，<br>如果先更新sq head，下发的新sq请求可能通过core产生cqe使得cq溢出。<br>(注意这里没有考虑cqe汇聚处理)</p>
<p>eq不溢出的条件是，考虑所有cq并发向eq写eqe，中断到来在写满eq时刻之后的情况，得出<br>eq深度至少要等于并发cq数N。假设eq已被写满，现在在中断处理函数中遍历各个cq处理，<br>并且处理完后打开cq写eq通道，并且N个cq上都有排队cqe(意味着会写N个新eqe给eq)。如果<br>eq的队列深度不增加(依然是N)，那么保证eq不溢出的条件是，irq中断中处理一个eq head<br>(相当于在eq里留出一个空位)，才能在处理一个cq的时候放开放cq写eq的通路(放开通路就<br>意味着可能有cq向eq写入一个eqe)。当然也可以处理n个eqe，放开&lt;=n个cq到eq的通路。</p>
<p>上面已经明确不溢出的条件，那么，下来就是看优化了。按照上面对eq的处理，需要在中断<br>中对每个eqe做eq head的更新，注意上面的硬件描述里提到，每次更新eq head都会打开<br>中断屏蔽，这样每个eq都会上报一个中断，性能会差。我们考虑把多个完成的eqe做一下汇聚，<br>然后更新eq head。注意, 在实际实现时，我们的处理大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">while (轮询已完成n个eqe) &#123;  /* 这里最大汇聚为n */</span><br><span class="line">	处理eqe对应的cqe;</span><br><span class="line">	打开cq到eq的通道；</span><br><span class="line">&#125;</span><br><span class="line">更新eq head到原来eq_head + n的位置;</span><br></pre></td></tr></table></figure>
<p>可以看到实际处理的时候为了把处理cq和打开cq到eq的通道放在一起，会在eq head处理<br>之前打开cq到eq的通道，这样和上面的eq不溢出的条件是冲突的。这样做eq会溢出。<br>注意到上面的分析是假设eq深度等于并发cq个数N。要使得eq不溢出，我们这里还可以增加<br>eq的深度来解决，可以看到我们需要至少把eq的深度加到N+n。</p>
<p>下面分析cqe汇聚处理的问题。一个eqe至少都对应着一个cqe，在cq写eq后，对应cq写eq的<br>通道被屏蔽，后续core处理完的请求将在cq上排队。在中断处理程序中，通过eq找到对应的<br>cq，然后处理cqe，如果只处理一个cqe后就打开cq到eq的通道，就会上报eq，eq可能会上报<br>中断(这个要看eq上的汇聚处理)。所以在处理cq的时候，我们可汇聚k个cq一起处理，然后<br>再打开cq到eq的通道。这个是一个和上面独立的逻辑。</p>
]]></content>
      <tags>
        <tag>软件架构</tag>
      </tags>
  </entry>
  <entry>
    <title>使用eBPF得到内核执行过程的时间分布</title>
    <url>/%E4%BD%BF%E7%94%A8eBPF%E5%BE%97%E5%88%B0%E5%86%85%E6%A0%B8%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%88%86%E5%B8%83/</url>
    <content><![CDATA[<h2 id="具体例子介绍"><a href="#具体例子介绍" class="headerlink" title="具体例子介绍"></a>具体例子介绍</h2><p> 在SVA的使用场景中，IO缺页比较影响系统的性能。为此，我们需要观测在一个段程序执行<br> 的时候，系统中发生IO缺页的次数, 以及IO缺页的统计特征，比如IO缺页的平均时间、方差<br> 和分布情况。我们可以在内核代码中加tracepoint点，然后用perf或者是ftrace得到trace<br> 信息，然后用脚本处理得到上面的信息。但是，本文介绍的是用eBPF的方法得到这样的<br> 信息。</p>
<p> 使用eBPF的方法，可以在把一段用户态写的代码放到内核执行，这段用户态写的代码可以<br> 在已有的内核tracepoint点上附着。就是每次内核里跑到tracepoint点的时候，都可以跑<br> 一下用户态写的代码，用户可以自由编程统计自己想要的信息。</p>
<p> eBPF在内核中提供里各种help函数，方便统计代码调用。同时现在也有关于eBPF的用户态<br> 库(e.g. <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2lvdmlzb3IvYmNjLmdpdA==">https://github.com/iovisor/bcc.git<i class="fa fa-external-link-alt"></i></span>)，方便在用户态编写代码，记录和处理<br> 得到的信息。</p>
<h2 id="内核配置"><a href="#内核配置" class="headerlink" title="内核配置"></a>内核配置</h2><p> 我们使用<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0xpbmFyby9saW51eC1rZXJuZWwtdWFkaw==">https://github.com/Linaro/linux-kernel-uadk<i class="fa fa-external-link-alt"></i></span> branch: uacce-devel-5.11<br> 的分支测试，需要打上如下第3节中的内核补丁。</p>
<p> 如下是需要打开的内核编译选项:</p>
<p> make ARCH=arm64 defconfig<br> CONFIG_UACCE=y<br> CONFIG_DEV_HISI_ZIP=y<br> CONFIG_DEV_HISI_QM=y<br> CONFIG_ARM_SMMU_V3=y (默认)<br> CONFIG_ARM_SMMU_V3_SVA=y (默认)<br> CONFIG_IOMMU_SVA_LIB=y (默认)<br> CONFIG_PCI_PASID=y (默认)<br> CONFIG_FTRACE=y<br> (以上是业务相关的)</p>
<p> CONFIG_BPF_SYSCALL=y<br> CONFIG_BPF_KPROBE_OVERRIDE=y<br> CONFIG_KPROBE=y</p>
<p> CONFIG_SQUASHFS_XZ=y<br> CONFIG_CGROUP_FREEZER=y<br> (以上是ebpf相关的)</p>
<h2 id="用户态环境搭建"><a href="#用户态环境搭建" class="headerlink" title="用户态环境搭建"></a>用户态环境搭建</h2><p> 本文测试所使用的环境是Ubuntu 16.04, 这个版本的ubuntu没有bcc相关的仓库，外部仓库<br> 现在也无法使用了(<span class="exturl" data-url="aHR0cDovL3d3dy5icmVuZGFuZ3JlZ2cuY29tL2Jsb2cvMjAxNi0wNi0xNC91YnVudHUteGVuaWFsLWJjYy1icGYuaHRtbA==">http://www.brendangregg.com/blog/2016-06-14/ubuntu-xenial-bcc-bpf.html<i class="fa fa-external-link-alt"></i></span>)。</p>
<p> 所以本文测试使用了snap来安装bcc相关的环境，snap是ubuntu apt之外的另一个包管理器。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install snap</span><br><span class="line">sudo snap install bcc</span><br></pre></td></tr></table></figure>
<p> 如上会把snap管理下的bcc包安装到/snap/*下。 ls -al /snap</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">total 28</span><br><span class="line">drwxr-xr-x  6 root root 4096 2月  20 02:34 .</span><br><span class="line">drwxr-xr-x 22 root root 4096 10月  1 15:48 ..</span><br><span class="line">drwxr-xr-x  3 root root 4096 2月  20 02:34 bcc</span><br><span class="line">drwxr-xr-x  2 root root 4096 2月  20 05:16 bin</span><br><span class="line">drwxr-xr-x  3 root root 4096 2月  20 02:33 core18</span><br><span class="line">-r--r--r--  1 root root  548 10月 17  2019 README</span><br><span class="line">drwxr-xr-x  3 root root 4096 2月  20 02:32 snapd</span><br></pre></td></tr></table></figure>

<p>为python增加搜索库的路径，可以看到snap管理的bcc python库都在snap自己的路劲下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PYTHONPATH=&quot;/snap/bcc/146/usr/lib/python3/dist-packages:&#123;$PYTHONPATH&#125;&quot;</span><br><span class="line">export PYTHONPATH</span><br><span class="line"></span><br><span class="line">LD_LIBRARY_PATH=&quot;/snap/bcc/146/usr/lib/aarch64-linux-gnu/:&#123;$LD_LIBRARY_PATH&#125;&quot;</span><br><span class="line">export LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure>

<p>注意，这里编译bcc代码的时候，会使用到内核头文件。本文尝试了各种内核头文件<br>导出的方式都还是一直有内核头文件找不到。索性把查找头文件的目录又重新软连接到<br>使用内核的源码路径上。为了简单起见，本文的测试使用了本地编译内核的方式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ln -s &lt;your kernel source&gt; /lib/modules/&lt;your_kernel_magic&gt;/build</span><br></pre></td></tr></table></figure>

<h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><p>内核补丁:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff --git a/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c b/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c</span><br><span class="line">index 8d29aa1be282..637e95c237a1 100644</span><br><span class="line">--- a/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c</span><br><span class="line">+++ b/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c</span><br><span class="line">@@ -32,6 +32,8 @@</span><br><span class="line"> </span><br><span class="line"> #include &lt;linux/amba/bus.h&gt;</span><br><span class="line"> </span><br><span class="line">+#include &lt;trace/events/smmu.h&gt;</span><br><span class="line">+</span><br><span class="line"> #include &quot;arm-smmu-v3.h&quot;</span><br><span class="line"> #include &quot;../../iommu-sva-lib.h&quot;</span><br><span class="line"> </span><br><span class="line">@@ -945,6 +947,8 @@ static int arm_smmu_page_response(struct device *dev,</span><br><span class="line"> 	 * forget.</span><br><span class="line"> 	 */</span><br><span class="line"> </span><br><span class="line">+	trace_io_fault_exit(dev, resp-&gt;pasid);</span><br><span class="line">+</span><br><span class="line"> 	return 0;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line">@@ -1474,6 +1478,8 @@ static int arm_smmu_handle_evt(struct arm_smmu_device *smmu, u64 *evt)</span><br><span class="line"> 	struct iommu_fault_event fault_evt = &#123; &#125;;</span><br><span class="line"> 	struct iommu_fault *flt = &amp;fault_evt.fault;</span><br><span class="line"> </span><br><span class="line">+	trace_io_fault_entry(smmu-&gt;dev, FIELD_GET(EVTQ_0_SSID, evt[0]));</span><br><span class="line">+</span><br><span class="line"> 	/* Stage-2 is always pinned at the moment */</span><br><span class="line"> 	if (evt[1] &amp; EVTQ_1_S2)</span><br><span class="line"> 		return -EFAULT;</span><br><span class="line">diff --git a/include/trace/events/smmu.h b/include/trace/events/smmu.h</span><br><span class="line">index e9b648407102..4d96bfd20726 100644</span><br><span class="line">--- a/include/trace/events/smmu.h</span><br><span class="line">+++ b/include/trace/events/smmu.h</span><br><span class="line">@@ -82,6 +82,28 @@ DEFINE_EVENT(smmu_mn, smmu_mn_free, TP_PROTO(unsigned int pasid), TP_ARGS(pasid)</span><br><span class="line"> DEFINE_EVENT(smmu_mn, smmu_mn_get, TP_PROTO(unsigned int pasid), TP_ARGS(pasid));</span><br><span class="line"> DEFINE_EVENT(smmu_mn, smmu_mn_put, TP_PROTO(unsigned int pasid), TP_ARGS(pasid));</span><br><span class="line"> </span><br><span class="line">+DECLARE_EVENT_CLASS(smmu_io_fault_class,</span><br><span class="line">+	TP_PROTO(struct device *dev, unsigned int pasid),</span><br><span class="line">+	TP_ARGS(dev, pasid),</span><br><span class="line">+</span><br><span class="line">+	TP_STRUCT__entry(</span><br><span class="line">+		__string(dev, dev_name(dev))</span><br><span class="line">+		__field(int, pasid)</span><br><span class="line">+	),</span><br><span class="line">+	TP_fast_assign(</span><br><span class="line">+		__assign_str(dev, dev_name(dev));</span><br><span class="line">+		__entry-&gt;pasid = pasid;</span><br><span class="line">+	),</span><br><span class="line">+	TP_printk(&quot;dev=%s pasid=%d&quot;, __get_str(dev), __entry-&gt;pasid)</span><br><span class="line">+);</span><br><span class="line">+</span><br><span class="line">+#define DEFINE_IO_FAULT_EVENT(name)       \</span><br><span class="line">+DEFINE_EVENT(smmu_io_fault_class, name,        \</span><br><span class="line">+	TP_PROTO(struct device *dev, unsigned int pasid), \</span><br><span class="line">+	TP_ARGS(dev, pasid))</span><br><span class="line">+</span><br><span class="line">+DEFINE_IO_FAULT_EVENT(io_fault_entry);</span><br><span class="line">+DEFINE_IO_FAULT_EVENT(io_fault_exit);</span><br><span class="line"> </span><br><span class="line"> #endif /* _TRACE_SMMU_H */</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如上，为了在eBPF的跟踪代码里跟踪io page fault的流程，我们在SMMU io page fault<br>的入口和出口出增加了新的tracepoint点。增加trancepoint的方法还可以参考<a href="https://wangzhou.github.io/%E4%BD%BF%E7%94%A8perf-trace%E8%B7%9F%E8%B8%AAIO%E7%BC%BA%E9%A1%B5/">这里</a>。</p>
<p>用户态python脚本, ebpf_smmu_iopf.py。可以参考bcc里自带的tools, 看看怎么写这些<br>脚本: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2lvdmlzb3IvYmNjLmdpdA==">https://github.com/iovisor/bcc.git<i class="fa fa-external-link-alt"></i></span></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line">#</span><br><span class="line">from __future__ import print_function</span><br><span class="line">from bcc import BPF</span><br><span class="line">from ctypes import c_ushort, c_int, c_ulonglong</span><br><span class="line">from time import sleep</span><br><span class="line">from sys import argv</span><br><span class="line"></span><br><span class="line">def usage():</span><br><span class="line">        print(&quot;USAGE: %s [interval [count]]&quot; % argv[0])</span><br><span class="line">        exit()</span><br><span class="line"></span><br><span class="line"># arguments</span><br><span class="line">interval = 5</span><br><span class="line">count = -1</span><br><span class="line">if len(argv) &gt; 1:</span><br><span class="line">        try:</span><br><span class="line">                interval = int(argv[1])</span><br><span class="line">                if interval == 0:</span><br><span class="line">                        raise</span><br><span class="line">                if len(argv) &gt; 2:</span><br><span class="line">                        count = int(argv[2])</span><br><span class="line">        except: # also catches -h, --help</span><br><span class="line">                usage()</span><br><span class="line"></span><br><span class="line"># load BPF program</span><br><span class="line">b = BPF(text=&quot;&quot;&quot;</span><br><span class="line">#include &lt;uapi/linux/ptrace.h&gt;</span><br><span class="line"></span><br><span class="line">BPF_HASH(start, u64);</span><br><span class="line">BPF_HISTOGRAM(dist);</span><br><span class="line"></span><br><span class="line">TRACEPOINT_PROBE(smmu, io_fault_entry)</span><br><span class="line">&#123;</span><br><span class="line">        u64 ts;</span><br><span class="line"></span><br><span class="line">        ts = bpf_ktime_get_ns();</span><br><span class="line">        start.update((unsigned int *)args-&gt;pasid, &amp;ts);</span><br><span class="line"></span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TRACEPOINT_PROBE(smmu, io_fault_exit)</span><br><span class="line">&#123;</span><br><span class="line">        u64 *tsp, delta;</span><br><span class="line">        u64 pasid;</span><br><span class="line"></span><br><span class="line">        tsp = start.lookup((unsigned int *)args-&gt;pasid);</span><br><span class="line"></span><br><span class="line">        if (tsp != 0) &#123;</span><br><span class="line">                delta = bpf_ktime_get_ns() - *tsp;</span><br><span class="line">                dist.increment(bpf_log2l(delta));</span><br><span class="line">                start.delete((unsigned int *)args-&gt;pasid);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br><span class="line">&quot;&quot;&quot;)</span><br><span class="line"></span><br><span class="line"># header</span><br><span class="line">print(&quot;Tracing... Hit Ctrl-C to end.&quot;)</span><br><span class="line"></span><br><span class="line"># output</span><br><span class="line">loop = 0</span><br><span class="line">do_exit = 0</span><br><span class="line">while (1):</span><br><span class="line">        if count &gt; 0:</span><br><span class="line">                loop += 1</span><br><span class="line">                if loop &gt; count:</span><br><span class="line">                        exit()</span><br><span class="line">        try:</span><br><span class="line">                sleep(interval)</span><br><span class="line">        except KeyboardInterrupt:</span><br><span class="line">                pass; do_exit = 1</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line">        b[&quot;dist&quot;].print_log2_hist(&quot;nsecs&quot;)</span><br><span class="line">        b[&quot;dist&quot;].clear()</span><br><span class="line">        if do_exit:</span><br><span class="line">                exit()</span><br></pre></td></tr></table></figure>

<h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ./ebpf_smmu_iopf.py</span><br><span class="line"></span><br><span class="line">另一个窗口运行：</span><br><span class="line">sudo zip_perf_test -b 8192 -s 819200000</span><br></pre></td></tr></table></figure>
<p>得到如下的iopf时延的分布图:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[...]</span><br></pre></td></tr></table></figure>

<p>在过程中测试eBPF的环境是否搭建ok，也可以跑下/snap/bin自带的程序，比如跑bcc.cpudist<br>我们得到了这样的分布图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Tracing on-CPU time... Hit Ctrl-C to end.</span><br><span class="line"></span><br><span class="line">     usecs               : count     distribution</span><br><span class="line">         0 -&gt; 1          : 193      |*************************               |</span><br><span class="line">         2 -&gt; 3          : 49       |******                                  |</span><br><span class="line">         4 -&gt; 7          : 35       |****                                    |</span><br><span class="line">         8 -&gt; 15         : 41       |*****                                   |</span><br><span class="line">        16 -&gt; 31         : 37       |****                                    |</span><br><span class="line">        32 -&gt; 63         : 116      |***************                         |</span><br><span class="line">        64 -&gt; 127        : 14       |*                                       |</span><br><span class="line">       128 -&gt; 255        : 2        |                                        |</span><br><span class="line">       256 -&gt; 511        : 0        |                                        |</span><br><span class="line">       512 -&gt; 1023       : 3        |                                        |</span><br><span class="line">      1024 -&gt; 2047       : 12       |*                                       |</span><br><span class="line">      2048 -&gt; 4095       : 61       |********                                |</span><br><span class="line">      4096 -&gt; 8191       : 96       |************                            |</span><br><span class="line">      8192 -&gt; 16383      : 97       |************                            |</span><br><span class="line">     16384 -&gt; 32767      : 162      |*********************                   |</span><br><span class="line">     32768 -&gt; 65535      : 301      |****************************************|</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件性能</tag>
      </tags>
  </entry>
  <entry>
    <title>使用SMMU PMU查看性能数据</title>
    <url>/%E4%BD%BF%E7%94%A8SMMU-PMU%E6%9F%A5%E7%9C%8B%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<ul>
<li><p>首先要确定使用的系统里有arm_smmuv3_pmu这个模块，或者它已经被编译进内核。<br>这个模块的代码在内核目录kernel/drivers/perf/arm_smmuv3_pmu.c, 内核配置是:<br>CONFIG_ARM_SMMU_V3_PMU</p>
</li>
<li><p>确定使用的单板上的UEFI里有你要测试的模块对应的SMMU PMCG节点，没有这个节点的<br>的话即使加载上面的驱动也无法使用SMMU PMCG</p>
</li>
<li><p>正常使用的话，dmesg | grep pmcg可以看见类似信息:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Ubuntu:/ # dmesg | grep pmcg</span><br><span class="line">[ 1232.379951] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.8.auto: option mask 0x1</span><br><span class="line">[ 1232.380040] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.8.auto: Registered PMU @ 0x0000000148020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380094] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.9.auto: option mask 0x1</span><br><span class="line">[ 1232.380142] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.9.auto: Registered PMU @ 0x0000000201020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380190] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.10.auto: option mask 0x1</span><br><span class="line">[ 1232.380241] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.10.auto: Registered PMU @ 0x0000000100020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380286] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.11.auto: option mask 0x1</span><br><span class="line">[ 1232.380337] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.11.auto: Registered PMU @ 0x0000000140020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380397] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.12.auto: option mask 0x1</span><br><span class="line">[ 1232.380445] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.12.auto: Registered PMU @ 0x0000200148020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380491] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.13.auto: option mask 0x1</span><br><span class="line">[ 1232.380542] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.13.auto: Registered PMU @ 0x0000200201020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380601] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.14.auto: option mask 0x1</span><br><span class="line">[ 1232.380653] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.14.auto: Registered PMU @ 0x0000200100020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380698] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.15.auto: option mask 0x1</span><br><span class="line">[ 1232.380770] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.15.auto: Registered PMU @ 0x0000200140020000 using 8 counters with Individual filter settings</span><br></pre></td></tr></table></figure></li>
<li><p>使用perf list | grep pmcg可以查看系统支持的pmcg相关的时间类型:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Ubuntu:/ # perf list | grep pmcg</span><br><span class="line">  [...]</span><br><span class="line">  smmuv3_pmcg_140020/config_cache_miss/              [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/config_struct_access/           [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/cycles/                         [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/pcie_ats_trans_passed/          [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/pcie_ats_trans_rq/              [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/tlb_miss/                       [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/trans_table_walk_access/        [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/transaction/                    [Kernel PMU event]</span><br><span class="line">  [...]</span><br></pre></td></tr></table></figure></li>
<li><p>使用pmcg之前需要先明确需要测试的设备是在哪个pmcg之下，pmcg的命名方式是:<br>smmuv3_pmcg_<phys_addr_page>, 这里的phys_addr_page是对应SMMU PMCG基地址去掉<br>低12bit。这里的设计有点不好，使用者很难找到对应的关系 :(</phys_addr_page></p>
</li>
<li><p>当想观测一个程序对应的SMMU上统计信息时我们可以, 比如这样:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf stat -e smmuv3_pmcg_&lt;phys_addr_page&gt;/tlb_miss/ &lt;your_program&gt;</span><br></pre></td></tr></table></figure>
<p>的到程序执行过程的smmu tlb miss数目。把这里的tlb_miss换成上面perf list | grep pmcg<br>所示的其他事件，就可以得到其他事件的统计。</p>
</li>
<li><p>实际系统上可能一个smmu下接着多个外设，只想看一个外设在smmu上统计数据可以，比如<br>这样:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf stat -e smmuv3_pmcg_&lt;phys_addr_page&gt;/tlb_miss/,filter_enable=1,filter_span=0,filter_stream_id=0x75 &lt;your_program&gt;</span><br></pre></td></tr></table></figure>
<p>上面的0x75是设备对应的stream_id，PCIe设备的话，一般就是这个设备的BDF号。<br>(fix me: device function number怎么表示?)</p>
<p>如果有smmu自定义的event实现，可以指定具体的event编号, 比如:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf stat -e smmuv3_pmcg_&lt;phys_addr_page&gt;/event=0x80/ &lt;your_program&gt;</span><br></pre></td></tr></table></figure>
<p>对于自定义的event，定义的命令是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf stat -e smmuv4_pmcg_&lt;phys_addr_page&gt;/event=0x80,filter_enable=1,filter_span=0,filter_stream_id=0x75/ &lt;your_program&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>使用ftrace跟踪函数</title>
    <url>/%E4%BD%BF%E7%94%A8ftrace%E8%B7%9F%E8%B8%AA%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>有些时候在调试内核代码时，我们想跟踪下内核代码的执行流程，以及函数执行时间。<br>这个时候我们可以用Linux内核自带的ftrace来跟踪。</p>
<ul>
<li><p>确定内核打开了ftrace的编译选项: CONFIG_FTRACE。这个Tracers目录下的子编译<br>选项可以都打开，我的经验是自测试的几个可以不开，不然开机过程加上自测会时间<br>很长，其他的都可以开打。</p>
</li>
<li><p>cd /sys/kernel/debug/tracing</p>
</li>
<li><p>echo 0 &gt; tracing_on //关闭trace</p>
</li>
<li><p>echo 0 &gt; trace //把之前跟踪buffer里的数据清空</p>
</li>
<li><p>echo function_graph &gt; current_tracer //设置当前的跟踪器是function_graph</p>
</li>
<li><p>echo funcgraph-proc &gt; trace_options</p>
</li>
<li><p>echo funcgraph-abstime &gt; trace_options //在输出结果里增加每个函数的时间戳</p>
</li>
<li><p>echo your_func &gt; set_ftrace_filter //把你要跟踪的函数配置进ftrace</p>
</li>
<li><p>grep -i your_func available_filter_functions //可以查看上个步骤的函数有没有设置好</p>
</li>
<li><p>echo 1 &gt; tracing_on //开始trace</p>
</li>
<li><p>启动你要跟踪的程序执行</p>
</li>
<li><p>echo 0 &gt; tracing_on //关闭trace</p>
</li>
<li><p>cat trace &gt; your_trace_result //输出得到的跟踪信息，可以将信息重定向到文件</p>
</li>
</ul>
<p>经过上面，你可以大概得到一个这样的输出信息:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tracer: function_graph</span><br><span class="line">#</span><br><span class="line">#     TIME        CPU  TASK/PID         DURATION                  FUNCTION CALLS</span><br><span class="line">#      |          |     |    |           |   |                     |   |   |   |</span><br><span class="line">[...]</span><br><span class="line"> 9520.457571 |     2)  dma0cha-1201  |   0.940 us    |  hisi_dma_issue_pending [hisi_dma]();</span><br><span class="line"> 9520.457583 |     0)    &lt;idle&gt;-0    |   0.680 us    |  hisi_dma_irq [hisi_dma]();</span><br><span class="line"> 9520.457584 |     0)    &lt;idle&gt;-0    |   0.490 us    |  hisi_dma_desc_free [hisi_dma]();</span><br><span class="line"> 9520.457589 |     2)  dma0cha-1201  |   0.420 us    |  hisi_dma_tx_status [hisi_dma]();</span><br><span class="line"> 9520.457666 |     2)  dma0cha-1201  |   0.930 us    |  hisi_dma_prep_dma_memcpy [hisi_dma]();</span><br><span class="line"> 9520.457668 |     2)  dma0cha-1201  |   0.900 us    |  hisi_dma_issue_pending [hisi_dma]();</span><br><span class="line"> 9520.462827 |     2)  dma0cha-1201  |   0.910 us    |  hisi_dma_issue_pending [hisi_dma]();</span><br><span class="line"> 9520.462833 |     0)    &lt;idle&gt;-0    |   0.620 us    |  hisi_dma_irq [hisi_dma]();</span><br><span class="line">[...] </span><br></pre></td></tr></table></figure>
<p>duration这一栏得到的是每个函数的执行时间。通过time一栏的时间戳，可以得到函数和<br>函数之间的时延, 比如可以得到从发起一个dma传输task(hisi_dma_issue_pending)到<br>中断函数执行(hisi_dma_irq)的大概时延是：9520.457583 -  9520.457571 = 12us。</p>
<p>如果要跟踪一个函数的调用链，可以使用set_graph_function。具体的使用设置是:</p>
<ul>
<li><p>echo function_graph &gt; current_tracer</p>
</li>
<li><p>echo your_func &gt; set_graph_function</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>Linux内核</tag>
        <tag>ftrace</tag>
      </tags>
  </entry>
  <entry>
    <title>使用TAILQ</title>
    <url>/%E4%BD%BF%E7%94%A8TAILQ/</url>
    <content><![CDATA[<p>TAILQ是BSD里实现的一套简单的链表，一般linux系统只要include sys/queue.h就可以使用。<br>在我使用的ARM版本的ubuntu系统，这个文件在/usr/include/aarch64-linux-gnu/sys/queue.h.</p>
<p>TAILQ作为链表使用的基本数据结构就是两个: TAILQ_ENTRY, TAILQ_HEAD。一个表示链表<br>的节点，一个表示链表头, 当使用的时候需要用TAILQ宏定义出相应的结构。具体使用的<br>方法和内核里链表的使用方式基本一致。在需要链表连起来的元素里每个埋一个TAILQ_ENTRY<br>的结构体。随后的各种操作宏里会大量的用到node这个变量。其实TAILQ_ENTRY就是定义了<br>一个包含指向本节点前后节点的指针结构。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct your_list_node &#123;</span><br><span class="line">	int data;</span><br><span class="line">	TAILQ_ENTRY(your_list_node) node;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">#define	_TAILQ_ENTRY(type, qual)					\</span><br><span class="line">struct &#123;								\</span><br><span class="line">	qual type *tqe_next;		/* next element */		\</span><br><span class="line">	qual type *qual *tqe_prev;	/* address of previous next element */\</span><br><span class="line">&#125;</span><br><span class="line">#define TAILQ_ENTRY(type)	_TAILQ_ENTRY(struct type,)</span><br></pre></td></tr></table></figure>
<p>使用TAILQ_HEAD定义一个类型是struct list_head, 所管理的链表元素类型是<br>truct your_list_node的链表头结构体。所以如下的宏其实只是定义了一个类型：<br>struct list_head。使用的时候还要有链表头实例的定义。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TAILQ_HEAD(list_head, your_list_node);	</span><br></pre></td></tr></table></figure>

<p>有了这两个基本数据结构，我们看几个基本的操作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct list_head head;</span><br><span class="line">TAILQ_INIT(&amp;head);</span><br></pre></td></tr></table></figure>
<p>初始化一个链表头。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct your_list_node e1;</span><br><span class="line">TAILQ_INSERT_TAIL(&amp;head, &amp;e1, node);</span><br></pre></td></tr></table></figure>
<p>把一个节点插入head链表头表示的链表的尾部。第二个参数是链表元素的指针。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TAILQ_FOREACH(tmp, &amp;head, node) &#123;</span><br><span class="line">	printf(&quot;---&gt; %d\n&quot;, tmp-&gt;data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>遍历head链表头表示的链表的所有元素，每次得到的元素指针放到tmp里。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct your_list_node e4;</span><br><span class="line">struct your_list_node e5;</span><br><span class="line">TAILQ_INSERT_BEFORE(&amp;e4, &amp;e5, node);</span><br></pre></td></tr></table></figure>
<p>把e5插到e4的前面，第一个, 第二个参数都是链表元素指针。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct your_list_node *tmp;</span><br><span class="line">tmp = TAILQ_LAST(&amp;head, list_head);</span><br></pre></td></tr></table></figure>
<p>得到head链表头表示的链表的最后一个元素的指针。head是链表头实例的名字，list_head<br>则是链表头的类型。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct your_list_node e2;</span><br><span class="line">TAILQ_REMOVE(&amp;head, &amp;e2, node);</span><br></pre></td></tr></table></figure>
<p>从head链表头表示的链表中删除e2节点，第二个参数是要删除节点的指针。</p>
<p>其他的宏基本逻辑和上面介绍的基本一样。<br>完整的测试代码可以参考: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL3RhaWxxL3Rlc3QuYw==">https://github.com/wangzhou/tests/blob/master/tailq/test.c<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>使用github做开源开发</title>
    <url>/%E4%BD%BF%E7%94%A8github%E5%81%9A%E5%BC%80%E6%BA%90%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<p>一个项目的参与者有开发者和维护者。开发者要做的是，从主分支拉代码，开发，提交代码。<br>维护者要做的是，review开发者提交的代码，合入代码。</p>
<p>开发者在github上使用发git pull request的方法向维护者提交合入请求。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                    repo fork</span><br><span class="line">+-----------------+ -------&gt; +-------------+</span><br><span class="line">| 主线repo master |          | 开发者 repo |</span><br><span class="line">+-----------------+ &lt;------- +-------------+</span><br><span class="line">         \      git pull request    ^</span><br><span class="line">          \                          \</span><br><span class="line">           \                          \ git push dev-branch到开发者github repo</span><br><span class="line">            \                          \</span><br><span class="line">             \                          \</span><br><span class="line">              \                          \</span><br><span class="line">               \             +----------------+</span><br><span class="line">                ----------&gt;  | 开发者本地repo |</span><br><span class="line">           跟踪主线master    +----------------+</span><br><span class="line">                         依据最新master分支创建开发分支: dev-branch</span><br></pre></td></tr></table></figure>
<p>如上，开发者本地可以维护一个主线repo master的跟踪分支，开发一个新特性的时候，<br>开发者建立基于最新主线master的开发分支，开发完成后，把开发分支push到开发者github<br>repo上，然后再在github页面上发起git pull request。</p>
<p>维护者在收到git pull request的时候，可以在线review，也可以把需要review的patch<br>拉到本地。维护者可以用 git pull origin pull/<pull_request_id>/head把对应patch拉到<br>本地的当前分支上。维护者直接在github页面就可以点击合入补丁，合入补丁的时候有几种<br>方式，”Create a merge commit”的方式会在合入的时候自动创建一个merge的commit，记录<br>整个merge的信息; “Rebase and merge”的方式会只合入pull request里的patch。</pull_request_id></p>
]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>两层qemu环境配置</title>
    <url>/%E4%B8%A4%E5%B1%82qemu%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id="第一层qemu"><a href="#第一层qemu" class="headerlink" title="第一层qemu"></a>第一层qemu</h2><p>第一层qemu正常编译即可。</p>
<p>第一层qemu启动的时候要打开vcpu的hypvisor extention特性，这样这一层qemu启动的系统<br>里就会带/dev/kvm，第二层qemu使用kvm加速，第二层qemu的运行速度会比较快，否则第二层<br>qemu慢的一塌糊涂。</p>
<p>arm构架下，打开vcpu hypvisor extention特性的配置是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 -cpu cortex-a57 \</span><br><span class="line">-smp 1 -m 1024M \</span><br><span class="line">-nographic \</span><br><span class="line">-M virt,virtualization=true \</span><br><span class="line">-kernel ~/Image \</span><br><span class="line">-append &quot;console=ttyAMA0 earlycon root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">-initrd ~/rootfs.cpio.gz</span><br></pre></td></tr></table></figure>
<p>其中 -M virt,virtualization=true 打开vcpu hypvisor extention。</p>
<h2 id="第二层qemu"><a href="#第二层qemu" class="headerlink" title="第二层qemu"></a>第二层qemu</h2><p>第二层qemu在编译的时候要打开kvm的支持：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">configure --target-list=aarch64-softmmu --enable-kvm</span><br></pre></td></tr></table></figure>
<p>理论上同样编译一个arm版本的qemu就好，本地编译一个arm版本的qemu目前已经比较方便，<br>在一台ubuntu 20.04 arm物理机器上直接编译就好，过程中缺什么库，直接安装就好。</p>
<p>但是，如果你的环境是在一台x86机器上，就需要交叉编译qemu，qemu的所有依赖库都要先<br>交叉编译下。如果是这种情况，一种比较好的解决办法是编译buildroot，同时指定自己的<br>qemu代码仓库给buildroot，指定自己的qemu代码仓库是为了随后比较方便修改qemu代码，<br>重复编译。直接用buildroot自己的qemu仓库的配置，会把qemu代码下载到buildroot/output/build/qemu<br>下面，去这里修改qemu的代码也是可以的，但是make clean会把output/build下的东西都删掉，<br>还是指定自己的qemu仓库方便一些。</p>
<p>使用这里介绍的方式，local.mk的配置是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">QEMU_OVERRIDE_SRCDIR = &lt;your_local_qemu_path&gt;/qemu/</span><br><span class="line">QEMU_OVERRIDE_SRCDIR_RSYNC_EXCLUSIONS = --include .git</span><br></pre></td></tr></table></figure>
<p>buildbood qemu的配置buildroot/package/qemu/qem.mk需要修改：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff --git a/package/qemu/qemu.mk b/package/qemu/qemu.mk</span><br><span class="line">index 88516678d1..a480673a68 100644</span><br><span class="line">--- a/package/qemu/qemu.mk</span><br><span class="line">+++ b/package/qemu/qemu.mk</span><br><span class="line">@@ -221,7 +221,6 @@ define QEMU_CONFIGURE_CMDS</span><br><span class="line"> 			--disable-opengl \</span><br><span class="line"> 			--disable-vhost-user-blk-server \</span><br><span class="line"> 			--disable-virtiofsd \</span><br><span class="line">-			--disable-tests \</span><br><span class="line"> 			$(QEMU_OPTS)</span><br><span class="line"> endef</span><br></pre></td></tr></table></figure>

<p>buildroot配置的时候，最好使用buildroot toolchain和musl c库，使用uclibc会报找不见<br>fenv.h的错误。编译qemu的时候只编译package qemu就好，BR2_PACKAGE_QEMU，<br>BR2_PACKAGE_QEMU_CUSTOM_TARGETS=aarch64-softmmu，BR2_PACKAGE_QEMU_FDT要配置下。</p>
<p>启动第二层qemu的时候加上-enable-kvm的启动参数:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 \</span><br><span class="line">-cpu host \</span><br><span class="line">-m 256M \</span><br><span class="line">-enable-kvm \</span><br><span class="line">-nographic \</span><br><span class="line">-machine virt \</span><br><span class="line">-kernel ~/Image \</span><br><span class="line">-append &quot;console=ttyAMA0 earlycon root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">-initrd ~/rootfs.cpio.gz</span><br></pre></td></tr></table></figure>

<p>如上整个环境的逻辑如下图所示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------------+</span><br><span class="line">|               | &lt;--- arm构架qemu</span><br><span class="line">|  第二层qemu    |</span><br><span class="line">|               |</span><br><span class="line">+---------------+ &lt;--- kvm加速</span><br><span class="line">+---------------+</span><br><span class="line">|               | &lt;--- arm构架qemu</span><br><span class="line">|  第一层qemu    |</span><br><span class="line">|               |</span><br><span class="line">+---------------+</span><br><span class="line">+----------------------------------------------+</span><br><span class="line">|                                              |</span><br><span class="line">|  物理机器(可能是arm构架,也可能是x86机器)          |</span><br><span class="line">|                                              |</span><br><span class="line">+----------------------------------------------+</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo NexT搭建个人博客</title>
    <url>/%E4%BD%BF%E7%94%A8hexo-NexT%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<ol>
<li>基础逻辑说明</li>
</ol>
<hr>
<p> github提供了静态网页展示的功能，我们可以在自己的github上创建username.github.io<br> 名字的仓库，然后在这个仓库的settings里开启Pages的功能。这里需要html文件才能以<br> 静态网页的形式把内容展示出来，而一般我们直接书写的是md或者是rst这样的文本文件。</p>
<p> 网上有各种各样的从文本文件生成html文件的框架，hexo就是其中的一个，它是基于JS的。<br> hexo支持主题可选，通过选不同的主题，博客的风格会不一样，我们选的是NexT主题，这个<br> 主题使用的人很多，相关的资料也容易找。</p>
<p> 我们把整个blog分成三个代码仓库来管理：</p>
<ul>
<li><p>hexo代码仓库，这个仓库是hexo代码，我们的博客原文也放在这个目录下。</p>
</li>
<li><p>静态网页仓库，这个仓库里的静态网页都是hexo生成的。我们把这个仓库推到如上的<br>username.github.io这个仓库，然后从浏览器就可有通过<span class="exturl" data-url="aHR0cHM6Ly91c2VybmFtZS5naXRodWIuaW8v">https://username.github.io<i class="fa fa-external-link-alt"></i></span><br>访问到。</p>
</li>
<li><p>NexT主题的仓库，我们把这个仓库fork一份到自己的github上，一般我们只需要改动<br>NexT的配置文件就好，但是我们这里还是把仓库本身也保存一份，日后可以直接修改NexT<br>的代码，然后提交到自己的仓库暂时保存。我们把自己的这个NexT仓库作为hexo仓库的<br>submodule保存。</p>
</li>
</ul>
<ol start="2">
<li>搭建过程</li>
</ol>
<hr>
<ul>
<li><p>安装npm、nodejs和hexo</p>
<p>sudo apt install npm nodejs<br>sudo npm install -g hexo-cli</p>
<p>Note: 上面安装的nodejs是10.x.x的，后面生成博客的时候会报错，我们这里需要安装</p>
<pre><code>  12.x.x的nodejs版本。nodejs的版本可以通过node -v 来看。
</code></pre>
</li>
<li><p>安装12.x.x nodejs</p>
<p>curl -sL <span class="exturl" data-url="aHR0cHM6Ly9kZWIubm9kZXNvdXJjZS5jb20vc2V0dXBfMTIueA==">https://deb.nodesource.com/setup_12.x<i class="fa fa-external-link-alt"></i></span> | sudo -E bash -<br>sudp apt update<br>sudo apt install -y nodejs</p>
</li>
<li><p>下载hexo和NexT</p>
<p>mkdir hexo; cd hexo<br>hexo init blog<br>cd blog<br>sudo npm install</p>
<p>把NexT fork到自己的github, 我们用url_priv_next表示自己NexT的github仓库地址。<br>创建一个空的github仓库用来存放hexo代码, 我们用url_priv_hexo表示自己的这个hexo<br>仓库的github地址。在hexo仓库把自己的NexT仓库设置成自己的hexo仓库的submodule:</p>
<p>git submodule add url_priv_next themes/next</p>
<p>到这里为止，基本的仓库以及他们之间的关系我们都搞定了。我们看看hexo里几个目录<br>里放什么：source里放博客相关的原文件，其中包括md文件和相关的图片；themes放各种<br>主题，所以NexT也放到这个目录下；scaffolds放各种模版，比如后面会讲到的生成文章<br>就会默认用到scaffolds/post.md这个模版，所以我们更改这个模版生成的文章也会带上<br>相关的改动，如下的description就是这样的一个例子, draft.md这个模版可以用来生成<br>草稿文档，比如我们可以用hexo new draft “draft_1”来生成名字是draft_1的草稿文档，<br>这个草稿文档以draft.md为模版，存放在source/_drafts下；public用来放后面生成<br>的html文档；node_modules是一些和Hexo nodejs相关的东西。</p>
</li>
<li><p>配置hexo以及NexT的配置文件</p>
<p>如上的配置已经可以生成静态网页。出于个人偏好，我自己的配置文件的改动如下，<br>具体的改动可以看相关地方的注释：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo _config.yml</span><br><span class="line"></span><br><span class="line">diff --git a/_config.yml b/_config.yml</span><br><span class="line">index 2dc35e6..97fc902 100644</span><br><span class="line">--- a/_config.yml</span><br><span class="line">+++ b/_config.yml</span><br><span class="line">@@ -97,7 +97,7 @@ ignore:</span><br><span class="line"> # Extensions</span><br><span class="line"> ## Plugins: https://hexo.io/plugins/</span><br><span class="line"> ## Themes: https://hexo.io/themes/</span><br><span class="line">-theme: landscape</span><br><span class="line">+theme: next  # 使用NexT主题</span><br><span class="line"></span><br><span class="line">diff --git a/scaffolds/post.md b/scaffolds/post.md</span><br><span class="line">index 1f9b9a4..91f86d0 100644</span><br><span class="line">--- a/scaffolds/post.md</span><br><span class="line">+++ b/scaffolds/post.md</span><br><span class="line">@@ -2,4 +2,5 @@</span><br><span class="line"> title: &#123;&#123; title &#125;&#125;</span><br><span class="line"> date: &#123;&#123; date &#125;&#125;</span><br><span class="line"> tags:</span><br><span class="line">+description:    # 如下使用hexo g 生成新md文档的时候会使用这个地方的模版，如果这里加上description</span><br><span class="line">                 # 主页就不会展示全部文件，而是只是显示这里的摘要。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NexT _config.yml</span><br><span class="line"></span><br><span class="line">diff --git a/_config.yml b/_config.yml</span><br><span class="line">index 61cc72d..957ae93 100644</span><br><span class="line">--- a/_config.yml</span><br><span class="line">+++ b/_config.yml</span><br><span class="line">@@ -67,7 +67,7 @@ footer:</span><br><span class="line">   copyright:</span><br><span class="line"> </span><br><span class="line">   # Powered by hexo &amp; NexT</span><br><span class="line">-  powered: true</span><br><span class="line">+  powered: false    # 个人喜欢极简的风格，所以去掉主页底部的&quot;Powered by hexo &amp; NexT&quot;</span><br><span class="line"> </span><br><span class="line">   # Beian ICP and gongan information for Chinese users. See: https://beian.miit.gov.cn, http://www.beian.gov.cn</span><br><span class="line">   beian:</span><br><span class="line">@@ -97,10 +97,10 @@ creative_commons:</span><br><span class="line"> # ---------------------------------------------------------------</span><br><span class="line"> </span><br><span class="line"> # Schemes</span><br><span class="line">-scheme: Muse</span><br><span class="line">+#scheme: Muse</span><br><span class="line"> #scheme: Mist</span><br><span class="line"> #scheme: Pisces</span><br><span class="line">-#scheme: Gemini</span><br><span class="line">+scheme: Gemini     # 换一个带边栏的风格，scheme是NexT内部的风格</span><br><span class="line"> </span><br><span class="line"> # Dark Mode</span><br><span class="line"> darkmode: false</span><br><span class="line">@@ -117,10 +117,10 @@ darkmode: false</span><br><span class="line"> # External url should start with http:// or https://</span><br><span class="line"> menu:</span><br><span class="line">   home: / || fa fa-home</span><br><span class="line">-  #about: /about/ || fa fa-user</span><br><span class="line">-  #tags: /tags/ || fa fa-tags</span><br><span class="line">+  about: /about || fa fa-user    # 打开about, tags, archives标签</span><br><span class="line">+  tags: /tags || fa fa-tags</span><br><span class="line">   #categories: /categories/ || fa fa-th</span><br><span class="line">-  archives: /archives/ || fa fa-archive</span><br><span class="line">+  archives: /archives || fa fa-archive</span><br><span class="line">   #schedule: /schedule/ || fa fa-calendar</span><br><span class="line">   #sitemap: /sitemap.xml || fa fa-sitemap</span><br><span class="line">   #commonweal: /404/ || fa fa-heartbeat</span><br><span class="line">@@ -138,8 +138,8 @@ menu_settings:</span><br><span class="line"> </span><br><span class="line"> sidebar:</span><br><span class="line">   # Sidebar Position.</span><br><span class="line">-  position: left</span><br><span class="line">-  #position: right</span><br><span class="line">+  #position: left</span><br><span class="line">+  position: right     # 边栏移动到右边</span><br><span class="line"> </span><br><span class="line">   # Manual define the sidebar width. If commented, will be default for:</span><br><span class="line">   # Muse | Mist: 320</span><br><span class="line">@@ -261,12 +261,12 @@ tag_icon: false</span><br><span class="line"> # Front-matter variable (unsupport animation).</span><br><span class="line"> reward_settings:</span><br><span class="line">   # If true, reward will be displayed in every article by default.</span><br><span class="line">-  enable: false</span><br><span class="line">+  enable: true        # 打开打赏的功能</span><br><span class="line">   animation: false</span><br><span class="line">   #comment: Donate comment here.</span><br><span class="line"> </span><br><span class="line"> reward:</span><br><span class="line">-  #wechatpay: /images/wechatpay.png</span><br><span class="line">+  wechatpay: /images/weixinpay.svg       # 把微信的付款二维码放到这个地方</span><br><span class="line">   #alipay: /images/alipay.png</span><br><span class="line">   #paypal: /images/paypal.png</span><br><span class="line">   #bitcoin: /images/bitcoin.png</span><br><span class="line">@@ -354,7 +354,7 @@ codeblock:</span><br><span class="line">   # Code Highlight theme</span><br><span class="line">   # Available values: normal | night | night eighties | night blue | night bright | solarized | solarized dark | galactic</span><br><span class="line">   # See: https://github.com/chriskempson/tomorrow-theme</span><br><span class="line">-  highlight_theme: normal</span><br><span class="line">+  highlight_theme: night blue            # 选择嵌入代码的显示风格，这里是深蓝色底</span><br><span class="line">   # Add copy button on codeblock</span><br><span class="line">   copy_button:</span><br><span class="line">     enable: false</span><br><span class="line">@@ -823,7 +823,7 @@ mermaid:</span><br><span class="line"> # Use velocity to animate everything.</span><br><span class="line"> # For more information: http://velocityjs.org</span><br><span class="line"> motion:</span><br><span class="line">-  enable: true</span><br><span class="line">+  enable: false          # 不禁止这个的话，打开一个文章会有动画，个人不喜欢这个</span><br><span class="line">   async: false</span><br><span class="line">   transition:</span><br><span class="line">     # Transition variants:</span><br></pre></td></tr></table></figure></li>
<li><p>添加文章和生成静态网页</p>
<p>hexo n “文章名字”<br>hexo clean &amp;&amp; hexo g</p>
<p>会在public目录里生成相关的静态网页，把public里的内容copy出来然后push到如上的<br>username.github.io仓库里。</p>
<p>本地调试的话可以先运行: hexo server, 然后在本地浏览器里通过<span class="exturl" data-url="aHR0cDovL2xvY2FsaG9zdDo0MDAwLw==">http://localhost:4000<i class="fa fa-external-link-alt"></i></span><br>观察生成的静态网页。</p>
</li>
<li><p>保存本地改动到GitHub</p>
<p>cd hexo/blog; git add &lt;改动文件&gt;; git commit -s -m &lt;改动描述&gt;;<br>cd hexo/blog/themes/next; git add &lt;改动文件&gt;; git commit -s -m &lt;改动描述&gt;;<br>git remote add url_priv_hexo<br>git push –recurse-submodules=on-demand origin master:master</p>
<p>在其他地方可以使用如下的方式迭代clone hexo和NexT仓库:<br>git clone –recursive url_priv_hexo</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>静态博客搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>使用linux zswap</title>
    <url>/%E4%BD%BF%E7%94%A8linux-zswap/</url>
    <content><![CDATA[<p>zswap是Linux内核里压缩swap内存的一个特性，他可以把需要swap到swap设备上内存先压缩<br>下，直到一定的门限值后再向swap设备写入。这个特性可以优化系统内存被大量使用，系统<br>有swap的时候的系统性能。本文简单介绍怎么使用。</p>
<ol>
<li><p>内核需要打开zswap的配置：CONFIG_ZSWAP。还需要打开zswap可能用到的用于压缩<br>内存存储的内存分配器，比如: CONFIG_ZBUD</p>
</li>
<li><p>在内核的启动cmdline里加zswap.enable=1可以自动加载zswap模块，zswap.compressor=xxx<br>可以选择压缩算法。如果不配置zswap.compressor, 默认的压缩算法是LZO。</p>
</li>
<li><p>启动系统，dmesg | grep zswap可以看到zswap正常加载，并且选择后端压缩内存分配器<br>的打印log。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">zswap: loaded using pool lzo/zbud</span><br></pre></td></tr></table></figure></li>
<li><p>zswap的控制参数也可以在/sys/module/zswap/parameters/*里配置。</p>
<p>compressor 选择压缩算法，enabled使能zswap功能，max_pool_percent配置压缩内存<br>池占系统内存的百分比(zswap是先把压缩的内存放在一个zpool里), zpool是后端压缩<br>内存分配器，这里是zbud, same_filled_pages_enable使能对内存值相同的情况做优化<br>处理。</p>
<p>zswap的debug信息可以在/sys/kernel/debug/zswap/*里显示。</p>
</li>
<li><p>如果系统没有swap分区，测试之前需要给系统加上swap分区，可以对一个空闲的磁盘分<br>区做：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkswap /dev/sda</span><br><span class="line">swapon /dev/sda</span><br><span class="line">用swapon -s可以查看系统中已经有的swap分区。</span><br></pre></td></tr></table></figure></li>
<li><p>如果要看下zswap是否运行，需要构造系统内存被大量使用的场景。如果在服务器上，<br>这样的场景不好构造。可以使用cgroup构造。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cgcreate -g memory:bob</span><br><span class="line">echo 0x4000000 &gt; /sys/fs/cgroup/memory/bob/memory.limit_in_bytes</span><br></pre></td></tr></table></figure>
<p>创建一个名字是bob的memory cgroup，限定在其中的进程使用内存大小是0x40MB。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cgexec -g memory:bob memhog 128m</span><br></pre></td></tr></table></figure>
<p>运行memhog这个程序，这个程序是一个内存测试程序，其操作的虚拟内存大小是128MB.</p>
<p>运行上面的命令后，查看/sys/kernel/debug/zswap/stored_pages，可以发现其值变<br>为非0。</p>
</li>
<li><p>以上的环境可以建在虚拟机里。可以基于<a href="https://wangzhou.github.io/Guest-and-host-communication-for-QEMU/">这里</a>的方法设置：</p>
<p>在host上使用 qemu-img create disk.img 1G 生成一个1G大小的虚拟磁盘。再在qemu<br>启动命令行里加上 -hdb path_of_disk/disk.img 把这个磁盘加给虚拟机。<br>再在虚拟机里配置以上各个步骤即可。如果需要调试带硬件加速的zswap，可以把硬件<br>的VF直通到虚拟机里调试。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>zswap</tag>
      </tags>
  </entry>
  <entry>
    <title>使用qemu虚拟机学习Linux内核</title>
    <url>/%E4%BD%BF%E7%94%A8qemu%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%A6%E4%B9%A0Linux%E5%86%85%E6%A0%B8/</url>
    <content><![CDATA[<p>当然可以在自己的PC机上装一个linux的发行版，然后写一些linux内核模块，插入到当前<br>的内核中。但是这样会使自己的PC机不稳定，内核模块有bug的话容易使得整个系统崩溃掉。<br>还有一点，现在做linux kernel开发的很多使用的不是X86的体系架构，如果做arm下的开发,<br>现在这样做也是不行的。本文以arm64为例子，介绍一个使用qemu虚拟机学习linux kernel<br>的环境。</p>
<ol start="0">
<li><p>PC机环境<br> 本人的PC机上的操作系统是ubuntu 14.04</p>
</li>
<li><p>qemu虚拟机</p>
<ol>
<li>下载qemu原码：<br> git clone git://git.qemu-project.org/qemu.git<br>(没有git的 sudo apt-get install git 安装一个)</li>
<li>编译arm64体系构架的qemu虚拟机：<br> cd qemu/<br>./configure -target-list=aarch64-softmmu<br>make<br> 在qemu/aarch64-softmmu下会有qemu-system-aarch64, 这就是我们将要用的虚拟机。</li>
</ol>
</li>
<li><p>linux kernel</p>
<ol>
<li>下载最新内核源码：<br> git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git</li>
<li>编译内核：<br> make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- defconfig<br>make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- Image<br> (需要提前下载好arm64的工具链，把工具链的路径加入PATH。这里CROSS_COMPILE示工具链而定)</li>
</ol>
</li>
<li><p>根文件系统<br> 根文件系统可以使用linaro网站上发布的openembedded的文件系统，如果遇到该文件系统里没有的<br> 工具，可以自己下载工具的源码，交叉编译后加入该文件系统中。</p>
<ol>
<li>下载openembedded文件系统：<br> <span class="exturl" data-url="aHR0cHM6Ly9yZWxlYXNlcy5saW5hcm8ub3JnL2xhdGVzdC9vcGVuZW1iZWRkZWQvYWFyY2g2NA==">https://releases.linaro.org/latest/openembedded/aarch64<i class="fa fa-external-link-alt"></i></span><br>中的vexpress64-openembedded_minimal-armv8-gcc-4.9_20141023-693.img.gz</li>
<li>解压：<br> gunzip vexpress64-openembedded_minimal-armv8-gcc-4.9_20140923-688.img.gz<br>mv vexpress64-openembedded_minimal-armv8-gcc-4.9_20140923-688.img fs.img</li>
<li>添加文件：<br> sudo kpartx -a fs.img<br>(没有kpartx的 sudo apt-get install kpartx 装一个)<br>sudo mount /dev/mapper/loopOp2 /mnt<br>这时可以访问文件系统，可以把自己编译的一些用户态程序放到文件系统中。初次尝试<br> 可以不做这一步骤。</li>
</ol>
</li>
<li><p>运行整个系统<br> ./qemu-system-aarch64 -machine virt -cpu cortex-a57 <br> -kernel ~/linux/arch/arm64/boot/Image <br> -drive if=none,file=/home/wangzhou/openembaded/fs.img,id=fs <br> -device virtio-blk-device,drive=fs <br> -append ‘console=ttyAMA0 root=/dev/vda2’ <br> -nographic</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>使用动态库实现私有驱动的加载</title>
    <url>/%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81%E5%BA%93%E5%AE%9E%E7%8E%B0%E7%A7%81%E6%9C%89%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%8A%A0%E8%BD%BD/</url>
    <content><![CDATA[<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">          +----------------------+</span><br><span class="line">          |       用户app        |</span><br><span class="line">          +----------------------+</span><br><span class="line">                     |</span><br><span class="line">                     |</span><br><span class="line">                     v</span><br><span class="line">          +----------------------+</span><br><span class="line">          |        api库         |</span><br><span class="line">          +----------------------+</span><br><span class="line">          /          |           \</span><br><span class="line">         /           |            \</span><br><span class="line">        v            v             v</span><br><span class="line">+----------+     +----------+     +----------+</span><br><span class="line">| drv lib1 |     | drv lib2 |     | drv lib3 | drv lib means driver libarary</span><br><span class="line">+----------+     +----------+     +----------+</span><br><span class="line">     |               |                 |</span><br><span class="line">     |               |                 |</span><br><span class="line">     v               v                 v</span><br><span class="line">+---------+      +---------+      +---------+</span><br><span class="line">|  hw 1   |      |  hw 2   |      |  hw 3   |   hw means hardware</span><br><span class="line">+---------+      +---------+      +---------+</span><br></pre></td></tr></table></figure>
<p> 如上图，用户通过api库提供的接口使用api库提供的功能，由于底层的硬件不一样，对应<br> 不同的驱动。为了方便, api库和各个驱动库被独立的编译成动态库。可以想像各个驱动<br> 库里需要实现api库里函数的各个回调函数，然后通过某种方式挂到api库里。我们考虑<br> 怎么组织软件实现这样的自动挂接。实际上rdma-core的代码里已经我们提供了实现的<br> 样板，我们下面把相关的骨架抽出来，简单demo下。</p>
<h2 id="设计demo代码"><a href="#设计demo代码" class="headerlink" title="设计demo代码"></a>设计demo代码</h2><p>app.c:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;                                                              </span><br><span class="line">#include &quot;api_dl.h&quot;                                                             </span><br><span class="line">                                                                                </span><br><span class="line">int main()                                                                      </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        int a = 1, b = 5, c = 0xffff;                                           </span><br><span class="line">                                                                                </span><br><span class="line">        c = add(a, b);                                                          </span><br><span class="line">        printf(&quot;a + b = %d\n&quot;, c);                                              </span><br><span class="line">                                                                                </span><br><span class="line">        c = multi(a, b);                                                        </span><br><span class="line">        printf(&quot;a * b = %d\n&quot;, c);                                              </span><br><span class="line">                                                                                </span><br><span class="line">        return 0;                                                               </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>api_dl.h:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int add(int a, int b);                                                          </span><br><span class="line">int multi(int a, int b);                                                         </span><br></pre></td></tr></table></figure>
<p>api_dl.c:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;                                                              </span><br><span class="line">#include &lt;dlfcn.h&gt;                                                              </span><br><span class="line">#include &quot;api_internal.h&quot;                                                       </span><br><span class="line">                                                                                </span><br><span class="line">struct api_driver *global_driver;                                               </span><br><span class="line">                                                                                </span><br><span class="line">void __set_driver(struct api_driver *drv)                                       </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        global_driver = drv;                                                    </span><br><span class="line">&#125;                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">int add(int a, int b)                                                           </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        return global_driver-&gt;add(a, b);                                        </span><br><span class="line">&#125;                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">int multi(int a, int b)                                                         </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        return global_driver-&gt;multi(a, b);                                      </span><br><span class="line">&#125;                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">static void __attribute__((constructor)) open_driver_dl(void)                    </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        void *driver_dl;                                                        </span><br><span class="line">                                                                                </span><br><span class="line">        driver_dl = dlopen(&quot;./libdriver.so&quot;, RTLD_NOW);                         </span><br><span class="line">        if (!driver_dl)                                                         </span><br><span class="line">                printf(&quot;Fail to open libdriver\n&quot;);                             </span><br><span class="line">&#125;                                                                               </span><br></pre></td></tr></table></figure>

<p>api库中针对驱动库的头文件(api_internal.h):</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct api_driver &#123;                                                             </span><br><span class="line">        int (*add)(int a, int b);                                               </span><br><span class="line">        int (*multi)(int a, int b);                                             </span><br><span class="line">&#125;;                                                                              </span><br><span class="line">                                                                                </span><br><span class="line">void __set_driver(struct api_driver *drv);                                      </span><br><span class="line">                                                                                </span><br><span class="line">#define SET_DRIVER(drv)                                                 \       </span><br><span class="line">static void __attribute__((constructor)) set_driver(void)               \       </span><br><span class="line">&#123;                                                                       \       </span><br><span class="line">        __set_driver(drv);                                              \       </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>一个驱动库的实现:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &quot;api_internal.h&quot;                                                       </span><br><span class="line">                                                                                </span><br><span class="line">static int add(int a, int b)                                                    </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        return a + b;                                                           </span><br><span class="line">&#125;                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">static int multi(int a, int b)                                                  </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        return a * b;                                                           </span><br><span class="line">&#125;                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">struct api_driver one_driver = &#123;                                                </span><br><span class="line">        .add = add,                                                             </span><br><span class="line">        .multi = multi,                                                         </span><br><span class="line">&#125;;                                                                              </span><br><span class="line">                                                                                </span><br><span class="line">SET_DRIVER(&amp;one_driver);                                                         </span><br></pre></td></tr></table></figure>

<p>编译app, api库，驱动库的命令:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># build libdriver.so                                                            </span><br><span class="line">        gcc -shared -fPIC -o libdriver.so driver_dl.c                           </span><br><span class="line">                                                                                </span><br><span class="line"># build libapi.so                                                               </span><br><span class="line">        gcc -shared -fPIC -o libapi.so api_dl.c                                 </span><br><span class="line">                                                                                </span><br><span class="line"># build app                                                                     </span><br><span class="line">        gcc -o app app.c -L. -lapi -ldl                                         </span><br></pre></td></tr></table></figure>

<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p> 可以看到app在加载libapi.so的时候可以先打开libdriver.so，libdrver.so在打开的时候<br> 调用libapi.so中提供给下层驱动的接口把驱动结构体的指针传给libapi.so里的驱动指针。</p>
<p> 这里把libapi里打开的驱动库名字写死了，实际上，我们应该通过某种选择机制把这个<br> 搞成动态可以配置的, 这样就可以选择加载不同的驱动库。这个动态的机制是另一个独立<br> 的逻辑，可以通过配置文件的方式，用户提前把需要加载的驱动库写到一个配置文件里，<br> libapi库通过读这个配置文件知道自己要加载哪个驱动库。</p>
]]></content>
      <tags>
        <tag>软件架构</tag>
        <tag>动态库</tag>
      </tags>
  </entry>
  <entry>
    <title>图解密码技术笔记</title>
    <url>/%E5%9B%BE%E8%A7%A3%E5%AF%86%E7%A0%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li>监听</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-----+               +-----+</span><br><span class="line">|  A  |------+------&gt; |  B  |</span><br><span class="line">+-----+      |        +-----+</span><br><span class="line">             v</span><br><span class="line">           +-----+</span><br><span class="line">           |  C  |</span><br><span class="line">           +-----+</span><br></pre></td></tr></table></figure>
<p>比如A发送信息给B，可能有C在链路上监听。应对的方法就是对要发送的信息加密, 比如我<br>们可以把发送的信息码字都加上1，B减去1就得到A原来发送的信息。这里就引入了密码学<br>上的几个基本概念, 这里对发送的信息加密就是原来的信息和额外的信息做运算，我们把<br>额外的信息叫做秘钥，把所做的运算叫做加解密算法。秘钥一般是不公开的，对于加解密<br>算法，有公开的算法，也有商业公司自己保密的算法，不过公开的算法的安全性要大大高于<br>私有的算法，现在一般的做法也是密码相关的行业组织会公开征集特定用途的算法，大家<br>通过竞争选出最好的算法。</p>
<p>可以看到上面的例子中，A用来加密的秘钥和B用来解密的秘钥是一样的。这种加解密的<br>方法叫对称加解密。实际使用中，秘钥和加解密算法是很复杂的，C即使听到了加密信息，<br>没有秘钥也解不出A发出的信息。</p>
<p>但是这样的加解密方法有一个要解决的问题: 怎么把一样的秘钥分发给A和B(前面我们已经<br>提到算法是公开的)。A直接传给B显然是不靠谱的，因为C完全可以听到。引入第三方，也不<br>靠谱，只要传输，C就可以听到秘钥。</p>
<p>为了应对是上面的问题，人们发明了非对称加解密。也就是说A用来加密的秘钥和B用来解密<br>的秘钥不是同一个。非对称加解密使用的方法就是: 把加密秘钥发给对方，请对方用这个加密<br>秘钥加密信息, 解密秘钥自己留着，用来解密信息。下面的图是基本流程:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> +-----+                      +-----+</span><br><span class="line"> |  A  |---------------------&gt;|  B  |</span><br><span class="line"> +-----+                      +-----+</span><br><span class="line"></span><br><span class="line">           +-------+</span><br><span class="line">           |pub key|  --&gt; </span><br><span class="line"> +-----+   +-------+          +-----+</span><br><span class="line"> |  A  |---------------------&gt;|  B  |</span><br><span class="line"> +-----+                      +-----+     +-------+   +---+</span><br><span class="line">                                          |message| + |key| --+</span><br><span class="line">             +--------------+             +-------+   +---+   |</span><br><span class="line">          &lt;--|crypto message| &lt;-------------------------------+</span><br><span class="line"> +-----+     +--------------+ +-----+</span><br><span class="line"> |  A  |---------------------&gt;|  B  |</span><br><span class="line"> +-----+                      +-----+</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">+--------+   +--------------+</span><br><span class="line">|priv key| + |crypto message|     </span><br><span class="line">+--------+ | +--------------+</span><br><span class="line">           v</span><br><span class="line">+-----------------+</span><br><span class="line">|message sent by B|</span><br><span class="line">+-----------------+</span><br></pre></td></tr></table></figure>
<p>可以看到C即使听到pub key也没有用，因为pub key是用来给发给A的信息加密的。上面又<br>有几个新的概念，公钥是可以公开的秘钥，私钥必须保密。</p>
<p>我们平时用的ssh用的就是非对称加密, 和上面的模型是可以对上的。非对称加密解决了<br>对称加密里对称秘钥分发的问题，我们可以使用非对称加解密代替对称加解密。当然，我们<br>也可以一开始采用非对称加解密解决对称加解密秘钥分发的问题，然后就可以使用对称<br>加解密了。</p>
<p>可以看到，非对称加解密的缺点是C可以拿到给A发信息用的加密秘钥。这样C可以把自己的<br>公钥发给B，诱导B用C的公钥加密信息，这样C监听B发给A的信息，就可以用C自己的私钥<br>解密信息。为了解决这个问题，就需要B可以确认他收到的是A的公钥。(to do: how to do)</p>
<p>非对称加解密还有一个缺点，就是秘钥生成和加解密都需要很大的算里。所以很适合做专门<br>的硬件加速器去offload cpu资源。</p>
<p>这里可以看下RSA算法步骤，RSA算法是经典的非对称加解密算法。我们这里只简单罗列RSA<br>算法的步骤，因为他表现的很简单有趣。这里E和N为加密秘钥，D和N为解密秘钥。加密和<br>解密做的运算就是:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                  E                            D</span><br><span class="line">密文 = 明文  mod N           明文 = 密文  mod N</span><br></pre></td></tr></table></figure>
<p>是不是很简单? 上面的密文和明文都是数字，加解密完全是一个求幂然后取模运算。但是<br>E, D是一个很大的数，大到用二进制表示，需要512bit，1024bit，2048bit，4096bit…<br>不同秘钥长度，当然密码的强度是不一样的。</p>
<p>E, D, N生成的算法是：</p>
<ol>
<li><p>找两个很大的素数p, q</p>
</li>
<li><p>N = p × q</p>
</li>
<li><p>L = 最小公倍数(p - 1, q - 1)</p>
</li>
<li><p>E为和L互质的数 —&gt; (E, N)是加密秘钥，公钥</p>
</li>
<li><p>(E × D) mod L = 1, 求出D —&gt; (D, N)是解密秘钥，私钥</p>
</li>
</ol>
<p>第2,5步的逆运算在数学上很困难，这个是RSA算法保密的根本。</p>
<ol start="2">
<li>篡改</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-----+                      +-----+</span><br><span class="line">|  A  |------+  +-----------&gt;|  B  |</span><br><span class="line">+-----+      |  |            +-----+</span><br><span class="line">             v  |</span><br><span class="line">           +----++</span><br><span class="line">           |  C  |</span><br><span class="line">           +-----+</span><br></pre></td></tr></table></figure>
<p>相比较上面的被动攻击，C还可以篡改A发给B的数据，然后发给B(其实上面也提到了主动<br>攻击, 我们先从简单的说起)。为了应对这样的攻击，B需要有办法验证收到信息的完整性。<br>一般用单向散列得到A发出信息的hash码，然后A把这个hash码也发给B，B对收到的信息做<br>同样的单向散列，把得到的hash码和收到的hash作对比，从而验证信息的完整性。</p>
<p>这要的算法有SHA3, MD5等。其实，一般我们发送大文件的时候，用md5sum算文件hash码,<br>然后在对接收到的文件做校验，就是一样的道理。</p>
<p>单向散列算法需要保证的就是防止碰撞发生，简单说就是不同数据得到的hash码要不一样。</p>
<p>单向散列只是单纯的验证数据的完整性，并不能确认数据就是A发出的。C可以把A发给B的<br>数据和hash值都截获，修改数据，然后对修改后的数据做下单向散列，然后把修改的数据<br>和新生成的hash值发给B。</p>
<ol start="3">
<li>认证</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-----+                      +-----+</span><br><span class="line">|  A  |--------------------&gt; |  B  |</span><br><span class="line">+-----+       +------------&gt; +-----+</span><br><span class="line">              |</span><br><span class="line">           +--+--+</span><br><span class="line">           |  C  |</span><br><span class="line">           +-----+</span><br></pre></td></tr></table></figure>
<p>如上，C也做下单向散列，然后把正确的hash码发送给B。B需要有办法确认他收到信息是A<br>发给他的，而不是别人伪造的。这里的问题和上面非对称加解密里C把自己的公钥发给B是<br>一样的问题。</p>
<p>当A，B有共享秘钥的时候，解决这个问题的方法是，信息和共享秘钥一起做单向散列。<br>但是非对称加密公钥配送的问题还是没有解决</p>
<p>对上面认证(也叫消息认证码)的攻击有: 重放攻击。有了消息认证码之后C无法篡改A发给B<br>的信息，但是C可以截获A发给B的信息，在A发给B信息后，再次发同样的信息给B。比如，A<br>给B的信息是请求B向A转一笔钱，那么B接收的C的信息后将会再次转钱给A。不过，应对这种<br>重放攻击，A只要在发送的信息加上编号或者是时间戳就好了。</p>
<p>消息认证码无法解决的问题: 对第三方证明，防止否认。对第三方证明比较好理解, 共享<br>秘钥存在于A和B，第三方的机构无法证明信息是A向B发的。防止否认的意思是, B也无法证<br>明A确实向B发了一条消息。其中的关键是消息认证码用的是共享秘钥, 比如A向B发了一个信<br>息说向B借了10万元钱(相当于A给B写了一个欠条)，这个信息用消息认证码加密, 当B拿着这<br>个消息向A去索要钱的时候，A完全可以否认自己发过这个消息，因为B也有A一样的秘钥，完<br>全是可以自己制造这个消息出来的。为了解决A否认消息是他发出的，就要引入数字签名，<br>就是给自己发出的信息签上自己的名字。</p>
<ol start="4">
<li>签名</li>
</ol>
<hr>
<p>如何给自己发出的消息上标记上自己无法否认的信息作为自己的签名？签名可以用非对称<br>加解密的相反运算实现: 用私钥加密信息给信息签名，用公钥解密加密的信息做验证。<br>因为私钥只有自己知道，而且用和私钥不对应的公钥算出来的数据不是被加密数据(算出<br>的数据接近随机值), 所以可以用这样的办法实现数字签名。</p>
<p>可以看出来，签名和签名验证的计算量是很大的，比如用RSA算法做签名，相当于对数据做<br>乘幂运算再做取模运算。当然，签名可以对数据的单向散列值做, 不过即使这样计算量也大。<br>可以看出签名和非对称加解密是可以用一个硬件加速器来做的。这里先做单向散列再做签名<br>是多个加解密算法级联使用的一个例子，在实际情况中这种情况很多，这对软件加解密框架<br>的设计提出了比较高的要求，关于这部分的设计实现可以参考Linux内核crypto子系统的<br>设计实现，同时该子系统也是c语言面向对象编程的一个很好的参考实例。</p>
<p>有很多针对签名的攻击, 其中利用签名解密信息的攻击很有意思。签名的本质是用私钥做<br>运算, 所做的运算如果数据是正好是加密数据，那么相当于做解密运算。攻击者如果手上<br>有一个别人发给签名者的加密信息(签名者对应的公钥加过密), 攻击者请求签名者给这段<br>信息签名, 如果签名，实际上签名者就被攻击者诱导做了解密运算。不过，一般情况下签名<br>者也不会对一段来历不明的信息做签名。</p>
<p>签名还有一个注意的地方是撤销签名，类比一下就是撕毁借条。但是数字签名无法撕毁，<br>办法就是再签一个说之前的签名无效了。</p>
<ol start="5">
<li>公钥证书</li>
</ol>
<hr>
<p>上面的各种办法其实还是没有办法解决非对称加解密里公钥配送的问题。</p>
<p>引入第三方机构办法公钥证书。</p>
<ol start="6">
<li>秘钥</li>
</ol>
<hr>
<p>上面各种加解密算法里多次提到秘钥。这里的秘钥其实就是一个巨大的数字。除了公钥<br>秘钥必须严格保密，因为加解密算法是公开的，秘钥的价值非常巨大，他的价值和明文是<br>相等的。</p>
<p>需要注意的是对称加解密里的秘钥配送还可以用秘钥配送去实现，比较经典的有DH算法。<br>直观的看，就是A和B预定几个数值(明文传送，C是可见的)，然后用公共的算法得到一个<br>相同的数值, C即使知道预定的值也无法计算出A和B的共享秘钥。</p>
<p>DH算法的一般步骤是:</p>
<p>秘钥管理里有秘钥作废比较有意思，秘钥作废之后要彻底销毁。这样做的原因是防止之前<br>的信息被解密，比如C长期截获并保存A和B之间的信息，如果C的到废弃的秘钥，他就可以<br>把之前的信息解密。</p>
<ol start="7">
<li>随机数</li>
</ol>
<hr>
<p>有些秘钥使用的是随机数，所以随机数是否是真随机这一点很关键。另外，并不是所有使用<br>随机数的地方都需要真随机数，但是加解密中使用的随机数一定要用真随机。单纯软件生<br>成的随机数一定是伪随机数。</p>
<p>和加解密类似，随机数的生成包括算法和种子，随机数生成算法类比加解密算法，是公开<br>的，种子类似秘钥，一定是私密的而且必须是真随机的。因为纯软件的种子一定不是随机<br>的，所以上面提到单纯软件生成的随机数一定是伪随机数。</p>
<p>Intel上硬件上已经提供了真随机数生成的指令。</p>
<ol start="8">
<li>PGP</li>
</ol>
<hr>
<ol start="9">
<li>SSL/TSL</li>
</ol>
<hr>
<ol start="10">
<li>区块链</li>
</ol>
<hr>
<ol start="11">
<li>硬件</li>
</ol>
<hr>
<p> SEC, HPRE, ZIP, RDE, TRNG, DMA</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>加解密</tag>
      </tags>
  </entry>
  <entry>
    <title>在qemu虚拟机上安装Linux发行版</title>
    <url>/%E5%9C%A8qemu%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%8A%E5%AE%89%E8%A3%85Linux%E5%8F%91%E8%A1%8C%E7%89%88/</url>
    <content><![CDATA[<ol>
<li><p>qemu-img create -f qcow2 debian.img 10G</p>
</li>
<li><p>sudo kvm -hda debian.img -cdrom debian-10.2.0-amd64-netinst.iso -m 2048</p>
<p>这一步是把这个debian的iso安装到debian.img这个文件上。</p>
</li>
<li><p>qemu command:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-x86_64 -cpu host -enable-kvm -smp 4 \</span><br><span class="line">-m 1G \</span><br><span class="line">-kernel ~/repos/linux/arch/x86/boot/bzImage \</span><br><span class="line">-append &quot;console=ttyS0 root=/dev/sda1&quot; \</span><br><span class="line">-hda ./debian.img \</span><br><span class="line">-nographic \</span><br></pre></td></tr></table></figure>
<p>如上启动虚拟机，可以发现自己已经可以使用如上安装的debian系统。我们在第二步<br>安装系统的时候可以把需要的程序都装上，在这样的虚拟机里做测试，会方便很多。<br>而且你在虚拟机里创建的文件下次启动虚拟机的时候都还在。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>多核启动基本逻辑分析</title>
    <url>/%E5%A4%9A%E6%A0%B8%E5%90%AF%E5%8A%A8%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> 计算机系统有多个核的时候，多个核之间启动的时候要遵守一定的逻辑关系。虽然，多核系统<br> 上每个核都可以独立的运行程序，但是总会有多个核共享的资源，对于这些资源配置和访问<br> 需要串行，比如，固件或者内核的BSS段，再比如固件的重定位，这些都只需要搞一次就好，<br> 一般就用一个核搞定就好，其他核后续可以再此基础上继续做各自核的初始化。</p>
<p> 本文分析多核启动中的这种逻辑关系。</p>
<h2 id="硬件逻辑"><a href="#硬件逻辑" class="headerlink" title="硬件逻辑"></a>硬件逻辑</h2><p> 我们从qemu启动多核看看硬件是怎么看待多核启动的。qemu里每个vcpu用一个线程模拟，<br> 多核的启动流程大概是:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">device_set_realized</span><br><span class="line">  +-&gt; riscv_cpu_realize</span><br><span class="line">    +-&gt; qemu_init_vcpu</span><br><span class="line">      [...]</span><br><span class="line">          /* 如下是vcpu模拟要执行的线程函数，代码在qemu/accel/tcg/tcg-accel-ops-mttcg.c */</span><br><span class="line">      +-&gt; mttcg_cpu_thread_fn</span><br><span class="line">    +-&gt; cpu_reset</span><br><span class="line">      +-&gt; device_cold_reset</span><br><span class="line">        +-&gt; resettable_reset</span><br><span class="line">          +-&gt; resettable_assert_reset</span><br><span class="line">            +-&gt; resettable_phase_hold</span><br><span class="line">                  /* CPU复位函数，CPU的初始状态在这里配置 */</span><br><span class="line">              +-&gt; riscv_cpu_reset</span><br><span class="line">    +-&gt; mcc-&gt;parent_realize (cpu_common_realizefn)</span><br><span class="line">      +-&gt; cpu_resume</span><br></pre></td></tr></table></figure>
<p> 其中我们要搞清楚CPU复位配置初始状态和CPU开始运行是怎么衔接起来的。如上，在qemu_init_vcpu<br> 里会拉起模拟vcpu的线程，该线程的主体逻辑就是在一个大循环里反复做取指令，翻译和执行，<br> 但是，CPU复位是在后面的cpu_reset里面才执行的。vcpu做取指令，翻译和执行之前会先<br> 判断CPU的状态，如果CPU在停止状态，就一直等待，qemu_init_vcpu一进来就会配置CPU在<br> stopped状态，这样vcpu的模拟线程起来也是在等待的状态。riscv_cpu_realize最后会调用<br> 父类的realize函数，也就是CPUClass的realize函数，这个里面会调用cpu_resume把CPU设置<br> 到可以运行的状态。(to check)</p>
<p> 总结下，从qemu的角度看，多核启动就是多个核独立开始执行指令，这个设计其实把多核启动<br> 的控制逻辑给到了随后的固件和内核。</p>
<p> qemu和opensbi的接口比较有意思，我们具体看下这里的实现。qemu在启动BIOS(opensbi)<br> 之前，会运行一小段rom上的指令，这段指令core id配置到a0，dts基地址配置到a1，qemu和<br> opensbi传递信息的一片内存的地址配置到a2。下面我们逐行来看看这段代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* hw/riscv/virt.c, 有&quot;n:&quot;前缀是本文新加的注释 */</span><br><span class="line">riscv_setup_rom_reset_vec</span><br><span class="line">  [...]</span><br><span class="line">  uint32_t reset_vec[10] = &#123;</span><br><span class="line">      /*</span><br><span class="line">       * n: 需要注意的是，这里汇编语意和二进制语意是不一样的。从汇编语意上看，这</span><br><span class="line">       *    句的意思是，%pcrel_hi(fw_dyn)表示计算fw_dyn这个符号相对于当前PC的偏移</span><br><span class="line">       *    的高20bit，而auipc t0, imm表示把imm和当前PC相加，结果保存到t0。所以，</span><br><span class="line">       *    这条指令整体上的结果是加载fw_dyn地址的高20bit到t0。</span><br><span class="line">       *</span><br><span class="line">       *    t0这里实际上是当前PC的值。</span><br><span class="line">       *</span><br><span class="line">       *    从汇编上看这条指令和下面一条指令一起完成计算fw_dyn地址的功能。下面</span><br><span class="line">       *    一条指令的意思是，找见位置是label 1b的指令，计算pcrel_hi修饰的</span><br><span class="line">       *    符号和auipc这条指令的偏差的低12bit, 这个就是%pcrel_lo(1b)的值。</span><br><span class="line">       *    然后&quot;addi a2, t0，imm&quot;把fw_dyn这个符号的低12bit补上，写到a2里，所以</span><br><span class="line">       *    这里a2就是fw_dyn的地址。</span><br><span class="line">       */</span><br><span class="line">      0x00000297,                  /* 1:  auipc  t0, %pcrel_hi(fw_dyn) */</span><br><span class="line">      0x02828613,                  /*     addi   a2, t0, %pcrel_lo(1b) */</span><br><span class="line">      /* n: a0是当前核的id */</span><br><span class="line">      0xf1402573,                  /*     csrr   a0, mhartid  */</span><br><span class="line">      0,</span><br><span class="line">      0,</span><br><span class="line">      /* n: 如下，t0是opensbi的地址，这里就直接跳到opensbi了 */</span><br><span class="line">      0x00028067,                  /*     jr     t0 */</span><br><span class="line">      /* n: 下个bootloader的地址，一般rv上就是opensbi的加载地址 */</span><br><span class="line">      start_addr,                  /* start: .dword */</span><br><span class="line">      start_addr_hi32,</span><br><span class="line">      /* n: dts的基地址 */</span><br><span class="line">      fdt_load_addr,               /* fdt_laddr: .dword */</span><br><span class="line">      fdt_load_addr_hi32,</span><br><span class="line">                                   /* fw_dyn: */</span><br><span class="line">  &#125;;</span><br><span class="line">  if (riscv_is_32bit(harts)) &#123;</span><br><span class="line">      reset_vec[3] = 0x0202a583;   /*     lw     a1, 32(t0) */</span><br><span class="line">      reset_vec[4] = 0x0182a283;   /*     lw     t0, 24(t0) */</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">      /* n: a1存放的是dts的基地址 */</span><br><span class="line">      reset_vec[3] = 0x0202b583;   /*     ld     a1, 32(t0) */</span><br><span class="line">      /* n: t0存放的是下一跳的地址，就是opensbi的加载地址 */</span><br><span class="line">      reset_vec[4] = 0x0182b283;   /*     ld     t0, 24(t0) */</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="固件逻辑"><a href="#固件逻辑" class="headerlink" title="固件逻辑"></a>固件逻辑</h2><p> 我们直接看opensbi中的多核启动逻辑，具体opensbi的代码分析可以看<a href="https://wangzhou.github.io/opensbi%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">这里</a>。</p>
<p> 总体上看，opensbi大概分为汇编部分和C代码部分，这两部分都会涉及到多核启动的逻辑。<br> qemu rom启动opensbi时会有一个主核选择的逻辑，opensbi的三种固件类型在这个逻辑上基<br> 本都是用所谓lottery算法，只有fw_dynamic_version_2用的是指定核启动。opensbi在C代码<br> 部分会再次随机的选一个核做主核。</p>
<p> lottery算法的逻辑很直白，就是多个核去抢做主启动核，主核就一个人去做公共资源的初始化，<br> 其它核(从核)就等着，直到主核把公共资源初始化完，从核继续做每个核各自的初始化内容。</p>
<p> 从硬件的视角看，riscv qemu virt上的各个核在系统初始化后都立即投入运行, 多核启动<br> 的逻辑都用软件搞定的，其中选核可以用类似lottery的算法，从核可以用wfi挂起，主从核<br> 可以用IPI来通信。我们把整个逻辑描述在如下的图里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">汇编部分:</span><br><span class="line">               core0     core1  ...  coreN      &lt;--- 这里core0抢到了。如果是指定核启动，就不用抢了</span><br><span class="line">                 |</span><br><span class="line">                 |       +-&gt;-+       +-&gt;-+</span><br><span class="line">                 |       ^   v       ^   v</span><br><span class="line">                 |       +-&lt;-+       +-&lt;-+</span><br><span class="line">                 |         |           |</span><br><span class="line">                 |         |           |</span><br><span class="line">C:             sbi_init  sbi_init     sbi_init   &lt;--- 这里core1抢到了coldboot</span><br><span class="line">                 |         |           |</span><br><span class="line">               +-&gt;-+       |         +-&gt;-+</span><br><span class="line">          wfi  ^   v       |    wfi  ^   v  &lt;------- warmboot先等coldboot完成</span><br><span class="line">               +-&lt;-+ &lt;+    |      +&gt; +-&lt;-+</span><br><span class="line">                 |    |    |      |    |</span><br><span class="line">                 |    |    |      |    |</span><br><span class="line">                 |    +- wake up -+    |</span><br><span class="line">                 |     core0 and N     |</span><br><span class="line">                 |         |           |</span><br><span class="line">                 |         |           |</span><br><span class="line">            HSM STOPPED    |        HSM STOPPED</span><br><span class="line">               (wfi)       |           (wfi)</span><br><span class="line">               |  ^        |            ^  |</span><br><span class="line">               |  |        |            |  |</span><br><span class="line">               |  |        |            |  |</span><br><span class="line">               |  |       mret          |  |</span><br><span class="line">               |  |        |            |  |</span><br><span class="line">               |  |        |            |  |</span><br><span class="line">               |  |        |            |  |</span><br><span class="line">               |  +-sbi_hsm_hart_start  |  |</span><br><span class="line">               |      (core0, addr)     |  |</span><br><span class="line">               |    sbi_hsm_hart_start -+  |</span><br><span class="line">               |      (coreN, addr)        |</span><br><span class="line">               |           |               |</span><br><span class="line">              mret         |              mret</span><br><span class="line">               |           |               |</span><br><span class="line">               |           |               |</span><br></pre></td></tr></table></figure>
<p> 如上C部分依然使用lottery算法，选择一个核先作为主核，初始化公共资源，这个时候各个<br> 从核是挂起的，在主核准备进入内核前，主核通过IPI解开挂起的从核，不过从核执行走几步<br> 之后，又会挂起。从核继续运行需要内核通过sbi的HSM接口下发启动命令，使用的是HSM的<br> hart start接口，这个接口可以指定需要启动的hart以及执行内核(S mode)执行的起始地址。<br> 从后面内核分析中可以看出，从核的内核启动地址并不是_start，而是secondary_start_sbi。</p>
<h2 id="内核逻辑"><a href="#内核逻辑" class="headerlink" title="内核逻辑"></a>内核逻辑</h2><p> 内核多核启动的逻辑和上面的逻辑类似，只不过内核里公共的资源更多，从核启动的时候，<br> 在各个核之间做同步的时候注意的问题会更多，比如，内核页表在各个核之间的同步问题。<br> Linux内核riscv下启动汇编的分析可以<a href="https://wangzhou.github.io/Linux%E5%86%85%E6%A0%B8riscv-head-S%E5%88%86%E6%9E%90/">这里</a>。</p>
<p> riscv的多核启动有两种实现，一种是基于spinwait的，在这种启动模拟下，多核均直接进入<br> 内核，内核使用lottery算法选出一个主核，其它核spinwait，riscv内核Kconfig里提示，<br> 这种方式不支持CPU hotplug和sparse hartid，这种方式只用在只支持M mode或者是BIOS<br> 不支持SBI HSM扩展的情况。另一种是ordered booting，这种方式要依赖HSM扩展，配合上面<br> 提到的fw_dynamic_version_2从固定核启动的方式，opensbi只容许主核进到内核，然后内核<br> 通过HSM方法启动从核，我们下面重点看下ordered booting。</p>
<p> riscv ordered booting的基本逻辑： </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start_kernel</span><br><span class="line">      /* arch/riscv/kernel/setup.c */</span><br><span class="line">  +-&gt; setup_arch</span><br><span class="line">    +-&gt; setup_smp</span><br><span class="line">          /* 根据内核的配置方式，挂接sbi或者是spinwait的回调函数 */</span><br><span class="line">      +-&gt; struct cpu_operations cpu_ops[cpuid] = &amp;cpu_ops_sbi</span><br></pre></td></tr></table></figure>
<p> 如上是cpu_ops的配置逻辑，根据是否支持SBI，决定是挂接spinwait还是SBI的回调。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* linux/init/main.c，kernel_init会拉起一号进程，在这之前会在smp_init里拉起从核 */</span><br><span class="line">kernel_init</span><br><span class="line">  +-&gt; smp_init</span><br><span class="line">        /* 在一个循环里一个接一个的拉起从核 */</span><br><span class="line">    +-&gt; bringup_noboot_cpus</span><br><span class="line">          /* 拉起一个core的函数 */</span><br><span class="line">      +-&gt; cpu_up</span><br><span class="line">        +-&gt; _cpu_up</span><br><span class="line">          [...]</span><br><span class="line">          /* 最终会一路调到riscv的实现函数(linux/arch/riscv/kernel/smpboot.c) */</span><br><span class="line">          +-&gt; __cpu_up</span><br><span class="line">            +-&gt; start_secondary_cpu</span><br><span class="line">                  /*</span><br><span class="line">                   * sbi_cpu_start, 传入的启动参数包括：启动地址，task_struct指针，</span><br><span class="line">                   * 栈指针。sbi_cpu_start的接口通过a0传启动地址，通过a1传打包</span><br><span class="line">                   * task_struct指针和栈指针的数据结构的地址。opensbi中不处理这</span><br><span class="line">                   * 两个参数，在启动内核的时候又通过a0和a1传给内核，内核解析a1</span><br><span class="line">                   * 地址上的数据，得到task_struct指针和栈指针。</span><br><span class="line">                   */</span><br><span class="line">              +-&gt; cpu_ops[cpu]-&gt;cpu_start</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>计算机体系结构</tag>
        <tag>Linux内核</tag>
        <tag>opensbi</tag>
      </tags>
  </entry>
  <entry>
    <title>使用perf trace跟踪IO缺页</title>
    <url>/%E4%BD%BF%E7%94%A8perf-trace%E8%B7%9F%E8%B8%AAIO%E7%BC%BA%E9%A1%B5/</url>
    <content><![CDATA[<p>使用perf list可以看到有trace point的软件定义的trace点。(fix me: 要开什么内核选项)<br>这些软件定义的trace point点要在代码里提前预埋，执行程序的时候可以用perf trace<br>把需要的信息统计出来。</p>
<p>我们拿Linux内核里IOMMU统计IO缺页的event做例子看看。这个event的定义在:<br>include/trace/events/iommu.h里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TRACE_EVENT(dev_fault,</span><br><span class="line"></span><br><span class="line">	TP_PROTO(struct device *dev,  struct iommu_fault *evt),</span><br><span class="line"></span><br><span class="line">	TP_ARGS(dev, evt),</span><br><span class="line"></span><br><span class="line">	TP_STRUCT__entry(</span><br><span class="line">		__string(device, dev_name(dev))</span><br><span class="line">		__field(int, type)</span><br><span class="line">		__field(int, reason)</span><br><span class="line">		__field(u64, addr)</span><br><span class="line">		__field(u64, fetch_addr)</span><br><span class="line">		__field(u32, pasid)</span><br><span class="line">		__field(u32, grpid)</span><br><span class="line">		__field(u32, flags)</span><br><span class="line">		__field(u32, prot)</span><br><span class="line">	),</span><br><span class="line"></span><br><span class="line">	TP_fast_assign(</span><br><span class="line">		__assign_str(device, dev_name(dev));</span><br><span class="line">		__entry-&gt;type = evt-&gt;type;</span><br><span class="line">		if (evt-&gt;type == IOMMU_FAULT_DMA_UNRECOV) &#123;</span><br><span class="line">			__entry-&gt;reason		= evt-&gt;event.reason;</span><br><span class="line">			__entry-&gt;flags		= evt-&gt;event.flags;</span><br><span class="line">			__entry-&gt;pasid		= evt-&gt;event.pasid;</span><br><span class="line">			__entry-&gt;grpid		= 0;</span><br><span class="line">			__entry-&gt;prot		= evt-&gt;event.perm;</span><br><span class="line">			__entry-&gt;addr		= evt-&gt;event.addr;</span><br><span class="line">			__entry-&gt;fetch_addr	= evt-&gt;event.fetch_addr;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			__entry-&gt;reason		= 0;</span><br><span class="line">			__entry-&gt;flags		= evt-&gt;prm.flags;</span><br><span class="line">			__entry-&gt;pasid		= evt-&gt;prm.pasid;</span><br><span class="line">			__entry-&gt;grpid		= evt-&gt;prm.grpid;</span><br><span class="line">			__entry-&gt;prot		= evt-&gt;prm.perm;</span><br><span class="line">			__entry-&gt;addr		= evt-&gt;prm.addr;</span><br><span class="line">			__entry-&gt;fetch_addr	= 0;</span><br><span class="line">		&#125;</span><br><span class="line">	),</span><br><span class="line"></span><br><span class="line">	TP_printk(&quot;IOMMU:%s type=%d reason=%d addr=0x%016llx fetch=0x%016llx pasid=%d group=%d flags=%x prot=%d&quot;,</span><br><span class="line">		__get_str(device),</span><br><span class="line">		__entry-&gt;type,</span><br><span class="line">		__entry-&gt;reason,</span><br><span class="line">		__entry-&gt;addr,</span><br><span class="line">		__entry-&gt;fetch_addr,</span><br><span class="line">		__entry-&gt;pasid,</span><br><span class="line">		__entry-&gt;grpid,</span><br><span class="line">		__entry-&gt;flags,</span><br><span class="line">		__entry-&gt;prot</span><br><span class="line">	)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>在需要打点的地方插入一个trace_dev_fault(dev, evt)就好，其中dev是TP_PROTO里定义的<br>struct device *dev, evt是里面定义的struct iommu_fault *evt。</p>
<p>TP_STRUCT__entry定义记录结构里各个域段的定义。TP_fast_assign定义域段记录的值。<br>TP_printk定义打印的方式。</p>
<p>以UADK里一个测试用力为例，我们看看怎么用perf trace收集IO page fault的信息。具体<br>的运行命令如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ./perf trace -o log_sva -a -e iommu:* numactl --cpubind 1 --membind 1  \</span><br><span class="line">test_hisi_sec --perf --async --pktlen 1024 --block 8192 --blknum 100000 \</span><br><span class="line">--times 1000000 --multi 1 --ctxnum 1</span><br></pre></td></tr></table></figure>
<p>-o后面加需要存放log的文件。注意, 需要sudo权限，需要-a，不然无法看到<br>iommu:dev_fault的事件，另外这个用力要使用block 8192才会观察到iommu:dev_fault事件</p>
<p>观察到的log_sva里的记录可能是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   0.000 :0/0 iommu:unmap:IOMMU: iova=0x00000000fdb67000 size=4096 unmapped_size=4096</span><br><span class="line">   0.030 :0/0 iommu:unmap:IOMMU: iova=0x00000000fdabe000 size=4096 unmapped_size=4096</span><br><span class="line">   0.396 test_hisi_sec/115486 iommu:map:IOMMU: iova=0x00000000fbfe9000 paddr=0x000000217e0c8000 size=4096</span><br><span class="line">   0.432 test_hisi_sec/115486 iommu:unmap:IOMMU: iova=0x00000000fbfe9000 size=4096 unmapped_size=4096</span><br><span class="line">   0.444 test_hisi_sec/115486 iommu:map:IOMMU: iova=0x00000000fbfe9000 paddr=0x000000217e0c8000 size=4096</span><br><span class="line">   0.465 test_hisi_sec/115486 iommu:unmap:IOMMU: iova=0x00000000fbfe9000 size=4096 unmapped_size=4096</span><br><span class="line"> 671.920 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x000000002321c000 fetch=0x0000000000000000 pasid=1 group=138 flags=3 prot=1</span><br><span class="line"> 671.961 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000023220000 fetch=0x0000000000000000 pasid=1 group=119 flags=3 prot=1</span><br><span class="line"> 671.983 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000023230000 fetch=0x0000000000000000 pasid=1 group=158 flags=3 prot=1</span><br><span class="line"> 672.003 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000023234000 fetch=0x0000000000000000 pasid=1 group=132 flags=3 prot=1</span><br><span class="line"> 672.024 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x000000002323c000 fetch=0x0000000000000000 pasid=1 group=135 flags=3 prot=1</span><br><span class="line"> 672.041 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000023232000 fetch=0x0000000000000000 pasid=1 group=120 flags=3 prot=2</span><br><span class="line"></span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">1946.610 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000084d82000 fetch=0x0000000000000000 pasid=1 group=122 flags=3 prot=2</span><br><span class="line">1946.636 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000084da6300 fetch=0x0000000000000000 pasid=1 group=88 flags=3 prot=2</span><br><span class="line">1946.659 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000084d8a180 fetch=0x0000000000000000 pasid=1 group=86 flags=3 prot=2</span><br><span class="line">3031.527 :0/0 iommu:unmap:IOMMU: iova=0x00000000fdbe2000 size=4096 unmapped_size=4096</span><br><span class="line">3031.550 :0/0 iommu:unmap:IOMMU: iova=0x00000000fdb68000 size=4096 unmapped_size=4096</span><br><span class="line">3031.499 test_hisi_sec/115486 iommu:map:IOMMU: iova=0x00000000fbfe9000 paddr=0x000000217e0c8000 size=4096</span><br><span class="line">3031.557 test_hisi_sec/115486 iommu:unmap:IOMMU: iova=0x00000000fbfe9000 size=4096 unmapped_size=4096</span><br></pre></td></tr></table></figure>

<p>除了用perf trace跟踪，也可以用ftrace跟踪。这需要ftrace的目录下(一般在/sys/kernel/debug/tracing)<br>的event里使能对应的trace point点，这样再去trace就可以看到输出的打印。</p>
<p>也可以在需要跟踪的地方简单的加一个trace_printk()的打印，把对应的模块写到<br>set_ftrace_filter: echo ‘:mod:xxx_module_name’ &gt; set_ftrace_filter。然后再去<br>trace，也可以看到输出的打印。</p>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>在qemu里增加指令的前端解码</title>
    <url>/%E5%9C%A8qemu%E9%87%8C%E5%A2%9E%E5%8A%A0%E6%8C%87%E4%BB%A4%E7%9A%84%E5%89%8D%E7%AB%AF%E8%A7%A3%E7%A0%81/</url>
    <content><![CDATA[<h2 id="qemu基础逻辑"><a href="#qemu基础逻辑" class="headerlink" title="qemu基础逻辑"></a>qemu基础逻辑</h2><p> qemu虚拟机提供两种CPU实现的方式，一种是基于中间码的实现，一种是基于KVM的实现。</p>
<p> 第一种方式，我们一般叫tcg(tiny code generator)，这种方式的基本思路是用纯软件的<br> 方式把target CPU的指令先翻译成所谓的中间码，然后再把中间码翻译成host CPU 的指令，<br> 我们把target CPU指令翻译成中间码的过程叫整个过程的前端，中间码翻译成host CPU的<br> 过程对应的叫做后端。给qemu增加一个新CPU的模型需要既增加前端也增加后端，如果要把<br> 整个系统支持起来，还要增加基础设备以及user mode的支持，整个系统的支持的逻辑不在<br> 本文档里展开，本文只聚焦于前端的相关逻辑。如果目的是在一个成熟的平台上验证另一个<br> 新的CPU，比如在x86机器上跑riscv的虚拟机，验证riscv的逻辑，只需要加上riscv指令到<br> 中间码这个前端支持就好，因为中间码到x86的后端已经存在；如果目的是，比如在一台riscv<br> 的机器上模拟x86，就需要加中间码到riscv的后端支持，我们这里以riscv为例子，把它作为<br> 一个需要支持的CPU构架。</p>
<p> 一个完整的riscv到中间码前端支持的例子可以参考：<br> <span class="exturl" data-url="aHR0cHM6Ly9sb3JlLmtlcm5lbC5vcmcvYWxsLzE1MTkzNDQ3MjktNzM0ODItMS1naXQtc2VuZC1lbWFpbC1tamNAc2lmaXZlLmNvbQ==">https://lore.kernel.org/all/1519344729-73482-1-git-send-email-mjc@sifive.com<i class="fa fa-external-link-alt"></i></span></p>
<p> 以riscv为例子，体系相关的前端的代码在：target/riscv/，后端的代码在：tcg/riscv/，<br> 基础外设和machine的代码在hw/riscv/</p>
<p> 基于KVM的方式，直接使用host CPU执行target CPU的指令，性能接近host上的性能，但是<br> 需要target CPU和host CPU是相同的构架。本文不讨论KVM的逻辑。</p>
<h2 id="qemu-tcg前端解码逻辑"><a href="#qemu-tcg前端解码逻辑" class="headerlink" title="qemu tcg前端解码逻辑"></a>qemu tcg前端解码逻辑</h2><p> 把target cpu指令翻译成host cpu指令有两种方式，一种是使用helper函数，一种是使用<br> tiny code generator函数的方式。helper函数的方式还没有分析，现在只看tcg的方式。</p>
<p> 我们把逻辑拉高一层看问题，所谓target CPU的运行，实际上是根据target CPU指令流去<br> 不断的改变target CPU软件描述结构里的数据状态，因为实际的代码要运行到host CPU上，<br> 所以，target代码要被翻译成host代码，才可以执行，通过执行改变target CPU的数据状态。<br> qemu为了解耦把target CPU代码先翻译成中间码，那么翻译成的中间码的语义也就是改变<br> target CPU数据状态的一组描述语句，所以target CPU状态参数会被当做入参传入中间码<br> 描述语句。这组中间码是改变CPU状态的抽象的描述，有些CPU上的状态不好抽象成一般的<br> 描述就用helper函数的方式补充，所以helper函数也是改变target CPU状态的描述。</p>
<p> tcg的方式，我们要使用tcg_gen_xxx的函数组织逻辑描述target CPU指令对target CPU状态<br> 的改变。一些公共的代码是可以自动生成的，qemu里使用decode tree的方式自动生成这一部<br> 分代码。</p>
<p> 以riscv的代码来具体说明。qemu定义了一组target CPU指令的描述格式，说明文档在：<br> docs/devel/decodetree.rst，riscv的指令描述在target/riscv/insn16.decode、insn32.decode<br> 里，qemu编译的时候会解析.decode文件，使用脚本(scripts/decodetree.py)生成对应的<br> 定义和函数，生成的文件放在qemu/build/libqemu-riscv64-softmmu.fa.p/decode-insn32.c.inc，<br> decode-insn16.c.inc里。这些文件生成的trans_xxx函数需要自己实现，riscv的这部分实现<br> 是放在了在target/riscv/insn_trans/*里。生成的文件里有两个很大的解码函数decode-insn32.c.inc和<br> decode-insn16.c.inc，qemu把target CPU指令翻译成中间码的时候就需要调用上面两个解码<br> 函数。</p>
<p> 我们用riscv user mode的代码来看看上层具体调用关系。qemu提供system mode和user mode<br> 的模拟方式，其中system mode会完整模拟整个系统，一个完整的OS可以运行在这个模拟的<br> 系统上，user mode只是支持加载一个target CPU构架的用户态程序来跑，对于一般指令<br> 使用tcg的方式翻译执行，对于用户态程序里的系统调用，user mode代码里模拟实现了系统<br> 调用的语意。linux user mode的代码在qemu/linux-user/*，具体的调用过程如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* qemu/linux-user/main.c */</span><br><span class="line">main</span><br><span class="line">  +-&gt; cpu_loop</span><br><span class="line">    +-&gt; cpu_exec</span><br><span class="line">      +-&gt; tb_gen_code</span><br><span class="line">      |     /* qemu/target/riscv/trannslate.c */</span><br><span class="line">      | +-&gt; gen_intermediate_code</span><br><span class="line">      | | +-&gt; translator_loop(&amp;riscv_tr_ops, xxx)</span><br><span class="line">      | |       /* riscv_tr_translate_insn */</span><br><span class="line">      | |   +-&gt; ops-&gt;translator_insn</span><br><span class="line">      | |     +-&gt; decode_ops</span><br><span class="line">      | |       +-&gt; decode_insn16</span><br><span class="line">      | |       +-&gt; decode_insn32</span><br><span class="line">      | +-&gt; tcg_gen_code</span><br><span class="line">      |   +-&gt; tcg_out_xxx</span><br><span class="line">      +-&gt; cpu_loop_exec_tb</span><br></pre></td></tr></table></figure>
<p> gen_intermediate_code是前端的解码函数，把target CPU的指令翻译成tcg中间码。tcg_gen_code<br> 是后端，把中间码翻译成host CPU上的指令，其中tcg_out_xxx的一组函数做具体的翻译工作。</p>
<p> 基本逻辑就是这样。下面展开其中的各个细节看下，细节上大概有这么几块：</p>
<ol>
<li>tcg整个翻译流程构架分析</li>
<li>decode tree的语法</li>
<li>tcg trans_xxx函数的语法</li>
</ol>
<h2 id="tcg翻译流程"><a href="#tcg翻译流程" class="headerlink" title="tcg翻译流程"></a>tcg翻译流程</h2><p> 整个tcg前后端的翻译流程按指令块的粒度来搞，收集一个指令块翻译成中间码，然后把<br> 中间码翻译成host CPU指令，整个过程动态执行。为了加速翻译，qemu把翻译成的host<br> CPU指令块做了缓存，tcg前端解码的时候，先在缓存里找，如果找见就直接执行。</p>
<p> 大致的代码调用关系如上。</p>
<h2 id="decode-tree语法"><a href="#decode-tree语法" class="headerlink" title="decode tree语法"></a>decode tree语法</h2><p> 因为CPU指令编码总是一组一组的，就可以用decode去描述这些固定的结构，然后qemu根据<br> 这些指令定义，使用一个脚本(scripts/decodetree.py)在编译的时候生成解码函数的框架。</p>
<p> decode tree里定义了几个描述：field，argument，format，pattern，group。依次看看<br> 他们是怎么用的。只记录要注意的点，细节还是直接看decodetree.rst这个文档。</p>
<p> CPU在解码的时候总要把指令中的特性field中的数据取出作为入参(寄存器编号，立即数，操作码等)，<br> field描述一个指令编码中特定的域段，根据描述可以生成取对应域段的函数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------------------------+---------------------------------------------+</span><br><span class="line">| Input                     | Generated code                              |</span><br><span class="line">+===========================+=============================================+</span><br><span class="line">| %disp   0:s16             | sextract(i, 0, 16)                          |</span><br><span class="line">+---------------------------+---------------------------------------------+</span><br><span class="line">| %imm9   16:6 10:3         | extract(i, 16, 6) &lt;&lt; 3 | extract(i, 10, 3)  |</span><br><span class="line">+---------------------------+---------------------------------------------+</span><br><span class="line">| %disp12 0:s1 1:1 2:10     | sextract(i, 0, 1) &lt;&lt; 11 |                   |</span><br><span class="line">|                           |    extract(i, 1, 1) &lt;&lt; 10 |                 |</span><br><span class="line">|                           |    extract(i, 2, 10)                        |</span><br><span class="line">+---------------------------+---------------------------------------------+</span><br><span class="line">| %shimm8 5:s8 13:1         | expand_shimm8(sextract(i, 5, 8) &lt;&lt; 1 |      |</span><br><span class="line">|   !function=expand_shimm8 |               extract(i, 13, 1))            |</span><br><span class="line">+---------------------------+---------------------------------------------+</span><br></pre></td></tr></table></figure>
<p> 上面的定义中，一个数据，比如一个立即数，可能是多个域段拼成的，所以就有相应的<br> 移位操作，再比如有些立即数是编码域段的数值取出来后再进过简单运算得到的，field定义<br> 中带的函数就可以完成这样的计算。</p>
<p> argument用来定义数据结构，比如，riscv insn32.decode里定义的: &amp;b imm rs2 rs1，<br> 编译后的decode-insn32.c.inc里生成的数据结构如下，这个结构可以做trans_xxx函数的入参：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">typedef struct &#123;</span><br><span class="line">    int imm;</span><br><span class="line">    int rs2;</span><br><span class="line">    int rs1;</span><br><span class="line">&#125; arg_b;</span><br></pre></td></tr></table></figure>

<p> format定义指令的格式。比如；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@opr    ...... ra:5 rb:5 ... 0 ....... rc:5</span><br><span class="line">@opi    ...... ra:5 lit:8    1 ....... rc:5</span><br></pre></td></tr></table></figure>
<p> 比如上面就是对一个32bit指令编码的描述，.表示一个0或者1的bit位，描述里可以用<br> field、之前定义的filed的引用、argument的引用，field的引用还可以赋值。field可以<br> 用来匹配，argument用来生成trans_xxx函数的入参。</p>
<p> pattern用来定义具体指令。比如riscv32里的lui指令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lui      ....................       ..... 0110111 @u</span><br><span class="line"></span><br><span class="line">@u       ....................      ..... ....... &amp;u      imm=%imm_u          %rd</span><br><span class="line"></span><br><span class="line">&amp;u    imm rd</span><br><span class="line"></span><br><span class="line">%imm_u    12:s20                 !function=ex_shift_12</span><br><span class="line">%rd        7:5</span><br></pre></td></tr></table></figure>
<p> 上面把相关的formate、argument、field的定义也列了出来。可以看到lui的操作码是0110111，<br> 这个指令的格式定义是@u，这个格式定义使用的参数定义是&amp;u，&amp;u就是trans_lui函数入参<br> 结构体里的变量的定义，其中定义的变量名字是imm、rd，这个imm实际的格式是%imm_i, 它<br> 是一个在指令编码31-12bit定义立即数，要把31-12bit的数值左移12bit得到最终结果，rd<br> 实际的格式是%rd，是一个在指令编码11-7bit定义的rd寄存器的标号。可以看到riscv里对应<br> 的trans函数的实现如下，在编译时，脚本只生成一个空函数，函数内容需要前端实现者编写。<br> 需要注意的是ex_shift_12这个函数是使用宏定义在target/riscv/translate.c里的，也是<br> 风骚。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static bool trans_lui(DisasContext *ctx, arg_lui *a)</span><br><span class="line">&#123;</span><br><span class="line">    if (a-&gt;rd != 0) &#123;</span><br><span class="line">        tcg_gen_movi_tl(cpu_gpr[a-&gt;rd], a-&gt;imm);</span><br><span class="line">    &#125;</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 用生成的立即数更新rd寄存器。</p>
<p> group定指令解码的组合，这个用的不多。</p>
<h2 id="trans-xxx函数的逻辑"><a href="#trans-xxx函数的逻辑" class="headerlink" title="trans_xxx函数的逻辑"></a>trans_xxx函数的逻辑</h2><p> tcg的trans_xxx函数在qemu/tcg/README里很好的介绍，这个文档里介绍了中间码的整套<br> 指令，可以比较容易的把这套指令和对应的trans_xxx函数对上，trans_xxxx函数的作用是<br> 生成这些中间码指令。以riscv上的add指令为例看下，如下是trans_rvi.c.inc里add指令<br> 的模拟。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static bool trans_add(DisasContext *ctx, arg_add *a)</span><br><span class="line">&#123;</span><br><span class="line">    return gen_arith(ctx, a, &amp;tcg_gen_add_tl);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static bool gen_arith(DisasContext *ctx, arg_r *a,</span><br><span class="line">                      void(*func)(TCGv, TCGv, TCGv))</span><br><span class="line">&#123;</span><br><span class="line">    TCGv source1, source2;</span><br><span class="line">    source1 = tcg_temp_new();</span><br><span class="line">    source2 = tcg_temp_new();</span><br><span class="line"></span><br><span class="line">    gen_get_gpr(source1, a-&gt;rs1);</span><br><span class="line">    gen_get_gpr(source2, a-&gt;rs2);</span><br><span class="line"></span><br><span class="line">    (*func)(source1, source1, source2);</span><br><span class="line"></span><br><span class="line">    gen_set_gpr(a-&gt;rd, source1);</span><br><span class="line">    tcg_temp_free(source1);</span><br><span class="line">    tcg_temp_free(source2);</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void tcg_gen_addi_i64(TCGv_i64 ret, TCGv_i64 arg1, int64_t arg2);</span><br></pre></td></tr></table></figure>
<p> tcg_gen_addi_i64可以看到tcg_gen_add_tl的函数入参，riscv的add指令从target CPU的<br> rs1，rs2里取两个加数，相加后放到rd寄存里，所以上面gen_get_gpr就表示生成这样的<br> 中间码：把rs1/2位置上的数据存到source1/2位置上，gen_get_gpr的实现就是:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_gen_mov_tl(t, cpu_gpr[reg_num])</span><br><span class="line">  -&gt; tcg_gen_mov_i64</span><br><span class="line">    -&gt; tcg_gen_op2_i64(INDEX_op_mov_i64, ret, arg)</span><br><span class="line">      -&gt; tcg_gen_op2(opc, tcgv_i64_arg(a1), tcgv_i64_arg(a2))</span><br><span class="line">        -&gt; TCGOp *op = tcg_emit_op(opc);</span><br><span class="line">        -&gt; op-&gt;args[0] = a1;</span><br><span class="line">        -&gt; op-&gt;args[1] = a2;</span><br></pre></td></tr></table></figure>
<p> 可以看到最后生成的mov指令先挂到了一个链表里，后面的后端解码会把这些指令翻译成<br> host指令，生成的指令就是qemu/tcg/README里介绍的mov_i32/i64 t0, t1这个指令。这里<br> 有几个逻辑要打通: 1. tcg_temp_new创建的变量存在哪里; 2. cpu_gpr[reg_num]是一个<br> 全局变量，它如何索引到target CPU的寄存器。</p>
<p> 首先tcg_temp_new分配的空间是在TCGContext tcg_ctx里的，所谓创建一个这样的TCGv就是<br> 在tcg_ctx里用去一个TCGTemp。cpu_gpr[reg_num]可以索引到target CPU寄存器的基本逻辑<br> 是，其实只要在前端和后端约定好描述target CPU的软件结构，cpu_gpr[reg_num]描述的就<br> 时相关寄存器在这个软件结构里的位置。我们再看下这个cpu_gpr[]的初始化逻辑和tcg_ctx<br> 的初始化逻辑，以及后端的编码逻辑就可以打通整个逻辑。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_translate_init</span><br><span class="line">  -&gt; cpu_gpr[i] = tcg_global_mem_new(cpu_env, offsetof(CPURISCVState, gpr[i]), riscv_int_regnames[i]);            </span><br></pre></td></tr></table></figure>
<p> cpu_env在tcg_context_init(unsigned max_cpus)里初始化，得到的是tcg_ctx里TCGTemp temps<br> 的地址。tcg_global_mem_new一次在tcg_ctx里从TCGTemp temps上分配空间，返回空间在<br> tcg_ctx上的相对地址。这样cpu_gpr[reg_name]就可以作为标记在前端和后端之间建立连接。</p>
<p> 后端的代码直接把中间码翻译成host指令，中间码中的TCGv直接映射到host CPU的寄存器上，<br> 从逻辑上讲，应该是翻译得到的host代码修改中间码对应TCGv对应的内存才对。这里的基本<br> 逻辑是qemu在生成的中间码中以及TB执行后做了host寄存器到target CPU描述内存之间的<br> 同步。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* qemu/tcg/riscv/tcg-target.c.inc */</span><br><span class="line">/* tcg_out_op是整个后端解码体系架构相关的入口函数，每个架构都要做具体实现 */</span><br><span class="line">tcg_out_op</span><br><span class="line">  -&gt; case INDEX_op_add_i64</span><br><span class="line">    -&gt; tcg_out_opc_reg</span><br><span class="line">      -&gt; tcg_out32</span><br></pre></td></tr></table></figure>
<p> 可以看到add_i64的中间码直接翻译到了host上的寄存器，这里后端的翻译还是拿riscv举例了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_gen_code</span><br><span class="line">     /* 如上提到的同步代码逻辑在这个函数中 */</span><br><span class="line">  -&gt; default: tcg_reg_alloc_op</span><br><span class="line">    -&gt; 生成用host指令描述的同步逻辑，放在TB里</span><br><span class="line">       /* 生成业务相关的host指令，后端译码的总入口 */</span><br><span class="line">    -&gt; tcg_out_op</span><br></pre></td></tr></table></figure>
<p> 如上是同步的一个大概逻辑，具体细节需要进一步分析。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>对称加解密</title>
    <url>/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E8%A7%A3%E5%AF%86/</url>
    <content><![CDATA[<ol>
<li>常用算法</li>
</ol>
<hr>
<p>对称加解密常用的算法有DES，3DES，AES, SM4。</p>
<p>DES算法现在已经破解，明文64bit，密文64bit，秘钥64bit(实际56bit，每隔7bit有一个<br>错误检查bit)。</p>
<p>3DES是DES计算三次(加密，解密，加密), 有三个64bit秘钥。</p>
<p>AES(提交时的算法名字叫Rijndael, 比利时人设计), 分支固定长度是128bit，秘钥长度可以<br>是128,192或者256bit。</p>
<p>SM4是中国自己的一种分组密码标准。分组长度和秘钥都是128bit。</p>
<p>这里有一个密码算法的分类: 分组密码和流密码。一般，上面提到的密码都采用分组密码。<br>流密码的意思是，对一串数据流进行加密，加密过程存在中间状态。</p>
<ol start="2">
<li>分组密码的模式</li>
</ol>
<hr>
<p>使用分组密码加密的时候，要把数据切成一块一块的分组(block)。根据分组之间的不同关系，<br>我们有不同的分组密码工作模式。模式有：ECB，CBC, CFB，OFB, CTR, XTS等。</p>
<p>ECB(Electronic CodeBook), block之间完全没有关系的分组加密。不推荐使用。</p>
<p>CBC(Cipher Block Chaining), 加密的逻辑如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">            +---------+      +---------+      +---------+</span><br><span class="line">            |明文分组1|      |明文分组2|      |明文分组3|</span><br><span class="line">            +---------+      +---------+      +---------+</span><br><span class="line">                v                v                 v</span><br><span class="line">              +---+            +---+             +---+</span><br><span class="line">       +-----&gt;|XOR|       +---&gt;|XOR|       +----&gt;|XOR|</span><br><span class="line">       |      +---+       |    +---+       |     +---+</span><br><span class="line">       |        v         |      v         |       v</span><br><span class="line">       |      +----+      |    +----+      |     +----+</span><br><span class="line">       |      |加密|      |    |加密|      |     |加密|</span><br><span class="line">       |      +----+      |    +----+      |     +----+</span><br><span class="line">       |        v         |      v         |       v</span><br><span class="line">+--+   |    +---------+   |  +---------+   |  +---------+</span><br><span class="line">|IV|---+    |密文分组1|---+  |密文分组2|---+  |密文分组3|</span><br><span class="line">+--+        +---------+      +---------+      +---------+</span><br></pre></td></tr></table></figure>
<p>注意，IV(初始化向量)是一个分组长度的一个随机数。<br>CBC解密的逻辑如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">            +---------+      +---------+      +---------+</span><br><span class="line">            |明文分组1|      |明文分组2|      |明文分组3|</span><br><span class="line">            +---------+      +---------+      +---------+</span><br><span class="line">                ^                ^                 ^</span><br><span class="line">              +---+            +---+             +---+</span><br><span class="line">       +-----&gt;|XOR|       +---&gt;|XOR|       +----&gt;|XOR|</span><br><span class="line">       |      +---+       |    +---+       |     +---+</span><br><span class="line">       |        ^         |      ^         |       ^</span><br><span class="line">       |      +----+      |    +----+      |     +----+</span><br><span class="line">       |      |解密|      |    |解密|      |     |解密|</span><br><span class="line">       |      +----+      |    +----+      |     +----+</span><br><span class="line">       |        ^         |      ^         |       ^</span><br><span class="line">+--+   |    +---------+   |  +---------+   |  +---------+</span><br><span class="line">|IV|---+    |密文分组1|---+  |密文分组2|---+  |密文分组3|</span><br><span class="line">+--+        +---------+      +---------+      +---------+</span><br></pre></td></tr></table></figure>
<p>可以看到CBC加密只能一个block一个block的顺序进行。解密时，如果一个密文分组出了问题<br>会影响到当前和下一个明文分组。翻转密文分组中的一个bit，会使得下一个明文分组中的<br>对应bit翻转，会使得当前明文分组中的若干个bit变化，接收方无法识别这种攻击，需要<br>引入消息认证码解决。</p>
<p>CFB(Cipher FeedBack), 密文反馈模式。OFB(Output-Feedback), 输出反馈模式。<br>CTR(Counter模式)。这几种模式的模型差不多，区别在于秘钥流生成的方式不一样。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     +--------+   	         +--------+                     +--------+</span><br><span class="line">            |明文分组|                  |明文分组|                     |明文分组|</span><br><span class="line">            +--------+                  +--------+                     +--------+</span><br><span class="line">                |                           |       +--------+             |     </span><br><span class="line">                v                           v       |        |             v     </span><br><span class="line">   +----+     +---+          +----+       +---+     |  +---+ |  +----+   +---+   </span><br><span class="line">+-&gt;|加密|----&gt;|XOR|       +-&gt;|加密|--+---&gt;|XOR|     +-&gt;|CTR|-+-&gt;|加密|--&gt;|XOR|   </span><br><span class="line">|  +----+     +---+       |  +----+  |    +---+        +---+    +----+   +---+   </span><br><span class="line">|               |         |          |      |                              |     </span><br><span class="line">|               v         |          |      v                              v     </span><br><span class="line">|           +--------+    |          |  +--------+                     +--------+</span><br><span class="line">+-----------|密文分组|    +----------+  |密文分组|                     |密文分组|</span><br><span class="line">            +--------+                  +--------+                     +--------+</span><br><span class="line">   </span><br><span class="line">     CFB                             OFB                           CTR</span><br></pre></td></tr></table></figure>
<p>这几种模式的明文和密文之间都只差一个异或操作，唯一不同的是异或操作的另一个操作数。<br>CFB用的是上次生成的密文加密得到；OFB用IV反复加密得到；CTR用一个计数器反复加密得到。<br>可以看到CFB的每次操作必须要依赖上一次的加密结果，OFB可以提前准备每次要用的异或值，<br>CTR甚至可以直接得到某个分组需要用的异或值，前后依赖程度逐个变小。</p>
<p>XTS模式主要用于磁盘加解密，可以做到block之间独立的加解密。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        +------------+            +----+</span><br><span class="line">        |128bit tweak|            |明文|</span><br><span class="line">        +------------+            +----+</span><br><span class="line">             |                      v</span><br><span class="line">             |                    +---+</span><br><span class="line">             |                +---+XOR|</span><br><span class="line">             |                |   +---+</span><br><span class="line">             v                |     v</span><br><span class="line">+----+     +----+     +--+    |   +----+      +----+</span><br><span class="line">|key2|----&gt;|加密|----&gt;|GF|----+   |加密|&lt;-----|key1|</span><br><span class="line">+----+     +----+     +--+    |   +----+      +----+</span><br><span class="line">                              |     v</span><br><span class="line">                              |   +---+</span><br><span class="line">                              +--&gt;|XOR|</span><br><span class="line">                                  +---+</span><br><span class="line">                                    v</span><br><span class="line">                                  +----+</span><br><span class="line">                                  |密文|</span><br><span class="line">                                  +----+</span><br></pre></td></tr></table></figure>

<p>Note: 消息认证码中的GCM模式依赖上述的CTR模式;基于CBC的消息认证码是CCM;使用hash<br>      来生成消息认证码(带秘钥的hash)叫HMAC。</p>
]]></content>
      <tags>
        <tag>加解密</tag>
      </tags>
  </entry>
  <entry>
    <title>如何尝试使用Linux SVA</title>
    <url>/%E5%A6%82%E4%BD%95%E5%B0%9D%E8%AF%95%E4%BD%BF%E7%94%A8Linux-SVA/</url>
    <content><![CDATA[<h2 id="硬件确认"><a href="#硬件确认" class="headerlink" title=" 硬件确认"></a> 硬件确认</h2><p> 首先你要有一台KunPeng920服务器，而且这台服务器上的压缩解压缩设备是可见的。你可以<br> lspci -s 75:00.0 -vv</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# lspci -s 75:00.0</span><br><span class="line">75:00.0 Processing accelerators: Device 19e5:a250 (rev 21)</span><br></pre></td></tr></table></figure>
<p> 如上，说明你的系统上有这个压缩解压缩的设备。</p>
<p> 系统的SMMU要在UEFI里打开。你可以看下系统启动日志，dmesg | grep iommu</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# dmesg | grep iommu</span><br><span class="line">[...]</span><br><span class="line">[   19.410490] hisi_zip 0000:75:00.0: Adding to iommu group 14</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p> 如上，可以认为SMMU的配置没有问题，当然group的编号可以是不同的。</p>
<h2 id="内核配置和编译"><a href="#内核配置和编译" class="headerlink" title="内核配置和编译"></a>内核配置和编译</h2><p> 目前内核的相关补丁还没有完全上主线，我们在Linaro的github上维护了一个完整的可以<br> 跑的分支：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0xpbmFyby9saW51eC1rZXJuZWwtd2FycGRyaXZlL3RyZWUvdWFjY2UtZGV2ZWw=">https://github.com/Linaro/linux-kernel-warpdrive/tree/uacce-devel<i class="fa fa-external-link-alt"></i></span></p>
<p> make defconfig</p>
<p> make menuconfig</p>
<p> 这里defconfig的配置是不够的，你需要确保如下的内核配置是打开的:<br>    CONFIG_ARM_SMMU_V3=y<br>    CONFIG_PCI_PASID=y<br>    CONFIG_IOMMU_SVA=y<br>    CONFIG_CRYPTO_DEV_HISI_QM=y<br>    CONFIG_CRYPTO_DEV_HISI_ZIP=y<br>    CONFIG_UACCE=y                                                                  </p>
<p> 然后编译内核即可。</p>
<h2 id="用户态代码配置和编译"><a href="#用户态代码配置和编译" class="headerlink" title="用户态代码配置和编译"></a>用户态代码配置和编译</h2><p> 对应的用户态代码的仓库也在Linaro的github上：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0xpbmFyby93YXJwZHJpdmUvdHJlZS9tYXN0ZXI=">https://github.com/Linaro/warpdrive/tree/master<i class="fa fa-external-link-alt"></i></span></p>
<p> ./autogen.sh<br> ./conf.sh<br> make</p>
<p> 在.lib目录下会生成编译出的用户态库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Sherlock@EstBuildSvr1:~/repos/linaro_wd/warpdrive/.libs$ ls *.so</span><br><span class="line">libhisi_qm.so  libwd_ciper.so  libwd_comp.so  libwd_digest.so  libwd.so</span><br></pre></td></tr></table></figure>
<p> 在test目录下有编译好的测试app：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test_sva_bind test_sva_perf</span><br></pre></td></tr></table></figure>
<p> 如上的两个测试app基于压缩解压缩设备，所以依赖的库是：<br> libhisi_qm.so libwd_comp.so libwd.so</p>
<h2 id="运行测试用例"><a href="#运行测试用例" class="headerlink" title="运行测试用例"></a>运行测试用例</h2><p> 使用如上编译好的内核Image启动系统, 把libhisi_qm.so libwd_comp.so libwd.so<br> 拷贝到系统上，然后尝试运行下 test_sva_perf。如果运行OK的话会有性能数据打印出来：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@ubuntu:/home/sherlock/warpdrive/test# ./test_sva_perf </span><br><span class="line">Compress bz=512000 nb=1×10, speed=1433.5 MB/s (±0.0% N=1) overall=1334.3 MB/s (±0.0%)</span><br></pre></td></tr></table></figure>

<p> test_sva_bind test_sva_perf里各个命令参数的用法可以参考help说明。</p>
]]></content>
      <tags>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>如何使用meson构建程序</title>
    <url>/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8meson%E6%9E%84%E5%BB%BA%E7%A8%8B%E5%BA%8F/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p>meson是用python写的一个程序构建工具，meson的官网在<span class="exturl" data-url="aHR0cHM6Ly9tZXNvbmJ1aWxkLmNvbS9pbmRleC5odG1s">这里<i class="fa fa-external-link-alt"></i></span>，这里有meson的使用手册，<br>这个手册很好用。meson和make一样，需要写描述文件告诉meson要构建什么，这个描述文件<br>就是meson.build，meson根据meson.build中的定义生成具体的构建定义文件build.ninja，<br>ninja根据build.ninja完成具体构建。所以，不像make直接根据Makefile文件完成构建，meson<br>需要和ninja配合一起完成构建。</p>
<p>我们通过一个简单程序具体看下使用meson的方法，具体使用meson还是要学习下如上官网上<br>的手册。</p>
<p>首先在源码根目录下创建meson.build文件，文件内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">project(&#x27;learn_meson&#x27;, &#x27;c&#x27;)</span><br><span class="line">executable(&#x27;hello&#x27;, &#x27;test.c&#x27;)</span><br></pre></td></tr></table></figure>
<p>这个文件定义了一个learn_meson的工程，并且定义了hello这个构建目标，以及test.c构建<br>使用的源文件。</p>
<p>在需要构建的源码根目录运行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">meson setup builddir</span><br></pre></td></tr></table></figure>
<p>这个是告诉meson在哪个目录下构建(这里是源码根目录下的builddir目录)，meson一定要在<br>一个和源码独立的目录里做构建，这样多次构建可以指定不同的构建目录和构建配置，相互<br>之间不受影响，比如对于同样的程序，构建一个riscv版本可以这样指定构建目录：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">meson setup --cross-file ./rv_cross_file rv_builddir</span><br></pre></td></tr></table></figure>
<p>其中，rv_cross_file是指定一些构建要用的参数，当然你的系统里要有riscv的工具链。<br>rv_cross_file内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[host_machine]</span><br><span class="line">system = &#x27;linux&#x27;</span><br><span class="line">cpu_family = &#x27;riscv64&#x27;</span><br><span class="line">cpu = &#x27;riscv64&#x27;</span><br><span class="line">endian = &#x27;little&#x27;</span><br><span class="line"></span><br><span class="line">[properties]</span><br><span class="line">c_args = []</span><br><span class="line">c_link_args = []</span><br><span class="line"></span><br><span class="line">[binaries]</span><br><span class="line">c = &#x27;riscv64-linux-gnu-gcc&#x27;</span><br><span class="line">cpp = &#x27;riscv64-linux-gnu-g++&#x27;</span><br><span class="line">ar = &#x27;riscv64-linux-gnu-ar&#x27;</span><br><span class="line">ld = &#x27;riscv64-linux-gnu-ld&#x27;</span><br><span class="line">objcopy = &#x27;riscv64-linux-gnu-objcopy&#x27;</span><br><span class="line">strip = &#x27;riscv64-linux-gnu-strip&#x27;</span><br></pre></td></tr></table></figure>

<p>运行如上命令后可以在源码根目录下发现对应的构建目录，里面有build.ninja文件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sherlock@m1:~/tests/meson_test/build$ ls</span><br><span class="line">build.ninja  compile_commands.json  hello.p  meson-info  meson-logs  meson-private</span><br></pre></td></tr></table></figure>

<p>在源码根目录运行meson compile -C builddir，在builddir目录下即可以看到编译好的hello，<br>可以看到编译好的hello是动态链接的。进入builddir，运行meson configure可以看到default_library<br>一项是shared，meson configure显示构建的配置，默认为动态链接，可以使用如下命令修改<br>为静态链接：(注意，要在builddir下运行)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">meson configure -Ddefault_library=static</span><br></pre></td></tr></table></figure>
<p>再次meson compile -C builddir即可只构建出静态链接的程序。</p>
<h2 id="一个例子：编译glib"><a href="#一个例子：编译glib" class="headerlink" title="一个例子：编译glib"></a>一个例子：编译glib</h2><p>如下的方式可以本地编译glib:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd glib_source_dir</span><br><span class="line">meson setup build</span><br><span class="line">meson compile -C build</span><br></pre></td></tr></table></figure>

<p>如果要静态编译可以：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd build</span><br><span class="line">meson configure -Ddefault_library=static</span><br><span class="line">cd ../</span><br><span class="line">meson compile -C build</span><br></pre></td></tr></table></figure>

<p>glib库会编译出libglib/libgio/libgmodule，如果想只编译libglib，直观的办法是可以修改<br>meson.build文件，我们可以把相关的模块这样注释掉，这样就可以只编译libglib：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff --git a/meson.build b/meson.build                                          </span><br><span class="line">index 0cbc9689f..f5acd5f61 100644                                               </span><br><span class="line">--- a/meson.build                                                               </span><br><span class="line">+++ b/meson.build                                                               </span><br><span class="line">@@ -82,9 +82,9 @@ darwin_versions = [current + 1, &#x27;@0@.@1@&#x27;.format(current + 1, interface_age)]</span><br><span class="line">                                                                                </span><br><span class="line"> configinc = include_directories(&#x27;.&#x27;)                                           </span><br><span class="line"> glibinc = include_directories(&#x27;glib&#x27;)                                          </span><br><span class="line">-gobjectinc = include_directories(&#x27;gobject&#x27;)                                    </span><br><span class="line">-gmoduleinc = include_directories(&#x27;gmodule&#x27;)                                    </span><br><span class="line">-gioinc = include_directories(&#x27;gio&#x27;)                                            </span><br><span class="line">+# gobjectinc = include_directories(&#x27;gobject&#x27;)                                  </span><br><span class="line">+# gmoduleinc = include_directories(&#x27;gmodule&#x27;)                                  </span><br><span class="line">+# gioinc = include_directories(&#x27;gio&#x27;)                                          </span><br><span class="line"></span><br><span class="line">@@ -2387,11 +2387,11 @@ pkg = import(&#x27;pkgconfig&#x27;)                               </span><br><span class="line"> windows = import(&#x27;windows&#x27;)                                                    </span><br><span class="line"> subdir(&#x27;tools&#x27;)                                                                </span><br><span class="line"> subdir(&#x27;glib&#x27;)                                                                 </span><br><span class="line">-subdir(&#x27;gobject&#x27;)                                                              </span><br><span class="line">-subdir(&#x27;gthread&#x27;)                                                              </span><br><span class="line">-subdir(&#x27;gmodule&#x27;)                                                              </span><br><span class="line">-subdir(&#x27;gio&#x27;)                                                                  </span><br><span class="line">-subdir(&#x27;fuzzing&#x27;)                                                              </span><br><span class="line">+# subdir(&#x27;gobject&#x27;)                                                            </span><br><span class="line">+# subdir(&#x27;gthread&#x27;)                                                            </span><br><span class="line">+# subdir(&#x27;gmodule&#x27;)                                                            </span><br><span class="line">+# subdir(&#x27;gio&#x27;)                                                                </span><br><span class="line">+# subdir(&#x27;fuzzing&#x27;)                                                            </span><br></pre></td></tr></table></figure>

<p>我们考虑交叉编译出riscv版本的libglib，使用如上交叉编译的方法会在setup过程中会自动<br>下载libffi，考虑到libglib并没有依赖libffi，我们直接把meson.build中的libffi依赖描述<br>这一行注释掉：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[...]</span><br><span class="line">libm = cc.find_library(&#x27;m&#x27;, required : false)                                   </span><br><span class="line"># libffi_dep = dependency(&#x27;libffi&#x27;, version : &#x27;&gt;= 3.0.0&#x27;)  &lt;--- 注释掉这行</span><br><span class="line">                                                                                </span><br><span class="line">libz_dep = dependency(&#x27;zlib&#x27;)                                                   </span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p>这样最后可以只编译出riscv版本的libglib。</p>
<h2 id="meson具体特性介绍"><a href="#meson具体特性介绍" class="headerlink" title="meson具体特性介绍"></a>meson具体特性介绍</h2><p>如上meson的官网有详细介绍meson的各种特性，我们这边持续的总结下，总结的思路是用类比<br>的方式看看make上的特性在meson上是怎么样，然后我们看meson特有的特性。</p>
<ul>
<li>target和构建文件</li>
</ul>
<p>首先meson一定要像make一样，有描述构建target和构建依赖的语法，有自己的数据结构的<br>定义、函数方法的定义。上面用executable()定义编译的目标文件，如果目标是要构建库出来，<br>可以用library()，比如glib里定义libglib这个target是这样搞的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">libglib = library(&#x27;glib-2.0&#x27;,                                                   </span><br><span class="line">  glib_dtrace_obj, glib_dtrace_hdr,                                             </span><br><span class="line">  sources : [deprecated_sources, glib_sources],                                 </span><br><span class="line">  version : library_version,                                                    </span><br><span class="line">  soversion : soversion,                                                        </span><br><span class="line">  darwin_versions : darwin_versions,                                            </span><br><span class="line">  install : true,                                                               </span><br><span class="line">  # intl.lib is not compatible with SAFESEH                                     </span><br><span class="line">  link_args : [noseh_link_args, glib_link_flags, win32_ldflags],                </span><br><span class="line">  include_directories : configinc,                                              </span><br><span class="line">  link_with: [charset_lib, gnulib_lib],                                         </span><br><span class="line">  dependencies : [                                                              </span><br><span class="line">    gnulib_libm_dependency,                                                     </span><br><span class="line">    libiconv,                                                                   </span><br><span class="line">    libintl_deps,                                                               </span><br><span class="line">    libm,                                                                       </span><br><span class="line">    librt,                                                                      </span><br><span class="line">    libsysprof_capture_dep,                                                     </span><br><span class="line">    pcre2,                                                                      </span><br><span class="line">    platform_deps,                                                              </span><br><span class="line">    thread_dep,                                                                 </span><br><span class="line">  ],                                                                            </span><br><span class="line">  c_args : glib_c_args,                                                         </span><br><span class="line">  objc_args : glib_c_args,                                                      </span><br><span class="line">  gnu_symbol_visibility : &#x27;hidden&#x27;,                                             </span><br><span class="line">)                                                                               </span><br></pre></td></tr></table></figure>
<p>其中各个域段的语法要查meson library这个函数的具体定义，其中的dependencies域段表示<br>目标的依赖，而dependencies中的语段，比如，thread_dep，又是通过dependency生成的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">thread_dep = dependency(&#x27;threads&#x27;)                                            </span><br></pre></td></tr></table></figure>
<p>如上使用dependency定义的是公共的库，比如上面定义的是一个对libthreads库的依赖，自<br>定义的依赖要用declare_dependency，比如，glib里定义了subproject，pcre库作为其中的<br>一个subproject，glib在构建的时候，如果在subproject下面没有找见pcre库，就会根据相关<br>定义(pcre.wrap)去下载pcre的代码，然后一起编译，所以glib的meson配置里也要定义glib<br>对subproject里的pcre的依赖，这个定义就使用了declare_dependency:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* glib/subprojects/pcre2-10.42/meson.build */</span><br><span class="line">pcre2_8_lib = library(                                                          </span><br><span class="line">  &#x27;pcre2-8&#x27;,                                                                    </span><br><span class="line">  sources,                                                                      </span><br><span class="line">  include_directories: includes,                                                </span><br><span class="line">  c_args: [config_h_defs, &#x27;-DHAVE_CONFIG_H&#x27;, &#x27;-DPCRE2_CODE_UNIT_WIDTH=8&#x27;],      </span><br><span class="line">  version: pcre2_8_lib_version,                                                 </span><br><span class="line">  install: true,                                                                </span><br><span class="line">)                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">libpcre2_8 = declare_dependency(                                                </span><br><span class="line">  link_with: pcre2_8_lib,                                                       </span><br><span class="line">  include_directories: includes,                                                </span><br><span class="line">  compile_args: static_defs,                                                    </span><br><span class="line">)                                                                               </span><br></pre></td></tr></table></figure>
<p>这里定义了pcre2-8这个target，又生成一个libpcre2_8的依赖供glib使用，glib顶层memson.build<br>里又用dependency定义了下libpcre2_8得到pcre2，最终在libglib的target定义里引用到了<br>pcre2。</p>
<p>那么有了target，怎么定义和target相关的构建文件，比如，libglib依赖一些库，但是他<br>本身也有一堆.o要编译出来，这个怎么去表述。相关源文件在sources域段描述，这个是一个<br>列表，可以把需要编译的文件都加进来。</p>
<ul>
<li>和python很像的语法</li>
</ul>
<p>如上可以看出来meson的语法和python的很像，这可能和meson是用python写的有关系。看meson.build<br>文件的时候，直接可以套用python的基本数据的定义，比如，列表、元组、字典、函数、模块。</p>
<ul>
<li>compiler property/检测系统信息</li>
</ul>
<p>在传统的Linux configure中会生成一些函数，并在配置阶段编译运行，以此来检测系统的<br>基础配置。meson的compiler properties特性支持做相关的系统检测，基本做法是先用<br>compiler = meson.get_compiler()拿到所使用编译器的对象，然后就可以调用编译器对象的<br>各种方法做检测，可以检测的项目包括对特定代码的编译、链接、运行、头文件以及函数是否<br>存在等等，meson网站的Compiler properties章节有详细的介绍。</p>
<p>这里看一个glib里的具体例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cc = meson.get_compiler(&#x27;c&#x27;)</span><br><span class="line">[...]</span><br><span class="line">uint128_t_src = &#x27;&#x27;&#x27;int main() &#123;</span><br><span class="line">static __uint128_t v1 = 100;</span><br><span class="line">static __uint128_t v2 = 10;</span><br><span class="line">static __uint128_t u;</span><br><span class="line">u = v1 / v2;</span><br><span class="line">&#125;&#x27;&#x27;&#x27;</span><br><span class="line">if cc.compiles(uint128_t_src, name : &#x27;__uint128_t available&#x27;)</span><br><span class="line">  glib_conf.set(&#x27;HAVE_UINT128_T&#x27;, 1)</span><br><span class="line">endif</span><br></pre></td></tr></table></figure>
<p>这些先定义了一段要编译的代码，用uint128_t_src表示，然后用cc.compiles去编译，如果<br>可以编译成功就会执行下面的configure语句把这个信息先记录下来，后面会把这些信息写<br>入到meson生成的config文件(config.h)里，一般在config文件里用宏去定义一个个的配置<br>信息，在源码文件中include相关的config头文件使用这些配置信息。</p>
<ul>
<li>subproject/wrap</li>
</ul>
<p>在上文中已经提到过subproject，这里我们进一步看下subproject是怎么定义的，subproject<br>被定义在subproject目录下的wrap描述文件里，我们还是以pcre为例子，对应的wrap文件是<br>这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* glib/subproject/pcre2.wrap */</span><br><span class="line">[wrap-file]</span><br><span class="line">directory = pcre2-10.42</span><br><span class="line">source_url = https://github.com/PhilipHazel/pcre2/releases/download/pcre2-10.42/pcre2-10.42.tar.bz2</span><br><span class="line">source_filename = pcre2-10.42.tar.bz2</span><br><span class="line">source_hash = 8d36cd8cb6ea2a4c2bb358ff6411b0c788633a2a45dabbf1aeb4b701d1b5e840</span><br><span class="line">patch_filename = pcre2_10.42-2_patch.zip</span><br><span class="line">patch_url = https://wrapdb.mesonbuild.com/v2/pcre2_10.42-2/get_patch</span><br><span class="line">patch_hash = 350dc342b81a1611af43e5cc23f1b10453c7df51d5bb60ab9ee247daf03802bc</span><br><span class="line">wrapdb_version = 10.42-2</span><br><span class="line"></span><br><span class="line">[provide]</span><br><span class="line">libpcre2-8 = libpcre2_8</span><br><span class="line">libpcre2-16 = libpcre2_16</span><br><span class="line">libpcre2-32 = libpcre2_32</span><br><span class="line">libpcre2-posix = libpcre2_posix</span><br></pre></td></tr></table></figure>
<p>如上pcre的wrap文件中，source_url是pcre库下载的地址，这里是直接下载的发布版本，meson<br>也支持不同的配置格式，比如使用git直接git clone源代码，这个使用需要用[wrap-git]定义。<br>patch_url不是我们直观想到的给基础的pcre新增的path，而是meson的配置文件，对于不是<br>使用meson构建的软件(比如这里的pcre)，meson维护了一个叫wrapdb的配置库，从这里可以<br>下载对应的meson配置文件，patch_url就是对应的meson配置文件的链接。如果需要给基础库<br>打业务相关的patch，这个需要定义到diff_files域段。</p>
<ul>
<li>生成代码/configuration</li>
</ul>
<p>上面已经提到menson会生成配置文件，这里进一步看下相关的语法，meson说明文档里的configuration<br>章节有详细的介绍。如下是glib里的一个例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">glib_conf = configuration_data()</span><br><span class="line">[...]</span><br><span class="line">glib_conf.set(&#x27;HAVE_UINT128_T&#x27;, 1)</span><br><span class="line">[...]</span><br><span class="line">configure_file(output : &#x27;config.h&#x27;, configuration : glib_conf)</span><br></pre></td></tr></table></figure>
<p>先用configuration_data创建一个配置收集的对象glib_conf，然后不断的调用set方法把相关<br>的配置放到glib_conf，最后使用configuration_file把收集到的配置写入config.h文件，比如<br>config.h文件里就会写入如下的配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define HAVE_UINT128_T 1</span><br></pre></td></tr></table></figure>

<ul>
<li>option</li>
</ul>
<p>meson还提供了一个叫meson_options.txt(1.1之后的版本改成meson.option)的配置文件，用来<br>定义一些用户可以动态配置的组件，这个特性最常用的就是定义一些可选的feature，可以把<br>相关feature的value域段配置成disable，从而禁止这个feature。</p>
<ul>
<li>调试技巧: 修改build.ninja</li>
</ul>
<p>meson setup会首先在build目录下生成build.ninja，随后ninja再根据这个文件做构建，所以<br>可以修改这个文件，不去构建某些target，最直观的修改方法就是从target列表里去掉不想<br>构建的target。</p>
]]></content>
      <tags>
        <tag>编译链接</tag>
        <tag>meson</tag>
      </tags>
  </entry>
  <entry>
    <title>尝试使用gem5/konata跟踪CPU流水线</title>
    <url>/%E5%B0%9D%E8%AF%95%E4%BD%BF%E7%94%A8gem5-konata%E8%B7%9F%E8%B8%AACPU%E6%B5%81%E6%B0%B4%E7%BA%BF/</url>
    <content><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>gem5是业界常用的CPU性能仿真模型，和qemu不同，gem5可以模拟CPU内部的微架构。gem5模拟<br>过程可以记录各种日志信息，帮助CPU开发人员了解CPU的实际行状况，我们这里介绍的是gem5<br>运行过程记录CPU流水线各个阶段的时间信息。使用图形化工具可以把记录的信息在图上画出<br>来，人们可以非常直观的看到CPU的整个运行过程，这里我们使用的是一个叫Konata的画图工具。</p>
<h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><p>整个步骤大概分为：1. 下载并编译gem5; 2. 下载并配置Konata；3. 用一个简单程序进行<br>展示。</p>
<p>可以从官网下载gem5，不过我是从<span class="exturl" data-url="aHR0cHM6Ly9naXRlZS5jb20va292ZXJsdS9nZW01LmdpdA==">https://gitee.com/koverlu/gem5.git<i class="fa fa-external-link-alt"></i></span>下载的，可能会快<br>一点。测试是在ARM64 ubuntu 20.04下进行的，一口气apt install了全部gem5依赖的库。<br>然后在gem5代码根目录下使用：scons build/RISCV/gem5.opt -j4 编译riscv处理器的gem5。<br>我的编译机器就是做测试的机器，是一台M1的Macbook Air上的虚拟机，系统是ubuntu 20.04，<br>整个编译可能要十几分钟，需要注意的是，gem5链接需要的内存比较大，我的虚拟机是4GB<br>内存，链接会报错，调整到8GB就可以完成链接了。</p>
<p>从<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NoaW95YWRhbi9Lb25hdGEuZ2l0">https://github.com/shioyadan/Konata.git<i class="fa fa-external-link-alt"></i></span>下载Konata。直接运行Konata代码根目录下的<br>install.sh，然后在ubuntu的图形化界面下打开terminal，并在其中运行Konata代码下的konata.sh，<br>这时就会跳出Konata的图形化界面，可以通过File-&gt;open打开Konata自带的log文件，log文件<br>的路径在Konata/docs/kanata-sample-[12].png，需要解压缩一下得到log文件，一开始是压缩的。</p>
<p>随意写一个简单代码，然后静态编译下：riscv64-linux-gnu-gcc –static test.c -o test</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int add(int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">	return a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	int a = 1, b = 2, c = 0;</span><br><span class="line">	</span><br><span class="line">	c = add(1, 2);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在gem5代码根目录下运行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./build/RISCV/gem5.opt --debug-flags=O3PipeView --debug-file=trace.out configs/example/se.py --cpu-type=O3CPU --caches --cmd /home/sherlock/test</span><br></pre></td></tr></table></figure>
<p>输出log在gem5/m5out/trace.out，用如上同样的方法在Konata里打开trace.out，就可以得到<br>如下的pipeline图形化输出:<br><img src="/%E5%B0%9D%E8%AF%95%E4%BD%BF%E7%94%A8gem5-konata%E8%B7%9F%E8%B8%AACPU%E6%B5%81%E6%B0%B4%E7%BA%BF/gem5_konata.png" alt="Konata展示gem5 pipeline"><br>上图中横坐标是时间，单位是CPU的cycle，纵坐标是指令进入流水线的先后顺序，单位就是<br>一条指令，横坐标每个不同的颜色表示不同的流水线阶段，比如F是fetch、Dc是decode、Rn<br>是Rename、Is是issue、Cm是commit。图中明亮的部分是流水线中正常退休的指令，灰色部分<br>是被flush掉的指令，比如一开始CPU一拍取8条指令，实际上第一条指令就是一个跳转指令，<br>后面的1-15条指令都取错了，后面CPU检测到分支预测失败后就会flush掉之前错误取入的指令。</p>
<h2 id="Konata调试"><a href="#Konata调试" class="headerlink" title="Konata调试"></a>Konata调试</h2><p>如果你自己hack了Konata这个软件，怎么去调试它呢？在它的help有Toggle Dev Tool这个按钮，<br>点击它就可以显示出调试界面，调试界面点击source可以打开Konata的代码，你可以点击代码<br>行数位置添加断点，如果简单的问题可能可以大概猜到出问题的地方，遇到复杂的问题就要<br>去熟悉JS前端的知识了。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>gem5</tag>
        <tag>Konata</tag>
      </tags>
  </entry>
  <entry>
    <title>如何在qemu里增加一个虚拟设备</title>
    <url>/%E5%A6%82%E4%BD%95%E5%9C%A8qemu%E9%87%8C%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA%E8%99%9A%E6%8B%9F%E8%AE%BE%E5%A4%87/</url>
    <content><![CDATA[<h2 id="一个简易PCIe-DMA-engine设备的定义"><a href="#一个简易PCIe-DMA-engine设备的定义" class="headerlink" title="一个简易PCIe DMA engine设备的定义"></a>一个简易PCIe DMA engine设备的定义</h2><p> 内存拷贝比较消耗CPU资源，定义一个专用的DMA设备帮助CPU做内存拷贝，CPU把数据的地址<br> 和需要拷贝到的目的地址，配置到这个设备的相关寄存器里，然后启动数据拷贝，数据拷贝<br> 完成后，设备的相关寄存器的值改变，从而向软件报告任务完成。也可以通过设备中断的<br> 方式向软件报告任务完成。</p>
<p> 本文选DMA engine设备，完全和具体业务没有关系，只是因为这个业务模型比较简单，容易<br> 说明问题。</p>
<p> 具体数据搬移的实现也很简单：先把数据搬移到这个设备内部的buffer里，然后再把buffer<br> 里的数据搬移到目的地址。</p>
<p> 这个搬移的过程可以过IOMMU设备，也可以不过IOMMU，我们可以控制qemu系统，使得被模拟<br> 的平台有IOMMU设备或者没有IOMMU设备。</p>
<p> 如下是这个设备MMIO寄存器的具体定义:</p>
<p> 这个设备有一个32bit non-prefetch BAR, BAR base + 0x1000的位置是我们定义的寄存器。</p>
<table>
<thead>
<tr>
<th align="left">offset</th>
<th align="left">定义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">0x0(只写)</td>
<td align="left">原数据的地址。</td>
</tr>
<tr>
<td align="left">0x8(只写)</td>
<td align="left">搬移的目的地址。</td>
</tr>
<tr>
<td align="left">0x10(只写)</td>
<td align="left">0-31 bit保留，32-63 bit表示搬移数据的长度。</td>
</tr>
<tr>
<td align="left">0x18(读写)</td>
<td align="left">bit0 置1表示开始拷贝数据。bit32 置1表示数据拷贝完成。</td>
</tr>
</tbody></table>
<h2 id="qemu设备的实现"><a href="#qemu设备的实现" class="headerlink" title="qemu设备的实现"></a>qemu设备的实现</h2><ul>
<li><p>加入qemu编译系统</p>
<p>本文用来arm64平台的qemu虚拟机测试。首先我们要保证整个平台的编译运行。测试使用<br>的qemu的版本是6.1.0</p>
<p>我们用如下的命令编译和启动基础的qemu虚拟机：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">configure --target-list=aarch64-softmmu --enable-kvm</span><br><span class="line">make -j64</span><br><span class="line"></span><br><span class="line">qemu-system-aarch64 -cpu cortex-a57 \</span><br><span class="line">-smp 1 -m 1024M \</span><br><span class="line">-nographic \</span><br><span class="line">-machine virt,iommu=smmuv3 \</span><br><span class="line">-kernel ~/Image \</span><br><span class="line">-append &quot;console=ttyAMA0 earlycon root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">-initrd ~/rootfs.cpio.gz \</span><br></pre></td></tr></table></figure>
<p>其中，如果想去掉smmuv3这个设备可以把machine那一行改成-machine virt \</p>
<p>qemu使用meson来编译，我们只要在对应的meson.build文件中加入我们要编译的文件就好。<br>把这个DMA engine设备的代码放到qemu/hw/misc/dma_engine.c里，所以在qemu/hw/misc/meson.build<br>里加入如下的代码:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">softmmu_ss.add(when: &#x27;CONFIG_DMA_ENGINE&#x27;, if_true: files(&#x27;dma_engine.c&#x27;))</span><br></pre></td></tr></table></figure>
<p>表示如果配置CONFIG_DMA_ENGINE打开，就把dma_engine.c编译进来。</p>
<p>在qemu/hw/misc/Kconfig里加入CONFIG_DMA_ENGINE，并配置把他直接编译到qemu里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">config DMA_ENGINE</span><br><span class="line">    bool</span><br><span class="line">    default y</span><br></pre></td></tr></table></figure>
<p>完成如上的配置后，我们在hw/misc目录加的dma_engine.c就可以参加到qemu编译里来。</p>
</li>
<li><p>定义一个PCIe设备</p>
<p>如下[1]中是DMA engine PCIe设备的代码，我们可以套用这个模板创建其他的PCIe设备。<br>改变PCIDeviceClass里的vendor_id/device_id/revision/class_id的数值，虚拟设备<br>PCIe配置空间中的对应域段的值可以被改变。</p>
<p>class中的realize函数和instance_init函数是主要要实现函数。可以使用pci_register_bar<br>来给这个PCIe设备增加BAR，通过pcie_xxxxx_cap_init来给这个设备增加PCIe的各种capability。</p>
<p>qemu里对PCI和PCIe设备是分开模拟的，如果你要加PCIe设备相关的capability，需要<br>创建一个PCIe设备，这个需要interfaces定义成 INTERFACE_PCIE_DEVICE，以及为这个<br>设备加上PCIe extend capability，使用pcie_endpoint_cap_init就可以了。我们这里的<br>DMA engine就是一个PCIe设备。在qemu里需要通过一个PCIe RP把一个PCIe设备接入到系统<br>里。下面的章节会提到具体的qemu命令。</p>
<p>模拟设备的代码里，用DmaEngState表示被模拟的设备，因为他是一个PCIe设备，所以在<br>这个结构体一开始的位置放一个PCIDevice的结构，后面的DmaRawState的结构用来放和具体<br>业务相关的东西。</p>
<p>使用lspci可以看到我们模拟出的是这样一个PCIe设备：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># ./lspci -s 00:04.0 -vvv</span><br><span class="line">00:04.0 Class 00ff: Device 1234:3456 (rev 10)</span><br><span class="line">        Subsystem: Device 1af4:1100</span><br><span class="line">        Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx+</span><br><span class="line">        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-</span><br><span class="line">        Latency: 0</span><br><span class="line">        Interrupt: pin A routed to IRQ 55</span><br><span class="line">        IOMMU group: 0</span><br><span class="line">        Region 0: Memory at 10260000 (32-bit, non-prefetchable) [size=16K]</span><br><span class="line">        Capabilities: [e0] Express (v2) Root Complex Integrated Endpoint, MSI 00</span><br><span class="line">                DevCap: MaxPayload 128 bytes, PhantFunc 0</span><br><span class="line">                        ExtTag- RBE+ FLReset-</span><br><span class="line">                DevCtl: CorrErr- NonFatalErr- FatalErr- UnsupReq-</span><br><span class="line">                        RlxdOrd- ExtTag- PhantFunc- AuxPwr- NoSnoop-</span><br><span class="line">                        MaxPayload 128 bytes, MaxReadReq 128 bytes</span><br><span class="line">                DevSta: CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-</span><br><span class="line">                DevCap2: Completion Timeout: Not Supported, TimeoutDis- NROPrPrP- LTR-</span><br><span class="line">                         10BitTagComp- 10BitTagReq- OBFF Not Supported, ExtFmt+ EETLPPrefix+, MaxEETLPPrefixes 4</span><br><span class="line">                         EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-</span><br><span class="line">                         FRS-</span><br><span class="line">                         AtomicOpsCap: 32bit- 64bit- 128bitCAS-</span><br><span class="line">                DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR- 10BitTagReq- OBFF Disabled,</span><br><span class="line">                         AtomicOpsCtl: ReqEn-</span><br><span class="line">        Capabilities: [40] MSI: Enable+ Count=1/1 Maskable- 64bit+</span><br><span class="line">                Address: 00000000fffff040  Data: 0050</span><br><span class="line">        Kernel driver in use: dma_engine</span><br></pre></td></tr></table></figure></li>
<li><p>MMIO</p>
<p>DmaRawState里的MemoryRegion用来表示这个模拟设备上BAR对应的MMIO。在realize函数里<br>把这个地址空间和BAR0绑定，在dma_engine_state_init函数里为这段MMIO挂上read/write<br>处理函数。read/write的回调函数定义在dma_engine_io_ops, 软件读写相关的寄存器最终<br>都会在这些回调函数中处理。可以看到我们BAR size配置成了16KB。</p>
</li>
<li><p>DMA</p>
<p>软件写0x1018 bit0为1触发一个DMA数据拷贝，相关实现代码在dma_engine_do_dma_copy里。<br>这个函数直接调用pci_dma_read/pci_dma_write做设备buffer和内存之间的数据搬移，这里<br>在使能smmuv3的时候，以上两个函数的内部实现会先调用smmuv3的translation函数做地址<br>翻译，然后再做数据搬移，在不使能smmuv3的时候，直接使用内存地址做数据搬移。</p>
<p>如果只是模拟一个PCIe EP设备，可以不用管和iommu相关的内部实现。如果要做iommu的模拟，<br>可以参考<a href="https://wangzhou.github.io/qemu-iommu%E6%A8%A1%E6%8B%9F%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90/">这里</a></p>
</li>
<li><p>中断</p>
<p>realize函数中使用pci_config_set_interrupt_pin给设备加一个INTx中断。使用msi_init<br>给设备加MSI中断。可以使用pci_irq_assert触发一个电平中断，通过msi_notify触发一个<br>MSI中断，通过qemu_irq_pulse触发一个边沿中断。</p>
</li>
</ul>
<h2 id="Linux内核驱动的实现"><a href="#Linux内核驱动的实现" class="headerlink" title="Linux内核驱动的实现"></a>Linux内核驱动的实现</h2><p> DMA engine设备的内核驱动比较简单，源码在[2]。这个驱动同时也通过sysfs接口向用户态<br> 暴露了一个叫copy_size的文件，向这个文件写入一个数值将触发一次该数值大小的DMA数据<br> 拷贝，为了方便起见，我们在这个驱动的内部生成需要拷贝的数据。</p>
<h2 id="编译运行"><a href="#编译运行" class="headerlink" title="编译运行"></a>编译运行</h2><p> qemu的编译运行在上面有提及。使用的Linux内核的基础版本是5.15-rc1，编译的时候打开<br> ARM_SMMU_V3和DMA_ENGINE_DEMO的配置即可。</p>
<p> 我们可以在启动qemu的时候带上–trace “smmuv3_*”，这样可以打开qemu smmuv3的trace，<br> 观察到smmuv3这个模拟设备内部的详细运行情况。我们也可以给DMA engine这个设备用类似<br> 的方法加上trace。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># echo 128 &gt; copy_size </span><br><span class="line">[  161.037614] dma_engine 0000:00:04.0: input size: 80</span><br><span class="line">smmuv3_config_cache_miss Config cache MISS for sid=0x20 (hits=0, misses=1, hit rate=0)</span><br><span class="line">smmuv3_find_ste sid=0x20 features:0x1, sid_split:0x8</span><br><span class="line">smmuv3_find_ste_2lvl strtab_base:0x4000000042c79000 l1ptr:0x42c79000 l1_off:0x0, l2ptr:0x7cc60000 l2_off:0x20 max_l2_ste:511</span><br><span class="line">smmuv3_get_ste STE addr: 0x7cc60800</span><br><span class="line">smmuv3_get_cd CD addr: 0x43bb4000</span><br><span class="line">smmuv3_decode_cd oas=44</span><br><span class="line">smmuv3_decode_cd_tt TT[0]:tsz:16 ttb:0x43dd5000 granule_sz:12 had:0</span><br><span class="line">smmuv3_translate_success smmuv3-iommu-memory-region-32-4 sid=0x20 iova=0xffffe900 translated=0x427ff900 perm=0x1</span><br><span class="line">smmuv3_config_cache_hit Config cache HIT for sid=0x20 (hits=1, misses=1, hit rate=50)</span><br><span class="line">smmuv3_translate_success smmuv3-iommu-memory-region-32-4 sid=0x20 iova=0xffffe980 translated=0x427ff980 perm=0x2</span><br><span class="line">smmuv3_cmdq_consume prod=42 cons=40 prod.wrap=0 cons.wrap=0</span><br><span class="line">smmuv3_cmdq_opcode &lt;--- SMMU_CMD_TLBI_NH_VA</span><br><span class="line">smmuv3_s1_range_inval vmid=0 asid=1 addr=0xffffe000 tg=1 num_pages=0x1 ttl=3 leaf=1</span><br><span class="line">smmuv3_cmdq_consume prod=42 cons=41 prod.wrap=0 cons.wrap=0</span><br><span class="line">smmuv3_cmdq_opcode &lt;--- SMMU_CMD_SYNC</span><br><span class="line">smmuv3_cmdq_consume_out prod:42, cons:42, prod_wrap:0, cons_wrap:0 </span><br><span class="line">smmuv3_write_mmio addr: 0x98 val:0x2a size: 0x4(0)</span><br><span class="line">smmuv3_read_mmio addr: 0x9c val:0x2a size: 0x4(0)</span><br><span class="line">[  161.043200] dma_engine 0000:00:04.0: dma engine test sucessed!</span><br></pre></td></tr></table></figure>
<p> 如上的日志，smmuv3前缀的是qemu的trace，时间戳开头的是内核驱动的打印。</p>
<p>[1]<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3FlbXUvYmxvYi80NjEyMTEzZGEwMjcxNmU4YzU2OTMwZTg4Y2E4YTE0MmUxODBmMTc1L2h3L21pc2MvZG1hX2VuZ2luZS5j">https://github.com/wangzhou/qemu/blob/4612113da02716e8c56930e88ca8a142e180f175/hw/misc/dma_engine.c<i class="fa fa-external-link-alt"></i></span><br>[2]<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L2xpbnV4L2Jsb2IvODc2OTU2OTVlNGQzZWE3MmU2MGQ5YzVkYTVmYzU4MDRhZTcxZmI0OC9kcml2ZXJzL21pc2MvZG1hX2VuZ2luZS9kbWFfZW5naW5lLmM=">https://github.com/wangzhou/linux/blob/87695695e4d3ea72e60d9c5da5fc5804ae71fb48/drivers/misc/dma_engine/dma_engine.c<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>模块复位的基本逻辑</title>
    <url>/%E6%A8%A1%E5%9D%97%E5%A4%8D%E4%BD%8D%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>设备复位的功能，需要软硬件一起联合起来考虑。本文试图梳理下要给一个linux设备驱动<br>加上复位功能需要考虑的基本逻辑。本文只从逻辑的层面给出我的认识，不涉及具体的<br>硬件和软件驱动。</p>
<p>首先我们对这个设备做出一些基本的假设。这个设备是一个PCIe设备，这个设备有多个PF，<br>一个PF又有多个附带的VF。每个function上又有很多可以独立工作的部件，比如很多队列。<br>这个设备可以工作在内核态，也可以工作在用户态, 也可以工作在虚拟机的内核态。</p>
<p>复位可以是整个设备的复位，我们称之为全局复位。全局复位发生的时候，这个设备上的<br>PF和VF都要被复位。复位也可以是function级别的，比如一个PF或者VF单独复位，其他<br>function的功能不受到影响, 我们把这种复位叫function复位，因为我们讨论的是PCIe<br>设备，这里也就是FLR。当然逻辑上，我们还有function里独立工作部件的复位，比如队列<br>的复位。</p>
<p>我们现在对复位做更清楚的定义。一般, 复位是对设备回归初始状态的操作，但它回归到的<br>状态不是刚刚上电的状态。用户在使用设备的时候，一般会先做一些初始的配置，比如，<br>enable几个VF, 配置一些队列的资源。这些配置在上下电或者驱动加卸载的时候要清除。<br>但是，在复位的时候，我们希望保留这些配置。一般需要复位的时候为系统发生了错误，无<br>法继续运行，复位只是叫系统回复之前可以运行的状态，清除用户之前的配置是不合适的。</p>
<p>所以，各种复位首先要考虑的问题是，是否需要保留用户配置，怎么保留，在复位完成后<br>怎么恢复这些配置。</p>
<p>我们还需要考虑带流量复位的问题，需要复位的时候，设备上可能还有部件在工作。冒然<br>做复位可能导致异常出现。一般的做法是要先把工作的部件停下来，再进行复位操作。</p>
<p>考虑到了以上两点，剩下的就是具体结合硬件提供的功能，写代码完成功能了。下面我们<br>再近一步说明以上两点。</p>
<p>一般，设备的全局配置保存在设备全局寄存器或者是PF里。所以，设备全局复位或者PF FLR<br>需要考虑配置保存，恢复相关的东西。全局复位中，如果硬件没有复位PCIe SRIOV相关的配置，<br>PCIe VF是一直保持的，设备驱动需要保留恢复设备业务相关的配置。但是PF的FLR，PCIe<br>协议规定VF要被disable，这也就意味着PF FLR会触发VF消失。不过，设备驱动并不需要在<br>复位完成后enable相应的VF，这是因为在PF FLR的流程里，PCIe总线驱动会保留VF数目的<br>配置，在PF FLR后enable相关VF(不过，全盘考虑这个问题，如果PF FLR的时候，VF在虚拟机<br>里正在使用，重新enable的VF在虚拟机里是否还可以继续使用，这里还不清楚)。</p>
<p>带流量复位的问题，可能是复位里最复杂的了。我们考虑全局复位的情况，一般，全局复位<br>的操作发生PF的驱动里, 我们可以在复位的时候先检查PF, 把PF的工作停下来再复位。但是，<br>我们怎么才能停下来正在虚拟机里工作的VF？这就需要PF和VF之间有硬件上的通知机制, PF<br>要进行全局复位的时候，先用相关的通知机制通知VF, VF收到通知后把它自己的工作停下<br>来，然后VF可以通知PF它已经停下工作，PF可以进行全局复位了。这个是一个合理的带流量<br>复位应该做的基本的软硬件配合的考虑。</p>
<p>如果，硬件没有PF/VF之间的通知机制，软件上可以做些什么来补救? 其实，补救的办法也<br>是要依赖硬件的行为。比如，如果全局复位的时候，VF不响应软件的请求(写入都丢弃，读<br>到的都是全1), 我们就可以在软件发送请求的时候加定时器，如果超时还没有完成，就猜测<br>发生了全局复位。但是，这样的补救一般运气的成分大，一不留神就有给你惊喜的地方。<br>比如，VF读到全1就有可能引起软件的误判, 超时之后VF怎么了解到是发生了全局复位，VF<br>又怎么判断全局复位完成了…</p>
]]></content>
      <tags>
        <tag>软件架构</tag>
      </tags>
  </entry>
  <entry>
    <title>搭建邮件客户端进行Linux kernel开发</title>
    <url>/%E6%90%AD%E5%BB%BA%E9%82%AE%E4%BB%B6%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9B%E8%A1%8CLinux-kernel%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<p>进行linux kernel开发需要实时关注kernel相关邮件列表的动态，本文介绍怎么搭建一个<br>邮件客户端，以便更好的查看邮件列表中的邮件。根据本文内容可以搭建一个这样的邮件<br>阅读环境。</p>
<p>收到内核邮件列表中的邮件需要首先订阅相应的邮件列表，具体订阅的方法请自行google.<br>订阅的本质是向一个地址发送特定内容的邮件。</p>
<p>每天来自一个邮件列表中的邮件可能会有几十封，如果同时订阅几个邮件列表，每天邮箱<br>中的邮件会有几百封。怎么管理这些邮件，有一种方法是用雷鸟邮件客户端(thunderbird),<br>还有一种方法就是本文将要介绍的 fetchmail + procmail + msmtp + mutt。当然你也可以<br>直接从你注册的邮箱里看邮件，不过从成千上百的邮件中找到你想看的，再找到别人的回复<br>的邮寄真的非常麻烦。</p>
<p>除了邮件客户端，还有一个问题：用什么邮箱。一般的公司邮箱可以用来注册邮件列表，<br>但是业余时间也想学习下大牛们的讨论，用公司邮箱显然不太好。于是需要注册一个私人<br>邮箱，用gmail显然要显得专业，但是用gmail邮箱设置thunderbird的时候比较麻烦，要先<br>生成一个第三方app接入gmail邮箱的密码，在设置thunderbird密码的时候设置进去，本人<br>用 gmail + thunderbird 的方式搭的环境，收邮件的时候时而连上时而连不上，所以本文<br>对这个不做介绍。用163,126邮箱注册了邮件列表，发了邮件就没有了下文，估计是给屏蔽<br>了，大家也不用忙活用163，126邮箱注册邮件列表了。在网上发现有人用139的邮箱成功<br>注册内核邮件列表，试了一下，果然可以。所以本文使用139邮箱。(update: 139用了一段<br>时间也不行了)</p>
<p>有了上面的背景，我们可以开始介绍整个环境的搭建了。首先想到的肯定是 139邮箱 + thunderbird<br>的方法，thunderbird图形化操作似乎也更方便一点。thunderbird只是一个邮件客户端，你<br>要配置它，告诉它发送/接收邮件使用的发送和接收服务器分别是什么。139邮箱的发送接收<br>服务器可以在邮箱的设置里找到，至于怎么设置thunderbird, 具体方法请自行google.<br>用thunderbird的好处是可以设置不同的邮件目录，对应的目录设置不同的过滤器以归类<br>邮件，还有就是可以以一个个thread的形式管理邮件，回复关系一目了然。需要注意的是<br>thunderbird的默认格式不符合linux kernel patch的格式，需要简单配置一下，想用<br>thunderbird的人也要google下了。</p>
<p>但是像本人这样，有时需要远程调试，然后在服务器上把调试log发回本地的情况时。图形<br>界面的邮件客户端显然满足不了需求，于是有了本文要介绍的命令行邮件客户端配置。<br>本文使用139邮箱，139邮箱默认是开启pop, smtp服务的，下面的配置脚本中可以看到<br>pop, smtp服务器地址。</p>
<p>首先简要说明fetchmail, procmail, msmtp, mutt的作用。<br>fetchmail是从邮箱下在邮件, procmail提供对邮件的过滤功能, msmtp用来发送邮件，<br>mutt用来看邮件和写邮件。本人的工作环境是ubuntu, 所以下面配置都是在ubuntu的环境<br>下完成的。ubuntu下安装上面四个软件apt-get install即可搞定。</p>
<p>fetchmail的配置文件在自己的home目录下的.fetchmailrc文件，需要自己创建一个这样<br>的文件。文件内容大概如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">poll pop.139.com</span><br><span class="line">protocol POP3</span><br><span class="line">user &quot;your_email_address@139.com&quot;</span><br><span class="line">password &quot;your_password&quot;</span><br><span class="line">keep</span><br><span class="line">ssl</span><br><span class="line">mda &quot;procmail -d %T&quot;</span><br></pre></td></tr></table></figure>

<p>配置完后，运行fetchmail, 可以看到在/var/mail/下有以你账号名作为文件名的文件。<br>里面就是下载的邮件。本人一开始没有配置最后一行，下载邮件失败, 加上最后一行后可以<br>下载。</p>
<p>procmail的配置文件也在自己的home目录下，为.procmailrc， 内容大概如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PATH=/usr/local/bin:/usr/bin:/bin</span><br><span class="line">MAILDIR=$HOME/Mail</span><br><span class="line">DEFAULT=$MAILDIR/mbox</span><br><span class="line">LOGFILE=$MAILDIR/log</span><br></pre></td></tr></table></figure>

<p>msmtp的配置文件也在自己的home目录下，为.msmtprc， 内容大概如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">account default</span><br><span class="line">host smtp.139.com</span><br><span class="line">from your_email_address@139.com</span><br><span class="line">auth login</span><br><span class="line">tls on</span><br><span class="line">tls_certcheck off</span><br><span class="line">user your_email_address@139.com</span><br><span class="line">password &quot;your_password&quot;</span><br></pre></td></tr></table></figure>

<p>配置好后可以用 msmtp -S 检查自己的配置是否正确。</p>
<p>mutt的配置文件也在自己的home目录下，为.muttrc， 内容大概如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set from=&quot;your name &lt;your_email_address@139.com&gt;&quot;</span><br><span class="line">set use_from=yes</span><br><span class="line">set sendmail=&quot;/usr/bin/msmtp&quot;</span><br><span class="line">set editor=vim</span><br><span class="line">set folder=&quot;$HOME/Mail&quot;</span><br><span class="line">set record=&quot;+sent&quot;</span><br><span class="line">set mbox=&quot;+mbox&quot;</span><br><span class="line">set postponed=&quot;+postponed&quot;</span><br></pre></td></tr></table></figure>

<p>配置好后每次可以先fetchmail, 然后用mutt -f ~/Mail/mbox打开邮件查看。</p>
<p>到现在就基本可以命令行浏览，收发邮件了。但是如果你一下订阅了几个邮件列表，想<br>分类管理邮件，下面给一个简单的示范：</p>
<p>想单独管理下来自<span class="exturl" data-url="bWFpbHRvOiYjMTA4OyYjMTA1OyYjeDZlOyYjeDc1OyYjeDc4OyYjNDU7JiN4NzA7JiM5OTsmIzEwNTsmIzY0OyYjeDc2OyYjeDY3OyYjeDY1OyYjMTE0OyYjeDJlOyYjeDZiOyYjMTAxOyYjeDcyOyYjMTEwOyYjeDY1OyYjeDZjOyYjNDY7JiMxMTE7JiN4NzI7JiN4Njc7">&#108;&#105;&#x6e;&#x75;&#x78;&#45;&#x70;&#99;&#105;&#64;&#x76;&#x67;&#x65;&#114;&#x2e;&#x6b;&#101;&#x72;&#110;&#x65;&#x6c;&#46;&#111;&#x72;&#x67;<i class="fa fa-external-link-alt"></i></span> 邮件列表里的邮件。先在家目录的Mail下<br>创建一个文件保存邮件，这里我们创建一个叫pci-mbox的文件。在.procmailrc文件中添加<br>一下几行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">:0</span><br><span class="line">* ^Cc:.*linux-pci@vger.kernel.org</span><br><span class="line">pci-mbox</span><br></pre></td></tr></table></figure>
<p>之后再fetchmail，来自pci邮件列表中的邮件就自动保存在~/Mail/pci-mbox中了。</p>
<p>注意：</p>
<ol>
<li>fetchmail失败: 在.procmailrc中配置不当（配置不当的过滤脚本）会导致fetchmail<br>下载不了邮件</li>
</ol>
]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>用mprotect定位踩内存问题</title>
    <url>/%E7%94%A8mprotect%E5%AE%9A%E4%BD%8D%E8%B8%A9%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>Linux用户态程序踩内存时可以用mprotect定位，mprotect本身是linux系统上的一个系统<br>调用，这个系统调用可以改变一段内存的读写属性, 当有非法的访问访问对应的内存的时候<br>会给进程发一个SIGSEGV信号，进程可以在信号处理函数中加调试信息进行定位。</p>
<p>mprotect的参数为要保护的虚拟地址，保护地址的大小，和保护地址空间的属性。这里<br>地址size必须是已页对齐的，地址空间的属性有读、写、执行和不可接入。</p>
<p>显然，当被踩内存本身就是只读的时候，我们一开始就可以用mprotect把这段内存保护起来,<br>别的执行流踩了这段内存就会触发信号。如果，被踩的内存是一段可读可写的内存，我们<br>可以在正常执行的时候调用mprotect设置为读写，正常执行完后用mprotect设置为只读。</p>
<p>在信号处理函数中，可以调用backtrace, backtrace_symbols相关函数把调用栈打出来。<br>如下的测试代码，在X86上用gcc -rdynamic test.c编译运行是OK的，可以打出调用栈，<br>加-rdynamic是为了打出调用栈里的函数名。但是在ARM64的环境下，需要用<br>gcc -rdynamic -funwind-tables test.c来编译测试代码，否则只能打出模块的名字。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;execinfo.h&gt;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">#include &lt;signal.h&gt;</span><br><span class="line">#include &lt;malloc.h&gt;</span><br><span class="line">#include &lt;sys/mman.h&gt;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * test in x86 with gcc -rdynamic test.c is OK.</span><br><span class="line"> *</span><br><span class="line"> * however, in aarch64, return of backtrace is alway 1.</span><br><span class="line"> * use gcc -rdynamic -funwind-tables test.c to solve this problem.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">void fun_3(void);</span><br><span class="line"></span><br><span class="line">void handler(int sig, siginfo_t *si, void *unused)</span><br><span class="line">&#123;</span><br><span class="line">	fun_3();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void fun_3(void)</span><br><span class="line">&#123;</span><br><span class="line">#define SIZE 10</span><br><span class="line">	void *buffer[SIZE];</span><br><span class="line">	char **strings;</span><br><span class="line">	int n, i;</span><br><span class="line"></span><br><span class="line">	n = backtrace(buffer, SIZE);</span><br><span class="line"></span><br><span class="line">	strings = backtrace_symbols(buffer, n);</span><br><span class="line"></span><br><span class="line">	for (i = 0; i &lt; n; i++) &#123;</span><br><span class="line">		printf(&quot;%s\n&quot;, strings[i]);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	free(strings);</span><br><span class="line"></span><br><span class="line">	exit(EXIT_FAILURE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void fun_2(void)</span><br><span class="line">&#123;</span><br><span class="line">	fun_3();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void fun_1(void)</span><br><span class="line">&#123;</span><br><span class="line">	fun_2();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void fun_0(void)</span><br><span class="line">&#123;</span><br><span class="line">	fun_1();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	struct sigaction sa;</span><br><span class="line">	char *buffer;</span><br><span class="line">	int pagesize;</span><br><span class="line"></span><br><span class="line">	sa.sa_flags = SA_SIGINFO;</span><br><span class="line">	sigemptyset(&amp;sa.sa_mask);</span><br><span class="line">	sa.sa_sigaction = handler;</span><br><span class="line">	sigaction(SIGSEGV, &amp;sa, NULL);</span><br><span class="line"></span><br><span class="line">	pagesize = sysconf(_SC_PAGE_SIZE);</span><br><span class="line">	buffer = memalign(pagesize, 4 * pagesize);</span><br><span class="line"></span><br><span class="line">	if (mprotect(buffer, pagesize, PROT_READ | PROT_WRITE) == -1)</span><br><span class="line">		printf(&quot;fail to set mprotect\n&quot;);</span><br><span class="line"></span><br><span class="line">	printf(&quot;write a in buffer a\n&quot;);</span><br><span class="line">	*buffer = &#x27;a&#x27;;</span><br><span class="line">	printf(&quot;write a in buffer b\n&quot;);</span><br><span class="line">	sleep(2);</span><br><span class="line">	printf(&quot;write a in buffer c\n&quot;);</span><br><span class="line"></span><br><span class="line">	if (mprotect(buffer, pagesize, PROT_READ) == -1)</span><br><span class="line">		printf(&quot;fail to set mprotect\n&quot;);</span><br><span class="line"></span><br><span class="line">	*buffer = &#x27;b&#x27;;</span><br><span class="line"></span><br><span class="line">	//fun_0();	</span><br><span class="line"></span><br><span class="line">	exit(EXIT_SUCCESS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>mprotect</tag>
      </tags>
  </entry>
  <entry>
    <title>构建riscv两层qemu的步骤</title>
    <url>/%E6%9E%84%E5%BB%BAriscv%E4%B8%A4%E5%B1%82qemu%E7%9A%84%E6%AD%A5%E9%AA%A4/</url>
    <content><![CDATA[<h2 id="第一层qemu"><a href="#第一层qemu" class="headerlink" title="第一层qemu"></a>第一层qemu</h2><p> 我们这里用qemu主线的当前最新版本，v7.0.0, 正常编译就好。</p>
<h2 id="host内核"><a href="#host内核" class="headerlink" title="host内核"></a>host内核</h2><p> 我们这里用v5.19主线内核，使用riscv的defconfig编译, 注意要加上KVM的编译选项。</p>
<h2 id="第二层qemu"><a href="#第二层qemu" class="headerlink" title="第二层qemu"></a>第二层qemu</h2><p> 这个qemu的编译比较有意思，因为qemu编译需要依赖很多动态库，我用的都是交叉编译编译<br> riscv的程序，所以，需要先交叉编译qemu依赖的动态库，然后再交叉编译qemu，太麻烦了。<br> 我们这里用编译buildroot的方式一同编译小文件系统里的qemu, buildroot编译qemu的时候<br> 就会一同编译qemu依赖的各种库, 这样编译出的host文件系统里就带了qemu。</p>
<p> 下载buildroot的源码：git clone git://git.buildroot.net/buildroot</p>
<p> 这个时间点下载的buildroot的源码的commit id是934788d1，这个版本的buildroot需要<br> hack下，使riscv构架下可以编译qemu和cmake:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff --git a/package/qemu/Config.in b/package/qemu/Config.in</span><br><span class="line">index e960a062cb..715d6571d9 100644</span><br><span class="line">--- a/package/qemu/Config.in</span><br><span class="line">+++ b/package/qemu/Config.in</span><br><span class="line">@@ -9,6 +9,7 @@ config BR2_PACKAGE_QEMU_ARCH_SUPPORTS_TARGET</span><br><span class="line">        default y if BR2_powerpc64</span><br><span class="line">        default y if BR2_powerpc64le</span><br><span class="line">        default y if BR2_x86_64</span><br><span class="line">+       default y if BR2_riscv</span><br><span class="line"></span><br><span class="line">diff --git a/package/cmake/Config.in b/package/cmake/Config.in</span><br><span class="line">index 3c47fdcb49..4ef5612b57 100644</span><br><span class="line">--- a/package/cmake/Config.in</span><br><span class="line">+++ b/package/cmake/Config.in</span><br><span class="line">@@ -6,7 +6,7 @@ config BR2_PACKAGE_CMAKE_ARCH_SUPPORTS</span><br><span class="line">                BR2_mipsel    || BR2_mips64el    || BR2_powerpc  || \</span><br><span class="line">                BR2_powerpc64 || BR2_powerpc64le || BR2_sparc    || \</span><br><span class="line">                BR2_i386      || BR2_x86_64      || BR2_xtensa   || \</span><br><span class="line">-               BR2_s390x</span><br><span class="line">+               BR2_s390x     || BR2_riscv</span><br></pre></td></tr></table></figure>

<p> 先选用这个defconfig: make qemu_riscv64_virt_defconfig, 然后我们用make menuconfig<br> 打开buildroot的图形配置界面，选上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">BR2_TOOLCHAIN_BUILDROOT_GLIBC=y</span><br><span class="line">BR2_USE_WCHAR=y</span><br><span class="line">BR2_PACKAGE_QEMU=y</span><br><span class="line">BR2_PACKAGE_QEMU_CUSTOM_TARGETS=riscv64-softmmu</span><br><span class="line"></span><br><span class="line">BR2_TARGET_ROOTFS_CPIO=y</span><br><span class="line">BR2_TARGET_ROOTFS_CPIO_GZIP=y</span><br></pre></td></tr></table></figure>
<p> 然后make -j编译，生成的小文件系统在buildroot/output/images下。</p>
<h2 id="guest内核"><a href="#guest内核" class="headerlink" title="guest内核"></a>guest内核</h2><p> guest内核可以和上面的host内核用同一个。</p>
<h2 id="运行脚本"><a href="#运行脚本" class="headerlink" title="运行脚本"></a>运行脚本</h2><p>启动第一层qemu:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">qemu-system-riscv64 \</span><br><span class="line">	-smp 1 -m 1024m \</span><br><span class="line">	-nographic \</span><br><span class="line">	-machine virt -cpu &#x27;rv64,h=true&#x27; \</span><br><span class="line">	-kernel ~/repos/linux/arch/riscv/boot/Image \</span><br><span class="line">	-append &quot;console=ttyS0 root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">	-initrd ~/repos/buildroot/output/images/rootfs.cpio.gz \</span><br><span class="line"> 	-device virtio-9p-pci,fsdev=p9fs,mount_tag=p9,bus=pcie.0 \</span><br><span class="line">	-fsdev local,id=p9fs,path=/home/sherlock/p9root,security_model=mapped</span><br></pre></td></tr></table></figure>
<p>注意，qemu之前的中间版本可能使用-cpu rv64,x-h=true使能H扩展，在qemu v7,0.0以及之后<br>的版本都用-cpu rv64,h=true使能H扩展。</p>
<p>启动第二层qemu:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">qemu-system-riscv64 \</span><br><span class="line">	-smp 1 -m 256m \</span><br><span class="line">	-nographic \</span><br><span class="line">	-machine virt --enable-kvm \</span><br><span class="line">	-kernel ./Image \</span><br><span class="line">	-append &quot;console=ttyS0 root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">	-initrd ./rootfs.cpio.gz</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>测试openssl的性能</title>
    <url>/%E6%B5%8B%E8%AF%95openssl%E7%9A%84%E6%80%A7%E8%83%BD/</url>
    <content><![CDATA[<ol>
<li>openssl基本命令</li>
</ol>
<hr>
<p>AES对称加解密：</p>
<p>openssl enc -aes-128-cbc -in data -out key_encrypt -K 12345678901234567890 -iv 12345678<br>openssl enc -aes-128-cbc -in key_encrypt -out key_decrypt -K 12345678901234567890 -iv 12345678 -d</p>
<p>公钥加密：<br>openssl rsautl -encrypt -in rsa_test -inkey test_pub.key -pubin -out rsa_test.en -engine uadk</p>
<p>公钥生成：<br>openssl rsa -in test.key -pubout -out test_pub.key -engine uadk</p>
<p>私钥生成:<br>openssl genrsa -out test.key -engine uadk 4096</p>
<p>私钥解密：<br>openssl rsautl -decrypt -in rsa_test.en -inkey test.key -out rsa_test.de -engine uadk</p>
<p>签名：<br>openssl rsautl -sign -in msg.txt -inkey test.key -out signed.txt -engine uadk</p>
<p>认证：<br>openssl rsautl -verify -in signed.txt -inkey test_pub.key -pubin -out verified.txt -engine uadk</p>
<p>哈希:<br>openssl md5/sha1/sha256/sm3 -engine uadk data<br>openssl md5/sha1/sha256/sm3 data</p>
<p>如上，有-engine xxx的表示用执行的硬件加解密engine做任务，没有指定就是用openssl<br>里提供的软件计算方法搞。</p>
<p>在非对称加解密的测试中，我们使用RSA算法。需要先生成私钥，然后生成公钥，然后用<br>秘钥进行加解密和签名、认证的测试。</p>
<ol start="2">
<li>openssl speed命令</li>
</ol>
<hr>
<p>openssl speed aes</p>
<p>openssl speed rsa</p>
<p>openssl speed -engine uadk -async_jobs 1 -evp md5</p>
<p>openssl speed -engine uadk -async_jobs 1 -evp aes-128-cbc</p>
<p>openssl speed -engine uadk -elapsed rsa2048  // 如下例子</p>
<p>openssl speed -engine uadk -elapsed -async_jobs 1 rsa2048</p>
<p>openssl speed -engine uadk -elapsed -async_jobs 36 rsa2048</p>
<p>如上，加了-async_jobs使用了openssl里的异步机制，如果engine里使用过了openssl里的<br>异步机制，这里就会触发engine里的异步机制生效。</p>
<p>openssl speed的代码在openssl/apps/speed.c，拿同步rsa2048为例。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">main</span><br><span class="line">  /* 分配input，output buffer */</span><br><span class="line">  +-&gt; app_malloc(buflen, &quot;input buffer&quot;);</span><br><span class="line">  /* 只看同步，即async_jobs = 0的情况 */</span><br><span class="line">  +-&gt; run_benchmark(async_jobs, RSA_sign_loop, loopargs);</span><br><span class="line">    /*</span><br><span class="line">     * 可以看到这里的内存使用模型是，反复的用一个buffer做sign。如果</span><br><span class="line">     * engine的实现没有另外申请内存，这个测试将反复用一块固定的buffer。</span><br><span class="line">     */</span><br><span class="line">    +-&gt; RSA_sign_loop</span><br><span class="line">      +-&gt; RSA_sign</span><br><span class="line">        /* 如果下面适配的是openssl engine, 可以实现如下的回调函数支持 */</span><br><span class="line">        +-&gt; rsa-&gt;meth-&gt;rsa_sign</span><br><span class="line">	+-&gt; RSA_private_encrypt</span><br><span class="line">	  +-&gt; rsa-&gt;meth-&gt;rsa_priv_enc</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>加解密</tag>
        <tag>openssl</tag>
      </tags>
  </entry>
  <entry>
    <title>用户态下使用gpio中断</title>
    <url>/%E7%94%A8%E6%88%B7%E6%80%81%E4%B8%8B%E4%BD%BF%E7%94%A8gpio%E4%B8%AD%E6%96%AD/</url>
    <content><![CDATA[<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>确定你的gpio驱动是好用的，这时会在/sys/class/gpio下发现gpio对应的文件:gpio***,<br>export, unexport, gpio***是gpio控制器对应的文件，export, unexport是gpio框架<br>提供的用来向用户态导出gpio的文件</p>
<p>假设使用66号gpio口作为中断端口，即产生中断的器件的中断管脚连接的是soc的66号gpio<br>管脚。echo “66” &gt; export 向外导出管脚，会发现/sys/class/gpio下多了目录gpio66<br>gpio66目录中有文件value, direction, edge, power, device等等</p>
<p>echo “in” &gt; direction 设置gpio66脚为输入<br>echo “falling” &gt; edge 设置gpio66脚为下降沿触发中断, 也可以把falling改成rising<br>即为上升沿触发，这时当gpio66管脚上存在一个falling时就会接收到一个中断，怎么把<br>这个接收到的中断在用户态反应出来呢？</p>
<p>在接收到中断的时候value的值会从原来的1变成0，这里假设是下降沿触发, 所以可以<br>使用poll()函数阻塞在value文件对应的文件描述符上，当文件发生变化的时候poll返回<br>相应的中断，具体代码:<br><span class="exturl" data-url="aHR0cDovL2Jsb2cuc2tlcnBhLmNvbS9kc2NobmVsbC9ibG9nLzIwMTMvMTAvMjcvbGludXgtYW5kLWdwaW8tdXNlcnNwYWNlLWludGVycnVwdHMv">http://blog.skerpa.com/dschnell/blog/2013/10/27/linux-and-gpio-userspace-interrupts/<i class="fa fa-external-link-alt"></i></span><br>…</p>
<h2 id="gpiolib-c中的实现"><a href="#gpiolib-c中的实现" class="headerlink" title="gpiolib.c中的实现"></a>gpiolib.c中的实现</h2><p>对edge的读写，最后会调用到： gpio_edge_show()，gpio_edge_store(), 可以看看<br>echo “falling” &gt; edge 内核的函数调用</p>
<p>内核首先找到falling对应的编码，然后在gpio_setup_irq中注册中断处理函数:<br>gpio_sysfs_irq, 所以当gpio管脚上发生中断时，最后会调用中断处理函数中的<br>wake_up_interrupt()通知对应文件上的等待队列?</p>
<p>request_irq(), free_irq()函数在注册中断，释放中断的时候，会对相应的中断线<br>做一定的处理, 包括使能中断等</p>
<p>但是有一个东西不清楚：假设是一个下降沿触发的中断，在接收到中断的时候对应的<br>/sys/class/gpio/gpio***/value 中的值应该从1变成0, 但是使用上面博客中的代码<br>可以发现value中的值一直是1，手动cat value发现其中的值是0。依然没有找见value<br>被设置为0的对应代码?</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>gpio</tag>
      </tags>
  </entry>
  <entry>
    <title>程序configure, compile, install的逻辑</title>
    <url>/%E7%A8%8B%E5%BA%8Fconfigure-compile-install%E7%9A%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p> 拿到一个linux用户态程序的源码，到使用该程序分为一下几步：</p>
<ol>
<li><p>配置(configure)，在源码目录下一般会有configure脚本文件，执行该文件可以检测<br>源码需要的编译链接环境，然后相应的生成makefile文件。</p>
<p>使用./configure –help可以得到该脚本的使用帮助, 一般的有一下几个参数：<br>./configure –host=*** –prefix=*** CC=*** LDFLAGS=*** LIBS=***<br>其中–host指定编译生成的可执行文件的执行环境， –prefix指定make install<br>的安装路径，CC指定用到的编译器，LDFLAGS指定链接时标准的库搜索路径之外的<br>库搜索路径。</p>
</li>
<li><p>编译链接(make)<br>根据Makefile文件中的配置，编译链接成可执行程序。</p>
</li>
<li><p>安装(make install)<br>第一步中(configure)中–prefix会把程序的安装路径写入到makefile中。在这一步会<br>依照该路径把相应的文件拷贝到相应的目录。<br>一个程序可以就只有一个可执行文件。也可能除了可以执行文件外，还需要一些静态<br>库或者是动态库的支持, 这时安装程序就包括把可执行文件和动态库文件拷贝到相应<br>的目录。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>超标量处理器ROB的理解</title>
    <url>/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8ROB%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>ROB是超标量处理器内部的一个硬件部件，一般是一个FIFO的队列，进入处理器的指令顺序<br>进入ROB，所以ROB里的指令顺序就是处理器内部指令的年龄顺序。注意，这里说的进入处理器<br>内部的指令就包括投机执行的指令，这个和软件看到的正常执行的指令是不一样的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  young                                          old</span><br><span class="line">+--------+--------+--------+--------+--------+--------+</span><br><span class="line">| insn_6 | insn_5 | insn_4 | insn_3 | insn_2 | insn_1 |</span><br><span class="line">+--------+--------+--------+--------+--------+--------+</span><br></pre></td></tr></table></figure>
<p>如上指令按照insn_1-&gt;insn_6的顺序顺序进入ROB，正常执行完的指令按照同样的顺序离开<br>ROB，也就离开了处理器。</p>
<p>但是，ROB里的指令也可能不按照先入先出的顺序离开，当投机执行错误，需要flush掉指令<br>时，处理器会抹去最年轻一段指令，这时就是年轻指令先离开。其中典型的例子就是分支预<br>测错了，比如上面insn_4是一个分支指令，某个时刻发现insn_4的分支预测做错了，也就是<br>insn_5/6取错了，那就要把insn_5/6从ROB里去掉，新取入的指令(insn_7/8)占据insn_5/6<br>的位置。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  young                                          old</span><br><span class="line">+--------+--------+--------+--------+--------+--------+</span><br><span class="line">|        |        | insn_4 | insn_3 | insn_2 | insn_1 |</span><br><span class="line">+--------+--------+--------+--------+--------+--------+</span><br><span class="line"> |</span><br><span class="line"> V</span><br><span class="line">+--------+--------+--------+--------+--------+--------+</span><br><span class="line">| insn_8 | insn_7 | insn_4 | insn_3 | insn_2 | insn_1 |</span><br><span class="line">+--------+--------+--------+--------+--------+--------+</span><br></pre></td></tr></table></figure>

<p>硬件里各个部件是并行在运行的(软件工程师有时会忽略这一点)，有哪些在同步操作ROB的硬<br>件部件呢？具体行为又是怎么样的？</p>
<ul>
<li>指令进入ROB</li>
</ul>
<p>指令rename后就会根据ROB上的指令进入点标记(allocate)写入ROB。</p>
<ul>
<li>指令正常离开ROB</li>
</ul>
<p>控制指令退休的部件(一般就是ROB自己控制)在一拍中要检测指令可以退休，如下commit位置<br>表示待提交指令的位置，如果处理器一拍可以提交两条指令，insn2/1又都满足退休条件，<br>insn2/1就可以都在这一拍里被提交，提交后的指令状态为retired，相当于离开了流水线，<br>指令对寄存器和内存(cache)的改动也变成外部可见。</p>
<ul>
<li>指令被flush离开ROB</li>
</ul>
<p>指令被flush离开流水线需要对流水线中很多部件做flush操作，清空里面和被flush指令相关<br>的信息，ROB是其中一个需要被flush的硬件部件，这里对ROB的操作主要就是更新ROB上指令<br>进入点的标记。可见flush指令会和指令进入ROB存在同步问题，逻辑上应该先flush掉ROB中<br>的指令再把新指令写入ROB。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  young                                                            old</span><br><span class="line">+--------+--------+--------+--------+--------+--------+--------+--------+</span><br><span class="line">|        |        | insn_4 | insn_3 | insn_2 | insn_1 | insn_x | insn_y |</span><br><span class="line">+--------+--------+--------+--------+--------+--------+--------+--------+</span><br><span class="line">             ^                          ^                  ^</span><br><span class="line">          allocate                    commit            retired</span><br></pre></td></tr></table></figure>

<p>ROB里allocate和commit之间(包括commit)的指令是当前正在处理器里执行的指令，这些指令<br>可能正在等资源，可能正在执行，也可能已经执行完毕正在等退休，但是这些指令的都是投机<br>执行的，在commit阶段处理器判断是正确执行的指令才可以退休。当这些正在处理器里运行的<br>指令在某个状态发现投机不对时就会触发指令的flush。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>转载: M1 Explainer-v0.70</title>
    <url>/%E8%BD%AC%E8%BD%BD-M1-Explainer-v0-70/</url>
    <content><![CDATA[<div class="pdfobject-container" data-target="M1_Explainer_070.pdf" data-height="500px"></div>
]]></content>
      <categories>
        <category>reprint</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机组成与设计-硬件软件接口笔记</title>
    <url>/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E4%B8%8E%E8%AE%BE%E8%AE%A1-%E7%A1%AC%E4%BB%B6%E8%BD%AF%E4%BB%B6%E6%8E%A5%E5%8F%A3%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><p> 近几年处理器发展放缓的原因是遇到了功耗墙。功耗正相关于1/2 × 负载电容 × 电压平方 × 开关频率。<br> 过去CPU的主频提升了1000多倍，但是功耗没有提升1000多倍的原因是电压降低了，电压和<br> 物理工艺的关系比较大，越高的工艺电压可以做的越低，现在整体工艺发展放缓，电压相对<br> 比较高，导致功耗越来越高。</p>
<p> 功耗分为动态功耗和静态功耗，动态功耗受开关频率的影响，静态功耗随着电压的降低会<br> 变大，因为电压降低漏电流增大了。目前，静态功耗占总功耗的40%。</p>
<p> 功耗墙迫使CPU设计从单核优化走向多核设计。</p>
<h2 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h2><ul>
<li>指令概括</li>
</ul>
<p> 可以以三种类型的指令概括下CPU的指令：1. 算数指令，2. 存储器操作指令，3. 流程控制指令。</p>
<p> 在risc-v上，算数指令的操作数必须是寄存器或是立即数，比如一个整数加法指令的汇编可能是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add x22, x22, x9 </span><br></pre></td></tr></table></figure>

<p> 存储器操作指令的代表就是load/store指令，比如risc-v上是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ld x9, 8(x22)</span><br><span class="line">sd x9, 96(x22)</span><br></pre></td></tr></table></figure>
<p> 第一条ld指令表示把x22地址偏移8Byte的地址上的数据load到x9寄存器里。<br> 第二条sd指令表示把x9寄存器里的值存到x22地址偏移96Byte的地址上。<br> 如上的指令里，x22叫基址寄存器，8和96叫偏移量。</p>
<p> 流程控制指令的代表就是beq，比如risc-v上是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">beq x0, x1, L1</span><br></pre></td></tr></table></figure>
<p> 如上的指令表示，如果x0和x1的值相等，那么程序跳到L1标签处执行。</p>
<ul>
<li>有符号和无符号数的表示</li>
</ul>
<p> 一个二进制数，作为有符号理解的时候，只要把最高位的符号变成负号就可以，比如一个<br> 8Byte的char，如果它做有符号计算，它的值是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">value = (-1)2^7 * bit[7] + 2^6 * bit[6] + ... + 2^0 * bit[0]</span><br></pre></td></tr></table></figure>
<p> 一个二进制数求相反数的快速方法是，对每个bit取反后整体加1。<br> 一个扩展二进制负数的方法是把扩展位用1补齐。</p>
<ul>
<li>汇编到指令</li>
</ul>
<p> 计算机真正可以执行的是二进制指令，所以汇编代码要被编译器编译成二进制的指令。<br> 二进制指令的格式设计会影响CPU的硬件设计，一个指令的格式被做成几种固定的样式。<br> 比如，risc-v上的整数加法指令的二进制表示是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add x9, x20, x21</span><br><span class="line">+--------+-------+-------+---------+-------+---------+</span><br><span class="line">|0000000 | 10101 | 10100 | 000     | 01001 | 0110011 |</span><br><span class="line">+--------+-------+-------+---------+-------+---------+</span><br><span class="line">| 7bits  |   5   |   5   |  3      |   5   |    7    |</span><br><span class="line">+--------+-------+-------+---------+-------+---------+</span><br><span class="line">| funct7 |  rs2  |  rs1  | funct3  |   rd  |  opcode |</span><br><span class="line">+--------+-------+-------+---------+-------+---------+</span><br><span class="line">|        |  x21  |  x20  |         |   x9  |    51   |</span><br><span class="line">+--------+-------+-------+---------+-------+---------+</span><br></pre></td></tr></table></figure>
<p> 可以看到一条加法指令的所有信息需要编码到一个32bit里，而且相关的算数指令都要复用<br> 上面的样式。如上，用5个bit表示操作数寄存器，是因为risc-v上有32个构架寄存器，可见<br> 如果要改变构架寄存器的数量，指令的编码也要跟着改。可以看到，对于立即数加的指令，<br> 立即数也要编码到32bit里，这个立即数的大小是有限制的。</p>
<p> 可见对于大一点的立即数加，需要有其他的指令，比如risc-v上有:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lui x19, immediate</span><br></pre></td></tr></table></figure>
<p> 加载立即数immediate到x19的31bit-12bit，可见其中的立即数最大可以是20bit。</p>
<ul>
<li>risc-v过程控制分析</li>
</ul>
<p> 也就是函数调用过程的分析。一段函数代码执行的时候不仅要占用CPU的计算资源，也要<br> 占用存储资源，比如算数指令要使用寄存器、临时变量可以存到寄存器里、寄存器不够的<br> 时候临时变量就要存到栈上。除了临时变量要使用栈，使用寄存器的时候也要提前把寄存器<br> 中的旧值保存到栈上，这样当函数执行完，才能恢复原来寄存器里的值，调用点之后继续<br> 执行的逻辑才是对的。</p>
<p> risc-v中的返回地址保存在x1中，传参放在x10-x17八个寄存器，栈指针是x2，x8是帧指针。<br> risc-v的过程调用的ABI中包括：x5-x7, x28-x31是临时寄存器，调用过程中可以不被被调用者<br> 保存，x8-x9, x18-x27，调用过程中必须被保存。</p>
<h2 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h2><p> 本章具体讲加减乘除以及浮点指令的实现。(跳过)</p>
<h2 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h2><p> 这一章是整本书的核心，可以先看这一章，有什么不懂再返回去看。这章循序渐进的把上面<br> 提到的三种指令都加到一个简单的CPU实现中，然后引出流水线的设计，基于流水线再次实现<br> 上面的三种指令，然后引出流水线里存在的数据冒险，控制冒险和结构冒险，然后引出相关<br> 的解决办法，数据冒险的解决办法是前递，就是数据提交前就回传给后面的指令使用，控制<br> 冒险的解决办法是各种分支预测。这章的最后引出了多发射的概念，然后得出超标量处理器<br> 的概念。</p>
<p> 从上面的指令分析里可以看出一个处理器需要的逻辑部件有哪些。算数指令、存储指令和<br> 流程控制指令都需要运算单元，存储指令要用基地址和偏移量来计算地址，流程控制指令<br> 要计算下一个PC的值，运算单元我们叫ALU。CPU里需要有取指令和译码的部件，这个部件<br> 根据pc值从存储器里取指令并且做指令译码。CPU里需要有数据存取的部件，ld、sd需要用<br> 这个部件访问数据存储器。实现上，CPU里还有寄存器堆(register fils)，这个部件提供<br> 其他部件对系统寄存器的读写接口，关于寄存器，CPU上一般有架构寄存器和物理寄存器，<br> 架构寄存器就是软件可以看到的那些寄存器，物理寄存器是CPU内部扩展的寄存器，软件<br> 不可见，一般CPU内部要对汇编代码里的架构寄存器做rename，把他们重定义成物理寄存器，<br> 这本书，关于rename的东西也基本没有介绍。CPU里需要有相关的控制逻辑，把如上的各个<br> 部件协调控制起来。</p>
<p> 一个指令的执行可以被拆分成几个独立的步骤，每个步骤在流水线的一步里完成。一个指令<br> 流顺序进入流水线，可以做到指令并行。这几个步骤一般是：取指，译码，执行，访存和写回。<br> 乱序执行的处理器中，相关的步骤有所不同。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+----+    +----+    +----+    +-----+    +----+</span><br><span class="line">| IF |---&gt;| ID |---&gt;| EX |---&gt;| MEM |---&gt;| WB |</span><br><span class="line">+----+    +----+    +----+    +-----+    +----+</span><br></pre></td></tr></table></figure>
<p> 如下的连续指令会导致流水线数据冒险：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add x19, x0, x1</span><br><span class="line">sub x2, x19, x3</span><br></pre></td></tr></table></figure>
<p> 原因是add的结果会存到x19里，这个在流水线的WB阶段才可以生效，而sub指令的EX步骤要<br> 依赖x19。当然，编译器可以做到不要产生这样的依赖。数据冒险会带来流水线停顿，直观<br> 的看，sub的指令可以在EX停顿，等到x19的值得到再继续。硬件上用前递的方式减轻数据冒险<br> 带来的流水线停顿，前递就是增加额外的路径，把x19的值一算出来就给sub指令，可以想到<br> 前递需要增加相关的数据冒险检测逻辑和前递数据通路的逻辑。</p>
<p> 如下的连续指令会导致流水线控制冒险:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add x4, x5, x6</span><br><span class="line">beq x1, x0, 40</span><br><span class="line">or x7, x8, x9</span><br></pre></td></tr></table></figure>
<p> 简单讲，流水线需要指令紧密进入执行，停顿就会性能下降。而像流程控制这样的指令会<br> 导致pc指针跳变，要等到x1、x0的计算结果得到后才知道有没有选错送入流水线的指令分支，<br> 如果选错了，之前执行的结果都不能要了，要重新执行。</p>
<p> 控制冒险是一定会存在的。缓解的办法就是各种分支预测手段，如果分支预测的基本准确，<br> 性能影响就不大。</p>
<p> 流水线的实现会在级与级之间增加多余的寄存器，保证各种控制逻辑和中间存储的实现。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">       |         |         |          |</span><br><span class="line">+----+ |  +----+ |  +----+ |  +-----+ |  +----+</span><br><span class="line">| IF |-+-&gt;| ID |-+-&gt;| EX |-+-&gt;| MEM |-+-&gt;| WB |</span><br><span class="line">+----+ |  +----+ |  +----+ |  +-----+ |  +----+</span><br><span class="line">       |         |         |          |</span><br></pre></td></tr></table></figure>
<p> 处理器的异常设计和流水线是结合在一起的，因为指令的执行随时可以产生异常。</p>
<p> 如上的内容都是流水线相关的，叫指令级并行(ILP)。曾加并行度的另外的方式还有多发射、<br> 多核。多发射是指每个周期可以发出多条指令，直观的看，在一个核里并行的多几条流水<br> 线可以支持多发射，多发射可以分为静态多发射和动态多发射。</p>
<p> 动态多发射处理器也称为超标量处理器。现在的处理器，一般4发射做的比较多。</p>
<h2 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h2><p> 本章讲和存储相关的东西，具体是cache、虚拟内存管理、页表、TLB相关的东西。内容基本<br> 是现在已知的东西，讲cache一致性的部分没有展开讲store buffer和invalid queue的东西，<br> barrier的东西都没有涉及。(跳过)</p>
<h2 id="第六章"><a href="#第六章" class="headerlink" title="第六章"></a>第六章</h2><p> 本章讲向量指令以及多核的东西，简单介绍了GPU的结构。 (跳过)</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>软件实现状态机要点</title>
    <url>/%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E7%8A%B6%E6%80%81%E6%9C%BA%E8%A6%81%E7%82%B9/</url>
    <content><![CDATA[<p>在比较复杂一点的软件系统建模的时候，有时需要理清楚系统的状态。这个时候画个系统<br>的状态机出来就很有帮助。注意对于一个系统，可能同时又好几个状态机，之所以这样，是<br>因为每个状态机关注的主题是不一样，每个独立的状态机只能保证关注的主题逻辑上是自恰<br>的。</p>
<p>我们以一个例子说明下。假设我们要为一个设备写一个Linux内核驱动，在驱动里这个设备<br>被抽象为:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct test_dev &#123;</span><br><span class="line">	int state;</span><br><span class="line">	int a;</span><br><span class="line">	int b;</span><br><span class="line">	int c;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这个设备需要初始化，然后配置下才能正常工作，这个设备出错的情况下需要复位这个设备。<br>所以我们给出的test_dev这个设备的状态有：初始化状态，正常工作状态，复位状态。在<br>判断可能的状态的时候，如果没有必要就不要新增加状态进来，一个状态机里每新增加一个<br>状态以后都会成为负担。</p>
<p>有了第一个状态，就可以找可以进入这个状态的激励(event), 比如，这里可能是:设备初始化。<br>基于第一个状态，分析在这个状态可能接收到的所有激励, 看看在这个状态的test_dev接收<br>到某个激励后对test_dev采取的动作(action)应该是怎么样的。比如这里对于初始化状态，<br>可能接收到的event可能有，1. 对test_dev配置，这个动作会使其进入正常工作状态；2.<br>用户还可以做初始化的逆操作，把这个设备释放掉。</p>
<p>重复对每一个状态做上面的分析。我们可以大概得到一个这样的状态机:<br><img src="/%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E7%8A%B6%E6%80%81%E6%9C%BA%E8%A6%81%E7%82%B9/test_dev_state.svg" alt="一个状态机示例"></p>
<p>接下来我们要针对每一个状态分析是否还有其他的event会发生，比如，我们有可能发现<br>这个设备驱动对应的fd文件上有ioctl，那就需要分析在每一种状态上，这个ioctl进来的<br>时候，test_dev是怎么变化的。当然，在某种状态的时候，我们是可以拒绝执行某种event<br>的。注意这里分析的中心还是test_dev的变化，因为我们这里要建立的是test_dev的状态机，<br>我们不能脱离test_dev而去分析比如fd的变化。再比如，我们这个设备驱动还mmap了一段<br>mmio空间到用户态去，用户态程序可以直接读写硬件寄存器，在test_dev的状态机中也不应<br>该去考虑用户态程序读写硬件寄存器这样的event，因为test_dev的状态机根本处理不了<br>这样的情况。针对用户态程序读写硬件寄存器这样的event，我们要单独分析。test_dev<br>状态机需要考虑的是test_dev结构里各个成员的变化。</p>
<p>完成对所有状态在全部event下的分析，我们的状态机就基本上建完了。剩下就是具体实现<br>的问题。实现中，要求所有对test_dev的action是原子的，也就是说状态变迁的过程是不能<br>被打断的，不然，可以看到我们会陷入各种各样同步带来的问题中。</p>
<p>还是看上面的图，从初始状态到正常工作状态的切换如果不是原子的，那么在这个过程中，<br>如果reset event来了，整个系统的状态将如何切换？如果，configure action里包括很多<br>步骤，而reset event可以在任何步骤到来，那么这个切换的分析工作将变得非常复杂。<br>状态切换变成原子操作将避免这样的情况发生，使得整个系统的状态变得可控。状态切换<br>是原子行为时，如果test_dev正从初始状态切换到工作状态，那么中间到来的reset event<br>将不得不排队，之后再在一个明确的状态响应reset event。</p>
<p>保证原子操作，一个简单的办法就是执行action的时候加锁。</p>
]]></content>
      <tags>
        <tag>软件架构</tag>
      </tags>
  </entry>
  <entry>
    <title>软件之间的兼容性问题分析</title>
    <url>/%E8%BD%AF%E4%BB%B6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     +-------+   +------+</span><br><span class="line">     |  app  |   | app  |</span><br><span class="line">     +-------+   +------+</span><br><span class="line">              \ /</span><br><span class="line">               X</span><br><span class="line">              / \</span><br><span class="line">             /   \</span><br><span class="line">     +-------+   +------+</span><br><span class="line">     |  lib  |   | lib  |</span><br><span class="line">     +-------+   +------+</span><br><span class="line">              \  /</span><br><span class="line">               \/</span><br><span class="line">               /\</span><br><span class="line">              /  \</span><br><span class="line">     +--------+  +--------+</span><br><span class="line">     | kernel |  | kernel |</span><br><span class="line">     +--------+  +--------+</span><br><span class="line"></span><br><span class="line">old ------------------------&gt; new</span><br></pre></td></tr></table></figure>
<p> 如上图，我们需要解决的是新老软件版本之间可以兼容使用的问题。</p>
<h2 id="kernel和user-space"><a href="#kernel和user-space" class="headerlink" title="kernel和user space"></a>kernel和user space</h2><p> 内核和用户态接口包括，系统调用、设备文件、sysfs/proc等。这些接口可以看成是独立<br> 的功能。所以，在用户态使用一个如上的接口时，可以先检测是否有这样的接口。对于<br> 设备文件和sysfs/proc很容易判断一个文件是否存在，对于系统调用如果一个接口底层<br> 驱动没有支持，用户态应该得到不支持的错误码，这需要内核驱动做必要的异常处理并<br> 返回错误码。</p>
<p> 我们考虑lib和kernel之间的新老兼容问题。对于老的内核，新的lib，lib中的代码依赖<br> 老的kernel接口编程，并先要检测kernel的接口是否可以使用，之所有要检测kernel接口<br> 是否可以使用，是为了防止随后kernel版本里删除之前的接口，造成lib的break; 内核<br> 升级但是lib还是老的情况，kernel里新增的接口lib里使用不到是正常现象，kernel里<br> 删除的接口(一般不会发生)，lib在之前使用的时候已经先判断是否支持，考虑的逻辑已经<br> 存在。</p>
<h2 id="user-space-lib和app"><a href="#user-space-lib和app" class="headerlink" title="user space lib和app"></a>user space lib和app</h2><p> lib和APP之间的接口一般是函数接口，比较难做成特性独立定义。那么在lib发展的过程<br> 中，删除一个特性，必然造成接口的不兼容。所以，要实现新旧库和APP相互兼容，我们<br> 可以lib库里的特性持续增加，并在增加特性的时候做好库版本的定义升级。APP在编程的<br> 时候根据lib版本决定是否可以使用某一个特性。</p>
<p> 持续增加lib中的接口必然造成库的膨胀，可以给将来不计划使用的接口加上deprecation<br> 的标记，提示用户相关接口将会在未来弃用。</p>
<p>[1] <a href="https://wangzhou.github.io/Linux%E9%A9%B1%E5%8A%A8%E8%BD%AF%E7%A1%AC%E4%BB%B6%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98%E7%9A%84%E8%80%83%E8%99%91/">https://wangzhou.github.io/Linux驱动软硬件兼容问题的考虑/</a></p>
]]></content>
      <tags>
        <tag>软件架构</tag>
      </tags>
  </entry>
  <entry>
    <title>超标量处理器设计笔记</title>
    <url>/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E8%AE%BE%E8%AE%A1%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p> 为了使处理器里的流水线全速工作，我们要尽量去掉指令和指令之间的依赖。指令之间的<br> 依赖有WAW，WAR，RAW以及控制依赖，只有RAW和控制依赖是真依赖，其他两种依赖都可以<br> 靠寄存器重命名来缓解。load/store地址之间也存在着依赖。</p>
<p> 一个周期可以去取多条指令送到流水线里执行，这个处理器就是超标量处理器了。超标量<br> 处理器又分顺序执行和乱序执行。所谓乱序执行，是指处理器内部执行指令的时候是乱序<br> 执行的，从程序员的视角看，程序是按顺序执行的，不过，对于不同处理器会有不同程度<br> 的放松，比如在没有数据依赖的时候，对不同地址的load/store可能是乱序完成的。</p>
<p> 一个周期执行多条指令并不是说一个处理器里有多条一样的流水线，一个处理器里有多个<br> 不同的执行单元就可以把不同的指令发射到不同的执行单元里执行。</p>
<p> 一般我们可以把执行单元分为：ALU(ADD/LOGIC)，ALU(MUL/DIV), LOAD/STORE, FP/SIMD等。<br> 一个超标量处理器的大概模型如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                            +-------+   +----+</span><br><span class="line">                  +-----+--&gt;|issue q|--&gt;| EX |\</span><br><span class="line">                  |issue|   +-------+   +----+ \</span><br><span class="line">                  |logic|                       \                     +---+</span><br><span class="line">+----+    +----+  |     |   +-------+   +----+   \+-----+    +----+--&gt;|ROB|</span><br><span class="line">| IF |---&gt;| ID |-&gt;|     |--&gt;|issue q|--&gt;| EX |---&gt;| MEM |---&gt;| WB |   +---+</span><br><span class="line">+----+    +----+  |     |   +-------+   +----+   /+-----+    +----+--&gt;+---+</span><br><span class="line">                  |     |                       /                     |SB |</span><br><span class="line">                  |     |   +-------+   +----+ /                      +---+</span><br><span class="line">                  +-----+--&gt;|issue q|--&gt;| EX |/</span><br><span class="line">                            +-------+   +----+</span><br></pre></td></tr></table></figure>
<p> 一般，我们把取指(IF)、译码(ID)看做前端(front-end)，发射(issue)、执行(EX)、内存操作(MEM)<br> 写回(WB)以及提交叫做后端(back-end)。</p>
<h2 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h2><p> todo</p>
<h2 id="虚拟存储器"><a href="#虚拟存储器" class="headerlink" title="虚拟存储器"></a>虚拟存储器</h2><p> todo</p>
<h2 id="分支预测"><a href="#分支预测" class="headerlink" title="分支预测"></a>分支预测</h2><p> 想要充分使用流水线就要预取指令执行，顺序执行的指令直接预取就可以，但是到了分支<br> 的时候，就要选择一条分支来预取，这时就存在各种分支预测的实现方式。分支预测的各种<br> 实现可以暂时当做黑盒来对待。预取的指令可以提前在流水线里执行，但是在分支结果出来<br> 之前不能提交。分支结果出来后，如果预取分支是对的，那么直接提交执行的结果就可以，<br> 但是，如果分支取错了就要把之前计算出的结果和正在流水线里跑的错误预取的指令和状态冲刷<br> 掉，而且还要恢复到指令分支时的状态，继续取正取的分支放到流水线里运行。</p>
<p> 可见，如果分支预测的不准就会老是冲刷流水线，后端的执行单元得不到充分利用，用perf<br> 看就是back-end stall增大。</p>
<p> 分支预测失败的时候要尽快回退到分支点的状态，硬件可以使用checkpoint的方式恢复，<br> 大概是分支预取的时候把整个状态统一保存，分支预测失败的时候统一一把恢复回来。</p>
<h2 id="指令解码"><a href="#指令解码" class="headerlink" title="指令解码"></a>指令解码</h2><p> 解码阶段需要做寄存器的重命名，有时还有把一条指令内部拆分成几条指令。对于超标量<br> 处理器，可以同时解码多条指令。这个阶段还是顺序进行的，比如4发射的处理器，就同时<br> 有四条指令送去解码。</p>
<p> 为了使的程序执行对外保持逻辑顺序，处理器实现里一般使用重排序缓冲(ROB reorder buffer)<br> 在提交之前重排序。ROB的设计逻辑很简单，就是以指令取入的顺序在取入的时候就按顺序<br> 存在ROB这个队列里，ROB为每个指令都维护了相关的状态，ROB里的指令提交的时候是顺序<br> 进行的，这样就可以保证先进入的指令先离开处理器。这会导致ROB会被执行慢的指令堵住，<br> 不过这也没有关系，虽然当前的执行堵住了，但是后面的指令已经在ROB里就绪，一旦这个<br> 堵住ROB的指令提交，后面的完成的指令可以一次都提交了。当然受ROB深度的限制，一旦<br> ROB慢了，就会反压前端的取指单元，这时就会导致front-end stall增大。</p>
<p> 我们可以先跑一个慢的指令比如浮点除法，后面不断的跑NOP指令，这样ROB会被填满，导致<br> front-end stall。执行单元以及执行的issue queue满了，都会导致前端反压，构造一个<br> 这样的场景看看？</p>
<p> 分支预测失败进行回退的时候也要清ROB里的内容。</p>
<h2 id="寄存器重命名"><a href="#寄存器重命名" class="headerlink" title="寄存器重命名"></a>寄存器重命名</h2><p> 使用寄存器重命名消除WAW和WAR依赖。具体的实现方式有很多，基本思路就是要识别出代码<br> 中的依赖，然后重命名消除依赖，指令执行完毕后要把重命名的寄存器重新返回到软件感知<br> 的构架寄存器上。硬件为了重命名要维护重命名相关的表格。当分支预测失败的时候，也要<br> 回退寄存器重命名占用的相关资源。</p>
<h2 id="发射"><a href="#发射" class="headerlink" title="发射"></a>发射</h2><p> 进入发射阶段之前已经完成寄存器重命名，发射把指令分发到各个执行单元之前的issue queue<br> 里，issue queue里的各个指令在资源OK时就被发送到对应的执行单元里执行。这里的资源<br> OK是指指令依赖的操作数的值都计算出来了、相关的计算单元空闲。可见，发射阶段要处理<br> 的逻辑是：1. 明确相关的依赖，依次决定何时执行指令；2. 需要有逻辑检测指令是否满足<br> 执行条件。</p>
<p> 详细逻辑后续看需要再补齐。</p>
<h2 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h2><p> 执行阶段除了要执行运算任务，还有就是要考虑旁路网络的逻辑。当指令有前后之间有依赖<br> 的时候，后一条指令需要等到前一条指令提交后再执行，这样会使流水线闲置，处理器里<br> 可以加入一定的前递逻辑，就是这里的旁路网络，这个逻辑在前一条指令计算结果得出后<br> 就可以把结果直接前递给后一条指令执行。</p>
<p> 对于load/store这种存储指令，因为他们执行的时候可能有比较大的时延，需要考虑相应<br> 的优化办法。一般的优化办法是，对于没有数据依赖的load/store指令，放松条件允许他们<br> 乱序执行，这本书里介绍的是，一般多个store指令是按循序执行的，没有数据依赖的时候<br> load指令可以提前到store指令前执行，因为load指令常常作为被依赖的指令。</p>
<h2 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h2><p> 提交阶段围绕ROB展开，ROB里的指令在完成时就可以提交，提交后软件就可以感知，可能<br> 是架构寄存器的值被更新，也可能是cache内容被更新。分支预取的指令也会排到ROB里，<br> 所以，当分支预测失败的时候，ROB还要有相关逻辑处理资源回退。处理器在执行指令的<br> 时候可能会出现异常和外部中断，比如存储器访问指令访问了错误地址、访问的物理页面<br> 不存在、外部设备给处理器发了一个中断，处理器要做到所谓精确异常，就是引发异常指令<br> 之前的指令都可以正确执行，引发异常的指令上报异常，处理器的PC需要跳到异常向量处<br> 执行代码。处理器很难在指令执行的时候就同步处理如上的逻辑，当执行指令异常发生的<br> 时候，处理器把指令异常的信息记录到ROB里，当指令提交的时候异常指令就可以得到处理，<br> 可以看到流水线里还存在异常指令之后的指令，异常处理逻辑需要把流水线里的这些残留指令<br> 排空再跳到异常向量处执行代码。可以看到异常很多的时候，不断排空流水线也会导致<br> back-end stall。外部中断的处理逻辑和异常的基本类似。</p>
<p> 注意，上面提到的store buffer存在于处理器内部，在store指令提交的时候把store buffer<br> 里的内容写到存储器上。如果写存储器时延太长，store buffer就有可能满，从而反压之前<br> load/store执行单元的issue queue，从而又有可能反压前端。</p>
<p> 有的处理器，比如ARM上，支持乱序，不只是内部执行可以乱序，从程序员的角度看，对于<br> 单个处理器(core)没有依赖的store/load也支持完成是乱序的。这里的逻辑需要引入处理器<br> core外部的逻辑看来，一般认为L1、L2 cache和core紧密相关，L3cache和内存是多核共享<br> 的，core和存储器(cache以及内存)之间靠总线互联，core和存储器之间的通信遵守总线协议，<br> 比如ARM的AXI协议，对于一个写操作，core发出后需要等对端应答，支持乱序完成的总线协议<br> (Out of order transaction completion)中，写操作可以不用对端应答继续发下一个写操作，<br> 经过总线传输，在存储器端完成的时候就有可能是乱序的，对于读操作也是一样可以乱序返回<br> 的。</p>
<p> 在多核的情况下，因为有store buffer和invalid queue会带来乱序完成，这个逻辑也需要<br> 整合进来。初步看，这个逻辑和上面的逻辑似乎是两个正交的逻辑。</p>
<p> 还有一个问题是，处理器怎么划定依赖的范围，比如，如果后续一个指令依赖之前的一个<br> 指令，之间间隔很大，处理器会不会看不到这样的依赖，先执行完成了后面的指令？仔细想下<br> 是不会出现这样的问题的，处理器在当前整个处理状态下，一定可以检测出所有的依赖，所以<br> 如果相互依赖而且跨度较大的两条指令，在处理器的当前状态内(包含整个流水线的范围内)<br> 是不会出问题的，如果第一条在流水线里，后面的一条在流水线外，因为后一条还没有执行，<br> 所以也不会出问题。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>锁使用的一些笔记</title>
    <url>/%E9%94%81%E4%BD%BF%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="使用的场景"><a href="#使用的场景" class="headerlink" title="使用的场景"></a>使用的场景</h2><p>   当一个数据结构有多个并发的流程去访问的时候，可以加锁去做互斥，这样数据的<br>   一致性的到保护。并发流程有多种表现形式，比如在linux内核里，内核线程，中断，<br>   来自用户态的系统调用等都可以并发起来; 用户态的话, 各个线程，信号(信号处理<br>   函数里不能使用锁)可以并发。</p>
<p>   加锁其实就是多个执行流在临界区外排队(这里不考虑try锁，就是那种试一下可以加上<br>   就加锁，不可以加上当场就返回的锁)。我们也可以给临界区配置一个原子变量标记，<br>   每个执行流在先抢到这个标记才可以操作保护的数据结构, 操作完保护的数据结构，<br>   退出临界区的时候释放这个标记。原子变量标记的方式，其实是用try锁去保护相应的<br>   数据结构。但是try锁没有了排队等待，需要在加锁失败时做必要的处理。</p>
<h2 id="使用时注意的事项"><a href="#使用时注意的事项" class="headerlink" title="使用时注意的事项"></a>使用时注意的事项</h2><ul>
<li><p>锁使用时最需要注意的就是死锁，死锁的最典型方式是两把锁交叉加锁:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  	thread1        thread2</span><br><span class="line"></span><br><span class="line">lock1          lock2</span><br><span class="line">[...]          [...]</span><br><span class="line">lock2          lock1</span><br></pre></td></tr></table></figure>
<p>如上，thread1加了lock1，执行下面的代码，thead2加了lock2。这时, thread1想<br>要加lock2，但是加不上，thread2想要加lock1的，但是也加不上。</p>
<p>同一个执行流重复加一把锁也会带来死锁:</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    	thread1</span><br><span class="line"></span><br><span class="line">lock1</span><br><span class="line">lock1</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<pre><code> 使用锁的时候要注意锁的不同种类，一般有spinlock和mutex，他们都有对应的读写
 锁的版本。一般情况我们可以先不用读写锁，直到确实是性能瓶颈, 我们才去做优化。
 spinlock是死循环等待获取锁的，mutex在获取锁失败后会sleep, 直到可以得到锁。
 所以在不可以sleep的场景里，我们要用spinlock锁, 和mutex比较，spinlock不sleep，
 所以，spinlock也适用于临界区很短的加锁保护。
</code></pre>
<h2 id="调试锁相关代码"><a href="#调试锁相关代码" class="headerlink" title="调试锁相关代码"></a>调试锁相关代码</h2><p>   一般我们写好Linux内核里所相关的代码，可以打开内核里死锁检测:<br>   <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vYXJub2xkbHUvcC84NTgwMzg3Lmh0bWw=">https://www.cnblogs.com/arnoldlu/p/8580387.html<i class="fa fa-external-link-alt"></i></span><br>   hange检测: Kernel hacking —&gt; Debug Lockups and Hangs</p>
<p>   死锁检测会检测出潜在的死锁位置，然后输出报告。<br>   hange检测在超出配置的超时时间后会把调用栈打出来。</p>
<p>   不过这些检测打开后会对系统性能有较大的影响，这些配置只能在调试版本里打开。</p>
<p>   (to do: 用户态死锁检测的工具)</p>
<h2 id="加锁后相关的性能问题"><a href="#加锁后相关的性能问题" class="headerlink" title="加锁后相关的性能问题"></a>加锁后相关的性能问题</h2><p>   为了维护并发访问的资源，所以需要加锁。所以，加锁之后有可能带来的性能下降，<br>   本质上是各个执行流相互等待带来的开销。所以，要提高性能，还是要把各个执行流的<br>   相互依赖解开。</p>
<h2 id="加锁对构架演进的影响"><a href="#加锁对构架演进的影响" class="headerlink" title="加锁对构架演进的影响"></a>加锁对构架演进的影响</h2><p>   锁临界区太大，会对后续添加新的锁进来产生影响，比较容易造成各种死锁问题。</p>
<h2 id="锁的实现"><a href="#锁的实现" class="headerlink" title="锁的实现"></a>锁的实现</h2><p>   (to do: spinlock, q spinlock, 原子变量简单实现spinlock, mutex)</p>
]]></content>
      <tags>
        <tag>Linux用户态编程</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title>网络精彩文章收集</title>
    <url>/%E7%BD%91%E7%BB%9C%E7%B2%BE%E5%BD%A9%E6%96%87%E7%AB%A0%E6%94%B6%E9%9B%86/</url>
    <content><![CDATA[<ul>
<li><p>论Linux页面迁移</p>
<p>宋宝华老师讲Linux迁移的文章:<br><span class="exturl" data-url="aHR0cDovL25ld3MuZWV3b3JsZC5jb20uY24vbXAveW1jL2E5Mzk0NC5qc3B4">http://news.eeworld.com.cn/mp/ymc/a93944.jspx<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>Memory Barriers: a Hardware View for Software Hackers</p>
<p>原文是Paul大神写的，从CPU硬件实现的角度为软件人员讲Barrier的原理。<br>转载的是蜗窝科技翻译的文章。此文甚好，系统软件人员值得阅读:</p>
<p><span class="exturl" data-url="aHR0cDovL3d3dy53b3dvdGVjaC5uZXQva2VybmVsX3N5bmNocm9uaXphdGlvbi9XaHktTWVtb3J5LUJhcnJpZXJzLmh0bWw=">http://www.wowotech.net/kernel_synchronization/Why-Memory-Barriers.html<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>原理和实战解析Linux中的如何正确地使用内存屏障</p>
<p>宋宝华老师讲解Linux中内存屏障的文章，内容深入浅出：<br><span class="exturl" data-url="aHR0cHM6Ly93d3cuZWV0LWNoaW5hLmNvbS9tcC9hMTU5MDA2Lmh0bWw=">https://www.eet-china.com/mp/a159006.html<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>深入理解cache对写好代码至关重要</p>
<p>宋宝华老师讲解cache的文章，内容深入浅出：<br><span class="exturl" data-url="aHR0cHM6Ly9jbG91ZC50ZW5jZW50LmNvbS9kZXZlbG9wZXIvYXJ0aWNsZS8xODQ0OTky">https://cloud.tencent.com/developer/article/1844992<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>Arm Neoverse N1 Core: Performance Analysis Methodology</p>
<p>基于Arm Neoverse N1 Core，讲解怎么利用PMU做软件调优：<br><span class="exturl" data-url="aHR0cHM6Ly9hcm1rZWlsLmJsb2IuY29yZS53aW5kb3dzLm5ldC9kZXZlbG9wZXIvRmlsZXMvcGRmL3doaXRlLXBhcGVyL25lb3ZlcnNlLW4xLWNvcmUtcGVyZm9ybWFuY2UtdjIucGRm">https://armkeil.blob.core.windows.net/developer/Files/pdf/white-paper/neoverse-n1-core-performance-v2.pdf<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>Writing a Linux Debugger</p>
<p>这个系列博客一共10篇，介绍怎么写一个基于DWARF调试信息的简单调试器：<br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLnRhcnRhbmxsYW1hLnh5ei93cml0aW5nLWEtbGludXgtZGVidWdnZXItc2V0dXAv">https://blog.tartanllama.xyz/writing-a-linux-debugger-setup/<i class="fa fa-external-link-alt"></i></span></p>
</li>
</ul>
]]></content>
      <categories>
        <category>reprint</category>
      </categories>
  </entry>
  <entry>
    <title>重读《程序员的自我修养》</title>
    <url>/%E9%87%8D%E8%AF%BB%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B/</url>
    <content><![CDATA[<h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p> 编译链接把代码翻译成机器可以执行的机器码，把编译得到的数据copy的内存上，把CPU<br> PC写成第一条指令的地址，机器就可以执行这些机器码。了解编译链接的过程，会使得我们<br> 更加透彻的理解计算机的行为。</p>
<p> 编译链接的过程，基本上可以分为：编译预处理，编译，汇编，链接，其中编译、链接最<br> 复杂，编译又可以分为词法分析、语法分析和语意分析，链接又分为静态链接和动态链接。<br> 上面的每个步骤都有专门工具来做，像gcc这样的编译器只是把这所有的相关工具打包在一起。</p>
<p> 要分析编译链接的整个过程，就要逐个分析上面提到的每一个点。汇编文件以及最终的可执行<br> 文件的格式也是我们重点要专注的，需要了解分析可执行文件的工具。</p>
<h2 id="预处理、编译和汇编"><a href="#预处理、编译和汇编" class="headerlink" title="预处理、编译和汇编"></a>预处理、编译和汇编</h2><p> 编译预处理做头文件展开和宏的替换，单独执行可以：gcc -E test.c -o test.i</p>
<p> 编译是把高级语言的文本转换成汇编语言的文本，只执行编译可以gcc -S test.c -o test.s<br> 编译得到的文件是汇编语言的文本，还是人可以直接阅读的程序，不是机器码。编译针对<br> 一个文件进行，当这个文件里要访问其他文件里定义的函数和变量时，编译阶段无法的到<br> 相关的地址。</p>
<p> 汇编是把汇编语言翻译成机器码，从高级语言到汇编完成可以：gcc -c test.c -o test.o<br> 得到的目标文件中的其他文件中定义的函数和变量的地址还是不能确定。</p>
<p> 链接是把多个目标文件，最终整合成机器可以执行的机器码。包括所有地址的确定，链接器<br> 的行为可以被链接脚本或者链接参数控制，比如，可以指定程序的起始地址，可以执行各个<br> 段的地址，gcc有默认的链接脚本，程序也可以提供自己的链接脚本，比如，risv内核的链接<br> 脚本在kernel/arch/riscv/kernel/vmlinux.lds</p>
<p> 我们配合一段小程序看看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define FUNC add</span><br><span class="line"></span><br><span class="line">int add(int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">        int c;        </span><br><span class="line"></span><br><span class="line">        c = a + b;</span><br><span class="line"></span><br><span class="line">        return c;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	int d;</span><br><span class="line"></span><br><span class="line">	d = FUNC(2, 5);</span><br><span class="line"></span><br><span class="line">        return 0;        </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 如下这样编译，nostartfiles是告诉gcc不要链接标准的startup相关的目标文件，-e main<br> 是告诉gcc把main函数的地址地址作为程序的入口地址。riscv64-linux-gnu-objdump -d test &gt; test.s<br> 反汇编可以看出test的二进制只有main和add函数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv64-linux-gnu-gcc test.c -nostartfiles -e main -o test --static</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test:     file format elf64-littleriscv</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Disassembly of section .text:</span><br><span class="line"></span><br><span class="line">000000000001010c &lt;add&gt;:</span><br><span class="line">   1010c:	7179                	addi	sp,sp,-48</span><br><span class="line">   1010e:	f422                	sd	s0,40(sp)</span><br><span class="line">   10110:	1800                	addi	s0,sp,48</span><br><span class="line">   10112:	87aa                	mv	a5,a0</span><br><span class="line">   10114:	872e                	mv	a4,a1</span><br><span class="line">   10116:	fcf42e23          	sw	a5,-36(s0)</span><br><span class="line">   1011a:	87ba                	mv	a5,a4</span><br><span class="line">   1011c:	fcf42c23          	sw	a5,-40(s0)</span><br><span class="line">   10120:	fdc42703          	lw	a4,-36(s0)</span><br><span class="line">   10124:	fd842783          	lw	a5,-40(s0)</span><br><span class="line">   10128:	9fb9                	addw	a5,a5,a4</span><br><span class="line">   1012a:	fef42623          	sw	a5,-20(s0)</span><br><span class="line">   1012e:	fec42783          	lw	a5,-20(s0)</span><br><span class="line">   10132:	853e                	mv	a0,a5</span><br><span class="line">   10134:	7422                	ld	s0,40(sp)</span><br><span class="line">   10136:	6145                	addi	sp,sp,48</span><br><span class="line">   10138:	8082                	ret</span><br><span class="line"></span><br><span class="line">000000000001013a &lt;main&gt;:</span><br><span class="line">   1013a:	1101                	addi	sp,sp,-32</span><br><span class="line">   1013c:	ec06                	sd	ra,24(sp)</span><br><span class="line">   1013e:	e822                	sd	s0,16(sp)</span><br><span class="line">   10140:	1000                	addi	s0,sp,32</span><br><span class="line">   10142:	4595                	li	a1,5</span><br><span class="line">   10144:	4509                	li	a0,2</span><br><span class="line">   10146:	fc7ff0ef          	jal	ra,1010c &lt;add&gt;</span><br><span class="line">   1014a:	87aa                	mv	a5,a0</span><br><span class="line">   1014c:	fef42623          	sw	a5,-20(s0)</span><br><span class="line">   10150:	4781                	li	a5,0</span><br><span class="line">   10152:	853e                	mv	a0,a5</span><br><span class="line">   10154:	60e2                	ld	ra,24(sp)</span><br><span class="line">   10156:	6442                	ld	s0,16(sp)</span><br><span class="line">   10158:	6105                	addi	sp,sp,32</span><br><span class="line">   1015a:	8082                	ret</span><br></pre></td></tr></table></figure>
<p> 用qemu-risv64运行它会segment fault, 运行的命令是qemu-riscv64 -d in_asm,cpu -D ./log test</p>
<p> 查看日志看看程序到底是怎么运行的，这里直接在必要的地方加上注释。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">----------------</span><br><span class="line">IN: main</span><br><span class="line">0x000000000001013a:  1101              addi            sp,sp,-32                // 可以看到程序直接是从main开始跑的</span><br><span class="line">0x000000000001013c:  ec06              sd              ra,24(sp)                // 当然如果链接gcc startup，main就不是程序入口了</span><br><span class="line">0x000000000001013e:  e822              sd              s0,16(sp)</span><br><span class="line">0x0000000000010140:  1000              addi            s0,sp,32</span><br><span class="line">0x0000000000010142:  4595              addi            a1,zero,5</span><br><span class="line">0x0000000000010144:  4509              addi            a0,zero,2</span><br><span class="line">0x0000000000010146:  fc7ff0ef          jal             ra,-58          # 0x1010c</span><br><span class="line"></span><br><span class="line"> pc       000000000001013a</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 0000000000000000 x2/sp 0000004000800390 x3/gp 0000000000000000</span><br><span class="line"> x4/tp 0000000000000000 x5/t0 0000000000000000 x6/t1 0000000000000000 x7/t2 0000000000000000</span><br><span class="line"> x8/s0 0000000000000000 x9/s1 0000000000000000 x10/a0 0000000000000000 x11/a1 0000000000000000</span><br><span class="line"> x12/a2 0000000000000000 x13/a3 0000000000000000 x14/a4 0000000000000000 x15/a5 0000000000000000</span><br><span class="line"> x16/a6 0000000000000000 x17/a7 0000000000000000 x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 0000000000000000 x29/t4 0000000000000000 x30/t5 0000000000000000 x31/t6 0000000000000000</span><br><span class="line">----------------</span><br><span class="line">IN: add</span><br><span class="line">0x000000000001010c:  7179              addi            sp,sp,-48               // 程序是跑到add了</span><br><span class="line">0x000000000001010e:  f422              sd              s0,40(sp)</span><br><span class="line">0x0000000000010110:  1800              addi            s0,sp,48</span><br><span class="line">0x0000000000010112:  87aa              mv              a5,a0</span><br><span class="line">0x0000000000010114:  872e              mv              a4,a1</span><br><span class="line">0x0000000000010116:  fcf42e23          sw              a5,-36(s0)</span><br><span class="line">0x000000000001011a:  87ba              mv              a5,a4</span><br><span class="line">0x000000000001011c:  fcf42c23          sw              a5,-40(s0)</span><br><span class="line">0x0000000000010120:  fdc42703          lw              a4,-36(s0)</span><br><span class="line">0x0000000000010124:  fd842783          lw              a5,-40(s0)</span><br><span class="line">0x0000000000010128:  9fb9              addw            a5,a5,a4</span><br><span class="line">0x000000000001012a:  fef42623          sw              a5,-20(s0)</span><br><span class="line">0x000000000001012e:  fec42783          lw              a5,-20(s0)</span><br><span class="line">0x0000000000010132:  853e              mv              a0,a5</span><br><span class="line">0x0000000000010134:  7422              ld              s0,40(sp)</span><br><span class="line">0x0000000000010136:  6145              addi            sp,sp,48</span><br><span class="line">0x0000000000010138:  8082              ret             </span><br><span class="line"></span><br><span class="line"> pc       000000000001010c</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 000000000001014a x2/sp 0000004000800370 x3/gp 0000000000000000</span><br><span class="line"> x4/tp 0000000000000000 x5/t0 0000000000000000 x6/t1 0000000000000000 x7/t2 0000000000000000</span><br><span class="line"> x8/s0 0000004000800390 x9/s1 0000000000000000 x10/a0 0000000000000002 x11/a1 0000000000000005     // add执行前的a0寄存器是2，a1是5</span><br><span class="line"> x12/a2 0000000000000000 x13/a3 0000000000000000 x14/a4 0000000000000000 x15/a5 0000000000000000   // qemu这里显示的本次执行前cpu的状态</span><br><span class="line"> x16/a6 0000000000000000 x17/a7 0000000000000000 x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 0000000000000000 x29/t4 0000000000000000 x30/t5 0000000000000000 x31/t6 0000000000000000</span><br><span class="line">----------------</span><br><span class="line">IN: main</span><br><span class="line">0x000000000001014a:  87aa              mv              a5,a0</span><br><span class="line">0x000000000001014c:  fef42623          sw              a5,-20(s0)</span><br><span class="line">0x0000000000010150:  4781              mv              a5,zero</span><br><span class="line">0x0000000000010152:  853e              mv              a0,a5</span><br><span class="line">0x0000000000010154:  60e2              ld              ra,24(sp)</span><br><span class="line">0x0000000000010156:  6442              ld              s0,16(sp)</span><br><span class="line">0x0000000000010158:  6105              addi            sp,sp,32</span><br><span class="line">0x000000000001015a:  8082              ret             </span><br><span class="line"></span><br><span class="line"> pc       000000000001014a</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 000000000001014a x2/sp 0000004000800370 x3/gp 0000000000000000</span><br><span class="line"> x4/tp 0000000000000000 x5/t0 0000000000000000 x6/t1 0000000000000000 x7/t2 0000000000000000</span><br><span class="line"> x8/s0 0000004000800390 x9/s1 0000000000000000 x10/a0 0000000000000007 x11/a1 0000000000000005   // a0是7, add的返回值</span><br><span class="line"> x12/a2 0000000000000000 x13/a3 0000000000000000 x14/a4 0000000000000002 x15/a5 0000000000000007</span><br><span class="line"> x16/a6 0000000000000000 x17/a7 0000000000000000 x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 0000000000000000 x29/t4 0000000000000000 x30/t5 0000000000000000 x31/t6 0000000000000000</span><br><span class="line"> // 后面就没有log了，这里也是segment fault的地方。这个地方出错是显然的，gcc的startup</span><br><span class="line"> // 会在main函数后用一个exit系统调用告诉内核自己退出，这里没有，程序一定会出错。</span><br></pre></td></tr></table></figure>

<h2 id="ELF文件"><a href="#ELF文件" class="headerlink" title="ELF文件"></a>ELF文件</h2><p> ELF文件的格式大体上是，ELF head + section headers + progam headers + 各个段的内容。<br> readelf, objdump, nm，strings等都可以解析ELF文件里的信息。progam headers描述的<br> 是segment的信息，我们可以自定义section，程序中是直接可以引用section相关的符号的<br> 地址的，比如内核就把初始化的函数放到一个.init.text section，内核初始化的时候统一<br> 执行一遍，不同的section可能有相同的读写执行的属性，可以把多个属性相同的section<br> 放到一个segment里。</p>
<p> riscv64-linux-gnu-readelf -h test 显示的ELF的头信息如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ELF Header:</span><br><span class="line">  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 </span><br><span class="line">  Class:                             ELF64</span><br><span class="line">  Data:                              2&#x27;s complement, little endian</span><br><span class="line">  Version:                           1 (current)</span><br><span class="line">  OS/ABI:                            UNIX - System V</span><br><span class="line">  ABI Version:                       0</span><br><span class="line">  Type:                              EXEC (Executable file)</span><br><span class="line">  Machine:                           RISC-V</span><br><span class="line">  Version:                           0x1</span><br><span class="line">  Entry point address:               0x1013a               // 程序的入口在main</span><br><span class="line">  Start of program headers:          64 (bytes into file)</span><br><span class="line">  Start of section headers:          896 (bytes into file)</span><br><span class="line">  Flags:                             0x5, RVC, double-float ABI</span><br><span class="line">  Size of this header:               64 (bytes)</span><br><span class="line">  Size of program headers:           56 (bytes)</span><br><span class="line">  Number of program headers:         3</span><br><span class="line">  Size of section headers:           64 (bytes)</span><br><span class="line">  Number of section headers:         7</span><br><span class="line">  Section header string table index: 6</span><br></pre></td></tr></table></figure>

<p> 最长看的ELF信息应该是符号表了，程序的符号信息一般放到符号表里，符号表是一个section。<br> readefl -s 或者nm都可以打出符号表，可以查看这个符号的地址，这个地址是编译生成的<br> 符号的地址，对于用户态程序，就是符号的虚拟地址，运行起来也是一样值，对于内核，<br> 是符号的虚拟地址，当内核运行在mmu打开之前，符号的实际运行地址和符号表里的地址是<br> 不一样的，比如，riscv内核的链接脚本把_start这个符号的地址配置成了0xffffffff80000000，<br> 而qemu把内核加载到了0x80200000的物理地址上，_start是内核的入口，实际运行时还没有<br> 开mmu，_start的地址运行的地址是0x80200000。</p>
<h2 id="静态链接"><a href="#静态链接" class="headerlink" title="静态链接"></a>静态链接</h2><p> 静态链接把库函数直接链接到了二进制中，增大了用户程序, 实际上不同用户态程序对应<br> 的库函数的代码只要有一套就好。</p>
<p> 静态库生成。</p>
<h2 id="动态链接"><a href="#动态链接" class="headerlink" title="动态链接"></a>动态链接</h2><p> (todo)</p>
<p> 动态库加载，动态库查找。</p>
<h2 id="内核链接"><a href="#内核链接" class="headerlink" title="内核链接"></a>内核链接</h2><p> 内核使用自己的链接脚本链接，riscv下在arch/riscv/kernel/vmlinux.ld.S，这个脚本<br> 主要就是指定符号以及section的链接地址，还指定了程序入口。</p>
<p> 比如用ENTRY指定程序的入口是_start，为_start这个符号分配的链接地址是LOAD_OFFSET，<br> riscv上是PAGE_OFFSET，一般这个值是0xffffffff80000000，不同的配置这个值是不一样<br> 的。</p>
<p> 链接地址和CPU取指时发出来来的地址可能是不一样，从硬件的角度看，CPU只时从PC寄存器<br> 指定的地方取指令，如果MMU时开着就翻译下，用翻译得到的地址去取指令，如果MMU没有<br> 开，那就直接用PC地址取指令执行了。一开始BIOS把内核Image加载到一段内存上执行，_start<br> 就直接放到那段内存的起始地址，比如用qemu+opensbi的化，这个地址是0x80200000，一直到<br> MMU开始前是无法用链接得到的虚拟地址直接寻址的，编译器和内核需要用地址无关的指令<br> 去寻址，具体可以参考《riscv head.S分析》。</p>
]]></content>
      <tags>
        <tag>编译链接</tag>
      </tags>
  </entry>
  <entry>
    <title>ARM SMMUv3 architecture</title>
    <url>/ARM-SMMUv3-architecture/</url>
    <content><![CDATA[<p>IOMMU是外设的MMU。原来的外设主动发起的DMA的操作使用的都是系统的物理地址，直接<br>使用物理地址有很多不方便的地方，在外设和内存之间引入一个新的IOMMU硬件，完成一些<br>诸如地址翻译的功能，这样又有很多新的玩法可以加进来。</p>
<p>加入IOMMU上的这个地址翻译，就可以引入翻译时候的权限管理，这样保证了外设发出的<br>访问系统内存的地址是安全的。加上地址翻译，还可以把一片连续的虚拟地址空间映射到<br>诸多离散的物理地址上，这样满足一部分设备要访问连续大地址的需要。IOMMU把外设<br>变得更像CPU，CPU和外设在使用内存方面都只看到虚拟地址, 如果虚拟地址到物理地址的<br>映射在CPU MMU和IOMMU上是一样的，CPU和外设将可以看到相同的虚拟地址空间。CPU MMU<br>上的七七八八的功能上都可以在IOMMU上都加上。</p>
<p>CPU core和MMU是绑在一起的，而IOMMU和外设是独立的两个设备, IOMMU一般是不做在<br>外设里的，不然带IOMMU的外设有了地址管理的功能，对系统不安全。MMU上地址翻译的<br>页表，不同进程切换的时候都要换一套, 外设并不能被进程独占，不同的进程可以同时<br>给外设发请求, 广义上作为外设状态一部分的IOMMU自然也用有别MMU的方法描述不同外设<br>在IOMMU的地址翻译配置。</p>
<p>为了说明白整个IOMMU/SMMU的大体架构, 大概要说清楚一下几个方面：</p>
<ol>
<li><p>IOMMU(以下都用SMMU)是一个什么设备，它在系统中的位置和作用是什么。</p>
</li>
<li><p>固件(UEFI)里如何描述SMMU和系统其他部件的关系(PCI, GIC), 如何描述SMMU和它<br>管理的外设的关系。系统软件(一下都用Linux内核)如何解析，进而构建这种关系。</p>
</li>
<li><p>SMMU硬件都提供怎么样的功能, SMMU使用怎么样的软硬件结构来支持这样的功能。<br>Linux内核如何构建SMMU硬件需要的执行环境。SMMU驱动运行时如何工作。</p>
</li>
<li><p>SMMU驱动怎么对外提供功能，外界访问IOMMU/SMMU的接口有哪些。IOMMU这一层如何支持<br>IOMMU的对外接口。</p>
</li>
<li><p>SMMU的虚拟化(S1 + S2)是怎么用起来的。</p>
</li>
</ol>
<p>下面来一一介绍下上面的内容。</p>
<h2 id="SMMU的固件描述和Linux解析"><a href="#SMMU的固件描述和Linux解析" class="headerlink" title="SMMU的固件描述和Linux解析"></a>SMMU的固件描述和Linux解析</h2><p>  参考[2][3], ACPI(先不关注DT里的描述方法)在IORT表格里描述SMMU和系统里其他部件<br>  的关系。</p>
<h2 id="SMMU功能简介"><a href="#SMMU功能简介" class="headerlink" title="SMMU功能简介"></a>SMMU功能简介</h2><p>  SMMU为外设提供和地址翻译相关的诸多功能。硬件上, SMMU用三个队列和两个表格支持<br>  其基本功能。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">               +----------+</span><br><span class="line">               | CPU core |</span><br><span class="line">               | +-----+  |</span><br><span class="line">               | | MMU |  |</span><br><span class="line">               +-+--+--+--+       +------------------------------------------+</span><br><span class="line">   system bus       v             |                                          |</span><br><span class="line">          -----------------------&gt;|  DDR     +-------+                       |</span><br><span class="line">                   ^              |          |pa     |                       |</span><br><span class="line">             pa    |              |          +-------+                       |</span><br><span class="line">                   |              |                                          |</span><br><span class="line">               +---+-----+        | +---+    +---+    +----------+           |</span><br><span class="line">               |  SMMU   |-------&gt;| |STE|---&gt;|CD |---&gt;|Page table|           |</span><br><span class="line">               |         |        | |   |    +---+    | va -&gt; ipa| (s1)      |</span><br><span class="line">               |         |        | |   |    |.. |    +----------+           |</span><br><span class="line">               |         |        | |   |    +---+                           |</span><br><span class="line">               |  TLB    |        | |   |    |CD |                           |</span><br><span class="line">               |         |        | |   |    +---+                           |</span><br><span class="line">               |STE cache|&lt;-------| |   |    +-----------+                   |</span><br><span class="line">               |         |        | |   |---&gt;|Page table | (s2)              |</span><br><span class="line">               |CD cache |        | |   |    | ipa -&gt; pa |                   |</span><br><span class="line">               |         |        | +---+    +-----------+                   |</span><br><span class="line">               |         |        | |.. |                                    |</span><br><span class="line">               |         |        | +---+                                    |</span><br><span class="line">               |         |        | |STE|                                    |</span><br><span class="line">               |         |        | +---+                                    |</span><br><span class="line">               |         |        |                                          |</span><br><span class="line">               |         |        | +------------------+                     |</span><br><span class="line">               |         |&lt;-------| |command queue     |                     |</span><br><span class="line">               |         |        | +------------------+                     |</span><br><span class="line">               |         |        | +------------------+                     |</span><br><span class="line">               |         |-------&gt;| |event queue       |                     |</span><br><span class="line">               |         |        | +------------------+                     |</span><br><span class="line">               |         |        | +------------------+                     |</span><br><span class="line">               |         |-------&gt;| |pri queue         |                     |</span><br><span class="line">               +---------+        | +------------------+                     |</span><br><span class="line">                   ^              +------------------------------------------+</span><br><span class="line">trasaction with va |              </span><br><span class="line">           +---+       +---+      </span><br><span class="line">           |dev|  ...  |dev|      </span><br><span class="line">           +---+       +---+      </span><br></pre></td></tr></table></figure>
<p>  如上是一个SMMU硬件的简单示意图, 为了把SMMU相关的一些内存里的数据结构也画下，<br>  上图把SMMU画的很大。一般的，一个SMMU物理硬件会同时服务几个外设，而每个外设又<br>  有可能可以独立的发出多个内存访问，这些内存访问需要靠SMMU相互区分，又要靠SMMU<br>  做地址翻译。SMMU硬件靠STE(stream table entry)和CD(context descriptor), 去区分<br>  不同硬件以及相同硬件上的不用内存访问流。如上STE和CD内存里的表格，SMMU硬件可以<br>  认知这些表格，一个外设相关的内存访问信息放在一个STE里，一个外设上的一部分资源<br>  的内存访问信息放在一个CD里。外设和STE的对应关系需要SID(stream id)建立联系，<br>  对于PCI设备，他的SID一般就是BDF，外设硬件在发出的内存访问请求中带上BDF信息，<br>  内存访问请求被SMMU解析，SMMU通过其中的SID找见对应的STE。外设的可以独立发内存<br>  访问请求的单位(比如外设的一个队列)和CD的关系需要SSID(substream id)建立联系，<br>  在PCI设备上，SSID对应的就是PCI协议里说的PASID，这个PASID一般从系统软件中申请<br>  得到，然后分别配置到外设的内存访问单元，和STE一样，设备发出内存访问的时候会<br>  带上这个PASID，SMMU根据PASID找见对应的CD。可以SMMU驱动需要先为对应的设备或者<br>  设备的独立内存访问单元建立STE或者CD，以及填充STE和CD中的域段以支持随后设备的<br>  内存访问。</p>
<p>  STE和CD里包含页表，和MMU一样，为了加快翻译速度，SMMU也做了TLB。为了加快STE和<br>  CD查找的速度，SMMU里也可能放STE和CD的cache。</p>
<p>  SMMU的软硬件控制接口，包括SMMU的基本的MMIO寄存器，三个硬件队列。如上，这是三个<br>  硬件队列中command queue用于软件向SMMU发送命令，event queue用于SMMU向软件报异常<br>  事件(包括缺页)，pri queue是和PCI设备配合一起用的，用于硬件向软件上报外设的<br>  page request请求。软件可以通过command queue向硬件发命令，SMMU的命令基本上可以<br>  分为，配置无效化命令，比如无效掉SMMU cache的STE和CD；TLB无效化命令，缺页相关<br>  的命令，比如用于继续stall请求的RESUME命令；prefetch命令；SYNC命令。</p>
<h2 id="SMMU驱动分析"><a href="#SMMU驱动分析" class="headerlink" title="SMMU驱动分析"></a>SMMU驱动分析</h2><p>  SMMUv3相关的驱动的文件包括arm-smmu-v3.c, io-pgtable-arm.c,<br>  drivers/perf/arm_smmuv3_pmu.c。第一个文件是smmu驱动的主体，第二个文件是和<br>  页表相关的操作，第三个文件是和SMMU PMU(PMCG)相关的东西。第一二个文件编译出来<br>  SMMU驱动，第三个文件编译出SMMU PMCG驱动。</p>
<p>  SMMU驱动是一个普通的平台设备驱动。这驱动的probe函数里初始化SMMU硬件，包括：<br>  ACPI/DTS解析，中断初始化，硬件特性解析，SMMU STE初始化，probe里还把SMMU向<br>  iommu子系统注册，以及把iommu_ops回调函数注册给SMMU结构和总线。</p>
<p>  这其中涉及的SMMU驱动相关的一些数据结构包括：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct arm_smmu_device 描述一个物理的SMMU设备。</span><br><span class="line">struct fwnode_handle 描述struct iommu_device的固件描述。</span><br><span class="line">struct arm_smmu_master 描述SMMU物理设备所管理的一个外设, 这个外设可以对应一组</span><br><span class="line">                       stream id, 但是一般是一个外设一个stream id。</span><br><span class="line">struct device</span><br><span class="line">  +-&gt; struct dev_iommu 一个外设device里和iommu相关的东西</span><br><span class="line">    +-&gt; struct iommu_fwspec 一个外设device和iommu硬件相关的东西</span><br><span class="line">      +-&gt; struct fwnode_handle iommu_device的固件描述</span><br></pre></td></tr></table></figure>
<p>  probe里的流程比较直白：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">arm_smmu_device_probe</span><br><span class="line">  ...</span><br><span class="line">  +-&gt; arm_smmu_device_hw_probe 探测各种硬件配置信息</span><br><span class="line">  +-&gt; arm_smmu_init_structures 初始化cmd, event, pri队列的内存, 初始化STE表,</span><br><span class="line">                               如果是两级STE表，只初始化L1 STE</span><br><span class="line">  +-&gt; iommu_device_register 向iommu子系统注册SMMU</span><br><span class="line">  +-&gt; arm_smmu_set_bus_ops 向pci，amba或者platform总线注册iommu_ops</span><br></pre></td></tr></table></figure>

<p>  struct iommu_ops和具体iommu设备相关的回调函数需要通过上层的接口使用。<br>    +-&gt; arm_smmu_add_device:<br>        创建外设对应的arm_smmu_master结构, 创建外设对应的iommu_group。<br>    外设和SMMU的关系怎么传给这个函数的？很明显在这个函数调用之前外设device<br>    之中的iommu_fwspece已经被赋值。这个赋值的地方在[1]中已经提到，就是<br>    pci_device_add(这里只看PCI设备的情况)，基本逻辑是PCI设备在被加入系统中<br>    的时候调用acpi_dma_configure找见它自己的RC，从而找见对应的SMMU。</p>
<pre><code>但是，内核代码后来修改了这部分，现在的内核代码(v5.7-rc1)，把xxx_dma_configure
移到了really_probe里。已PCI总线为例，这里调用的是pci_dma_configure:

pci_dma_configure
  +-&gt; acpi_dma_configure
    +-&gt; iort_iommu_configure 这个函数里创建device的iommu_fwspec
    +-&gt; arch_setup_dma_ops 调到比如arm64的回调中
      +-&gt; iommu_setup_dma_ops 在有iommu的情况下给device-&gt;dma_ops挂上iommu_dma_ops
      (诸如dma_alloc_coherent的dma接口在做dma相关的操作时，在有iommu的
       情况下，调用的就是这里挂在device-&gt;dma_ops里的各种回调函数)

上面说的是为啥arm_smmu_add_device这个函数调用到的时候，device入参里已经
有device-&gt;iommu_fwspec的域段，下面说arm_smmu_add_device这个函数怎么调用到：

arm_smmu_add_device这个函数在iommu层里被封装, 然后注册成总线的一个notifier:
smmu probe -&gt; arm_smmu_set_bus_ops -&gt; bus_set_iommu
    +-&gt; iommu_bus_init
      +-&gt; iommu_bus_notifier  BUS_NOTIFY_ADD_DEVICE会触发
        +-&gt; iommu_probe_device
      +-&gt; ops-&gt;add_device

如上pci_device_add的最后会调用devcie_add向bus添加外设device, 这个过程
会触发以上notifier回调函数，最终调用到arm_smmu_add_device。注意，这里
really_probe在后，触发notifier执行在前。触发notifier在设备加载的时候，
而really_probe在驱动加载的时候。

下面看add_device都做了什么:
add_device
  +-&gt; blocking_notifier_call_chain
  +-&gt; bus_probe_device
        +-&gt; device_initial_probe
      +-&gt; __device_attach
        +-&gt; __device_attch_driver 注意：如果驱动没有加载不会再继续下面的调用
    ...
      +-&gt; really_probe

也就是说，一般先枚举设备，再insmod驱动的场景，只有在驱动加载的时候才会
生成device结构里的iommu_fwspec。

但是，实际跟踪执行流程，你会发现对于一个外设的arm_smmu_add_device调用是
发生在对应的设备驱动加载的时候。这是因为smmu驱动加载比pci驱动晚，add_device
想要触发notifier时，因为smmu驱动没有加载，notifier都还没有注册，自然也
不会在add_device这个流程中触发arm_smmu_add_device这个函数。那么really_probe
为啥会最终调用到arm_smmu_add_device? 这是因为在iort_iommu_configure里
调用了iort_add_device_replay-&gt;iommu_probe_device。综上struct device里的
和iommu有关的struct dev_iommu，以及iommu_fwspec都是在iort_iommu_configure
里创建的。

如最开始所述，这个arm_smmu_add_device创建特定外设在smmu处的各种描述信息。
其中包括，创建arm_smmu_master, 建立设备对应的STE entry，初始化设备的
pasid，pri功能，建立这个设备相关的iommu_group和iommu_domain, 在这个过程
中会调用smmu ops里的device_group, domain_alloc以及attach_dev。

    架构设计上，iommu_group描述共享一个地址空间的设备的集合, iommu_domain
打包地址翻译的类型以及为具体的iomm_domain实现提供桥梁，比如arm_smmu_domain。
iommu_ops里的有些操作，比如，map会最终调用到arm_smmu_domain-&gt;io_pagetable_ops-&gt;map

arm_smmu_add_device
  +-&gt; iommu_group_get_for_dev
    +-&gt; iommu_group_add_device
      +-&gt; __iommu_attach_device
        +-&gt; arm_smmu_attach_dev
里创建iommu_group, iommu_domain, 并最终调用到arm_smmu_attach_dev。可以看到
先按全局变量iommu_def_domain_type创建domain, 再用iommu_domain_set_attr
补充设置domain的attr。一般domain的type有IDENTITY, UNMANAGED, DMA。IDENTITY
    是IOMMU不做处理，DMA是在内核里使用IOMMU，UNMANAGED是把IOMMU的能力暴露出去
给其他模块使用，现在一般是VFIO和SVA再用UNMANAGED domain。attr用来区分
domain里更细一步的配置，比如，arm_smmu_domain_set_attr里，对于DMA domain,
用attr区分non_strict mode; 对于UNMANAGED domain, 用attr区分Nested的使用
方式。可以看到UNMANAGED domain时, 如果只用一个stage，用的是stage1; 对于
内核的DMA流程，一般顺着这个调用关系下来，但是，对于UNMANAGED domain和
Nested，iommu还有其他对外接口，提供配置的方法。

下面看arm_smmu_attach_dev里具体干了什么。

+-&gt; arm_smmu_attach_dev:
    这个函数的入参是，struct iommu_domain, struct device。如上，这里的domain
表示的是一种使用方式。也就是说这里是把一个device和一种使用方式建立起联系。
注意，上面的add_device是把一个外设加入到smmu这个系统里来。

如上，我们可以看下iommu_domain的各种不同类型，以及内核里的不同子系统是
怎么调用这个函数的，以及这个函数到底做了什么。

这个函数对于特定的设备，根据传进来的domain类型, 初始化具体domain的相关
操作函数，然后配置STE生效:
arm_smmu_attach_dev
  +-&gt; arm_smmu_domain_finalise
    +-&gt; arm_smmu_domain_finalise_xxx[cd, s1, s2]
    +-&gt; smmu_domain-&gt;pgtbl_ops = alloc_io_pgtable_ops
  +-&gt; arm_smmu_install_ste_for_dev
可见arm_smmu_domain_finalise里首先根绝smmu_domain里stage的配置，去配置
smmu的CD,或者STE和s2相关的内容。然后，把对应的页表操作函数赋值给smmu_domain
里的回调函数: smmu_domain-&gt;pgtbl_ops。以后，iommu_ops里的mmap，unmap函数
直接会调用到这里。

iommu_attach_group, iommu_attach_device, iommu_group_add_device,
iommu_request_dm_for_dev, iommu_request_dma_domain_for_dev都会最终调用到
attach_dev这个回调。在vfio里会使用到iommu_attach_group, iommu_attach_device,
vfio会用UNMANAGED domain。(具体逻辑： todo)
</code></pre>
<h2 id="IOMMU接口分析"><a href="#IOMMU接口分析" class="headerlink" title="IOMMU接口分析"></a>IOMMU接口分析</h2><p>  DMA接口和IOMMU的关系在上面已给出。</p>
<p>  目前社区正在上传SVA的相关补丁，SVA的接口也是IOMMU子系统对外接口的一部分, 这<br>  一部分的分析可以参见[5]。目前的SVA接口使用的是DMA domain, 通过SVA的接口触发<br>  在STE表里建立SSID非零的CD表，从而可以实现一个外设，一部分在内核态使用(使用CD0),<br>  一部风在用户态使用(使用其他CD)。</p>
<p>  VFIO重度使用IOMMU接口, 从这个角度看IOMMU子系统的对外接口参见[6]</p>
<h2 id="SMMU虚拟化分析"><a href="#SMMU虚拟化分析" class="headerlink" title="SMMU虚拟化分析"></a>SMMU虚拟化分析</h2><p>  Nested SMMU的分析参见[4]。</p>
<p>Reference</p>
<p>[1] <a href="https://wangzhou.github.io/PCI-SMMU-parse-in-ACPI/">https://wangzhou.github.io/PCI-SMMU-parse-in-ACPI/</a><br>[2] <a href="https://wangzhou.github.io/PCI-ACPI%E7%AC%94%E8%AE%B02/">https://wangzhou.github.io/PCI-ACPI笔记2/</a><br>[3] <a href="https://wangzhou.github.io/PCI-SMMU-parse-in-ACPI/">https://wangzhou.github.io/PCI-SMMU-parse-in-ACPI/</a><br>[4] <a href="https://wangzhou.github.io/vSVA%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">https://wangzhou.github.io/vSVA逻辑分析/</a><br>[5] <a href="https://wangzhou.github.io/Linux-SVA%E7%89%B9%E6%80%A7%E5%88%86%E6%9E%90/">https://wangzhou.github.io/Linux-SVA特性分析/</a><br>[6] <a href="https://wangzhou.github.io/Linux-vfio-driver-arch-analysis/">https://wangzhou.github.io/Linux-vfio-driver-arch-analysis/</a></p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Dump Linux内核和用户进程页表</title>
    <url>/Dump-Linux%E5%86%85%E6%A0%B8%E5%92%8C%E7%94%A8%E6%88%B7%E8%BF%9B%E7%A8%8B%E9%A1%B5%E8%A1%A8/</url>
    <content><![CDATA[<ol>
<li><p>dump 内核页表</p>
<ol>
<li><p>打开内核编译选项：CONFIG_PTDUMP_CORE, CONFIG_PTDUMP_DEBUGFS, 编译内核。</p>
</li>
<li><p>在/sys/kernel/debug/kernel_page_tables下可以dump kernel page table。dump<br>出的数据大概如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0x0000000000000000-0xffff000000000000    16776960T PGD</span><br><span class="line">0xffff000000000000-0xffff0000002dd000        2932K PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff0000002dd000-0xffff0000002de000           4K PTE       ro NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff0000002de000-0xffff00002d800000      742536K PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff00002d800000-0xffff00002f200000          26M PMD       ro NX SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xffff00002f200000-0xffff00002f3a0000        1664K PTE       ro NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff00002f3a0000-0xffff0000385ec000      149808K PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff0000385ec000-0xffff0000386c0000         848K PTE</span><br><span class="line">0xffff0000386c0000-0xffff0000386e0000         128K PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff0000386e0000-0xffff000038750000         448K PTE</span><br><span class="line">0xffff000038750000-0xffff00003bc20000       54080K PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff00003bc20000-0xffff00003be00000        1920K PTE</span><br><span class="line">0xffff00003be00000-0xffff00003c000000           2M PMD</span><br><span class="line">0xffff00003c000000-0xffff000040000000          64M PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff000040000000-0xffff008000000000         511G PUD</span><br><span class="line">0xffff008000000000-0xffff800000000000      130560G PGD</span><br><span class="line">---[ Linear Mapping end ]---</span><br><span class="line">---[ BPF start ]---</span><br><span class="line">0xffff800000000000-0xffff800000001000           4K PTE       ro x  SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800000001000-0xffff800000200000        2044K PTE</span><br><span class="line">0xffff800000200000-0xffff800008000000         126M PMD</span><br><span class="line">---[ BPF end ]---</span><br><span class="line">---[ Modules start ]---</span><br><span class="line">0xffff800008000000-0xffff800010000000         128M PMD</span><br><span class="line">---[ Modules end ]---</span><br><span class="line">---[ vmalloc() area ]---</span><br><span class="line">0xffff800010000000-0xffff800011200000          18M PMD       ro x  SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xffff800011200000-0xffff800011240000         256K PTE       ro x  SHD AF    CON     UXN    MEM/NORMAL</span><br><span class="line">0xffff800011240000-0xffff800011400000        1792K PTE       ro NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800011400000-0xffff800011a00000           6M PMD       ro NX SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xffff800011a00000-0xffff800011ba0000        1664K PTE       ro NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800011ba0000-0xffff800011e00000        2432K PTE</span><br><span class="line">0xffff800011e00000-0xffff800012400000           6M PMD</span><br><span class="line">0xffff800012400000-0xffff8000125d0000        1856K PTE</span><br><span class="line">0xffff8000125d0000-0xffff800012600000         192K PTE       RW NX SHD AF    CON     UXN    MEM/NORMAL</span><br><span class="line">0xffff800012600000-0xffff800012a00000           4M PMD       RW NX SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xffff800012a00000-0xffff800012b90000        1600K PTE       RW NX SHD AF    CON     UXN    MEM/NORMAL</span><br><span class="line">0xffff800012b90000-0xffff800012b91000           4K PTE</span><br><span class="line">0xffff800012b91000-0xffff800012b92000           4K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012b92000-0xffff800012b93000           4K PTE</span><br><span class="line">0xffff800012b93000-0xffff800012b94000           4K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012b94000-0xffff800012b95000           4K PTE</span><br><span class="line">0xffff800012b95000-0xffff800012b96000           4K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012b96000-0xffff800012b98000           8K PTE</span><br><span class="line">0xffff800012b98000-0xffff800012b9c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012b9c000-0xffff800012b9d000           4K PTE</span><br><span class="line">0xffff800012b9d000-0xffff800012b9e000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800012b9e000-0xffff800012ba0000           8K PTE</span><br><span class="line">0xffff800012ba0000-0xffff800012bb0000          64K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800012bb0000-0xffff800012bb1000           4K PTE</span><br><span class="line">0xffff800012bb1000-0xffff800012bb2000           4K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bb2000-0xffff800012bb3000           4K PTE</span><br><span class="line">0xffff800012bb3000-0xffff800012bb4000           4K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bb4000-0xffff800012bb8000          16K PTE</span><br><span class="line">0xffff800012bb8000-0xffff800012bbc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bbc000-0xffff800012bc0000          16K PTE</span><br><span class="line">0xffff800012bc0000-0xffff800012bd0000          64K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800012bd0000-0xffff800012bd8000          32K PTE</span><br><span class="line">0xffff800012bd8000-0xffff800012bdc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bdc000-0xffff800012be0000          16K PTE</span><br><span class="line">0xffff800012be0000-0xffff800012be4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012be4000-0xffff800012be8000          16K PTE</span><br><span class="line">0xffff800012be8000-0xffff800012bec000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bec000-0xffff800012bf0000          16K PTE</span><br><span class="line">0xffff800012bf0000-0xffff800012bf4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bf4000-0xffff800012bf8000          16K PTE</span><br><span class="line">0xffff800012bf8000-0xffff800012bfc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bfc000-0xffff800012c00000          16K PTE</span><br><span class="line">0xffff800012c00000-0xffff800012c04000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c04000-0xffff800012c08000          16K PTE</span><br><span class="line">0xffff800012c08000-0xffff800012c0c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c0c000-0xffff800012c10000          16K PTE</span><br><span class="line">0xffff800012c10000-0xffff800012c14000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c14000-0xffff800012c18000          16K PTE</span><br><span class="line">0xffff800012c18000-0xffff800012c1c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c1c000-0xffff800012c20000          16K PTE</span><br><span class="line">0xffff800012c20000-0xffff800012c24000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c24000-0xffff800012c28000          16K PTE</span><br><span class="line">0xffff800012c28000-0xffff800012c2c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c2c000-0xffff800012c30000          16K PTE</span><br><span class="line">0xffff800012c30000-0xffff800012c34000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c34000-0xffff800012c38000          16K PTE</span><br><span class="line">0xffff800012c38000-0xffff800012c3c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c3c000-0xffff800012c40000          16K PTE</span><br><span class="line">0xffff800012c40000-0xffff800012c44000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c44000-0xffff800012c48000          16K PTE</span><br><span class="line">0xffff800012c48000-0xffff800012c4c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c4c000-0xffff800012c50000          16K PTE</span><br><span class="line">0xffff800012c50000-0xffff800012c54000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c54000-0xffff800012c55000           4K PTE</span><br><span class="line">0xffff800012c55000-0xffff800012c75000         128K PTE       RW NX SHD AF            UXN    MEM/NORMAL-NC</span><br><span class="line">0xffff800012c75000-0xffff800012c76000           4K PTE</span><br><span class="line">0xffff800012c76000-0xffff800012c96000         128K PTE       RW NX SHD AF            UXN    MEM/NORMAL-NC</span><br><span class="line">0xffff800012c96000-0xffff800012c97000           4K PTE</span><br><span class="line">0xffff800012c97000-0xffff800012cb7000         128K PTE       RW NX SHD AF            UXN    MEM/NORMAL-NC</span><br><span class="line">0xffff800012cb7000-0xffff800012cb8000           4K PTE</span><br><span class="line">0xffff800012cb8000-0xffff800012cbc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cbc000-0xffff800012cc0000          16K PTE</span><br><span class="line">0xffff800012cc0000-0xffff800012cc4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cc4000-0xffff800012cc8000          16K PTE</span><br><span class="line">0xffff800012cc8000-0xffff800012ccc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012ccc000-0xffff800012cd0000          16K PTE</span><br><span class="line">0xffff800012cd0000-0xffff800012cd4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cd4000-0xffff800012cd8000          16K PTE</span><br><span class="line">0xffff800012cd8000-0xffff800012cdc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cdc000-0xffff800012ce0000          16K PTE</span><br><span class="line">0xffff800012ce0000-0xffff800012ce4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012ce4000-0xffff800012ce8000          16K PTE</span><br><span class="line">0xffff800012ce8000-0xffff800012cec000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cec000-0xffff800012cf0000          16K PTE</span><br><span class="line">0xffff800012cf0000-0xffff800012cf4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cf4000-0xffff800012cf8000          16K PTE</span><br><span class="line">0xffff800012cf8000-0xffff800012cfc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cfc000-0xffff800013000000        3088K PTE</span><br><span class="line">0xffff800013000000-0xffff800013f60000       15744K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013f60000-0xffff800013fb0000         320K PTE</span><br><span class="line">0xffff800013fb0000-0xffff800013fb4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fb4000-0xffff800013fbd000          36K PTE</span><br><span class="line">0xffff800013fbd000-0xffff800013fbe000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fbe000-0xffff800013fc0000           8K PTE</span><br><span class="line">0xffff800013fc0000-0xffff800013fc4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fc4000-0xffff800013fc5000           4K PTE</span><br><span class="line">0xffff800013fc5000-0xffff800013fc6000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fc6000-0xffff800013fc8000           8K PTE</span><br><span class="line">0xffff800013fc8000-0xffff800013fcc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fcc000-0xffff800013fcd000           4K PTE</span><br><span class="line">0xffff800013fcd000-0xffff800013fce000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fce000-0xffff800013fd0000           8K PTE</span><br><span class="line">0xffff800013fd0000-0xffff800013fd4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fd4000-0xffff800013fd5000           4K PTE</span><br><span class="line">0xffff800013fd5000-0xffff800013fd6000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fd6000-0xffff800013fd8000           8K PTE</span><br><span class="line">0xffff800013fd8000-0xffff800013fdc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fdc000-0xffff800013fdd000           4K PTE</span><br><span class="line">0xffff800013fdd000-0xffff800013fde000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fde000-0xffff800013fe0000           8K PTE</span><br><span class="line">0xffff800013fe0000-0xffff800013fe4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fe4000-0xffff800013fe5000           4K PTE</span><br><span class="line">0xffff800013fe5000-0xffff800013fe6000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fe6000-0xffff800013fe8000           8K PTE</span><br><span class="line">0xffff800013fe8000-0xffff800013fec000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fec000-0xffff800013fed000           4K PTE</span><br><span class="line">0xffff800013fed000-0xffff800013fee000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fee000-0xffff800013ff0000           8K PTE</span><br><span class="line">0xffff800013ff0000-0xffff800013ff4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013ff4000-0xffff800013ff5000           4K PTE</span><br><span class="line">0xffff800013ff5000-0xffff800013ff6000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013ff6000-0xffff800013ff8000           8K PTE</span><br><span class="line">0xffff800013ff8000-0xffff800013ffc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013ffc000-0xffff800013ffd000           4K PTE</span><br><span class="line">0xffff800013ffd000-0xffff800013ffe000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013ffe000-0xffff800014000000           8K PTE</span><br><span class="line">0xffff800014000000-0xffff800014004000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014004000-0xffff800014005000           4K PTE</span><br><span class="line">0xffff800014005000-0xffff800014006000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800014006000-0xffff80001400d000          28K PTE</span><br><span class="line">0xffff80001400d000-0xffff80001400e000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff80001400e000-0xffff800014015000          28K PTE</span><br><span class="line">0xffff800014015000-0xffff800014016000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800014016000-0xffff800014038000         136K PTE</span><br><span class="line">0xffff800014038000-0xffff80001403c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff80001403c000-0xffff800014040000          16K PTE</span><br><span class="line">0xffff800014040000-0xffff800014044000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014044000-0xffff800014048000          16K PTE</span><br><span class="line">0xffff800014048000-0xffff80001404c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff80001404c000-0xffff800014050000          16K PTE</span><br><span class="line">0xffff800014050000-0xffff800014054000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014054000-0xffff800014058000          16K PTE</span><br><span class="line">0xffff800014058000-0xffff80001405c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff80001405c000-0xffff800014060000          16K PTE</span><br><span class="line">0xffff800014060000-0xffff800014064000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014064000-0xffff800014065000           4K PTE</span><br><span class="line">0xffff800014065000-0xffff8000140a7000         264K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff8000140a7000-0xffff8000140a8000           4K PTE</span><br><span class="line">0xffff8000140a8000-0xffff8000140b3000          44K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff8000140b3000-0xffff8000140b8000          20K PTE</span><br><span class="line">0xffff8000140b8000-0xffff8000140bc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff8000140bc000-0xffff8000140c8000          48K PTE</span><br><span class="line">0xffff8000140c8000-0xffff8000140cc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff8000140cc000-0xffff8000140e8000         112K PTE</span><br><span class="line">0xffff8000140e8000-0xffff8000140ec000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff8000140ec000-0xffff800014128000         240K PTE</span><br><span class="line">0xffff800014128000-0xffff80001412c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff80001412c000-0xffff800014130000          16K PTE</span><br><span class="line">0xffff800014130000-0xffff800014134000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014134000-0xffff800014135000           4K PTE</span><br><span class="line">0xffff800014135000-0xffff800014138000          12K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014138000-0xffff800014200000         800K PTE</span><br><span class="line">0xffff800014200000-0xffff800020000000         190M PMD</span><br><span class="line">0xffff800020000000-0xffff800030000000         256M PTE       RW NX SHD AF            UXN    DEVICE/nGnRnE</span><br><span class="line">0xffff800030000000-0xffff800040000000         256M PMD</span><br><span class="line">0xffff800040000000-0xffff808000000000         511G PUD</span><br><span class="line">0xffff808000000000-0xfffffd8000000000         125T PGD</span><br><span class="line">0xfffffd8000000000-0xfffffdff80000000         510G PUD</span><br><span class="line">0xfffffdff80000000-0xfffffdffbfe00000        1022M PMD</span><br><span class="line">0xfffffdffbfe00000-0xfffffdffbffd1000        1860K PTE</span><br><span class="line">0xfffffdffbffd1000-0xfffffdffbffd4000          12K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xfffffdffbffd4000-0xfffffdffbfff0000         112K PTE</span><br><span class="line">---[ vmalloc() end ]---</span><br><span class="line">0xfffffdffbfff0000-0xfffffdffc0000000          64K PTE</span><br><span class="line">0xfffffdffc0000000-0xfffffdfffe400000         996M PMD</span><br><span class="line">0xfffffdfffe400000-0xfffffdfffe5f9000        2020K PTE</span><br><span class="line">---[ Fixmap start ]---</span><br><span class="line">0xfffffdfffe5f9000-0xfffffdfffe5fa000           4K PTE</span><br><span class="line">0xfffffdfffe5fa000-0xfffffdfffe5fb000           4K PTE       ro x  SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xfffffdfffe5fb000-0xfffffdfffe5fc000           4K PTE       ro NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xfffffdfffe5fc000-0xfffffdfffe5ff000          12K PTE</span><br><span class="line">0xfffffdfffe5ff000-0xfffffdfffe600000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xfffffdfffe600000-0xfffffdfffe800000           2M PMD       ro NX SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xfffffdfffe800000-0xfffffdfffea00000           2M PMD</span><br><span class="line">---[ Fixmap end ]---</span><br><span class="line">0xfffffdfffea00000-0xfffffdfffec00000           2M PMD</span><br><span class="line">---[ PCI I/O start ]---</span><br><span class="line">0xfffffdfffec00000-0xfffffdfffec10000          64K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xfffffdfffec10000-0xfffffdfffee00000        1984K PTE</span><br><span class="line">0xfffffdfffee00000-0xfffffdffffc00000          14M PMD</span><br><span class="line">---[ PCI I/O end ]---</span><br><span class="line">0xfffffdffffc00000-0xfffffdffffe00000           2M PMD</span><br><span class="line">---[ vmemmap start ]---</span><br><span class="line">0xfffffdffffe00000-0xfffffe0000e00000          16M PMD       RW NX SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xfffffe0000e00000-0xfffffe0040000000        1010M PMD</span><br><span class="line">0xfffffe0040000000-0xfffffe8000000000         511G PUD</span><br><span class="line">0xfffffe8000000000-0x0000000000000000        1536G PGD</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>dump 进程页表</p>
<p>linux系统上，通过对/proc/&lt;pid&gt;/pagemap的操作可以dump进程的页表信息。<br>这个文件的介绍可以参考linux/Documentation/vm/pagemap.txt。直接cat<br>这个文件的无法得到信息，我们需要按照上面文件中描述的格式写代码解析。<br>内核代码在tools/vm/page-types.c提供了解析的工具。可以使用:<br>make -C tools/vm 编译得到page-types这个工具。使用的效果类似:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ./page-types --pid 91228</span><br><span class="line">             flags	page-count       MB  symbolic-flags			long-symbolic-flags</span><br><span class="line">0x0000000000000800	         1        0  ___________M________________________________	mmap</span><br><span class="line">0x000000000000086c	       706        2  __RU_lA____M________________________________	referenced,uptodate,lru,active,mmap</span><br><span class="line">0x0000000000005828	       520        2  ___U_l_____Ma_b_____________________________	uptodate,lru,mmap,anonymous,swapbacked</span><br><span class="line">0x000000000000586c	         1        0  __RU_lA____Ma_b_____________________________	referenced,uptodate,lru,active,mmap,anonymous,swapbacked</span><br><span class="line">             total	      1228        4</span><br></pre></td></tr></table></figure>
<p>使用 ./page-types –pid <pid> –list-each 可以看到具体va到pa的映射：</pid></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">voffset	offset	flags</span><br><span class="line">aaaab6b57	6e938	__RU_lA____M________________________________</span><br><span class="line">aaaab6b58	aa3cb	__RU_lA____M________________________________</span><br><span class="line">aaaab6b59	55172	__RU_lA____M________________________________</span><br><span class="line">[...]</span><br><span class="line">ffffc744c	a17e7	___U_lA____Ma_b_____________________________</span><br><span class="line">ffffc744d	117a3a	___U_lA____Ma_b_____________________________</span><br><span class="line">ffffc744e	109913	___U_lA____Ma_b_____________________________</span><br><span class="line">ffffc744f	136574	__RU_lA____Ma_b_____________________________</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">             flags	page-count       MB  symbolic-flags			long-symbolic-flags</span><br><span class="line">0x0000000000000800	         1        0  ___________M________________________________	mmap</span><br><span class="line">0x0000000000000804	         1        0  __R________M________________________________	referenced,mmap</span><br><span class="line">0x000000000004082c	       429        1  __RU_l_____M______u_________________________	referenced,uptodate,lru,mmap,unevictable</span><br><span class="line">0x0000000000000868	         3        0  ___U_lA____M________________________________	uptodate,lru,active,mmap</span><br><span class="line">0x000000000000086c	      2009        7  __RU_lA____M________________________________	referenced,uptodate,lru,active,mmap</span><br><span class="line">0x0000000000005868	      1073        4  ___U_lA____Ma_b_____________________________	uptodate,lru,active,mmap,anonymous,swapbacked</span><br><span class="line">0x000000000000586c	         1        0  __RU_lA____Ma_b_____________________________	referenced,uptodate,lru,active,mmap,anonymous,swapbacked</span><br><span class="line">             total	      3517       13</span><br></pre></td></tr></table></figure>
<p>这里显示的是页之间的映射，所以还要乘上页大小，如果是4K页的话，上面的va到pa的<br>映射就是：(左边是va, 右边是pa)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aaaab6b57000	6e938000</span><br><span class="line">aaaab6b58000	aa3cb000</span><br><span class="line">aaaab6b59000	55172000</span><br><span class="line">[...]</span><br><span class="line">ffffc744c000	a17e7000</span><br><span class="line">ffffc744d000	117a3a000</span><br><span class="line">ffffc744e000	109913000</span><br><span class="line">ffffc744f000	136574000</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux SVA特性分析</title>
    <url>/Linux-SVA%E7%89%B9%E6%80%A7%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="使用场景介绍"><a href="#使用场景介绍" class="headerlink" title="使用场景介绍"></a>使用场景介绍</h2><p>  如上，SVA特性可以做到进程虚拟地址在进程和设备之间共享。最直观的使用场景就是在<br>  用户态做DMA。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">            </span><br><span class="line">   +------------------+     +----------------+</span><br><span class="line">   |   process        |     |  process       |       ...</span><br><span class="line">   |                  |     |                |</span><br><span class="line">   | va = malloc();   |     |                |</span><br><span class="line">   | set dma dst: va  |     |                |</span><br><span class="line">   |      +---------+ |     |                |          用户态</span><br><span class="line">   +------+ mmap io +-+     +----------------+</span><br><span class="line">&lt;---------+         +------------------------------------------&gt; </span><br><span class="line">          +---------+</span><br><span class="line">                       内核</span><br><span class="line"></span><br><span class="line">&lt;--------------------------------------------------------------&gt; </span><br><span class="line">               +--------------+</span><br><span class="line">               |  DMA dst: va |</span><br><span class="line">               |              |</span><br><span class="line">               |  设备        |</span><br><span class="line">               +--------------+</span><br></pre></td></tr></table></figure>
<p>  如上图所示，在SVA的支持下，我们可以在用户态进程malloc一段内存，然后把得到va<br>  直接配置给设备的DMA，然后启动DMA向va对应的虚拟地址写数据，当然也可以从va对应<br>  的虚拟地址上往设备读数据。这里我们把设备DMA相关的寄存器先mmap到用户态，这样<br>  DMA操作在用户态就可以完成。</p>
<p>  可以注意到，SVA可以支持功能很大一部分取决于设备的功能，SVA就是提供一个进程和<br>  设备一致的虚拟地址空间，其他的设备想怎么用都可以。如上，如果设备做的足够强，<br>  设备完全可以执行va上对应的代码。</p>
<p>  可以看到，设备完全可以把自身的资源分配给不同的进程同时使用。</p>
<p>  为了满足上面的使用场景，SVA特性需要硬件支持IOMMU以及设备发起缺页。在下一节<br>  先介绍硬件，再基于此分析SVA的软件实现。</p>
<h2 id="硬件基础介绍"><a href="#硬件基础介绍" class="headerlink" title="硬件基础介绍"></a>硬件基础介绍</h2><p>  本文以ARM64体系结构为基础分析，在ARM64下，IOMMU指的就是SMMU。对于设备，ARM64<br>  下有平台设备和PCI设备。整体的硬件示意图如下，图中也画出了硬件工作时相关的内存<br>  里的数据结构。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">             +-----+</span><br><span class="line">             | CPU |</span><br><span class="line">             +--+--+</span><br><span class="line">                |             </span><br><span class="line">                v             </span><br><span class="line">             +-----+           +---------------------------------------------+</span><br><span class="line">             | MMU |-----------+------------------------------------+        |</span><br><span class="line">             +--+--+           | DDR                                |        |</span><br><span class="line">                |              |                                    |        |</span><br><span class="line">                v              |                                    |        |</span><br><span class="line">system bus ------------------&gt; |                                    |        |</span><br><span class="line">                ^              |                                    v        |</span><br><span class="line">                |   SID/SSID   |      +-----+     +----+      +------------+ |</span><br><span class="line">                |       +------+----&gt; | STE |----&gt;| CD |-----&gt;| page table | |</span><br><span class="line">                |       |      |      +-----+     +----+      +------------+ |</span><br><span class="line">                |       |      |       ...        | CD |-----&gt;| page table | |</span><br><span class="line">                |       |      |                  +----+      +------------+ |</span><br><span class="line">        IRQs    |       |      |                  | .. |      | ..         | |</span><br><span class="line">              ^ |^  ^   |      |                  +----+      +------------+ |</span><br><span class="line">              | ||  |   |      |                  | CD |-----&gt;| page table | |</span><br><span class="line">    +---------+-++--+---+---+  |                  +----+      +------------+ |</span><br><span class="line">    | SMMU    |  |  |   |   |  |                                             |</span><br><span class="line">    |         |  |  |       |  |     +-------------+                         |</span><br><span class="line">    |  +------+--+-CMD Q ---+--+----&gt;| CMD queue   |                         |</span><br><span class="line">    +--v--+   |  |          |  |     +-------------+                         |</span><br><span class="line">    | PRI |---&gt; PRI Q ------+--+----&gt;| PRI queue   |                         |</span><br><span class="line">    +-----+   |             |  |     +-------------+                         |</span><br><span class="line">    | ATS |  EVENT Q -------+--+----&gt;| EVENT queue |                         |</span><br><span class="line">    +-----+-----------------+  |     +-------------+                         |</span><br><span class="line">      ^ |     ^            ^   +---------------------------------------------+</span><br><span class="line">      | |     |            |</span><br><span class="line">      | v     | BDF/PASID  +--------------+</span><br><span class="line">    +---------+-------------+             | </span><br><span class="line">    |         RP            |             | </span><br><span class="line">    +-----------------------+             | </span><br><span class="line">      ^ |       ^                         | </span><br><span class="line">      | v       |  BDF/PASID              | </span><br><span class="line">    +-+----+----+-+---------+  +-----+----+----------+</span><br><span class="line">    | ATC  |      |         |  |     |               |                       </span><br><span class="line">    +------+      |         |  | DMA |               |                       </span><br><span class="line">    | PRI  |      |  EP     |  |     |               |                       </span><br><span class="line">    +------+ DMA  |         |  +-----+  platform dev |                       </span><br><span class="line">    +-------------+         |  |                     |                       </span><br><span class="line">    +-----------------------+  +---------------------+</span><br></pre></td></tr></table></figure>
<p>   基于上一节中提到的使用场景, 我们梳理硬件中的逻辑关系。调用malloc后，其实只是<br>   拿到了一个虚拟地址，内核并没有为申请的地址空间分配实际的物理内存，直到访问这<br>   块地址空间时引发缺页，内核在缺页流程里分配实际的物理内存，然后建立虚拟地址到<br>   物理内存的映射。这个过程需要MMU的参与。设想SVA的场景中，先malloc得到va, 然后<br>   把这个va传给设备，配置设备DMA去访问该地址空间，这时内核并没有为va分配实际的<br>   物理内存，所以设备一侧的访问流程必然需要进行类似的缺页请求。支持设备侧缺页<br>   请求的硬件设备就是上面所示的SMMU，其中对于PCI设备，还需要ATS、PRI硬件特性支持。<br>   平台设备需要SMMU stall mode支持(使用event queue)。PCI设备和平台设备都需要<br>   PASID特性的支持。</p>
<p>   如上图所示，引入SVA后，MMU和SMMU使用相同的进程页表, SMMU使用STE表和CD表管理<br>   当前SMMU下设备的页表，其中STE表用来区分设备，CD表用来区分同一个设备上分配给<br>   不同进程使用的硬件资源所对应的进程页表。STE表和CD表都需要SMMU驱动预先分配好。</p>
<p>   SMMU内部使用command queue，event queue，pri queue做基本的事件管理。当有相应<br>   硬件事件发生时，硬件把相应的描述符写入event queue或者pri queue, 然后上报中断。<br>   软件使用command queue下发相应的命令操作硬件。</p>
<p>   PCI设备和平台设备的硬件缺页流程有一些差异，下面分别介绍。对于PCI设备，ATS,<br>   PRI和PASID的概念同时存在于PCIe和SMMU规范中。ATS的介绍可以参考<a href="https://wangzhou.github.io/PCIe-ATS%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90/">这里</a>简单讲，ATS特性<br>   由设备侧的ATC和SMMU侧的ATS同时支持，其目的是在设备中缓存va对应的pa，设备随后<br>   使用pa做内存访问时无需经过SMMU页表转换，可以提高性能。PRI(page request<br>   interface)也是需要设备和SMMU一起工作，PCIe设备可以发出缺页请求，SMMU硬件在解<br>   析到缺页请求后可以直接将缺页请求写入PRI queueu, 软件在建立好页表后，可以通过<br>   CMD queue发送PRI response给PCIe设备。具体的ATS和PRI的实现是硬件相关的，目前<br>   市面上还没有实现这两个硬件特性的PCIe设备，但是我们可以设想一下ATS和PRI的硬件<br>   实现，最好的实现应该是软件透明的，也就是软件配置给设备DMA的访问地址是va, 软件<br>   控制DMA发起后，硬件先发起ATC请求，从SMMU请求该va对应的pa，如果SMMU里已经有va<br>   到pa的映射，那么设备可以得到pa，然后设备再用pa发起一次内存访问，该访问将直接<br>   访问对应pa地址，不在SMMU做地址翻译，如果SMMU没有va到pa的映射, 那么设备得到<br>   这个消息后会继续向SMMU发PRI请求，设备得到从SMMU来的PRI response后发送内存访问<br>   请求，该请求就可以在SMMU中翻译得到pa, 最终访问到物理内存。</p>
<p>   PRI请求是基于PCIe协议的, 平台设备无法用PRI发起缺页请求。实际上，平台设备是无法<br>   靠自身发起缺页请求的，SMMU用stall模式支持平台设备的缺页，当一个平台设备的内存<br>   访问请求到达SMMU后，如果SMMU里没有为va做到pa的映射，硬件会给SMMU的event queue<br>   里写一个信息，SMMU的event queue中断处理里可以做缺页处理，然后SMMU可以回信息给<br>   设备(fix me: 请求设备重发，还是smmu缺页处理后已经把该访问翻译后送到上游总线)。<br>   实际上, SMMU使用event queue来处理各种错误异常，这里的stall模式是借用了event<br>   queue来处理缺页。</p>
<p>   可以注意到PRI和stall模式完成缺页的区别是，PRI缺页的时候并不是在IO实际发生的<br>   时候，因为如果PRI response表示PRI请求失败，硬件完全可以不发起后续的IO操作。<br>   而stall模式，完全发生在IO请求的途中。所以，他被叫做stall模式。</p>
<h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct device</span><br><span class="line">        +-&gt; struct iommu_param</span><br><span class="line">                +-&gt; struct iommu_fault_param</span><br><span class="line">                        +-&gt; handler(struct iommu_fault, void *)</span><br><span class="line">                        +-&gt; faults list</span><br><span class="line">                +-&gt; struct iopf_device_param</span><br><span class="line">                     +-&gt; struct iopf_queue</span><br><span class="line">                             +-&gt; work queue</span><br><span class="line">                             +-&gt; devices list</span><br><span class="line">                     +-&gt; wait queue</span><br><span class="line">                +-&gt; struct iommu_sva_param</span><br></pre></td></tr></table></figure>
<p>  在引起缺页的外设的device里，需要添加缺页相关的数据结构。handler是缺页要执行<br>  的函数，具体见下面动态流程分析。iopf_queue在smmu驱动初始化时添加，这里iopf_queue<br>  可能是eventq对应的，也可能是priq对应的，iopf_queue是smmu里的概念，所以同一个<br>  iopf_queue会服务同一个smmu下的所有外设, 上面的devices list链表就是用来收集这个<br>  smmu下的外设。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct iommu_bond</span><br><span class="line">      +-&gt; struct iommu_sva</span><br><span class="line">              +-&gt; device</span><br><span class="line">              +-&gt; struct iommu_sva_ops</span><br><span class="line">                      +-&gt; iommu_mmu_exit_handle ?</span><br><span class="line">      +-&gt; struct io_mm</span><br><span class="line">              +-&gt; pasid</span><br><span class="line">              +-&gt; device list</span><br><span class="line">              +-&gt; mm</span><br><span class="line">              +-&gt; mmu_notifier</span><br><span class="line">              +-&gt; iommu_ops</span><br><span class="line">                      +-&gt; attach</span><br><span class="line">                      +-&gt; dettach</span><br><span class="line">                      +-&gt; invalidat</span><br><span class="line">                      +-&gt; release</span><br><span class="line">      +-&gt; mm list</span><br><span class="line">      +-&gt; device list</span><br><span class="line">      +-&gt; wait queue</span><br></pre></td></tr></table></figure>
<p>  下面的图引用自JPB的补丁，该图描述的是建立好的静态数据结构之间的关系，以及IOMMU<br>  (e.g. SMMU的STE和CD表在这种数据结构下的具体配置情况)表格的配置。用这个图可以<br>  很好说明iommu_bond中的各个数据结构的意义以及之间的关系。</p>
<p>  iommu_bond这个结构并不对外, 用下面的iommu_sva_bind_device/unbind接口时，函数<br>  参数都是iommu_sva。使用SVA的设备可以把一个设备的一些资源和一个进程地址空间绑定，<br>  这种绑定关系是灵活的，比如可以一个设备上的不同资源和不用的进程地址空间绑定<br>  (bond 1, bond 2), 还可以同一个设备上的资源都绑定在一个进程的地址空间上(bond 3,<br>  bond 4)。从进程地址空间的角度看，一个进程地址空间可能和多个设备资源绑定。</p>
<p>  iommu_bond指的就是一个绑定，io_mm指的是绑定了外设资源的一个进程地址空间。<br>  io_pgtables是指内核dma接口申请内存的页表。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        ___________________________</span><br><span class="line">       |  IOMMU domain A           |</span><br><span class="line">       |  ________________         |</span><br><span class="line">       | |  IOMMU group   |        +------- io_pgtables</span><br><span class="line">       | |                |        |</span><br><span class="line">       | |   dev 00:00.0 ----+------- bond 1 --- io_mm X</span><br><span class="line">       | |________________|   \    |</span><br><span class="line">       |                       &#x27;----- bond 2 ---.</span><br><span class="line">       |___________________________|             \</span><br><span class="line">        ___________________________               \</span><br><span class="line">       |  IOMMU domain B           |             io_mm Y</span><br><span class="line">       |  ________________         |             / /</span><br><span class="line">       | |  IOMMU group   |        |            / /</span><br><span class="line">       | |                |        |           / /</span><br><span class="line">       | |   dev 00:01.0 ------------ bond 3 -&#x27; /</span><br><span class="line">       | |   dev 00:01.1 ------------ bond 4 --&#x27;</span><br><span class="line">       | |________________|        |</span><br><span class="line">       |                           +------- io_pgtables</span><br><span class="line">       |___________________________|</span><br><span class="line"></span><br><span class="line">                          PASID tables</span><br><span class="line">                           of domain A</span><br><span class="line">                        .-&gt;+--------+</span><br><span class="line">                       / 0 |        |-------&gt; io_pgtable</span><br><span class="line">                      /    +--------+</span><br><span class="line">      Device tables  /   1 |        |-------&gt; pgd X</span><br><span class="line">        +--------+  /      +--------+</span><br><span class="line">00:00.0 |      A |-&#x27;     2 |        |--.</span><br><span class="line">        +--------+         +--------+   \</span><br><span class="line">        :        :       3 |        |    \</span><br><span class="line">        +--------+         +--------+     --&gt; pgd Y</span><br><span class="line">00:01.0 |      B |--.                    /</span><br><span class="line">        +--------+   \                  |</span><br><span class="line">00:01.1 |      B |----+   PASID tables  |</span><br><span class="line">        +--------+     \   of domain B  |</span><br><span class="line">                        &#x27;-&gt;+--------+   |</span><br><span class="line">                         0 |        |-- | --&gt; io_pgtable</span><br><span class="line">                           +--------+   |</span><br><span class="line">                         1 |        |   |</span><br><span class="line">                           +--------+   |</span><br><span class="line">                         2 |        |---&#x27;</span><br><span class="line">                           +--------+</span><br><span class="line">                         3 |        |</span><br><span class="line">                           +--------+</span><br></pre></td></tr></table></figure>

<p>  相关接口:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- iommu_dev_enable_feature: 准备和sva相关的smmu中的管理结构, 该接口可以在</span><br><span class="line">                                设备驱动里调用，用来使能sva的功能</span><br><span class="line">  +-&gt; arm_smmu_dev_enable_feature</span><br><span class="line">      +-&gt; arm_smmu_dev_enable_sva</span><br><span class="line">        +-&gt; iommu_sva_enable</span><br><span class="line">          +-&gt; iommu_register_device_fault_handler(dev, iommu_queue_iopf, dev)</span><br><span class="line">              /* 动态部分将执行iommu_queue_iopf */</span><br><span class="line">              把iommu_queue_iopf赋值给iommu_fault_param里的handler                  </span><br><span class="line">        +-&gt; iopf_queue_add_device(struct iopf_queue, dev)</span><br><span class="line">            把相应的iopf_queue赋值给iopf_device_param里的iopf_queue, 这里有</span><br><span class="line">            pri对应的iopf_queue或者是stall mode对应的iopf_queue。初始化</span><br><span class="line">            iopf_device_param里的wait queue</span><br><span class="line"></span><br><span class="line">            对应iopf_queue的初始化在在smmu驱动probe流程中: e.g.</span><br><span class="line">            arm_smmu_init_queues</span><br><span class="line">              +-&gt; smmu-&gt;prq.iopf = iopf_queue_alloc</span><br><span class="line">                                       +-&gt; alloc_workqueue</span><br><span class="line">                                         分配以及初始化iopf_queue里的工作队列</span><br><span class="line">        +-&gt; arm_smmu_enable_pri</span><br><span class="line">            调用PCI函数是能EP设备的PRI功能</span><br><span class="line"></span><br><span class="line">- iommu_sva_bind_device: 将设备和mm绑定, 该接口可以在设备驱动里调用，把一个</span><br><span class="line">                         设备和mm绑定在一起。返回struct iommu_sva *</span><br><span class="line">  +-&gt; iommu_sva_bind_group</span><br><span class="line">    +-&gt; iommu_group_do_bind_dev</span><br><span class="line">      +-&gt; arm_smmu_sva_bind</span><br><span class="line">        +-&gt; arm_smmu_alloc_shared_cd(mm)</span><br><span class="line">            分配相应的CD表项，并且把CD表项里的页表地址指向mm里保存的进程页表</span><br><span class="line">            地址。这个函数主要配置SMMU的硬件</span><br><span class="line">        +-&gt; iommu_sva_bind_generic(dev, mm, cd, &amp;arm_smmu_mm_ops, drvdata)</span><br><span class="line">          +-&gt; io_mm_alloc</span><br><span class="line">              分配io_mm以及初始化其中的数据域段, 向mm注册io_mm的notifier</span><br><span class="line">              /* to do: mm发生变化的时候通知io_mm */</span><br><span class="line">              +-&gt; mmu_notifier_register</span><br><span class="line">          +-&gt; io_mm_attach</span><br><span class="line">            +-&gt; init_waitqueue_head</span><br><span class="line">                初始化iommu_bond里的等待队列mm_exit_wq /* to do: 作用 */</span><br><span class="line">            +-&gt; io_mm-&gt;ops-&gt;attach(bond-&gt;sva.dev, io_mm-&gt;pasid, io_mm-&gt;ctx)</span><br><span class="line">                调用e.g.SMMU arm_smmu_mm_ops里的attach函数</span><br><span class="line">                +-&gt; arm_smmu_mm_attach</span><br><span class="line">                  +-&gt; __arm_smmu_write_ctx_desc</span><br><span class="line">                    下发SMMU command使能CD配置。可以看到arm_smmu_mm_ops里的</span><br><span class="line">                    这一组回调函数基本都是下发SMMU命令控制CD/ATC/TLB相关的</span><br><span class="line">                    配置</span><br><span class="line"></span><br><span class="line">- iommu_sva_unbind_device</span><br><span class="line">    +-&gt; iopf_queue_flush_dev</span><br><span class="line">    +-&gt; iommu_unbind_locked(to_iommu_bond(handle))</span><br><span class="line">    这里的handle是一个struct iommu_sva</span><br></pre></td></tr></table></figure>
<pre><code>- iommu_sva_set_ops(iommu_sva, iommu_sva_ops)

  这个接口把iommu_sva_ops传递给iommu_sva, iommu_sva_ops包含mm_exit回调。
  在上面的iommu_sva_bind_generic里会调用mmu_notifier_register给当前的mm里
  注册一个notifier，在mm发生改变的时候，mm子系统可以触发notifier里的回调
  函数。当前的代码里，是在notifier的release回调里去调用iommu_sva里保存的
  mm_exit回调函数。

  SVA特性使得设备可以看到进程虚拟地址空间，这样在进程虚拟地址空间销毁的时候
  应该调用设备驱动提供的函数停止设备继续访问进程虚拟地址空间。这里iommu_sva_set_ops
  就是把设备驱动的回调函数注册给进程mm。

  注意上面的mmu_notifier_register注册的iommu_mmu_notifier_ops回调里。release
  只在进程异常时调用到，用户态进程正常退出时并不会调用。在进程正常退出时，
  如何保证设备停止访问将要释放的进程地址空间，这里还有疑问。

  进程退出的调用链是:
  kernel/exit.c:
  do_exit
    +-&gt; exit_mm
  +-&gt; mmput
    +-&gt; exit_mmap
      +-&gt; mmu_notifier_release

- iommu_sva_get_pasid

  这个接口返回返回smmu_sva对应的pasid数值，设备驱动需要把pasid配置给与这个
  smmu_sva相关的硬件资源。
</code></pre>
<p>  需要使用SVA特性的社区驱动在调用上面的接口后，可以建立起静态的数据结构。</p>
<h2 id="动态分析"><a href="#动态分析" class="headerlink" title="动态分析"></a>动态分析</h2><ul>
<li><p>缺页流程</p>
<p>当一个PRI或者是一个stall event上报后, 软件会在缺页流程里建立页表，然后控制<br>SMMU给设备返送reponse信息。我们可以从SMMU PRI queue或者是event queue的中断<br>处理流程入手跟踪: e.g.PRI中断流程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">devm_request_threaded_irq(..., arm_smmu_priq_thread, ...)</span><br><span class="line">arm_smmu_priq_thread</span><br><span class="line">  +-&gt; arm_smmu_handle_ppr</span><br><span class="line">    +-&gt; iommu_report_device_fault</span><br><span class="line">      +-&gt; iommu_fault_param-&gt;handler</span><br><span class="line">        +-&gt; iommu_queue_iopf /* 初始化参见上面第2部分 */</span><br><span class="line">          +-&gt; iopf_group = kzalloc</span><br><span class="line">          +-&gt; list_add(faults list in group, fault)</span><br><span class="line">          +-&gt; INIT_WORK(&amp;group-&gt;work, iopf_handle_group)</span><br><span class="line">          +-&gt; queue_work(iopf_param-&gt;queue-&gt;wq, &amp;group-&gt;work)</span><br><span class="line">          这段代码创建缺页的group，并把当前的缺页请求挂入group里的链表，然后</span><br><span class="line">          创建一个任务，并调度这个任务运行</span><br><span class="line"></span><br><span class="line">          在工作队列线程中:</span><br><span class="line">          +-&gt; iopf_handle_group</span><br><span class="line">            +-&gt; iopf_handle_single</span><br><span class="line">              +-&gt; handle_mm_fault</span><br><span class="line">                  这里会最终申请内存并建立页表</span><br><span class="line"></span><br><span class="line">    +-&gt; arm_smmu_page_response</span><br><span class="line">        软件执行完缺页流程后，软件控制SMMU向设备回响应。</span><br></pre></td></tr></table></figure></li>
<li><p>Invalid流程</p>
<p>当软件释放申请的内存时，SMMU中关于这些内存的tlb以及设备ATC里都要Invalid。<br>进程mm变动的时候，调用注册的io_mm notifier完成相关的tlb、atc的invalid。</p>
<p>[…]</p>
</li>
</ul>
<h2 id="性能分析"><a href="#性能分析" class="headerlink" title="性能分析"></a>性能分析</h2><pre><code>[...]
</code></pre>
<h2 id="虚拟化"><a href="#虚拟化" class="headerlink" title="虚拟化"></a>虚拟化</h2><p> SVA虚拟化的基本逻辑可以参考<a href="https://wangzhou.github.io/vSVA%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">这里</a>。</p>
]]></content>
      <tags>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
        <tag>iommu</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核riscv head.S分析</title>
    <url>/Linux%E5%86%85%E6%A0%B8riscv-head-S%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* n: 这个是 kernel execute in place, 就是可以从一个持久存储里上直接启动内核 */</span><br><span class="line">#ifdef CONFIG_XIP_KERNEL</span><br><span class="line">.macro XIP_FIXUP_OFFSET reg</span><br><span class="line">	REG_L t0, _xip_fixup</span><br><span class="line">	add \reg, \reg, t0</span><br><span class="line">.endm</span><br><span class="line">.macro XIP_FIXUP_FLASH_OFFSET reg</span><br><span class="line">	la t0, __data_loc</span><br><span class="line">	REG_L t1, _xip_phys_offset</span><br><span class="line">	sub \reg, \reg, t1</span><br><span class="line">	add \reg, \reg, t0</span><br><span class="line">.endm</span><br><span class="line">_xip_fixup: .dword CONFIG_PHYS_RAM_BASE - CONFIG_XIP_PHYS_ADDR - XIP_OFFSET</span><br><span class="line">_xip_phys_offset: .dword CONFIG_XIP_PHYS_ADDR + XIP_OFFSET</span><br><span class="line">#else</span><br><span class="line">.macro XIP_FIXUP_OFFSET reg</span><br><span class="line">.endm</span><br><span class="line">.macro XIP_FIXUP_FLASH_OFFSET reg</span><br><span class="line">.endm</span><br><span class="line">#endif /* CONFIG_XIP_KERNEL */</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * n: 定义了一个代码段：.section &quot;.head.test&quot;,&quot;ax&quot;</span><br><span class="line"> *    前面是名字，后面是属性，a表示可分配？x表示可执行</span><br><span class="line"> */</span><br><span class="line">__HEAD</span><br><span class="line">/*</span><br><span class="line"> * n: ENTRY这个宏定义了一个叫_start的符号，一直到对应的END处。在END处也生成了对应</span><br><span class="line"> * 符号的size：.size name, .-name</span><br><span class="line"> */</span><br><span class="line">ENTRY(_start)</span><br><span class="line">	/*</span><br><span class="line">	 * Image header expected by Linux boot-loaders. The image header data</span><br><span class="line">	 * structure is described in asm/image.h.</span><br><span class="line">	 * Do not modify it without modifying the structure and all bootloaders</span><br><span class="line">	 * that expects this header format!!</span><br><span class="line">	 */</span><br><span class="line">#ifdef CONFIG_EFI</span><br><span class="line">	/*</span><br><span class="line">	 * n: 这里是riscv上UEFI和内核约定格式，具体的说明可以看内核文档:</span><br><span class="line">	 *    linux/Documentation/riscv/boot-image-header.rst</span><br><span class="line">	 *    _start最开头的格式要符合如上文档里的定义。</span><br><span class="line">	 */</span><br><span class="line">	/*</span><br><span class="line">	 * This instruction decodes to &quot;MZ&quot; ASCII required by UEFI.</span><br><span class="line">	 */</span><br><span class="line">	c.li s4,-13</span><br><span class="line">	j _start_kernel</span><br><span class="line">#else</span><br><span class="line">	/* jump to start kernel */</span><br><span class="line">	j _start_kernel</span><br><span class="line">	/* reserved */</span><br><span class="line">	.word 0</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">	.balign 8</span><br><span class="line">	/*</span><br><span class="line">	 * n: M mode下MMU是不开的，所以，一般内核都不开这个，而是S mode，</span><br><span class="line">	 *    一般是bios跑在M mode，在进入内核的时候把机器切到S mode。</span><br><span class="line">	 */</span><br><span class="line">#ifdef CONFIG_RISCV_M_MODE</span><br><span class="line">	/* Image load offset (0MB) from start of RAM for M-mode */</span><br><span class="line">	.dword 0</span><br><span class="line">#else</span><br><span class="line">#if __riscv_xlen == 64</span><br><span class="line">	/* n: 把Image加载到内存2MB偏移处 */</span><br><span class="line">	/* Image load offset(2MB) from start of RAM */</span><br><span class="line">	.dword 0x200000</span><br><span class="line">#else</span><br><span class="line">	/* Image load offset(4MB) from start of RAM */</span><br><span class="line">	.dword 0x400000</span><br><span class="line">#endif</span><br><span class="line">#endif</span><br><span class="line">	/* Effective size of kernel image */</span><br><span class="line">	.dword _end - _start</span><br><span class="line">	.dword __HEAD_FLAGS</span><br><span class="line">	.word RISCV_HEADER_VERSION</span><br><span class="line">	.word 0</span><br><span class="line">	.dword 0</span><br><span class="line">	.ascii RISCV_IMAGE_MAGIC</span><br><span class="line">	.balign 4</span><br><span class="line">	.ascii RISCV_IMAGE_MAGIC2</span><br><span class="line">#ifdef CONFIG_EFI</span><br><span class="line">	.word pe_head_start - _start</span><br><span class="line">pe_head_start:</span><br><span class="line"></span><br><span class="line">	__EFI_PE_HEADER</span><br><span class="line">#else</span><br><span class="line">	.word 0</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">.align 2</span><br><span class="line">#ifdef CONFIG_MMU</span><br><span class="line">relocate:</span><br><span class="line">	/* n: 把kernel_map这个符号的地址放到a1里 */</span><br><span class="line">	/* Relocate return address */</span><br><span class="line">	la a1, kernel_map</span><br><span class="line">	XIP_FIXUP_OFFSET a1</span><br><span class="line">	/*</span><br><span class="line">	 * n: 把内核Image的虚拟地址放到a1里，kernle_map.virt_addr这个地址在setup_vm</span><br><span class="line">	 *    里确定，是KERNEL_LINK_ADDR，一般的就是0xffffffff80000000这个地址，</span><br><span class="line">	 *    就是内核的起始地址，在Documentation/riscv/vm-layout.rst里也有说明。</span><br><span class="line">	 */</span><br><span class="line">	REG_L a1, KERNEL_MAP_VIRT_ADDR(a1)</span><br><span class="line">	/*</span><br><span class="line">	 * n: _start这个符号的地址是在内核链接脚本里定义的arch/riscv/kernel/vmlinux.lds.S:</span><br><span class="line">	 *</span><br><span class="line">	 *    . = LOAD_OFFSET;</span><br><span class="line">	 *    _start = .;</span><br><span class="line">	 *</span><br><span class="line">	 *    LOAD_OFFSET是KERNEL_LINK_ADDR的封装。但是，这里加载到a2里的值并不是</span><br><span class="line">	 *    链接脚本里静态定义的值，la是一个伪指令，要被编译器替换成auipc和ld</span><br><span class="line">	 *    指令，编译器处理auipc时使用地址无关的处理方式，当前PC和_start符号的</span><br><span class="line">	 *    相对偏移是编译时已知的，所以，地址无关的处理方式使用和当前PC的偏移</span><br><span class="line">	 *    得到运行是_start的具体地址。这里还没有开MMU，得到的是_start的物理地址，</span><br><span class="line">	 *    也就是内核被加载到的物理地址。qemu virt机器上，这个地址是0x80200000</span><br><span class="line">	 *</span><br><span class="line">	 *    一个符号是不是位置无关(PIC)，可以由编译器的选项控制，比如-fPIC，从</span><br><span class="line">	 *    汇编指令的角度看，la、auipc编译出来的就是位置无关的代码。</span><br><span class="line">	 */</span><br><span class="line">	la a2, _start</span><br><span class="line">	/* n: a1放内核Image加载虚拟地址和物理地址的差值 */</span><br><span class="line">	sub a1, a1, a2</span><br><span class="line">	/* n: 调用relocate的时候ra放的物理地址，这里加一个偏移得到对应的虚拟地址 */</span><br><span class="line">	add ra, ra, a1</span><br><span class="line"></span><br><span class="line">	/* Point stvec to virtual address of intruction after satp write */</span><br><span class="line">	la a2, 1f</span><br><span class="line">	/* n: 得到label 1的虚拟地址，放到a2里 */</span><br><span class="line">	add a2, a2, a1</span><br><span class="line">	csrw CSR_TVEC, a2</span><br><span class="line"></span><br><span class="line">	/* Compute satp for kernel page tables, but don&#x27;t load it yet */</span><br><span class="line">	/* n: a0传入页表page，这里计算得到页表基地址，注意这个是物理地址 */</span><br><span class="line">	srl a2, a0, PAGE_SHIFT</span><br><span class="line">	la a1, satp_mode</span><br><span class="line">	REG_L a1, 0(a1)</span><br><span class="line">	or a2, a2, a1</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * n: 这几个pg_dir的逻辑，先看trampoline_pg_dir和early_pg_dir。看setup_vm</span><br><span class="line">	 *    里的逻辑，trampoline_pg_dir的页表只是覆盖了内核Image的开头2MB。</span><br><span class="line">	 *</span><br><span class="line">	 *    这里先把trampoline_pg_dir配置成当前页表。SATP是页表的基地址寄存器，</span><br><span class="line">	 *    页表相关信息在这里配置，包括：页表的种类、ASID和页表基地址。</span><br><span class="line">	 */</span><br><span class="line">	/*</span><br><span class="line">	 * Load trampoline page directory, which will cause us to trap to</span><br><span class="line">	 * stvec if VA != PA, or simply fall through if VA == PA.  We need a</span><br><span class="line">	 * full fence here because setup_vm() just wrote these PTEs and we need</span><br><span class="line">	 * to ensure the new translations are in use.</span><br><span class="line">	 */</span><br><span class="line">	la a0, trampoline_pg_dir</span><br><span class="line">	XIP_FIXUP_OFFSET a0</span><br><span class="line">	srl a0, a0, PAGE_SHIFT</span><br><span class="line">	or a0, a0, a1</span><br><span class="line">	sfence.vma</span><br><span class="line">	/* n: 这条指令后MMU就生效了，CPU访问内存都要翻译下 */</span><br><span class="line">	csrw CSR_SATP, a0</span><br><span class="line">.align 2</span><br><span class="line">1:</span><br><span class="line">	/*</span><br><span class="line">	 * n: CPU在当前的PC上取这条指令的时候，会发生异常。CPU的执行逻辑是，先</span><br><span class="line">	 *    尝试把当前PC(还是物理地址)做翻译，trampoline_pg_dir页表里没有PC</span><br><span class="line">	 *    相关的页表项导致CPU异常，CPU跳到异常向量入口执行指令，这个异常向量</span><br><span class="line">	 *    的地址就是上面配置到CSR_TVEC里的值，上面在配置的时候已经把label 1</span><br><span class="line">	 *    的虚拟地址计算出来，并把这个虚拟地址写到了CSR_TVEC。因为已经打开</span><br><span class="line">	 *    MMU，CPU在执行异常向量地址上的指令时，先通过MMU得到label 1的物理地</span><br><span class="line">	 *    址，然后再次执行下面的指令。</span><br><span class="line">	 *</span><br><span class="line">	 *    需要注意的是，当前PC已经是虚拟地址了，这样相对寻址得到.Lsecondary_park</span><br><span class="line">	 *    的虚拟地址，a0里是.Lsecondary_park的虚拟地址。</span><br><span class="line">	 */</span><br><span class="line">	/* Set trap vector to spin forever to help debug */</span><br><span class="line">	la a0, .Lsecondary_park</span><br><span class="line">	csrw CSR_TVEC, a0</span><br><span class="line"></span><br><span class="line">	/* Reload the global pointer */</span><br><span class="line">.option push</span><br><span class="line">.option norelax</span><br><span class="line">	/* n: 同理，这里重新加载下__global_pointer$，gp里放的是虚拟地址了 */</span><br><span class="line">	la gp, __global_pointer$</span><br><span class="line">.option pop</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * Switch to kernel page tables.  A full fence is necessary in order to</span><br><span class="line">	 * avoid using the trampoline translations, which are only correct for</span><br><span class="line">	 * the first superpage.  Fetching the fence is guaranteed to work</span><br><span class="line">	 * because that first superpage is translated the same way.</span><br><span class="line">	 */</span><br><span class="line">	/* n: 把页表改成early_pg_dir，这个页表覆盖了全部内核Image和DT的内存 */</span><br><span class="line">	csrw CSR_SATP, a2</span><br><span class="line">	sfence.vma</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * n: 跳转到ra，call relocate的下一条指令的地址，这个地址已经在上面被</span><br><span class="line">	 *    改成虚拟地址。</span><br><span class="line">	 */</span><br><span class="line">	ret</span><br><span class="line">#endif /* CONFIG_MMU */</span><br><span class="line">#ifdef CONFIG_SMP</span><br><span class="line">	.global secondary_start_sbi</span><br><span class="line">secondary_start_sbi:</span><br><span class="line">	/* Mask all interrupts */</span><br><span class="line">	csrw CSR_IE, zero</span><br><span class="line">	csrw CSR_IP, zero</span><br><span class="line"></span><br><span class="line">	/* Load the global pointer */</span><br><span class="line">	.option push</span><br><span class="line">	.option norelax</span><br><span class="line">		la gp, __global_pointer$</span><br><span class="line">	.option pop</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * Disable FPU to detect illegal usage of</span><br><span class="line">	 * floating point in kernel space</span><br><span class="line">	 */</span><br><span class="line">	li t0, SR_FS</span><br><span class="line">	csrc CSR_STATUS, t0</span><br><span class="line"></span><br><span class="line">	/* Set trap vector to spin forever to help debug */</span><br><span class="line">	la a3, .Lsecondary_park</span><br><span class="line">	csrw CSR_TVEC, a3</span><br><span class="line"></span><br><span class="line">	/* a0 contains the hartid &amp; a1 contains boot data */</span><br><span class="line">	li a2, SBI_HART_BOOT_TASK_PTR_OFFSET</span><br><span class="line">	XIP_FIXUP_OFFSET a2</span><br><span class="line">	add a2, a2, a1</span><br><span class="line">	REG_L tp, (a2)</span><br><span class="line">	li a3, SBI_HART_BOOT_STACK_PTR_OFFSET</span><br><span class="line">	XIP_FIXUP_OFFSET a3</span><br><span class="line">	add a3, a3, a1</span><br><span class="line">	REG_L sp, (a3)</span><br><span class="line"></span><br><span class="line">.Lsecondary_start_common:</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_MMU</span><br><span class="line">	/* Enable virtual memory and relocate to virtual address */</span><br><span class="line">	la a0, swapper_pg_dir</span><br><span class="line">	XIP_FIXUP_OFFSET a0</span><br><span class="line">	call relocate</span><br><span class="line">#endif</span><br><span class="line">	call setup_trap_vector</span><br><span class="line">	tail smp_callin</span><br><span class="line">#endif /* CONFIG_SMP */</span><br><span class="line"></span><br><span class="line">.align 2</span><br><span class="line">setup_trap_vector:</span><br><span class="line">	/* n: handle_exception定义在arch/riscv/kernel/entry.S里 */</span><br><span class="line">	/* Set trap vector to exception handler */</span><br><span class="line">	la a0, handle_exception</span><br><span class="line">	csrw CSR_TVEC, a0</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * Set sup0 scratch register to 0, indicating to exception vector that</span><br><span class="line">	 * we are presently executing in kernel.</span><br><span class="line">	 */</span><br><span class="line">	csrw CSR_SCRATCH, zero</span><br><span class="line">	ret</span><br><span class="line"></span><br><span class="line">.align 2</span><br><span class="line">.Lsecondary_park:</span><br><span class="line">	/* We lack SMP support or have too many harts, so park this hart */</span><br><span class="line">	wfi</span><br><span class="line">	j .Lsecondary_park</span><br><span class="line"></span><br><span class="line">END(_start)</span><br><span class="line"></span><br><span class="line">ENTRY(_start_kernel)</span><br><span class="line">	/*</span><br><span class="line">	 * n: 配置相关的CSR寄存器，mask掉中断，调试的时候注意，M mode的相关bit</span><br><span class="line">	 *    是清不掉的</span><br><span class="line">	 */</span><br><span class="line">	/* Mask all interrupts */</span><br><span class="line">	csrw CSR_IE, zero</span><br><span class="line">	csrw CSR_IP, zero</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_RISCV_M_MODE</span><br><span class="line">	/* flush the instruction cache */</span><br><span class="line">	fence.i</span><br><span class="line"></span><br><span class="line">	/* Reset all registers except ra, a0, a1 */</span><br><span class="line">	call reset_regs</span><br><span class="line"></span><br><span class="line">	/* n: pmp的操作，pmp是M mode的地址保护机制，S mode的时候用MMU */</span><br><span class="line">	/*</span><br><span class="line">	 * Setup a PMP to permit access to all of memory.  Some machines may</span><br><span class="line">	 * not implement PMPs, so we set up a quick trap handler to just skip</span><br><span class="line">	 * touching the PMPs on any trap.</span><br><span class="line">	 */</span><br><span class="line">	la a0, pmp_done</span><br><span class="line">	csrw CSR_TVEC, a0</span><br><span class="line"></span><br><span class="line">	li a0, -1</span><br><span class="line">	csrw CSR_PMPADDR0, a0</span><br><span class="line">	li a0, (PMP_A_NAPOT | PMP_R | PMP_W | PMP_X)</span><br><span class="line">	csrw CSR_PMPCFG0, a0</span><br><span class="line">.align 2</span><br><span class="line">pmp_done:</span><br><span class="line"></span><br><span class="line">	/* n: 得到当前的cpu id */</span><br><span class="line">	/*</span><br><span class="line">	 * The hartid in a0 is expected later on, and we have no firmware</span><br><span class="line">	 * to hand it to us.</span><br><span class="line">	 */</span><br><span class="line">	csrr a0, CSR_MHARTID</span><br><span class="line">#endif /* CONFIG_RISCV_M_MODE */</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * n: .option是给riscv编译器看的，push就是保存编译器当前的配置，比如，</span><br><span class="line">	 *    下面的norelax就是一个编译器的配置，编译器处理push就是把它自己目前</span><br><span class="line">	 *    编译代码用的配置先存起来，之后再用.option pop来恢复。</span><br><span class="line">	 */</span><br><span class="line">	/* Load the global pointer */</span><br><span class="line">.option push</span><br><span class="line">.option norelax</span><br><span class="line">	/*</span><br><span class="line">	 * n: gp是riscv的一个全局寄存器，这里把__global_pointer$这个符号的地址</span><br><span class="line">	 *    存到gp里也是一个编译器的优化手段，编译器后续可以使用这个地址做地址</span><br><span class="line">	 *    索引，编译器会自动在编译生成的汇编中使用gp。在MMU起来之前，还没法</span><br><span class="line">	 *    使用虚拟地址，所以就使用相对gp的相对寻址方式。</span><br><span class="line">	 */</span><br><span class="line">	la gp, __global_pointer$</span><br><span class="line">.option pop</span><br><span class="line">	</span><br><span class="line">	/* n: 禁用浮点 */</span><br><span class="line">	/*</span><br><span class="line">	 * Disable FPU to detect illegal usage of</span><br><span class="line">	 * floating point in kernel space</span><br><span class="line">	 */</span><br><span class="line">	li t0, SR_FS</span><br><span class="line">	csrc CSR_STATUS, t0</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_RISCV_BOOT_SPINWAIT</span><br><span class="line">	li t0, CONFIG_NR_CPUS</span><br><span class="line">	/*</span><br><span class="line">	 * n: cpu id没有超过最大范围，就继续跑，否则直接跳到Lsecondary_park，</span><br><span class="line">	 *    这里会停在wfi</span><br><span class="line">	 */</span><br><span class="line">	blt a0, t0, .Lgood_cores</span><br><span class="line">	tail .Lsecondary_park</span><br><span class="line">.Lgood_cores:</span><br><span class="line"></span><br><span class="line">	/* The lottery system is only required for spinwait booting method */</span><br><span class="line">#ifndef CONFIG_XIP_KERNEL</span><br><span class="line">	/* n: hart_lottery是外部定义的一个原子变量，在arch/riscv/kernel/setup.c */</span><br><span class="line">	/* Pick one hart to run the main boot sequence */</span><br><span class="line">	la a3, hart_lottery</span><br><span class="line">	li a2, 1</span><br><span class="line">	/* n: 把a2的值和a3地址上的值相加，并存入a3地址，a3地址上原来的值写入a3 */</span><br><span class="line">	amoadd.w a3, a2, (a3)</span><br><span class="line">	/* n: a3不是0就要跳转，这里a3在上一步中更新成0，所以下面走到bss清理的逻辑 */</span><br><span class="line">	bnez a3, .Lsecondary_start</span><br><span class="line"></span><br><span class="line">#else</span><br><span class="line">	/* hart_lottery in flash contains a magic number */</span><br><span class="line">	la a3, hart_lottery</span><br><span class="line">	mv a2, a3</span><br><span class="line">	XIP_FIXUP_OFFSET a2</span><br><span class="line">	XIP_FIXUP_FLASH_OFFSET a3</span><br><span class="line">	lw t1, (a3)</span><br><span class="line">	amoswap.w t0, t1, (a2)</span><br><span class="line">	/* first time here if hart_lottery in RAM is not set */</span><br><span class="line">	beq t0, t1, .Lsecondary_start</span><br><span class="line"></span><br><span class="line">#endif /* CONFIG_XIP */</span><br><span class="line">#endif /* CONFIG_RISCV_BOOT_SPINWAIT */</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_XIP_KERNEL</span><br><span class="line">	la sp, _end + THREAD_SIZE</span><br><span class="line">	XIP_FIXUP_OFFSET sp</span><br><span class="line">	mv s0, a0</span><br><span class="line">	call __copy_data</span><br><span class="line"></span><br><span class="line">	/* Restore a0 copy */</span><br><span class="line">	mv a0, s0</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifndef CONFIG_XIP_KERNEL</span><br><span class="line">	/* Clear BSS for flat non-ELF images */</span><br><span class="line">	la a3, __bss_start</span><br><span class="line">	la a4, __bss_stop</span><br><span class="line">	ble a4, a3, clear_bss_done</span><br><span class="line">clear_bss:</span><br><span class="line">	REG_S zero, (a3)</span><br><span class="line">	add a3, a3, RISCV_SZPTR</span><br><span class="line">	blt a3, a4, clear_bss</span><br><span class="line">clear_bss_done:</span><br><span class="line">#endif</span><br><span class="line">	/* n：a0放cpu id, a1放DTB的地址，这个从openSBI代码里可以看到 */</span><br><span class="line">	/* Save hart ID and DTB physical address */</span><br><span class="line">	mv s0, a0</span><br><span class="line">	mv s1, a1</span><br><span class="line"></span><br><span class="line">	/* n: boot_cpu_hartid全局变量，初始化是0 */</span><br><span class="line">	la a2, boot_cpu_hartid</span><br><span class="line">	/* n: 这个是XIP用来调整地址的，没有XIP这个是空，可以不看 */</span><br><span class="line">	XIP_FIXUP_OFFSET a2</span><br><span class="line">	REG_S a0, (a2)</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * n: sp保存栈指针，这里是物理地址，不过init_thread_union在什么地方定义？</span><br><span class="line">	 *    init_thread_union这里是thread_info的基地址，thread_info在task_struct</span><br><span class="line">	 *    里，存放硬件架构相关的信息，内核栈在内存中的位置在thread_info上方，</span><br><span class="line">	 *    和thread_info紧邻。THREAD_SIZE就是内核栈大小，64BIT下是PAGE_SIZE * 4</span><br><span class="line">	 */</span><br><span class="line">	/* Initialize page tables and relocate to virtual addresses */</span><br><span class="line">	la sp, init_thread_union + THREAD_SIZE</span><br><span class="line">	XIP_FIXUP_OFFSET sp</span><br><span class="line">#ifdef CONFIG_BUILTIN_DTB</span><br><span class="line">	la a0, __dtb_start</span><br><span class="line">	XIP_FIXUP_OFFSET a0</span><br><span class="line">#else</span><br><span class="line">	/* n: dtb地址给a0，为后面call setup_vm做入参 */</span><br><span class="line">	mv a0, s1</span><br><span class="line">#endif /* CONFIG_BUILTIN_DTB */</span><br><span class="line">	/*</span><br><span class="line">	 * n: 建立初始化阶段的页表，start_kernel里的mm_init会创建正式页表，</span><br><span class="line">	 *    setup_vm在arch/riscv/mm/init.c里，单独分析这块的逻辑。</span><br><span class="line">	 */</span><br><span class="line">	call setup_vm</span><br><span class="line">#ifdef CONFIG_MMU</span><br><span class="line">	/* n: setup_vm里会更新这个页表的总入口 */</span><br><span class="line">	la a0, early_pg_dir</span><br><span class="line">	XIP_FIXUP_OFFSET a0</span><br><span class="line">	/* n: relocate把setup_vm里创建的初始阶段页表配置给硬件 */</span><br><span class="line">	call relocate</span><br><span class="line">#endif /* CONFIG_MMU */</span><br><span class="line">	/* n: 把异常向量配置给硬件 */</span><br><span class="line">	call setup_trap_vector</span><br><span class="line">	/* Restore C environment */</span><br><span class="line">	/* n: tp, sp在这里赋值，tp是内核的task_struct的指针，sp是内核栈指针 */</span><br><span class="line"></span><br><span class="line">	la tp, init_task</span><br><span class="line">	la sp, init_thread_union + THREAD_SIZE</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_KASAN</span><br><span class="line">	call kasan_early_init</span><br><span class="line">#endif</span><br><span class="line">	/* Start the kernel */</span><br><span class="line">	call soc_early_init</span><br><span class="line">	/* n: 开始跑start_kernel，注意是单核在跑，多核启动逻辑另外分析 */</span><br><span class="line">	tail start_kernel</span><br><span class="line"></span><br><span class="line">#if CONFIG_RISCV_BOOT_SPINWAIT</span><br><span class="line">.Lsecondary_start:</span><br><span class="line">	/* Set trap vector to spin forever to help debug */</span><br><span class="line">	la a3, .Lsecondary_park</span><br><span class="line">	csrw CSR_TVEC, a3</span><br><span class="line"></span><br><span class="line">	slli a3, a0, LGREG</span><br><span class="line">	la a1, __cpu_spinwait_stack_pointer</span><br><span class="line">	XIP_FIXUP_OFFSET a1</span><br><span class="line">	la a2, __cpu_spinwait_task_pointer</span><br><span class="line">	XIP_FIXUP_OFFSET a2</span><br><span class="line">	add a1, a3, a1</span><br><span class="line">	add a2, a3, a2</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * This hart didn&#x27;t win the lottery, so we wait for the winning hart to</span><br><span class="line">	 * get far enough along the boot process that it should continue.</span><br><span class="line">	 */</span><br><span class="line">.Lwait_for_cpu_up:</span><br><span class="line">	/* FIXME: We should WFI to save some energy here. */</span><br><span class="line">	REG_L sp, (a1)</span><br><span class="line">	REG_L tp, (a2)</span><br><span class="line">	beqz sp, .Lwait_for_cpu_up</span><br><span class="line">	beqz tp, .Lwait_for_cpu_up</span><br><span class="line">	fence</span><br><span class="line"></span><br><span class="line">	tail .Lsecondary_start_common</span><br><span class="line">#endif /* CONFIG_RISCV_BOOT_SPINWAIT */</span><br><span class="line"></span><br><span class="line">END(_start_kernel)</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_RISCV_M_MODE</span><br><span class="line">ENTRY(reset_regs)</span><br><span class="line">	li	sp, 0</span><br><span class="line">	li	gp, 0</span><br><span class="line">	li	tp, 0</span><br><span class="line">	li	t0, 0</span><br><span class="line">	li	t1, 0</span><br><span class="line">	li	t2, 0</span><br><span class="line">	li	s0, 0</span><br><span class="line">	li	s1, 0</span><br><span class="line">	li	a2, 0</span><br><span class="line">	li	a3, 0</span><br><span class="line">	li	a4, 0</span><br><span class="line">	li	a5, 0</span><br><span class="line">	li	a6, 0</span><br><span class="line">	li	a7, 0</span><br><span class="line">	li	s2, 0</span><br><span class="line">	li	s3, 0</span><br><span class="line">	li	s4, 0</span><br><span class="line">	li	s5, 0</span><br><span class="line">	li	s6, 0</span><br><span class="line">	li	s7, 0</span><br><span class="line">	li	s8, 0</span><br><span class="line">	li	s9, 0</span><br><span class="line">	li	s10, 0</span><br><span class="line">	li	s11, 0</span><br><span class="line">	li	t3, 0</span><br><span class="line">	li	t4, 0</span><br><span class="line">	li	t5, 0</span><br><span class="line">	li	t6, 0</span><br><span class="line">	csrw	CSR_SCRATCH, 0</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_FPU</span><br><span class="line">	csrr	t0, CSR_MISA</span><br><span class="line">	/* n: 判断有没有浮点，如果没有，可以直接退出了 */</span><br><span class="line">	andi	t0, t0, (COMPAT_HWCAP_ISA_F | COMPAT_HWCAP_ISA_D)</span><br><span class="line">	beqz	t0, .Lreset_regs_done</span><br><span class="line"></span><br><span class="line">	/* n: 把status寄存器里浮点相关的两bit配置上 */</span><br><span class="line">	li	t1, SR_FS</span><br><span class="line">	csrs	CSR_STATUS, t1</span><br><span class="line">	fmv.s.x	f0, zero</span><br><span class="line">	fmv.s.x	f1, zero</span><br><span class="line">	fmv.s.x	f2, zero</span><br><span class="line">	fmv.s.x	f3, zero</span><br><span class="line">	fmv.s.x	f4, zero</span><br><span class="line">	fmv.s.x	f5, zero</span><br><span class="line">	fmv.s.x	f6, zero</span><br><span class="line">	fmv.s.x	f7, zero</span><br><span class="line">	fmv.s.x	f8, zero</span><br><span class="line">	fmv.s.x	f9, zero</span><br><span class="line">	fmv.s.x	f10, zero</span><br><span class="line">	fmv.s.x	f11, zero</span><br><span class="line">	fmv.s.x	f12, zero</span><br><span class="line">	fmv.s.x	f13, zero</span><br><span class="line">	fmv.s.x	f14, zero</span><br><span class="line">	fmv.s.x	f15, zero</span><br><span class="line">	fmv.s.x	f16, zero</span><br><span class="line">	fmv.s.x	f17, zero</span><br><span class="line">	fmv.s.x	f18, zero</span><br><span class="line">	fmv.s.x	f19, zero</span><br><span class="line">	fmv.s.x	f20, zero</span><br><span class="line">	fmv.s.x	f21, zero</span><br><span class="line">	fmv.s.x	f22, zero</span><br><span class="line">	fmv.s.x	f23, zero</span><br><span class="line">	fmv.s.x	f24, zero</span><br><span class="line">	fmv.s.x	f25, zero</span><br><span class="line">	fmv.s.x	f26, zero</span><br><span class="line">	fmv.s.x	f27, zero</span><br><span class="line">	fmv.s.x	f28, zero</span><br><span class="line">	fmv.s.x	f29, zero</span><br><span class="line">	fmv.s.x	f30, zero</span><br><span class="line">	fmv.s.x	f31, zero</span><br><span class="line">	/* n: 清空浮点控制寄存器fcsr */</span><br><span class="line">	csrw	fcsr, 0</span><br><span class="line">	/* note that the caller must clear SR_FS */</span><br><span class="line">#endif /* CONFIG_FPU */</span><br><span class="line">.Lreset_regs_done:</span><br><span class="line">	ret</span><br><span class="line">END(reset_regs)</span><br><span class="line">#endif /* CONFIG_RISCV_M_MODE */</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>Linux内核</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核riscv entry.S分析</title>
    <url>/Linux%E5%86%85%E6%A0%B8riscv-entry-S%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* n: 在非抢占的情况下，下面两个符号等价 */</span><br><span class="line">#if !IS_ENABLED(CONFIG_PREEMPTION)</span><br><span class="line">.set resume_kernel, restore_all</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">/* n: 在head.S里配置给CSR_TVEC */</span><br><span class="line">ENTRY(handle_exception)</span><br><span class="line">	/*</span><br><span class="line">	 * If coming from userspace, preserve the user thread pointer and load</span><br><span class="line">	 * the kernel thread pointer.  If we came from the kernel, the scratch</span><br><span class="line">	 * register will contain 0, and we should continue on the current TP.</span><br><span class="line">	 */</span><br><span class="line">	/*</span><br><span class="line">	 * n: 交换tp寄存器和scratch寄存器里的值。如果exception从用户态进来，scratch</span><br><span class="line">	 * 的值应该是task_struct(下面在离开内核的时候，会把用户进程的task_struct</span><br><span class="line">	 * 指针赋给tp)，如果exception从内核态进来，tp应该是当前线程的task_struct</span><br><span class="line">	 * 指针，scratch应该是0。</span><br><span class="line">	 *</span><br><span class="line">	 * 所以，下面紧接着的处理中，如果从内核进来，要恢复下tp。</span><br><span class="line">	 *</span><br><span class="line">	 * 总结下在用户态和内核态时tp和scratch的值是什么：</span><br><span class="line">	 *</span><br><span class="line">	 *  +----------+--------------+---------------+</span><br><span class="line">	 *  |          | user space   |   kernel      |</span><br><span class="line">	 *  +----------+--------------+---------------+</span><br><span class="line">	 *  | tp:      | tls base     |   task_struct |</span><br><span class="line">	 *  +----------+--------------+---------------+</span><br><span class="line">	 *  | scratch: | task_struct  |   0           |</span><br><span class="line">	 *  +----------+--------------+---------------+</span><br><span class="line">	 *</span><br><span class="line">	 * 注意：配置tp为tls地址的函数是：copy_thread，如果用户态没有使用TLS，tp</span><br><span class="line">	 *       在用户态的值是？</span><br><span class="line">	 */</span><br><span class="line">	csrrw tp, CSR_SCRATCH, tp</span><br><span class="line">	/* n: tp不为0，异常来自用户态，直接跳到上下文保存的地方 */</span><br><span class="line">	bnez tp, _save_context</span><br><span class="line"></span><br><span class="line">_restore_kernel_tpsp:</span><br><span class="line">	/*</span><br><span class="line">	 * n: csrr伪指令，把scratch寄存器的值写入tp，上面为了判断是否在内核把tp</span><br><span class="line">	 * 的值和CSR_SCRATCH的值做了交换。这里恢复tp寄存器。</span><br><span class="line">	 */</span><br><span class="line">	csrr tp, CSR_SCRATCH</span><br><span class="line">	/* n: 把内核sp保存到内核thread_info上 */</span><br><span class="line">	REG_S sp, TASK_TI_KERNEL_SP(tp)</span><br><span class="line">_save_context:</span><br><span class="line">	/*</span><br><span class="line">	 * n: 保存sp到thread_info用户态指针的位置，如果是就在内核态，那么把内核栈</span><br><span class="line">	 * sp保存到了thread_info用户态栈指针的位置。</span><br><span class="line">	 *</span><br><span class="line">	 * 异常或中断来自内核或者用户态，再下面合并处理。当来自用户态时，tp的值和</span><br><span class="line">	 * scratch寄存器的值是一样的，所以这里不需要恢复tp。</span><br><span class="line">	 */</span><br><span class="line">	REG_S sp, TASK_TI_USER_SP(tp)</span><br><span class="line">	/* n: sp换成内核栈 */</span><br><span class="line">	REG_L sp, TASK_TI_KERNEL_SP(tp)</span><br><span class="line">	/*</span><br><span class="line">	 * n: 扩大栈的范围，扩大的范围用来保存相关的寄存器。移动sp其实就相当于在栈上分配空间。</span><br><span class="line">	 *    sp移动之前的值是中断或者异常打断的上下文，也就是中断或异常处理完后要恢复的值。</span><br><span class="line">	 */</span><br><span class="line">	addi sp, sp, -(PT_SIZE_ON_STACK)</span><br><span class="line">	/*</span><br><span class="line">	 * n: 下面的一段代码把各个系统寄存器保存到栈上刚刚开辟出来的空间, 注意需要</span><br><span class="line">	 * 特殊处理的是sp(x2)和tp(x4)。当前的sp，由于上面的变动已经不是需要保存的sp，</span><br><span class="line">	 * 但是，之前我们已经把需要保存的sp放到了thread_info里，所以下面把thread_info</span><br><span class="line">	 * 里的sp取出后再入栈。</span><br><span class="line">	 */</span><br><span class="line">	REG_S x1,  PT_RA(sp)</span><br><span class="line">	REG_S x3,  PT_GP(sp)</span><br><span class="line">	REG_S x5,  PT_T0(sp)</span><br><span class="line">	REG_S x6,  PT_T1(sp)</span><br><span class="line">	REG_S x7,  PT_T2(sp)</span><br><span class="line">	REG_S x8,  PT_S0(sp)</span><br><span class="line">	REG_S x9,  PT_S1(sp)</span><br><span class="line">	REG_S x10, PT_A0(sp)</span><br><span class="line">	REG_S x11, PT_A1(sp)</span><br><span class="line">	REG_S x12, PT_A2(sp)</span><br><span class="line">	REG_S x13, PT_A3(sp)</span><br><span class="line">	REG_S x14, PT_A4(sp)</span><br><span class="line">	REG_S x15, PT_A5(sp)</span><br><span class="line">	REG_S x16, PT_A6(sp)</span><br><span class="line">	REG_S x17, PT_A7(sp)</span><br><span class="line">	REG_S x18, PT_S2(sp)</span><br><span class="line">	REG_S x19, PT_S3(sp)</span><br><span class="line">	REG_S x20, PT_S4(sp)</span><br><span class="line">	REG_S x21, PT_S5(sp)</span><br><span class="line">	REG_S x22, PT_S6(sp)</span><br><span class="line">	REG_S x23, PT_S7(sp)</span><br><span class="line">	REG_S x24, PT_S8(sp)</span><br><span class="line">	REG_S x25, PT_S9(sp)</span><br><span class="line">	REG_S x26, PT_S10(sp)</span><br><span class="line">	REG_S x27, PT_S11(sp)</span><br><span class="line">	REG_S x28, PT_T3(sp)</span><br><span class="line">	REG_S x29, PT_T4(sp)</span><br><span class="line">	REG_S x30, PT_T5(sp)</span><br><span class="line">	REG_S x31, PT_T6(sp)</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * Disable user-mode memory access as it should only be set in the</span><br><span class="line">	 * actual user copy routines.</span><br><span class="line">	 *</span><br><span class="line">	 * Disable the FPU to detect illegal usage of floating point in kernel</span><br><span class="line">	 * space.</span><br><span class="line">	 */</span><br><span class="line">	/* n: 配置先放到t0寄存器里，SR_SUM为1容许S mode接入U mode地址，反之不容许 */</span><br><span class="line">	li t0, SR_SUM | SR_FS</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * n: 上面已经保存了寄存器现场，下面可以使用系统寄存器了，s0保存用户态sp。</span><br><span class="line">	 * 把task_struct里保存的用户态栈指针提取出来，然后在后面保存到内核栈上。</span><br><span class="line">	 */</span><br><span class="line">	REG_L s0, TASK_TI_USER_SP(tp)</span><br><span class="line">	/* n: 把中断异常相关的csr寄存器读出来, ~t0 &amp; csr_status旧值，然后写入csr_status，相当于清SR_SUM和SR_FS */</span><br><span class="line">	csrrc s1, CSR_STATUS, t0</span><br><span class="line">	csrr s2, CSR_EPC</span><br><span class="line">	csrr s3, CSR_TVAL</span><br><span class="line">	csrr s4, CSR_CAUSE</span><br><span class="line">	csrr s5, CSR_SCRATCH</span><br><span class="line">	/* n: 把中断异常相关的csr寄存器以及用户态sp保存到栈上 */</span><br><span class="line">	REG_S s0, PT_SP(sp)</span><br><span class="line">	REG_S s1, PT_STATUS(sp)</span><br><span class="line">	REG_S s2, PT_EPC(sp)</span><br><span class="line">	REG_S s3, PT_BADADDR(sp)</span><br><span class="line">	REG_S s4, PT_CAUSE(sp)</span><br><span class="line">	REG_S s5, PT_TP(sp)</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * Set the scratch register to 0, so that if a recursive exception</span><br><span class="line">	 * occurs, the exception vector knows it came from the kernel</span><br><span class="line">	 */</span><br><span class="line">	csrw CSR_SCRATCH, x0</span><br><span class="line"></span><br><span class="line">	/* Load the global pointer */</span><br><span class="line">.option push</span><br><span class="line">.option norelax</span><br><span class="line">	la gp, __global_pointer$</span><br><span class="line">.option pop</span><br><span class="line"></span><br><span class="line">	/* n: todo */</span><br><span class="line">#ifdef CONFIG_TRACE_IRQFLAGS</span><br><span class="line">	call trace_hardirqs_off</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">	/* n: todo */</span><br><span class="line">#ifdef CONFIG_CONTEXT_TRACKING</span><br><span class="line">	/* If previous state is in user mode, call context_tracking_user_exit. */</span><br><span class="line">	li   a0, SR_PP</span><br><span class="line">	and a0, s1, a0</span><br><span class="line">	bnez a0, skip_context_tracking</span><br><span class="line">	call context_tracking_user_exit</span><br><span class="line">skip_context_tracking:</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * MSB of cause differentiates between</span><br><span class="line">	 * interrupts and exceptions</span><br><span class="line">	 */</span><br><span class="line">	/* n: 最高位为1会解释为一个负数，如果是异常最高位为0，会跳到lable 1f */</span><br><span class="line">	bge s4, zero, 1f</span><br><span class="line"></span><br><span class="line">	/* n: 如果是中断，会走到这里，先把返回地址保存到ra */</span><br><span class="line">	la ra, ret_from_exception</span><br><span class="line"></span><br><span class="line">	/* Handle interrupts */</span><br><span class="line">	/* n: 上面可以知道sp就是pt_regs的地址，a0存放pt_regs的地址 */</span><br><span class="line">	move a0, sp /* pt_regs */</span><br><span class="line">	/* n: a1存handle_arch_irq的地址 */</span><br><span class="line">	la a1, handle_arch_irq</span><br><span class="line">	/* n: handle_arch_irq看起来像一个数组，这个在哪里定义？*/</span><br><span class="line">	REG_L a1, (a1)</span><br><span class="line">	/* n: 返回地址已经配置到ra里。这里是中断处理函数的入口，下面的处理都和异常有关了 */</span><br><span class="line">	jr a1</span><br><span class="line">1:</span><br><span class="line">	/*</span><br><span class="line">	 * Exceptions run with interrupts enabled or disabled depending on the</span><br><span class="line">	 * state of SR_PIE in m/sstatus.</span><br><span class="line">	 */</span><br><span class="line">	/* n: s1之前被存入sstatus的值，这里把SR_PIE的值存入t0 */</span><br><span class="line">	andi t0, s1, SR_PIE</span><br><span class="line">	/*</span><br><span class="line">	 * n: PIE表示进入中断或者异常之前IE的配置情况，如果之前就是关中断，现在</span><br><span class="line">	 * 继续关中断跑，如果之前是开中断的，需要打开中断继续跑，但是ebreak异常中</span><br><span class="line">	 * 不能开中断，所以，如果之前是开中断的，我们继续检测是不是ebreak异常，</span><br><span class="line">	 * 如果是ebreak异常，在关中断下继续跑，如果不是ebreak异常，会走到csrs CSR_STATUS, SR_IE，</span><br><span class="line">	 * 这里会把中断打开。</span><br><span class="line">	 *</span><br><span class="line">	 * 注意，这里是在处理异常。硬件在处理中断或异常时会改变SPIE/SPP/SIE的值。</span><br><span class="line">	 * SPP会保存中断或异常时CPU的mode，SPIE会保存sstatus里的SIE值，SIE表示S</span><br><span class="line">	 * mode中断是否被enable，最后硬件会关中断就是把SIE配置成0。</span><br><span class="line">	 */</span><br><span class="line">	beqz t0, 1f</span><br><span class="line">	/* kprobes, entered via ebreak, must have interrupts disabled. */</span><br><span class="line">	li t0, EXC_BREAKPOINT</span><br><span class="line">	beq s4, t0, 1f</span><br><span class="line">	/* n: todo */</span><br><span class="line">#ifdef CONFIG_TRACE_IRQFLAGS</span><br><span class="line">	call trace_hardirqs_on</span><br><span class="line">#endif</span><br><span class="line">	/*</span><br><span class="line">	 * n: csrs是csrrs x0, csr, rs, 意思是把csr的值存入x0, csr的旧值和rs做或运算后写入csr。</span><br><span class="line">	 * 这里是把S mode全局中断打开。注意，上面的逻辑是如果检测到是ebreak异常是直接跳过开中断的。</span><br><span class="line">	 */</span><br><span class="line">	csrs CSR_STATUS, SR_IE</span><br><span class="line"></span><br><span class="line">	/* n: 开始处理异常，先配置返回地址。非ebreak，且之前是开中断的，异常处理前开中断 */</span><br><span class="line">1:</span><br><span class="line">	la ra, ret_from_exception</span><br><span class="line">	/* Handle syscalls */</span><br><span class="line">	li t0, EXC_SYSCALL</span><br><span class="line">	/* n: 如果是ecall异常就跳到系统调用的地方处理 */</span><br><span class="line">	beq s4, t0, handle_syscall</span><br><span class="line"></span><br><span class="line">	/* Handle other exceptions */</span><br><span class="line">	/* n: 向左移动3bit就是得到函数指针的偏移，其他异常的入口函数排到了excp_vect_table这个表里 */</span><br><span class="line">	slli t0, s4, RISCV_LGPTR</span><br><span class="line">	la t1, excp_vect_table</span><br><span class="line">	la t2, excp_vect_table_end</span><br><span class="line">	move a0, sp /* pt_regs */</span><br><span class="line">	/* n: t1是异常向量表的基地址，再加上对应异常处理函数的偏移，t0的值就是对应异常处理函数的地址 */</span><br><span class="line">	add t0, t1, t0</span><br><span class="line">	/* Check if exception code lies within bounds */</span><br><span class="line">	/* n: 计算的异常处理函数的地址超出了异常向量表的结尾 */</span><br><span class="line">	bgeu t0, t2, 1f</span><br><span class="line">	/* n: t0是存放函数指针的地址，这里把t0更新成函数指针 */</span><br><span class="line">	REG_L t0, 0(t0)</span><br><span class="line">	/* n: 跳到对应异常处理函数执行，返回地址还是之前配置的ret_from_exception */</span><br><span class="line">	jr t0</span><br><span class="line">1:</span><br><span class="line">	tail do_trap_unknown</span><br><span class="line"></span><br><span class="line">	/* 这个是handle_syscall这个函数的独立代码了, 异常的处理被分为系统调用和其他异常 */</span><br><span class="line">handle_syscall:</span><br><span class="line">	/* n: todo */</span><br><span class="line">#ifdef CONFIG_RISCV_M_MODE</span><br><span class="line">	/*</span><br><span class="line">	 * When running is M-Mode (no MMU config), MPIE does not get set.</span><br><span class="line">	 * As a result, we need to force enable interrupts here because</span><br><span class="line">	 * handle_exception did not do set SR_IE as it always sees SR_PIE</span><br><span class="line">	 * being cleared.</span><br><span class="line">	 */</span><br><span class="line">	csrs CSR_STATUS, SR_IE</span><br><span class="line">#endif</span><br><span class="line">	/* n: todo */</span><br><span class="line">#if defined(CONFIG_TRACE_IRQFLAGS) || defined(CONFIG_CONTEXT_TRACKING)</span><br><span class="line">	/* Recover a0 - a7 for system calls */</span><br><span class="line">	REG_L a0, PT_A0(sp)</span><br><span class="line">	REG_L a1, PT_A1(sp)</span><br><span class="line">	REG_L a2, PT_A2(sp)</span><br><span class="line">	REG_L a3, PT_A3(sp)</span><br><span class="line">	REG_L a4, PT_A4(sp)</span><br><span class="line">	REG_L a5, PT_A5(sp)</span><br><span class="line">	REG_L a6, PT_A6(sp)</span><br><span class="line">	REG_L a7, PT_A7(sp)</span><br><span class="line">#endif</span><br><span class="line">	/* save the initial A0 value (needed in signal handlers) */</span><br><span class="line">	/* n: 把a0保存到栈的PT_ORIG_A0这个位置，a0为什么要存都这里？(a0会作为系统调用的返回值，估计这里是为了做区分) */</span><br><span class="line">	REG_S a0, PT_ORIG_A0(sp)</span><br><span class="line">	/*</span><br><span class="line">	 * Advance SEPC to avoid executing the original</span><br><span class="line">	 * scall instruction on sret</span><br><span class="line">	 */</span><br><span class="line">	addi s2, s2, 0x4</span><br><span class="line">	REG_S s2, PT_EPC(sp)</span><br><span class="line">	/* Trace syscalls, but only if requested by the user. */</span><br><span class="line">	/* n: 读一个thread_info里的flag，判断是否要走syscall trace */</span><br><span class="line">	REG_L t0, TASK_TI_FLAGS(tp)</span><br><span class="line">	andi t0, t0, _TIF_SYSCALL_WORK</span><br><span class="line">	bnez t0, handle_syscall_trace_enter</span><br><span class="line">check_syscall_nr:</span><br><span class="line">	/* Check to make sure we don&#x27;t jump to a bogus syscall number. */</span><br><span class="line">	li t0, __NR_syscalls</span><br><span class="line">	/* n: ni的意思是no implemented，下面可以看出如果系统调用号大于等于最大值了，就会进入这个函数 */</span><br><span class="line">	la s0, sys_ni_syscall</span><br><span class="line">	/*</span><br><span class="line">	 * Syscall number held in a7.</span><br><span class="line">	 * If syscall number is above allowed value, redirect to ni_syscall.</span><br><span class="line">	 */</span><br><span class="line">	bgeu a7, t0, 1f</span><br><span class="line">	/* Call syscall */</span><br><span class="line">	/*</span><br><span class="line">	 * n: sys_call_table这个表定义在arch/riscv/kernel/syscall_table.c，又include</span><br><span class="line">	 * 到了其他地方，最终系统调用都在linux/include/uapi/asm-generic/unistd.h</span><br><span class="line">	 * 里定义。</span><br><span class="line">	 *</span><br><span class="line">	 * 注意riscv系统调用的参数使用a0-a7传递，a7里放系统调用号。</span><br><span class="line">	 */</span><br><span class="line">	la s0, sys_call_table</span><br><span class="line">	slli t0, a7, RISCV_LGPTR</span><br><span class="line">	add s0, s0, t0</span><br><span class="line">	REG_L s0, 0(s0)</span><br><span class="line">	/* n: 从这里返回，返回到ret_from_syscall, 之前的几个跳转返回，会返回到之前配置的ra，就是ret_from_exception */</span><br><span class="line">1:</span><br><span class="line">	jalr s0</span><br><span class="line"></span><br><span class="line">ret_from_syscall:</span><br><span class="line">	/* Set user a0 to kernel a0 */</span><br><span class="line">	/* n: a0保存返回值，这里把a0存入内核栈保存的寄存器是为了后面恢复？*/</span><br><span class="line">	REG_S a0, PT_A0(sp)</span><br><span class="line">	/*</span><br><span class="line">	 * We didn&#x27;t execute the actual syscall.</span><br><span class="line">	 * Seccomp already set return value for the current task pt_regs.</span><br><span class="line">	 * (If it was configured with SECCOMP_RET_ERRNO/TRACE)</span><br><span class="line">	 */</span><br><span class="line">ret_from_syscall_rejected:</span><br><span class="line">	/* Trace syscalls, but only if requested by the user. */</span><br><span class="line">	REG_L t0, TASK_TI_FLAGS(tp)</span><br><span class="line">	andi t0, t0, _TIF_SYSCALL_WORK</span><br><span class="line">	bnez t0, handle_syscall_trace_exit</span><br><span class="line"></span><br><span class="line">ret_from_exception:</span><br><span class="line">	/* n: 把sstatus的值加载到s0里 */</span><br><span class="line">	REG_L s0, PT_STATUS(sp)</span><br><span class="line">	/* n: 关中断。处理异常的时候，除了ebreak异常，其他异常处理可能是开中断的 */</span><br><span class="line">	csrc CSR_STATUS, SR_IE</span><br><span class="line">#ifdef CONFIG_TRACE_IRQFLAGS</span><br><span class="line">	call trace_hardirqs_off</span><br><span class="line">#endif</span><br><span class="line">#ifdef CONFIG_RISCV_M_MODE</span><br><span class="line">	/* the MPP value is too large to be used as an immediate arg for addi */</span><br><span class="line">	li t0, SR_MPP</span><br><span class="line">	and s0, s0, t0</span><br><span class="line">#else</span><br><span class="line">	/* n: 恢复进入S mode之前的mode, 并保存到s0 */</span><br><span class="line">	andi s0, s0, SR_SPP</span><br><span class="line">#endif</span><br><span class="line">	/* n: s0不是0，即不是user mode，那只恢复内核的上下文就好, 否则要先恢复用户态上下文 */</span><br><span class="line">	bnez s0, resume_kernel</span><br><span class="line"></span><br><span class="line">	/* n: 用户态和内核态恢复唯一的区别就在resume_userspace这一段 */</span><br><span class="line">resume_userspace:</span><br><span class="line">	/* Interrupts must be disabled here so flags are checked atomically */</span><br><span class="line">	REG_L s0, TASK_TI_FLAGS(tp) /* current_thread_info-&gt;flags */</span><br><span class="line">	/* n: todo: 哪里会配置这个值？(这个估计要结合调度看下) 定义在arch/riscv/include/asm/thread_info.h */</span><br><span class="line">	andi s1, s0, _TIF_WORK_MASK</span><br><span class="line">	bnez s1, work_pending</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_CONTEXT_TRACKING</span><br><span class="line">	call context_tracking_user_enter</span><br><span class="line">#endif</span><br><span class="line">	/* n: 这里把相当于sp向上移动，相当于退栈，释放保存寄存器上下文的栈空间 */</span><br><span class="line">	/* Save unwound kernel stack pointer in thread_info */</span><br><span class="line">	addi s0, sp, PT_SIZE_ON_STACK</span><br><span class="line">	/* n: 把当前内核栈sp保存到task_struct */</span><br><span class="line">	REG_S s0, TASK_TI_KERNEL_SP(tp)</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * Save TP into the scratch register , so we can find the kernel data</span><br><span class="line">	 * structures again.</span><br><span class="line">	 */</span><br><span class="line">	/*</span><br><span class="line">	 * n: 每次离开内核的时候把当前进程的tp存入scratch寄存器。注意，只有进入</span><br><span class="line">	 * 用户态的时候才这样，这里也和一开始的分析呼应。</span><br><span class="line">	 */</span><br><span class="line">	csrw CSR_SCRATCH, tp</span><br><span class="line"></span><br><span class="line">restore_all:</span><br><span class="line">#ifdef CONFIG_TRACE_IRQFLAGS</span><br><span class="line">	REG_L s1, PT_STATUS(sp)</span><br><span class="line">	andi t0, s1, SR_PIE</span><br><span class="line">	beqz t0, 1f</span><br><span class="line">	call trace_hardirqs_on</span><br><span class="line">	j 2f</span><br><span class="line">1:</span><br><span class="line">	call trace_hardirqs_off</span><br><span class="line">2:</span><br><span class="line">#endif</span><br><span class="line">	/* n: 注意，前面虽然已经&quot;退栈&quot;，但是sp的指向还没有变，这里依然可以继续使用sp */</span><br><span class="line">	REG_L a0, PT_STATUS(sp)</span><br><span class="line">	/*</span><br><span class="line">	 * The current load reservation is effectively part of the processor&#x27;s</span><br><span class="line">	 * state, in the sense that load reservations cannot be shared between</span><br><span class="line">	 * different hart contexts.  We can&#x27;t actually save and restore a load</span><br><span class="line">	 * reservation, so instead here we clear any existing reservation --</span><br><span class="line">	 * it&#x27;s always legal for implementations to clear load reservations at</span><br><span class="line">	 * any point (as long as the forward progress guarantee is kept, but</span><br><span class="line">	 * we&#x27;ll ignore that here).</span><br><span class="line">	 *</span><br><span class="line">	 * Dangling load reservations can be the result of taking a trap in the</span><br><span class="line">	 * middle of an LR/SC sequence, but can also be the result of a taken</span><br><span class="line">	 * forward branch around an SC -- which is how we implement CAS.  As a</span><br><span class="line">	 * result we need to clear reservations between the last CAS and the</span><br><span class="line">	 * jump back to the new context.  While it is unlikely the store</span><br><span class="line">	 * completes, implementations are allowed to expand reservations to be</span><br><span class="line">	 * arbitrarily large.</span><br><span class="line">	 */</span><br><span class="line">	/* n: 处理和lr/sc相关的逻辑? */</span><br><span class="line">	REG_L  a2, PT_EPC(sp)</span><br><span class="line">	REG_SC x0, a2, PT_EPC(sp)</span><br><span class="line"></span><br><span class="line">	csrw CSR_STATUS, a0</span><br><span class="line">	csrw CSR_EPC, a2</span><br><span class="line"></span><br><span class="line">	REG_L x1,  PT_RA(sp)</span><br><span class="line">	REG_L x3,  PT_GP(sp)</span><br><span class="line">	REG_L x4,  PT_TP(sp)</span><br><span class="line">	REG_L x5,  PT_T0(sp)</span><br><span class="line">	REG_L x6,  PT_T1(sp)</span><br><span class="line">	REG_L x7,  PT_T2(sp)</span><br><span class="line">	REG_L x8,  PT_S0(sp)</span><br><span class="line">	REG_L x9,  PT_S1(sp)</span><br><span class="line">	REG_L x10, PT_A0(sp)</span><br><span class="line">	REG_L x11, PT_A1(sp)</span><br><span class="line">	REG_L x12, PT_A2(sp)</span><br><span class="line">	REG_L x13, PT_A3(sp)</span><br><span class="line">	REG_L x14, PT_A4(sp)</span><br><span class="line">	REG_L x15, PT_A5(sp)</span><br><span class="line">	REG_L x16, PT_A6(sp)</span><br><span class="line">	REG_L x17, PT_A7(sp)</span><br><span class="line">	REG_L x18, PT_S2(sp)</span><br><span class="line">	REG_L x19, PT_S3(sp)</span><br><span class="line">	REG_L x20, PT_S4(sp)</span><br><span class="line">	REG_L x21, PT_S5(sp)</span><br><span class="line">	REG_L x22, PT_S6(sp)</span><br><span class="line">	REG_L x23, PT_S7(sp)</span><br><span class="line">	REG_L x24, PT_S8(sp)</span><br><span class="line">	REG_L x25, PT_S9(sp)</span><br><span class="line">	REG_L x26, PT_S10(sp)</span><br><span class="line">	REG_L x27, PT_S11(sp)</span><br><span class="line">	REG_L x28, PT_T3(sp)</span><br><span class="line">	REG_L x29, PT_T4(sp)</span><br><span class="line">	REG_L x30, PT_T5(sp)</span><br><span class="line">	REG_L x31, PT_T6(sp)</span><br><span class="line"></span><br><span class="line">	REG_L x2,  PT_SP(sp)</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_RISCV_M_MODE</span><br><span class="line">	mret</span><br><span class="line">#else</span><br><span class="line">	sret</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">/* n: 开抢占的时候，在恢复上下文之前要先处理下抢占 */</span><br><span class="line">#if IS_ENABLED(CONFIG_PREEMPTION)</span><br><span class="line">resume_kernel:</span><br><span class="line">	/*</span><br><span class="line">	 * n: 取thread_info里的preempt_count, 如果是非0，那么是禁止抢占的, 就直接</span><br><span class="line">	 *    跳到restore_all, 不做后面的调度, 否则就继续往下走，查看是否需要调度，</span><br><span class="line">	 *    需要做调度，就执行下调度。</span><br><span class="line">	 *</span><br><span class="line">	 *    所谓抢占就是一个线程执行被被动的中断，然后CPU上换另一个线程的上下文</span><br><span class="line">	 *    执行，所以，一个线程主动放弃CPU，换另一个线程上CPU执行肯定不是抢占，</span><br><span class="line">	 *    所以，抢占具体实施的点就是像在中断或异常的时候，处理完中断或异常，</span><br><span class="line">	 *    然后调度一下，这个时候就可能调度其他的线程到CPU上跑。</span><br><span class="line">	 *</span><br><span class="line">	 *    内核里又加了一个禁止抢占的标记，就是上面preempt_count这个变量，当</span><br><span class="line">	 *    这个变量是0的时候，是可以抢占的，当这个变量大于0，是不能抢占的。</span><br><span class="line">	 *    这样，就有可能出现，中断处理完但是不能抢占的情况。</span><br><span class="line">	 */</span><br><span class="line">	REG_L s0, TASK_TI_PREEMPT_COUNT(tp)</span><br><span class="line">	bnez s0, restore_all</span><br><span class="line">	REG_L s0, TASK_TI_FLAGS(tp)</span><br><span class="line">	andi s0, s0, _TIF_NEED_RESCHED</span><br><span class="line">	beqz s0, restore_all</span><br><span class="line">	call preempt_schedule_irq</span><br><span class="line">	j restore_all</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">work_pending:</span><br><span class="line">	/* Enter slow path for supplementary processing */</span><br><span class="line">	la ra, ret_from_exception</span><br><span class="line">	andi s1, s0, _TIF_NEED_RESCHED</span><br><span class="line">	/* n: 异常或中断返回时主动调度 */</span><br><span class="line">	bnez s1, work_resched</span><br><span class="line">work_notifysig:</span><br><span class="line">	/* Handle pending signals and notify-resume requests */</span><br><span class="line">	csrs CSR_STATUS, SR_IE /* Enable interrupts for do_notify_resume() */</span><br><span class="line">	move a0, sp /* pt_regs */</span><br><span class="line">	move a1, s0 /* current_thread_info-&gt;flags */</span><br><span class="line">	tail do_notify_resume</span><br><span class="line">work_resched:</span><br><span class="line">	tail schedule</span><br><span class="line"></span><br><span class="line">/* Slow paths for ptrace. */</span><br><span class="line">handle_syscall_trace_enter:</span><br><span class="line">	move a0, sp</span><br><span class="line">	call do_syscall_trace_enter</span><br><span class="line">	move t0, a0</span><br><span class="line">	REG_L a0, PT_A0(sp)</span><br><span class="line">	REG_L a1, PT_A1(sp)</span><br><span class="line">	REG_L a2, PT_A2(sp)</span><br><span class="line">	REG_L a3, PT_A3(sp)</span><br><span class="line">	REG_L a4, PT_A4(sp)</span><br><span class="line">	REG_L a5, PT_A5(sp)</span><br><span class="line">	REG_L a6, PT_A6(sp)</span><br><span class="line">	REG_L a7, PT_A7(sp)</span><br><span class="line">	bnez t0, ret_from_syscall_rejected</span><br><span class="line">	j check_syscall_nr</span><br><span class="line">handle_syscall_trace_exit:</span><br><span class="line">	move a0, sp</span><br><span class="line">	call do_syscall_trace_exit</span><br><span class="line">	j ret_from_exception</span><br><span class="line"></span><br><span class="line">END(handle_exception)</span><br><span class="line"></span><br><span class="line">ENTRY(ret_from_fork)</span><br><span class="line">	la ra, ret_from_exception</span><br><span class="line">	tail schedule_tail</span><br><span class="line">ENDPROC(ret_from_fork)</span><br><span class="line"></span><br><span class="line">ENTRY(ret_from_kernel_thread)</span><br><span class="line">	call schedule_tail</span><br><span class="line">	/* Call fn(arg) */</span><br><span class="line">	la ra, ret_from_exception</span><br><span class="line">	move a0, s1</span><br><span class="line">	jr s0</span><br><span class="line">ENDPROC(ret_from_kernel_thread)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * Integer register context switch</span><br><span class="line"> * The callee-saved registers must be saved and restored.</span><br><span class="line"> *</span><br><span class="line"> *   a0: previous task_struct (must be preserved across the switch)</span><br><span class="line"> *   a1: next task_struct</span><br><span class="line"> *</span><br><span class="line"> * The value of a0 and a1 must be preserved by this function, as that&#x27;s how</span><br><span class="line"> * arguments are passed to schedule_tail.</span><br><span class="line"> */</span><br><span class="line">/*</span><br><span class="line"> * n: sched/core.c里的context_switch会调用到这里。__switch_to这个函数的第一个入参</span><br><span class="line"> * 是前task_struct, 后一个是要switch到的线程的task_struct。task_struct里有一个架</span><br><span class="line"> * 构相关的结构thread_struct。riscv的定义在arch/riscv/include/asm/processor.h, </span><br><span class="line"> * TASK_THREAD_RA是这个结构在task_struct里的偏移，所以a3、a4得到thread_struct在</span><br><span class="line"> * 切出、切入线程task_struct里thread_struct的地址。</span><br><span class="line"> * </span><br><span class="line"> */</span><br><span class="line">ENTRY(__switch_to)</span><br><span class="line">	/* Save context into prev-&gt;thread */</span><br><span class="line">	li    a4,  TASK_THREAD_RA</span><br><span class="line">	add   a3, a0, a4</span><br><span class="line">	add   a4, a1, a4</span><br><span class="line">	/* n: 保存这些就够了，剩下的是caller save寄存器，已经保存到对应的栈了 */</span><br><span class="line">	REG_S ra,  TASK_THREAD_RA_RA(a3)</span><br><span class="line">	REG_S sp,  TASK_THREAD_SP_RA(a3)</span><br><span class="line">	REG_S s0,  TASK_THREAD_S0_RA(a3)</span><br><span class="line">	REG_S s1,  TASK_THREAD_S1_RA(a3)</span><br><span class="line">	REG_S s2,  TASK_THREAD_S2_RA(a3)</span><br><span class="line">	REG_S s3,  TASK_THREAD_S3_RA(a3)</span><br><span class="line">	REG_S s4,  TASK_THREAD_S4_RA(a3)</span><br><span class="line">	REG_S s5,  TASK_THREAD_S5_RA(a3)</span><br><span class="line">	REG_S s6,  TASK_THREAD_S6_RA(a3)</span><br><span class="line">	REG_S s7,  TASK_THREAD_S7_RA(a3)</span><br><span class="line">	REG_S s8,  TASK_THREAD_S8_RA(a3)</span><br><span class="line">	REG_S s9,  TASK_THREAD_S9_RA(a3)</span><br><span class="line">	REG_S s10, TASK_THREAD_S10_RA(a3)</span><br><span class="line">	REG_S s11, TASK_THREAD_S11_RA(a3)</span><br><span class="line">	/* Restore context from next-&gt;thread */</span><br><span class="line">	REG_L ra,  TASK_THREAD_RA_RA(a4)</span><br><span class="line">	REG_L sp,  TASK_THREAD_SP_RA(a4)</span><br><span class="line">	REG_L s0,  TASK_THREAD_S0_RA(a4)</span><br><span class="line">	REG_L s1,  TASK_THREAD_S1_RA(a4)</span><br><span class="line">	REG_L s2,  TASK_THREAD_S2_RA(a4)</span><br><span class="line">	REG_L s3,  TASK_THREAD_S3_RA(a4)</span><br><span class="line">	REG_L s4,  TASK_THREAD_S4_RA(a4)</span><br><span class="line">	REG_L s5,  TASK_THREAD_S5_RA(a4)</span><br><span class="line">	REG_L s6,  TASK_THREAD_S6_RA(a4)</span><br><span class="line">	REG_L s7,  TASK_THREAD_S7_RA(a4)</span><br><span class="line">	REG_L s8,  TASK_THREAD_S8_RA(a4)</span><br><span class="line">	REG_L s9,  TASK_THREAD_S9_RA(a4)</span><br><span class="line">	REG_L s10, TASK_THREAD_S10_RA(a4)</span><br><span class="line">	REG_L s11, TASK_THREAD_S11_RA(a4)</span><br><span class="line">	/* Swap the CPU entry around. */</span><br><span class="line">	/* n: 交换两个task_struct里的thread_info里的cpu域段，cpu的语意是？*/</span><br><span class="line">	lw a3, TASK_TI_CPU(a0)</span><br><span class="line">	lw a4, TASK_TI_CPU(a1)</span><br><span class="line">	sw a3, TASK_TI_CPU(a1)</span><br><span class="line">	sw a4, TASK_TI_CPU(a0)</span><br><span class="line">	/* The offset of thread_info in task_struct is zero. */</span><br><span class="line">	/* n: tp指向新的task_struct */</span><br><span class="line">	move tp, a1</span><br><span class="line">	ret</span><br><span class="line">ENDPROC(__switch_to)</span><br><span class="line"></span><br><span class="line">#ifndef CONFIG_MMU</span><br><span class="line">#define do_page_fault do_trap_unknown</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">	.section &quot;.rodata&quot;</span><br><span class="line">	.align LGREG</span><br><span class="line">	/* Exception vector table */</span><br><span class="line">ENTRY(excp_vect_table)</span><br><span class="line">	RISCV_PTR do_trap_insn_misaligned</span><br><span class="line">	RISCV_PTR do_trap_insn_fault</span><br><span class="line">	RISCV_PTR do_trap_insn_illegal</span><br><span class="line">	RISCV_PTR do_trap_break</span><br><span class="line">	RISCV_PTR do_trap_load_misaligned</span><br><span class="line">	RISCV_PTR do_trap_load_fault</span><br><span class="line">	RISCV_PTR do_trap_store_misaligned</span><br><span class="line">	RISCV_PTR do_trap_store_fault</span><br><span class="line">	RISCV_PTR do_trap_ecall_u /* system call, gets intercepted */</span><br><span class="line">	RISCV_PTR do_trap_ecall_s</span><br><span class="line">	RISCV_PTR do_trap_unknown</span><br><span class="line">	RISCV_PTR do_trap_ecall_m</span><br><span class="line">	RISCV_PTR do_page_fault   /* instruction page fault */</span><br><span class="line">	RISCV_PTR do_page_fault   /* load page fault */</span><br><span class="line">	RISCV_PTR do_trap_unknown</span><br><span class="line">	RISCV_PTR do_page_fault   /* store page fault */</span><br><span class="line">excp_vect_table_end:</span><br><span class="line">END(excp_vect_table)</span><br><span class="line"></span><br><span class="line">#ifndef CONFIG_MMU</span><br><span class="line">ENTRY(__user_rt_sigreturn)</span><br><span class="line">	li a7, __NR_rt_sigreturn</span><br><span class="line">	scall</span><br><span class="line">END(__user_rt_sigreturn)</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">       -+- task_struct           low address      &lt;--- tp</span><br><span class="line">        |  (thread_info)</span><br><span class="line">        |</span><br><span class="line">        |  struct thread_struct thread</span><br><span class="line">stack   |</span><br><span class="line">size    |</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">        |</span><br><span class="line">        +-+-</span><br><span class="line">        | |                                          ^</span><br><span class="line">        | |  PT_SIZE_ON_STACK                        |</span><br><span class="line">        | |                                          |</span><br><span class="line">       -+-+- sp end                high address      |</span><br></pre></td></tr></table></figure>
<p> 如上是一个线程对应的内核栈和task_struct在内存中的存储格式。栈底在sp end的位置，<br> task_struct在sp end - 栈大小的位置，内核栈会分出PT_SIZE_ON_STACK的大小，用来保存<br> 中断异常时的寄存器上下文，PT_XX的宏就是这个区里的偏移。thread_info放在task_struct<br> 的一开始，用来暂存体系结构相关的一些数据。thread也在task_struct里，在__switch_to<br> 里使用，用TASK_THREAD_XX_XX表示相关的偏移。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>Linux内核</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>RISCV DTS描述分析</title>
    <url>/RISCV-DTS%E6%8F%8F%E8%BF%B0%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p> 我们使用如下的qemu命令可以直接dump virt平台的dts文件:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-riscv64 -machine virt,aclint=on -machine dumpdtb=rv_dtb.dtb \</span><br><span class="line">    -m 256M \</span><br><span class="line">    -smp 8 \</span><br><span class="line">    -numa node,nodeid=0,mem=128M,cpus=0-3 \</span><br><span class="line">    -numa node,nodeid=1,mem=128M,cpus=4-7 \</span><br><span class="line">    -kernel ~/repos/linux/arch/riscv/boot/Image \</span><br><span class="line">    -initrd ~/repos/buildroot/output/images/rootfs.cpio.gz</span><br><span class="line">dtc rv_dtb.dtb -I dtb -o rv.dts</span><br></pre></td></tr></table></figure>
<p> 我们可以用如上的numa配置观察在numa系统下生成的dts。riscv virt qemu支持不同的中断<br> 控制器配置，所有我们也可以把上面默认的plic中断控制器改成只用aplic、aplic-imsic，<br> 它们的配置分别是-machine virt,aia=aplic和-machine virt,aia=aplic-imsic，我们也<br> 把上面的aclint去掉，那么我们将用回clint。aclint也可以和aia的配置组合，比如可以用<br> -machine virt,aclint=on,aia=aplic-imsic表示同时使用aclint和aplic-imsic。</p>
<p> 我们生成一个最复杂的numa + aclint + aia的配置，相关的dts文件在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvMWI5Njk1MTYzZDk1ZTc0YzI0NDUzMGZiNGVjYTc3MWZhMjVkZGQwOC9ydl9kdHMvcnYuZHRzX3dpdGhfMl9udW1hX25vZGU=">这里<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="CPU相关dts节点"><a href="#CPU相关dts节点" class="headerlink" title="CPU相关dts节点"></a>CPU相关dts节点</h2><p> 可以看到每个CPU都有一个类似这样的描述节点:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cpus &#123;                                                                  </span><br><span class="line">        #address-cells = &lt;0x01&gt;;                                        </span><br><span class="line">        #size-cells = &lt;0x00&gt;;                                           </span><br><span class="line">        timebase-frequency = &lt;0x989680&gt;;                                </span><br><span class="line">                                                                        </span><br><span class="line">        cpu@0 &#123;                                                         </span><br><span class="line">                phandle = &lt;0x0f&gt;;                                       </span><br><span class="line">                numa-node-id = &lt;0x00&gt;;                                  </span><br><span class="line">                device_type = &quot;cpu&quot;;                                    </span><br><span class="line">                reg = &lt;0x00&gt;;                                           </span><br><span class="line">                status = &quot;okay&quot;;                                        </span><br><span class="line">                compatible = &quot;riscv&quot;;                                   </span><br><span class="line">                riscv,isa = &quot;rv64imafdch_zicsr_zifencei_zihintpause_zawrs_zba_zbb_zbc_zbs_smaia_ssaia_sstc&quot;;</span><br><span class="line">                mmu-type = &quot;riscv,sv48&quot;;                                </span><br><span class="line">                                                                        </span><br><span class="line">                interrupt-controller &#123;                                  </span><br><span class="line">                        #interrupt-cells = &lt;0x01&gt;;                      </span><br><span class="line">                        interrupt-controller;                           </span><br><span class="line">                        compatible = &quot;riscv,cpu-intc&quot;;                  </span><br><span class="line">                        phandle = &lt;0x10&gt;;                               </span><br><span class="line">                &#125;;                                                      </span><br><span class="line">        &#125;;                                                              </span><br></pre></td></tr></table></figure>
<p> reg表示CPU相关寄存器的基地址? riscv,isa描述CPU支持的特性。mmu-type描述MMU支持的<br> 特性。CPU里的这个interrupt-controller描述CPU core的本地”中断控制器”，在CPU core<br> 看来，像software interrupt、timer interrupt以及external interrupt这样的中断输入<br> 都可以中断CPU core当前的执行，这里软件抽象一个本地”中断控制器”出来。</p>
<p> CPU和numa相关的信息描述在cpu-map里，一个cluster表示一个numa节点，相关的CPU core<br> 写到cluster里。</p>
<p> 相关的内核代码分析:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start_kernel</span><br><span class="line">      /* arch/riscv/kernel/setup.c */</span><br><span class="line">  +-&gt; setup_arch</span><br><span class="line">  | +-&gt; parse_dtb</span><br><span class="line">  | | +-&gt; early_init_dt_scan</span><br><span class="line">  | |       /*</span><br><span class="line">  | |        * dts中memory节点的解析和添加memblock，这里会去掉0x80000000-0x80200000</span><br><span class="line">  | |        * 这段BIOS占用的内存。</span><br><span class="line">  | |        */</span><br><span class="line">  | |   +-&gt; early_init_dt_scan_memory</span><br><span class="line">  | |     +-&gt; early_init_dt_add_memory_arch</span><br><span class="line">  | |       +-&gt; memblock_add</span><br><span class="line">  | |</span><br><span class="line">  | |   /* 展开dtb的入口? */</span><br><span class="line">  | +-&gt; unflatten_device_tree</span><br><span class="line">  | |   /* 解析riscv.isa字符串 */</span><br><span class="line">  | +-&gt; riscv_fill_hwcap</span><br><span class="line">  |   /*</span><br><span class="line">  |    * riscv上在arch/riscv/kernel/smpboot.c, 这里是CPU的numa拓扑的解析代码的</span><br><span class="line">  |    * 入口。</span><br><span class="line">  |    *</span><br><span class="line">  |    * 如何和内核numa相关结构建立上联系？</span><br><span class="line">  |    */</span><br><span class="line">  +-&gt; smp_prepare_boot_cpu</span><br><span class="line">    +-&gt; init_cpu_topology</span><br><span class="line">      +-&gt; parse_dt_topology</span><br></pre></td></tr></table></figure>
<p> plic内核代码分析可以参考这里<a href="https://wangzhou.github.io/riscv-plic%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">这里</a>。</p>
<h2 id="中断相关dts节点"><a href="#中断相关dts节点" class="headerlink" title="中断相关dts节点"></a>中断相关dts节点</h2><p> 除了上面的本地”中断控制器”，riscv的平台上还可能有PLIC/AIA等核外中断控制器。</p>
<p> 在只有plic时，plic的dts节点大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plic@c000000 &#123;                                                  </span><br><span class="line">        phandle = &lt;0x12&gt;;                                       </span><br><span class="line">        numa-node-id = &lt;0x00&gt;;                                  </span><br><span class="line">        riscv,ndev = &lt;0x5f&gt;;                                    </span><br><span class="line">        reg = &lt;0x00 0xc000000 0x00 0x600000&gt;;                   </span><br><span class="line">        interrupts-extended = &lt;0x10 0x0b 0x10 0x09 0x0e 0x0b 0x0e 0x09 0x0c 0x0b 0x0c 0x09 0x0a 0x0b 0x0a 0x09&gt;;</span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        compatible = &quot;sifive,plic-1.0.0\0riscv,plic0&quot;;          </span><br><span class="line">        #address-cells = &lt;0x00&gt;;                                </span><br><span class="line">        #interrupt-cells = &lt;0x01&gt;;                              </span><br><span class="line">&#125;;                                                              </span><br><span class="line">                                                                </span><br><span class="line">plic@c600000 &#123;                                                  </span><br><span class="line">        phandle = &lt;0x11&gt;;                                       </span><br><span class="line">        numa-node-id = &lt;0x01&gt;;                                  </span><br><span class="line">        riscv,ndev = &lt;0x5f&gt;;                                    </span><br><span class="line">        reg = &lt;0x00 0xc600000 0x00 0x600000&gt;;                   </span><br><span class="line">        interrupts-extended = &lt;0x08 0x0b 0x08 0x09 0x06 0x0b 0x06 0x09 0x04 0x0b 0x04 0x09 0x02 0x0b 0x02 0x09&gt;;</span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        compatible = &quot;sifive,plic-1.0.0\0riscv,plic0&quot;;          </span><br><span class="line">        #address-cells = &lt;0x00&gt;;                                </span><br><span class="line">        #interrupt-cells = &lt;0x01&gt;;                              </span><br><span class="line">&#125;;                                                              </span><br></pre></td></tr></table></figure>
<p> 可见这样的情况下是一个numa节点配置一个plic，riscv.ndev是一个plic支持的输入中断<br> 个数，interrupts-extended描述plic和CPU的连接关系，具体可以这样看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">interrupts-extended = &lt;0x08 0x0b &lt;--- 0x08是local irq controller的phandle，</span><br><span class="line">                       0x08 0x09      后面是是接哪个中断的标号，0xb是M ext irq。</span><br><span class="line">                       0x06 0x0b      0x9是S ext irq。</span><br><span class="line">                       0x06 0x09</span><br><span class="line">                       0x04 0x0b</span><br><span class="line">                       0x04 0x09</span><br><span class="line">                       0x02 0x0b</span><br><span class="line">                       0x02 0x09&gt;;</span><br></pre></td></tr></table></figure>
<p> 可以看见interrupts-extended完整描述了plic的输出和CPU的关系，包括M mode和S mode<br> 的外部中断。这种情况的最大问题是, 在numa系统里会有多个plic，一个numa节点上的plic<br> 只能连接本numa节点上的CPU。plic和CPU的逻辑关系可以参考<a href="https://wangzhou.github.io/riscv-plic%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">这里</a>。</p>
<p> 如下是只有aplic的dts节点，这里只写了numa0上的节点：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aplic@d000000 &#123;                                                 </span><br><span class="line">        phandle = &lt;0x14&gt;;                                       </span><br><span class="line">        numa-node-id = &lt;0x00&gt;;                                  </span><br><span class="line">        riscv,num-sources = &lt;0x60&gt;;                             </span><br><span class="line">        reg = &lt;0x00 0xd000000 0x00 0x8000&gt;;                     </span><br><span class="line">        interrupts-extended = &lt;0x10 0x09 0x0e 0x09 0x0c 0x09 0x0a 0x09&gt;;</span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        #interrupt-cells = &lt;0x02&gt;;                              </span><br><span class="line">        compatible = &quot;riscv,aplic&quot;;                             </span><br><span class="line">&#125;;                                                              </span><br><span class="line">                                                                </span><br><span class="line">aplic@c000000 &#123;                                                 </span><br><span class="line">        phandle = &lt;0x13&gt;;                                       </span><br><span class="line">        numa-node-id = &lt;0x00&gt;;                                  </span><br><span class="line">        riscv,delegate = &lt;0x14 0x01 0x60&gt;;                      </span><br><span class="line">        riscv,children = &lt;0x14&gt;;                                </span><br><span class="line">        riscv,num-sources = &lt;0x60&gt;;                             </span><br><span class="line">        reg = &lt;0x00 0xc000000 0x00 0x8000&gt;;                     </span><br><span class="line">        interrupts-extended = &lt;0x10 0x0b 0x0e 0x0b 0x0c 0x0b 0x0a 0x0b&gt;;</span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        #interrupt-cells = &lt;0x02&gt;;                              </span><br><span class="line">        compatible = &quot;riscv,aplic&quot;;                             </span><br><span class="line">&#125;;                                                              </span><br></pre></td></tr></table></figure>
<p> 可以见到如上的aplic把M mode和S mode描述到了两个独立的节点上，分析原因？</p>
<p> 如下是aplic-imsic的dts节点：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aplic@d000000 &#123;                                                 </span><br><span class="line">        phandle = &lt;0x16&gt;;                                       </span><br><span class="line">        numa-node-id = &lt;0x00&gt;;                                  </span><br><span class="line">        riscv,num-sources = &lt;0x60&gt;;                             </span><br><span class="line">        reg = &lt;0x00 0xd000000 0x00 0x8000&gt;;                     </span><br><span class="line">        msi-parent = &lt;0x12&gt;;                                    </span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        #interrupt-cells = &lt;0x02&gt;;                              </span><br><span class="line">        compatible = &quot;riscv,aplic&quot;;                             </span><br><span class="line">&#125;;                                                              </span><br><span class="line">                                                                </span><br><span class="line">aplic@c000000 &#123;                                                 </span><br><span class="line">        phandle = &lt;0x15&gt;;                                       </span><br><span class="line">        numa-node-id = &lt;0x00&gt;;                                  </span><br><span class="line">        riscv,delegate = &lt;0x16 0x01 0x60&gt;;                      </span><br><span class="line">        riscv,children = &lt;0x16&gt;;                                </span><br><span class="line">        riscv,num-sources = &lt;0x60&gt;;                             </span><br><span class="line">        reg = &lt;0x00 0xc000000 0x00 0x8000&gt;;                     </span><br><span class="line">        msi-parent = &lt;0x11&gt;;                                    </span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        #interrupt-cells = &lt;0x02&gt;;                              </span><br><span class="line">        compatible = &quot;riscv,aplic&quot;;                             </span><br><span class="line">&#125;;                                                              </span><br><span class="line">                                                                </span><br><span class="line">aplic@d008000 &#123;                                                 </span><br><span class="line">        phandle = &lt;0x14&gt;;                                       </span><br><span class="line">        numa-node-id = &lt;0x01&gt;;                                  </span><br><span class="line">        riscv,num-sources = &lt;0x60&gt;;                             </span><br><span class="line">        reg = &lt;0x00 0xd008000 0x00 0x8000&gt;;                     </span><br><span class="line">        msi-parent = &lt;0x12&gt;;                                    </span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        #interrupt-cells = &lt;0x02&gt;;                              </span><br><span class="line">        compatible = &quot;riscv,aplic&quot;;                             </span><br><span class="line">&#125;;                                                              </span><br><span class="line">                                                                </span><br><span class="line">aplic@c008000 &#123;                                                 </span><br><span class="line">        phandle = &lt;0x13&gt;;                                       </span><br><span class="line">        numa-node-id = &lt;0x01&gt;;                                  </span><br><span class="line">        riscv,delegate = &lt;0x14 0x01 0x60&gt;;                      </span><br><span class="line">        riscv,children = &lt;0x14&gt;;                                </span><br><span class="line">        riscv,num-sources = &lt;0x60&gt;;                             </span><br><span class="line">        reg = &lt;0x00 0xc008000 0x00 0x8000&gt;;                     </span><br><span class="line">        msi-parent = &lt;0x11&gt;;                                    </span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        #interrupt-cells = &lt;0x02&gt;;                              </span><br><span class="line">        compatible = &quot;riscv,aplic&quot;;                             </span><br><span class="line">&#125;;                                                              </span><br><span class="line"></span><br><span class="line">imsics@28000000 &#123;                                               </span><br><span class="line">        phandle = &lt;0x12&gt;;                                       </span><br><span class="line">        riscv,group-index-shift = &lt;0x18&gt;;                       </span><br><span class="line">        riscv,group-index-bits = &lt;0x01&gt;;                        </span><br><span class="line">        riscv,hart-index-bits = &lt;0x02&gt;;                         </span><br><span class="line">        riscv,num-ids = &lt;0xff&gt;;                                 </span><br><span class="line">        reg = &lt;0x00 0x28000000 0x00 0x4000 0x00 0x29000000 0x00 0x4000&gt;;</span><br><span class="line">        interrupts-extended = &lt;0x10 0x09 0x0e 0x09 0x0c 0x09 0x0a 0x09 0x08 0x09 0x06 0x09 0x04 0x09 0x02 0x09&gt;;</span><br><span class="line">        msi-controller;                                         </span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        #interrupt-cells = &lt;0x00&gt;;                              </span><br><span class="line">        compatible = &quot;riscv,imsics&quot;;                            </span><br><span class="line">&#125;;                                                              </span><br><span class="line">                                                                </span><br><span class="line">imsics@24000000 &#123;                                               </span><br><span class="line">        phandle = &lt;0x11&gt;;                                       </span><br><span class="line">        riscv,group-index-shift = &lt;0x18&gt;;                       </span><br><span class="line">        riscv,group-index-bits = &lt;0x01&gt;;                        </span><br><span class="line">        riscv,hart-index-bits = &lt;0x02&gt;;                         </span><br><span class="line">        riscv,num-ids = &lt;0xff&gt;;                                 </span><br><span class="line">        reg = &lt;0x00 0x24000000 0x00 0x4000 0x00 0x25000000 0x00 0x4000&gt;;</span><br><span class="line">        interrupts-extended = &lt;0x10 0x0b 0x0e 0x0b 0x0c 0x0b 0x0a 0x0b 0x08 0x0b 0x06 0x0b 0x04 0x0b 0x02 0x0b&gt;;</span><br><span class="line">        msi-controller;                                         </span><br><span class="line">        interrupt-controller;                                   </span><br><span class="line">        #interrupt-cells = &lt;0x00&gt;;                              </span><br><span class="line">        compatible = &quot;riscv,imsics&quot;;                            </span><br><span class="line">&#125;;                                                              </span><br></pre></td></tr></table></figure>
<p> 上面列出了numa系统中完整的aplic/imsic的dts节点，有imsic的情况下，aplic接到了imsic<br> 上，除此之外，aplic的dts没有变化。系统中新加入两个imsics dts节点，注意这两个imsics<br> 节点并不是按照numa节点划分，而是按照CPU mode划分，S mode的aplic接入第一个imsics,<br> M mode的plic接入第二个imsics，每个imsics分别接入每个CPU core的对应mode的外部中断。</p>
<p> (todo: aplic的输出怎么转化成一个MSI给imsics的?)</p>
<h2 id="timer相关dts节点"><a href="#timer相关dts节点" class="headerlink" title="timer相关dts节点"></a>timer相关dts节点</h2><p> 每个numa节点一个mtimer。stimer只是增加了相关的CSR寄存器，并没有抽象出一个timer<br> 设备，所以dts并没有stimer的节点。其实，从下面的内核代码可以看出，内核是不需要这<br> 里mtimer的节点信息的(todo: 原因分析)。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mtimer@2000000 &#123;                                                </span><br><span class="line">        numa-node-id = &lt;0x00&gt;;                                  </span><br><span class="line">        interrupts-extended = &lt;0x10 0x07 0x0e 0x07 0x0c 0x07 0x0a 0x07&gt;;</span><br><span class="line">        reg = &lt;0x00 0x2007ff8 0x00 0x08 0x00 0x2000000 0x00 0x7ff8&gt;;</span><br><span class="line">        compatible = &quot;riscv,aclint-mtimer&quot;;                     </span><br><span class="line">&#125;;                                                              </span><br><span class="line">                                                                </span><br><span class="line">mtimer@2008000 &#123;                                                </span><br><span class="line">        numa-node-id = &lt;0x01&gt;;                                  </span><br><span class="line">        interrupts-extended = &lt;0x08 0x07 0x06 0x07 0x04 0x07 0x02 0x07&gt;;</span><br><span class="line">        reg = &lt;0x00 0x200fff8 0x00 0x08 0x00 0x2008000 0x00 0x7ff8&gt;;</span><br><span class="line">        compatible = &quot;riscv,aclint-mtimer&quot;;                     </span><br><span class="line">&#125;;                                                              </span><br></pre></td></tr></table></figure>
<p> timer的内核代码分析可以参考<a href="https://wangzhou.github.io/riscv-timer%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/">这里</a>。</p>
<h2 id="PCIe相关dts节点"><a href="#PCIe相关dts节点" class="headerlink" title="PCIe相关dts节点"></a>PCIe相关dts节点</h2><p> 如下是PCIe控制器的dts节点，我们整理了下interrupt-map和ranges的输出格式，这样看起<br> 来更清楚一点：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci@30000000 &#123;                                                  </span><br><span class="line">        interrupt-map-mask = &lt;0x1800 0x00 0x00 0x07&gt;;           </span><br><span class="line">        interrupt-map = &lt;0x00 0x00 0x00 0x01 0x14 0x20 0x04     </span><br><span class="line">                         0x00 0x00 0x00 0x02 0x14 0x21 0x04     </span><br><span class="line">                         0x00 0x00 0x00 0x03 0x14 0x22 0x04     </span><br><span class="line">                         0x00 0x00 0x00 0x04 0x14 0x23 0x04     </span><br><span class="line">                         0x800 0x00 0x00 0x01 0x14 0x21 0x04    </span><br><span class="line">                         0x800 0x00 0x00 0x02 0x14 0x22 0x04    </span><br><span class="line">                         0x800 0x00 0x00 0x03 0x14 0x23 0x04    </span><br><span class="line">                         0x800 0x00 0x00 0x04 0x14 0x20 0x04    </span><br><span class="line">                         0x1000 0x00 0x00 0x01 0x14 0x22 0x04   </span><br><span class="line">                         0x1000 0x00 0x00 0x02 0x14 0x23 0x04   </span><br><span class="line">                         0x1000 0x00 0x00 0x03 0x14 0x20 0x04   </span><br><span class="line">                         0x1000 0x00 0x00 0x04 0x14 0x21 0x04   </span><br><span class="line">                         0x1800 0x00 0x00 0x01 0x14 0x23 0x04   </span><br><span class="line">                         0x1800 0x00 0x00 0x02 0x14 0x20 0x04   </span><br><span class="line">                         0x1800 0x00 0x00 0x03 0x14 0x21 0x04   </span><br><span class="line">                         0x1800 0x00 0x00 0x04 0x14 0x22 0x04&gt;; </span><br><span class="line">        ranges = &lt;0x1000000 0x00 0x00</span><br><span class="line">		  0x00 0x3000000 0x00 0x10000        &lt;--- IO空间</span><br><span class="line">		  0x2000000</span><br><span class="line">		  0x00 0x40000000</span><br><span class="line">		  0x00 0x40000000 0x00 0x40000000    &lt;--- MEM空间</span><br><span class="line">		  0x3000000</span><br><span class="line">		  0x04 0x00</span><br><span class="line">		  0x04 0x00 0x04 0x00&gt;;</span><br><span class="line">        reg = &lt;0x00 0x30000000 0x00 0x10000000&gt;;     &lt;--- ECAM空间</span><br><span class="line">        msi-parent = &lt;0x12&gt;;                                    </span><br><span class="line">        dma-coherent;                                           </span><br><span class="line">        bus-range = &lt;0x00 0xff&gt;;                                </span><br><span class="line">        linux,pci-domain = &lt;0x00&gt;;                              </span><br><span class="line">        device_type = &quot;pci&quot;;                                    </span><br><span class="line">        compatible = &quot;pci-host-ecam-generic&quot;;                   </span><br><span class="line">        #size-cells = &lt;0x02&gt;;                                   </span><br><span class="line">        #interrupt-cells = &lt;0x01&gt;;                              </span><br><span class="line">        #address-cells = &lt;0x03&gt;;                                </span><br><span class="line">&#125;;                                                              </span><br></pre></td></tr></table></figure>
<p> interrupt-map/interrupt-map-map是PCIe控制器INTx中断的配置，ranges是MEM/IO窗口的<br> 配置，reg是ECAM空间的配置。</p>
]]></content>
      <tags>
        <tag>riscv</tag>
        <tag>dts</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu monitor介绍</title>
    <url>/qemu-monitor%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h2 id="直观使用"><a href="#直观使用" class="headerlink" title="直观使用"></a>直观使用</h2><p> 启动qemu后，ctrl + a + c可以进入monitor的界面，再次ctrl + a + c可以从monitor里退出，<br> 输入help可以列出monitor里可以使用的命令。我们把部分命令的含义直接用注释的形式写到下面，<br> 后面用到再持续补充进来吧。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># QEMU 6.2.0 monitor - type &#x27;help&#x27; for more information</span><br><span class="line">(qemu) help</span><br><span class="line">announce_self [interfaces] [id] -- Trigger GARP/RARP announcements</span><br><span class="line">balloon target -- request VM to change its memory allocation (in MB)</span><br><span class="line">block_job_cancel [-f] device -- stop an active background block operation (use -f</span><br><span class="line">                         if you want to abort the operation immediately</span><br><span class="line">                         instead of keep running until data is in sync)</span><br><span class="line">block_job_complete device -- stop an active background block operation</span><br><span class="line">block_job_pause device -- pause an active background block operation</span><br><span class="line">block_job_resume device -- resume a paused background block operation</span><br><span class="line">block_job_set_speed device speed -- set maximum speed for a background block operation</span><br><span class="line">block_resize device size -- resize a block image</span><br><span class="line">block_set_io_throttle device bps bps_rd bps_wr iops iops_rd iops_wr -- change I/O throttle limits for a block drive</span><br><span class="line">block_stream device [speed [base]] -- copy data from a backing file into a block device</span><br><span class="line">boot_set bootdevice -- define new values for the boot device list</span><br><span class="line">calc_dirty_rate [-r] [-b] second [sample_pages_per_GB] -- start a round of guest dirty rate measurement (using -r to</span><br><span class="line">                         specify dirty ring as the method of calculation and</span><br><span class="line">                         -b to specify dirty bitmap as method of calculation)</span><br><span class="line">change device filename [format [read-only-mode]] -- change a removable medium, optional format</span><br><span class="line">chardev-add args -- add chardev</span><br><span class="line">chardev-change id args -- change chardev</span><br><span class="line">chardev-remove id -- remove chardev</span><br><span class="line">chardev-send-break id -- send a break on chardev</span><br><span class="line">client_migrate_info protocol hostname port tls-port cert-subject -- set migration information for remote display</span><br><span class="line">closefd closefd name -- close a file descriptor previously passed via SCM rights</span><br><span class="line">commit device|all -- commit changes to the disk images (if -snapshot is used) or backing files</span><br><span class="line">cont|c  -- resume emulation</span><br><span class="line">cpu index -- set the default CPU</span><br><span class="line">delvm tag -- delete a VM snapshot from its tag</span><br><span class="line">device_add driver[,prop=value][,...] -- add device, like -device on the command line</span><br><span class="line">device_del device -- remove device</span><br><span class="line">drive_add [-n] [[&lt;domain&gt;:]&lt;bus&gt;:]&lt;slot&gt;</span><br><span class="line">[file=file][,if=type][,bus=n]</span><br><span class="line">[,unit=m][,media=d][,index=i]</span><br><span class="line">[,snapshot=on|off][,cache=on|off]</span><br><span class="line">[,readonly=on|off][,copy-on-read=on|off] -- add drive to PCI storage controller</span><br><span class="line">drive_backup [-n] [-f] [-c] device target [format] -- initiates a point-in-time</span><br><span class="line">                        copy for a device. The device&#x27;s contents are</span><br><span class="line">                        copied to the new image file, excluding data that</span><br><span class="line">                        is written after the command is started.</span><br><span class="line">                        The -n flag requests QEMU to reuse the image found</span><br><span class="line">                        in new-image-file, instead of recreating it from scratch.</span><br><span class="line">                        The -f flag requests QEMU to copy the whole disk,</span><br><span class="line">                        so that the result does not need a backing file.</span><br><span class="line">                        The -c flag requests QEMU to compress backup data</span><br><span class="line">                        (if the target format supports it).</span><br><span class="line"></span><br><span class="line">drive_del device -- remove host block device</span><br><span class="line">drive_mirror [-n] [-f] device target [format] -- initiates live storage</span><br><span class="line">                        migration for a device. The device&#x27;s contents are</span><br><span class="line">                        copied to the new image file, including data that</span><br><span class="line">                        is written after the command is started.</span><br><span class="line">                        The -n flag requests QEMU to reuse the image found</span><br><span class="line">                        in new-image-file, instead of recreating it from scratch.</span><br><span class="line">                        The -f flag requests QEMU to copy the whole disk,</span><br><span class="line">                        so that the result does not need a backing file.</span><br><span class="line"></span><br><span class="line">dump-guest-memory [-p] [-d] [-z|-l|-s|-w] filename [begin length] -- dump guest memory into file &#x27;filename&#x27;.</span><br><span class="line">                        -p: do paging to get guest&#x27;s memory mapping.</span><br><span class="line">                        -d: return immediately (do not wait for completion).</span><br><span class="line">                        -z: dump in kdump-compressed format, with zlib compression.</span><br><span class="line">                        -l: dump in kdump-compressed format, with lzo compression.</span><br><span class="line">                        -s: dump in kdump-compressed format, with snappy compression.</span><br><span class="line">                        -w: dump in Windows crashdump format (can be used instead of ELF-dump converting),</span><br><span class="line">                            for Windows x64 guests with vmcoreinfo driver only.</span><br><span class="line">                        begin: the starting physical address.</span><br><span class="line">                        length: the memory size, in bytes.</span><br><span class="line">eject [-f] device -- eject a removable medium (use -f to force it)</span><br><span class="line">exit_preconfig  -- exit the preconfig state</span><br><span class="line">expire_password protocol time -- set spice/vnc password expire-time</span><br><span class="line">gdbserver [device] -- start gdbserver on given device (default &#x27;tcp::1234&#x27;), stop with &#x27;none&#x27;</span><br><span class="line">getfd getfd name -- receive a file descriptor via SCM rights and assign it a name</span><br><span class="line">gpa2hpa addr -- print the host physical address corresponding to a guest physical address</span><br><span class="line">gpa2hva addr -- print the host virtual address corresponding to a guest physical address</span><br><span class="line">gva2gpa addr -- print the guest physical address corresponding to a guest virtual address</span><br><span class="line">help|? [cmd] -- show the help</span><br><span class="line">hostfwd_add [netdev_id] [tcp|udp]:[hostaddr]:hostport-[guestaddr]:guestport -- redirect TCP or UDP connections from host to guest (requires -net user)</span><br><span class="line">hostfwd_remove [netdev_id] [tcp|udp]:[hostaddr]:hostport -- remove host-to-guest TCP or UDP redirection</span><br><span class="line">i /fmt addr -- I/O port read</span><br><span class="line">info [subcommand] -- show various information about the system state</span><br><span class="line">loadvm tag -- restore a VM snapshot from its tag</span><br><span class="line">log item1[,...] -- activate logging of the specified items              &lt;--- 似乎可以动态的触发某种打印log</span><br><span class="line">logfile filename -- output logs to &#x27;filename&#x27;</span><br><span class="line">memsave addr size file -- save to disk virtual memory dump starting at &#x27;addr&#x27; of size &#x27;size&#x27;</span><br><span class="line">migrate [-d] [-b] [-i] [-r] uri -- migrate to URI (using -d to not wait for completion)</span><br><span class="line">                         -b for migration without shared storage with full copy of disk</span><br><span class="line">                         -i for migration without shared storage with incremental copy of disk (base image shared between src and destination)</span><br><span class="line">                         -r to resume a paused migration</span><br><span class="line">migrate_cancel  -- cancel the current VM migration</span><br><span class="line">migrate_continue state -- Continue migration from the given paused state</span><br><span class="line">migrate_incoming uri -- Continue an incoming migration from an -incoming defer</span><br><span class="line">migrate_pause  -- Pause an ongoing migration (postcopy-only)</span><br><span class="line">migrate_recover uri -- Continue a paused incoming postcopy migration</span><br><span class="line">migrate_set_capability capability state -- Enable/Disable the usage of a capability for migration</span><br><span class="line">migrate_set_parameter parameter value -- Set the parameter for migration</span><br><span class="line">migrate_start_postcopy  -- Followup to a migration command to switch the migration to postcopy mode. The postcopy-ram capability must be set on both source an</span><br><span class="line">d destination before the original migration command .</span><br><span class="line">mouse_button state -- change mouse button state (1=L, 2=M, 4=R)</span><br><span class="line">mouse_move dx dy [dz] -- send mouse move events</span><br><span class="line">mouse_set index -- set which mouse device receives events</span><br><span class="line">nbd_server_add nbd_server_add [-w] device [name] -- export a block device via NBD</span><br><span class="line">nbd_server_remove nbd_server_remove [-f] name -- remove an export previously exposed via NBD</span><br><span class="line">nbd_server_start nbd_server_start [-a] [-w] host:port -- serve block devices on the given host and port</span><br><span class="line">nbd_server_stop nbd_server_stop -- stop serving block devices using the NBD protocol</span><br><span class="line">netdev_add [user|tap|socket|vde|bridge|hubport|netmap|vhost-user],id=str[,prop=value][,...] -- add host network device</span><br><span class="line">netdev_del id -- remove host network device</span><br><span class="line">nmi  -- inject an NMI</span><br><span class="line">o /fmt addr value -- I/O port write</span><br><span class="line">object_add [qom-type=]type,id=str[,prop=value][,...] -- create QOM object</span><br><span class="line">object_del id -- destroy QOM object</span><br><span class="line">pcie_aer_inject_error [-a] [-c] id &lt;error_status&gt; [&lt;tlp header&gt; [&lt;tlp header prefix&gt;]] -- inject pcie aer error</span><br><span class="line">                         -a for advisory non fatal error</span><br><span class="line">                         -c for correctable error</span><br><span class="line">                        &lt;id&gt; = qdev device id</span><br><span class="line">                        &lt;error_status&gt; = error string or 32bit</span><br><span class="line">                        &lt;tlp header&gt; = 32bit x 4</span><br><span class="line">                        &lt;tlp header prefix&gt; = 32bit x 4</span><br><span class="line">pmemsave addr size file -- save to disk physical memory dump starting at &#x27;addr&#x27; of size &#x27;size&#x27;</span><br><span class="line">print|p /fmt expr -- print expression value (use $reg for CPU register access)</span><br><span class="line">qemu-io [-d] [device] &quot;[command]&quot; -- run a qemu-io command on a block device</span><br><span class="line">                        -d: [device] is a device ID rather than a drive ID or node name</span><br><span class="line">qom-get path property -- print QOM property</span><br><span class="line">qom-list path -- list QOM properties</span><br><span class="line">qom-set [-j] path property value -- set QOM property.</span><br><span class="line">                        -j: the value is specified in json format.</span><br><span class="line">quit|q  -- quit the emulator</span><br><span class="line">replay_break icount -- set breakpoint at the specified instruction count</span><br><span class="line">replay_delete_break  -- remove replay breakpoint</span><br><span class="line">replay_seek icount -- replay execution to the specified instruction count</span><br><span class="line">ringbuf_read device size -- Read from a ring buffer character device</span><br><span class="line">ringbuf_write device data -- Write to a ring buffer character device</span><br><span class="line">savevm tag -- save a VM snapshot. If no tag is provided, a new snapshot is created</span><br><span class="line">screendump filename [device [head]] -- save screen from head &#x27;head&#x27; of display device &#x27;device&#x27; into PPM image &#x27;filename&#x27;</span><br><span class="line">sendkey keys [hold_ms] -- send keys to the VM (e.g. &#x27;sendkey ctrl-alt-f1&#x27;, default hold time=100 ms)</span><br><span class="line">set_link name on|off -- change the link status of a network adapter</span><br><span class="line">set_password protocol password action-if-connected -- set spice/vnc password</span><br><span class="line">singlestep [on|off] -- run emulation in singlestep mode or switch to normal mode      &lt;--- 似乎还可以动态的进入和退出singlestep?</span><br><span class="line">snapshot_blkdev [-n] device [new-image-file] [format] -- initiates a live snapshot</span><br><span class="line">                        of device. If a new image file is specified, the</span><br><span class="line">                        new image file will become the new root image.</span><br><span class="line">                        If format is specified, the snapshot file will</span><br><span class="line">                        be created in that format.</span><br><span class="line">                        The default format is qcow2.  The -n flag requests QEMU</span><br><span class="line">                        to reuse the image found in new-image-file, instead of</span><br><span class="line">                        recreating it from scratch.</span><br><span class="line">snapshot_blkdev_internal device name -- take an internal snapshot of device.</span><br><span class="line">                        The format of the image used by device must</span><br><span class="line">                        support it, such as qcow2.</span><br><span class="line"></span><br><span class="line">snapshot_delete_blkdev_internal device name [id] -- delete an internal snapshot of device.</span><br><span class="line">                        If id is specified, qemu will try delete</span><br><span class="line">                        the snapshot matching both id and name.</span><br><span class="line">                        The format of the image used by device must</span><br><span class="line">                        support it, such as qcow2.</span><br><span class="line"></span><br><span class="line">stopcapture capture index -- stop capture</span><br><span class="line">stop|s  -- stop emulation</span><br><span class="line">sum addr size -- compute the checksum of a memory region</span><br><span class="line">sync-profile [on|off|reset] -- enable, disable or reset synchronization profiling. With no arguments, prints whether profiling is on or off.</span><br><span class="line">system_powerdown  -- send system power down event</span><br><span class="line">system_reset  -- reset the system</span><br><span class="line">system_wakeup  -- wakeup guest from suspend</span><br><span class="line">trace-event name on|off [vcpu] -- changes status of a specific trace event (vcpu: vCPU to set, default is all)</span><br><span class="line">watchdog_action [reset|shutdown|poweroff|pause|debug|none] -- change watchdog action</span><br><span class="line">wavcapture path audiodev [frequency [bits [channels]]] -- capture audio to a wave file (default frequency=44100 bits=16 channels=2)</span><br><span class="line">x /fmt addr -- virtual memory dump starting at &#x27;addr&#x27;          &lt;--- 打印虚拟地址上的数据</span><br><span class="line">x_colo_lost_heartbeat  -- Tell COLO that heartbeat is lost,</span><br><span class="line">                        a failover or takeover is needed.</span><br><span class="line">xp /fmt addr -- physical memory dump starting at &#x27;addr&#x27;        &lt;--- 打印物理地址上的数据</span><br></pre></td></tr></table></figure>
<p> 其中info命令还有自命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(qemu) info</span><br><span class="line">info balloon  -- show balloon information</span><br><span class="line">info block [-n] [-v] [device] -- show info of one block device or all block devices (-n: show named nodes; -v: show details)</span><br><span class="line">info block-jobs  -- show progress of ongoing block device operations</span><br><span class="line">info blockstats  -- show block device statistics</span><br><span class="line">info capture  -- show capture information</span><br><span class="line">info chardev  -- show the character devices</span><br><span class="line">info cpus  -- show infos for each CPU                          &lt;--- 打印vCPU的线程id</span><br><span class="line">info dirty_rate  -- show dirty rate information</span><br><span class="line">info dump  -- Display the latest dump status</span><br><span class="line">info history  -- show the command line history</span><br><span class="line">info hotpluggable-cpus  -- Show information about hotpluggable CPUs</span><br><span class="line">info iothreads  -- show iothreads</span><br><span class="line">info irq  -- show the interrupts statistics (if available)</span><br><span class="line">info jit  -- show dynamic compiler info                        &lt;--- 可以查看tcg的相关信息，比如平均TB的平均大小</span><br><span class="line">info kvm  -- show KVM information</span><br><span class="line">info mem  -- show the active virtual memory mappings           &lt;--- dump出当前guest系统中所有虚拟地址到物理地址的映射(dump时刻的进程?)</span><br><span class="line">info memdev  -- show memory backends</span><br><span class="line">info memory-devices  -- show memory devices</span><br><span class="line">info memory_size_summary  -- show the amount of initially allocated and present hotpluggable (if enabled) memory in bytes.</span><br><span class="line">info mice  -- show which guest mouse is receiving events</span><br><span class="line">info migrate  -- show migration status</span><br><span class="line">info migrate_capabilities  -- show current migration capabilities</span><br><span class="line">info migrate_parameters  -- show current migration parameters</span><br><span class="line">info mtree [-f][-d][-o][-D] -- show memory tree (-f: dump flat view for address spaces;-d: dump dispatch tree, valid with -f only);-o: dump region owners/pare</span><br><span class="line">nts;-D: dump disabled regions</span><br><span class="line">info name  -- show the current VM name</span><br><span class="line">info network  -- show the network state</span><br><span class="line">info numa  -- show NUMA information</span><br><span class="line">info opcount  -- show dynamic compiler opcode counters</span><br><span class="line">info pci  -- show PCI info</span><br><span class="line">info pic  -- show PIC state</span><br><span class="line">info profile  -- show profiling information</span><br><span class="line">info qdm  -- show qdev device model list</span><br><span class="line">info qom-tree [path] -- show QOM composition tree</span><br><span class="line">info qtree  -- show device tree</span><br><span class="line">info ramblock  -- Display system ramblock information</span><br><span class="line">info rdma  -- show RDMA state</span><br><span class="line">info registers [-a] -- show the cpu registers (-a: all - show register info for all cpus)     &lt;--- 打印CPU的寄存器值，多核的时候可以加上-a</span><br><span class="line">info replay  -- show record/replay information</span><br><span class="line">info rocker name -- Show rocker switch</span><br><span class="line">info rocker-of-dpa-flows name [tbl_id] -- Show rocker OF-DPA flow tables</span><br><span class="line">info rocker-of-dpa-groups name [type] -- Show rocker OF-DPA groups</span><br><span class="line">info rocker-ports name -- Show rocker ports</span><br><span class="line">info roms  -- show roms</span><br><span class="line">info snapshots  -- show the currently saved VM snapshots</span><br><span class="line">info status  -- show the current VM status (running|paused)</span><br><span class="line">info sync-profile [-m] [-n] [max] -- show synchronization profiling info, up to max entries (default: 10), sorted by total wait time. (-m: sort by mean wait t</span><br><span class="line">ime; -n: do not coalesce objects with the same call site)</span><br><span class="line">info tpm  -- show the TPM device</span><br><span class="line">info trace-events [name] [vcpu] -- show available trace-events &amp; their state (name: event name pattern; vcpu: vCPU to query, default is any)</span><br><span class="line">info usb  -- show guest USB devices</span><br><span class="line">info usbhost  -- show host USB devices</span><br><span class="line">info usernet  -- show user network stack connection states</span><br><span class="line">info uuid  -- show the current VM UUID</span><br><span class="line">info version  -- show the version of QEMU</span><br><span class="line">info vm-generation-id  -- Show Virtual Machine Generation ID</span><br><span class="line">info vnc  -- show the vnc server status</span><br></pre></td></tr></table></figure>
<p> 需要注意的时候，运行如上命令的时候，qemu上的guest系统还处于活动的状态，可以使用<br> stop|s暂停模拟，用cont｜c继续模拟。</p>
<h2 id="增加选项"><a href="#增加选项" class="headerlink" title="增加选项"></a>增加选项</h2><p> qemu的源码目录下有一个怎么增加monitor命令的指导：qemu/docs/devel/writing-monitor-commands.rst</p>
<h2 id="QMP分析和使用"><a href="#QMP分析和使用" class="headerlink" title="QMP分析和使用"></a>QMP分析和使用</h2><p> qemu monitor支持两种模式的对外交互，一种是方便人理解的文本方式，一种是方便代码处理<br> 的json的格式，我们叫后者QMP，叫前者HMP，qemu的官方文档说，代码演进的方向是，HMP<br> 底层都用QMP实现。</p>
<p> QMP可以直接使用，用户需要使用json格式和qemu monitor交互，基于QMP的工具有virsh/libvirt。</p>
<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p> ctrl + a + c可以进入monitor HMP的界面，这里对应的代码逻辑是怎么样的。要分析清楚<br> 这个就先要对qemu的线程模型有一定的了解。qemu为每个vCPU创建一个线程，qemu在主线程<br> 里处理IO，qemu还为其他的业务起了相关线程，比如，为虚拟机热迁移起了单独的线程。</p>
<p> qemu主线程里使用poll fd的方式监控IO，具体编码实现上qemu使用了glib库里提供的事件处理<br> 方式。所以，我们要找见monitor对应的fd是在哪里插入到qemu的事件监控里的。</p>
<p> qemu启动使用-nographic时，monitor HMP会使用标准输入输出作为用户界面的输入输出，<br> 具体上，把monitor fd插入到qemu事件监控里的代码路径如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">main</span><br><span class="line">  +-&gt; qemu_init</span><br><span class="line">    +-&gt; qemu_create_late_backends</span><br><span class="line">      +-&gt; foreach_device_config</span><br><span class="line">        +-&gt; serial_parse</span><br><span class="line">          +-&gt; qemu_chr_new_mux_mon</span><br><span class="line">            +-&gt; qemu_chr_new_permit_mux_mon</span><br><span class="line">              +-&gt; qemu_chr_new_noreplay</span><br><span class="line">         +-&gt; monitor_init_hmp</span><br><span class="line">	   +-&gt; qemu_char_fe_set_handlers  &lt;--- 这里所谓的前端似乎只是抽象了一个统一的后端配置入口</span><br><span class="line">	     +-&gt; qemu_char_fe_set_handlers_full</span><br><span class="line">	       +-&gt; mux_chr_update_read_handlers</span><br><span class="line">	         +-&gt; qemu_char_fe_set_handlers_full</span><br><span class="line">	           +-&gt; fd_chr_update_read_handlers</span><br><span class="line">		     +-&gt; g_source_attach  &lt;--- 最后在这里加入到事件监控</span><br></pre></td></tr></table></figure>

<p> 在qemu代码里的hmp_info_kvm的函数上打断点，使用gdb运行qemu，在断点处打印调用栈如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">main</span><br><span class="line">  +-&gt; qemu_main_loop</span><br><span class="line">    +-&gt; main_loop_wait</span><br><span class="line">      +-&gt; os_host_main_loop_wait</span><br><span class="line">        +-&gt; glib_pollfds_poll</span><br><span class="line">          +-&gt; g_main_context_dispatch</span><br><span class="line">            +-&gt; fd_chr_read</span><br><span class="line">       +-&gt; mux_chr_read</span><br><span class="line">         +-&gt; monitor_read</span><br><span class="line">           +-&gt; readline_handle_byte</span><br><span class="line">             +-&gt; monitor_command_cb</span><br><span class="line">               +-&gt; handle_hmp_command</span><br><span class="line">                 +-&gt; handle_hmp_command_exec</span><br><span class="line">                   +-&gt; hmp_info_kvm</span><br></pre></td></tr></table></figure>
<p> 可以看出当monitor上有输入的时候，qemu主线程会poll到相关fd，然后以来glib的事件处理<br> 模型做事件分发处理，然后一路调用下来。</p>
<p>Note: chardev目录下的abstract class有chardev(char.c)、chardev-fd(char-fd.c)</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>软件调试</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu plugin基本逻辑分析</title>
    <url>/qemu-plugin%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="qemu-plugin基本概念以及使用"><a href="#qemu-plugin基本概念以及使用" class="headerlink" title="qemu plugin基本概念以及使用"></a>qemu plugin基本概念以及使用</h2><p>qemu tcg支持用插件的方式为qemu新增功能，qemu源码里作为示例自带了几个插件，我们先<br>编译使用下qemu自带的cache插件。</p>
<p>一般情况下qemu tcg是不模拟cache的，qemu自带了一个简单模拟cache的插件。我们在配置<br>qemu的时候要带上–enable-plugins，编译完qemu后，进入qemu/build/contrib/plugins/,<br>在这个目录下运行make，之后可以看见qemu的自带的plugin都编译出了，其中就有cache的<br>plugin: libcache.so。</p>
<p>按如下运行命令，可以带着cache plugin运行qemu，-d plugin输出plugin里的打印信息，<br>-D指定打印信息输出的文件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-riscv64 -m 256m -nographic -machine virt \</span><br><span class="line">-kernel ~/repos/linux/arch/riscv/boot/Image \</span><br><span class="line">-append &quot;console=ttyS0 root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">-initrd ~/repos/buildroot/output/images/rootfs.cpio.gz \</span><br><span class="line">-plugin ~/repos/qemu/build/contrib/plugins/libcache.so \</span><br><span class="line">-d plugin -D ~/qemu_cache_plugin_log</span><br></pre></td></tr></table></figure>
<p>如上运行qemu后，linux内核启动从1s到了30s，可见qemu带plugin运行的速度是很慢的。</p>
<h2 id="qemu-plugin机制分析"><a href="#qemu-plugin机制分析" class="headerlink" title="qemu plugin机制分析"></a>qemu plugin机制分析</h2><p>qemu要实现plugin就必然向外，也就是向plugin提供一组API接口，plugin使用这组API向qemu<br>注册以及获取qemu模拟的guest的信息。qemu内部为了支持plugin机制也会增加plugin的核心<br>实现代码。</p>
<p>首先我们从qemu user mode入手看下qemu是如何解析命令行里输入的plugin so的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">main</span><br><span class="line">  +-&gt; handle_arg_plugin(const char *arg)</span><br><span class="line">    +-&gt; qemu_plugin_opt_parse(const char *optarg, QemuPluginList *head)</span><br></pre></td></tr></table></figure>
<p>可以看见qemu user mode会把解析到的每个plugin的信息放到qemu_plugin_desc，再把所有<br>的qemu_plugin_desc保存到一个叫plugins的全局链表里。</p>
<p>main函数里随后会使用qemu_plugin_load_list加载plugin so:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu_plugin_load_list()</span><br><span class="line">  +-&gt; plugin_load()</span><br><span class="line">    +-&gt; ctx-&gt;handle = g_module_open(desc-&gt;path, G_MODULE_BIND_LOCAL);</span><br><span class="line">    +-&gt; g_module_symbol(ctx-&gt;handle, &quot;qemu_plugin_install&quot;, &amp;sym));</span><br><span class="line">        install = (qemu_plugin_install_func_t) sym;                                 </span><br><span class="line">    +-&gt; g_hash_table_lookup(plugin.id_ht, &amp;ctx-&gt;id);</span><br><span class="line">    +-&gt; QTAILQ_INSERT_TAIL(&amp;plugin.ctxs, ctx, entry);</span><br><span class="line">    +-&gt; install(ctx-&gt;id, info, desc-&gt;argc, desc-&gt;argv);                        </span><br></pre></td></tr></table></figure>
<p>qemu内部对一个plugin的信息保存在qemu_plugin_ctx里，qemu全局的plugin信息保存到<br>struct qemu_plugin_state plugin里，注意上面的qemu_plugin_desc保存的只是plugin对应<br>的文件路径、参数等信息。struct qemu_plugin_state plugin里会用链表和哈希表分别记录<br>所有plugin。</p>
<p>qemu针对每个plugin，打开对应的动态库，执行动态库里名为qemu_plugin_install的函数。<br>每个plugin必须实现这个注册接口。</p>
<p>我们以libcache看看plugin怎么实现qemu_plugin_install。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* qemu/contrib/plugins/cache.c */</span><br><span class="line">qemu_plugin_install(qemu_plugin_id_t id, const qemu_info_t *info, int argc, char **argv)</span><br><span class="line">  [...]</span><br><span class="line">  +-&gt; qemu_plugin_register_vcpu_tb_trans_cb(id, vcpu_tb_trans);                   </span><br><span class="line">    +-&gt; plugin_register_cb(id, QEMU_PLUGIN_EV_VCPU_TB_TRANS, cb);                   </span><br><span class="line">  +-&gt; qemu_plugin_register_atexit_cb(id, plugin_exit, NULL);                      </span><br></pre></td></tr></table></figure>
<p>这个函数做了一些自己plugin的初始化后，调用了qemu plugin API注册了vcpu_tb_trans和<br>plugin_exit两个函数。所谓注册，就是把这两个函数保存到了plugin对应qemu_plugin_ctx<br>(后面简称ctx)的qemu_plugin_cb(后面简称cb)里，注意ctx里的cb是个数组，不同的数组项<br>描述不同的event，比如如上的QEMU_PLUGIN_EV_VCPU_TB_TRANS就是一个event，qemu在翻译<br>执行主流程里会调用这些注册的回调函数。</p>
<p>我们可以先观察下如上vcpu_tb_trans的行为，去掉其中业务相关的逻辑，其主逻辑如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vcpu_tb_trans()</span><br><span class="line">  +-&gt; n_insns = qemu_plugin_tb_n_insns(tb);                                       </span><br><span class="line">  +-&gt; for (i = 0; i &lt; n_insns; i++) &#123;                                             </span><br><span class="line">      struct qemu_plugin_insn *insn = qemu_plugin_tb_get_insn(tb, i);         </span><br><span class="line">      effective_addr = (uint64_t) qemu_plugin_insn_vaddr(insn);</span><br><span class="line">      [...]</span><br><span class="line">      qemu_plugin_register_vcpu_mem_cb(insn, vcpu_mem_access, QEMU_PLUGIN_CB_NO_REGS, rw, data);</span><br><span class="line">      qemu_plugin_register_vcpu_insn_exec_cb(insn, vcpu_insn_exec, QEMU_PLUGIN_CB_NO_REGS, data);</span><br><span class="line">  +-&gt; &#125;                                                                           </span><br></pre></td></tr></table></figure>
<p>这个函数对于一个tb，调用qemu plugin的API得到guest指令数目、每个指令相关信息，并<br>针对每条guest指令使用qemu plugin API注册相关的回调函数，比如这里对每条guest指令<br>注册了vcpu_mem_access和vcpu_insn_exec两个函数。不同注册函数注册的回调函数在qemu<br>翻译执行主循环的不同位置被触发，plugin需要根据API的语意使用API注册回调函数。</p>
<p>我们进一步分析qemu内部是怎么实现回调函数的注册和触发的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void qemu_plugin_register_vcpu_mem_cb(struct qemu_plugin_insn *insn,            </span><br><span class="line">                                      qemu_plugin_vcpu_mem_cb_t cb,             </span><br><span class="line">                                      enum qemu_plugin_cb_flags flags,          </span><br><span class="line">                                      enum qemu_plugin_mem_rw rw,               </span><br><span class="line">                                      void *udata)                              </span><br><span class="line">&#123;                                                                               </span><br><span class="line">    plugin_register_vcpu_mem_cb(&amp;insn-&gt;cbs[PLUGIN_CB_MEM][PLUGIN_CB_REGULAR],   </span><br><span class="line">                                    cb, flags, rw, udata);                      </span><br><span class="line">&#125;                                                                               </span><br></pre></td></tr></table></figure>
<p>从qemu_plugin_insn的内部结构可以看出来，针对一个guest指令，plugin可以注册两大类<br>回调函数：PLUGIN_CB_MEM和PLUGIN_CB_INSN，每一类里又分: PLUGIN_CB_REGULAR和PLUGIN_CB_INLINE。<br>被注册的函数以及相关的参数统统保存在insn的对应cb里(qemu_plugin_dyn_cb)。这里只是<br>保存了相关注册函数信息，被注册的函数还没有和qemu主流程关联在一起，和qemu主流程关<br>联的过程还是在qemu主流程里实现。</p>
<p>整个qemu翻译的主流程中被插入了plugin的桩函数以及桩函数的替换逻辑:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* qemu/accel/tcg/translator.c */</span><br><span class="line">translator_loop</span><br><span class="line">  +-&gt; plugin_enabled = plugin_gen_tb_start()</span><br><span class="line">  +-&gt; while (true) &#123;</span><br><span class="line">          if (plugin_enabled) &#123;                                                   </span><br><span class="line">              plugin_gen_insn_start(cpu, db);                                     </span><br><span class="line">                +-&gt; plugin_gen_empty_callback(PLUGIN_GEN_FROM_INSN)</span><br><span class="line">                  +-&gt; gen_wrapped(from, PLUGIN_GEN_ENABLE_MEM_HELPER, gen_empty_mem_helper)</span><br><span class="line">                    +-&gt; gen_plugin_cb_start(from, type, 0);                                         </span><br><span class="line">                    +-&gt; func();                                                                     </span><br><span class="line">                    +-&gt; tcg_gen_plugin_cb_end();                                                    </span><br><span class="line">                  +-&gt; gen_wrapped(from, PLUGIN_GEN_CB_UDATA, gen_empty_udata_cb);             </span><br><span class="line">                  +-&gt; gen_wrapped(from, PLUGIN_GEN_CB_INLINE, gen_empty_inline_cb);           </span><br><span class="line">          &#125;                                                                       </span><br><span class="line"></span><br><span class="line">          ops-&gt;translate_insn(db, cpu);                                       </span><br><span class="line"></span><br><span class="line">          if (plugin_enabled) &#123;                                                   </span><br><span class="line">              plugin_gen_insn_end();                                              </span><br><span class="line">          &#125;                                                                       </span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">  +-&gt; if (plugin_enabled) &#123;                                                       </span><br><span class="line">          plugin_gen_tb_end(cpu);                                                 </span><br><span class="line">      &#125;                                                                           </span><br></pre></td></tr></table></figure>
<p>plugin_gen_tb_start/plugin_gen_insn_start/plugin_gen_insn_end用来插入桩函数，<br>plugin_gen_tb_end主要用来做桩函数的替换。</p>
<p>我们顺序看一个plugin_gen_insn_start的处理。可以看见，这里插入了一些中间码，大概的<br>情况是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plugin_cb_start PLUGIN_GEN_FROM_INSN, PLUGIN_GEN_ENABLE_MEM_HELPER</span><br><span class="line">movi_64 ptr, 0</span><br><span class="line">/* 清空CPUState里plugin_mem_cbs保存的memory相关的回调函数 */</span><br><span class="line">st_i64 ptr CPUState, offset of plugin_mem_cbs</span><br><span class="line">plugin_cb_end</span><br><span class="line"></span><br><span class="line">plugin_cb_start PLUGIN_GEN_FROM_TB, PLUGIN_GEN_CB_UDATA</span><br><span class="line">ld_i32 cpu_index CPUState, offset of cpu_index</span><br><span class="line">call plugin_vcpu_udata_cb cpu_index, udata</span><br><span class="line">plugin_cb_end</span><br><span class="line"></span><br><span class="line">plugin_cb_start PLUGIN_GEN_FROM_TB, PLUGIN_GEN_CB_INLINE</span><br><span class="line">ld_i64 val, ptr, 0</span><br><span class="line">addi_i64 val, val, 0xdeadface</span><br><span class="line">st_i64 val, val, 0</span><br><span class="line">plugin_cb_end</span><br></pre></td></tr></table></figure>
<p>如上的plugin_vcpu_udata_cb是一个空的桩函数。</p>
<p>qemu在plugin_gen_tb_end把plugin注册的回调函数插入qemu翻译执行逻辑里，使用的基本方<br>法就是使用qemu_plugin_insn中保存的回调函数替换如上的桩函数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plugin_gen_tb_end</span><br><span class="line">  +-&gt; qemu_plugin_tb_trans_cb(cpu, ptb);                                          </span><br><span class="line">  +-&gt; plugin_gen_inject(ptb);                                                     </span><br></pre></td></tr></table></figure>
<p>qemu_plugin_tb_trans_cb扫描qemu全局的plugin，针对每个plugin，调用之前注册的<br>QEMU_PLUGIN_EV_VCPU_TB_TRANS event对应的回调函数。具体到上面的cache plugin就是<br>其中的vcpu_tb_trans函数。</p>
<p>我们就用cache的vcpu_tb_trans继续分析，这个函数里最主要的针对tb里的每个guest指令<br>调用qemu plugin API注册回调函数，如上，其实这里的注册就是把回调函数保存到guest指<br>令insn结构体里，我们具体看其中一个：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu_plugin_register_vcpu_mem_cb(insn, vcpu_mem_access, QEMU_PLUGIN_CB_NO_REGS, rw, data);</span><br><span class="line">  +-&gt; plugin_register_vcpu_mem_cb(&amp;insn-&gt;cbs[PLUGIN_CB_MEM][PLUGIN_CB_REGULAR], cb, flags, rw, udata);</span><br></pre></td></tr></table></figure>

<p>plugin_gen_inject用注册的回调函数替换掉如上call中间码里的函数地址，并对输入参数做<br>必要的调整：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plugin_gen_inject(ptb)</span><br><span class="line"></span><br><span class="line">  QTAILQ_FOREACH(op, &amp;tcg_ctx-&gt;ops, link)</span><br><span class="line">    switch (op-&gt;opc)</span><br><span class="line">    [...]</span><br><span class="line">    case INDEX_op_plugin_cb_start:                                          </span><br><span class="line"></span><br><span class="line">      enum plugin_gen_from from = op-&gt;args[0];                            </span><br><span class="line">      enum plugin_gen_cb type = op-&gt;args[1];                              </span><br><span class="line">                                                                          </span><br><span class="line">      switch (from)</span><br><span class="line">      [...]</span><br><span class="line">      case PLUGIN_GEN_FROM_INSN:                                          </span><br><span class="line">        switch (type)</span><br><span class="line">        case PLUGIN_GEN_CB_UDATA:                                       </span><br><span class="line">          plugin_gen_insn_udata(plugin_tb, op, insn_idx);             </span><br><span class="line">          break;                                                      </span><br><span class="line">        case PLUGIN_GEN_CB_INLINE:                                      </span><br><span class="line">          plugin_gen_insn_inline(plugin_tb, op, insn_idx);            </span><br><span class="line">          break;                                                      </span><br><span class="line">        case PLUGIN_GEN_ENABLE_MEM_HELPER:                              </span><br><span class="line">          plugin_gen_enable_mem_helper(plugin_tb, op, insn_idx);      </span><br><span class="line">          break;                                                      </span><br></pre></td></tr></table></figure>
<p>如上代码识别plugin_tb_start开头的一段中间码，然后做匹配位置的函数替换以及参数生成。<br>还是看如上PLUGIN_GEN_FROM_INSN对应的处理(语意是获取指令执行之前的信息)，如上插入<br>了三段以plugin_cb_start开头的中间码，这里的三个case分别处理相关的中间码。</p>
<p>我们深入看下plugin_gen_enable_mem_helper的处理。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plugin_gen_enable_mem_helper</span><br><span class="line">  +-&gt; inject_mem_enable_helper(insn, begin_op);                                   </span><br><span class="line">        /*</span><br><span class="line">         * 如果对这个指令没有注册memory类型的回调, 或者没有calls_helpers? 就删掉</span><br><span class="line">         * 之前的中间码桩。不做任何操作直接返回了。</span><br><span class="line">         */</span><br><span class="line">    +-&gt; plugin_insn-&gt;mem_helper = plugin_insn-&gt;calls_helpers &amp;&amp; n_cbs;              </span><br><span class="line">        if (likely(!plugin_insn-&gt;mem_helper)) &#123;                                     </span><br><span class="line">            rm_ops(begin_op);                                                       </span><br><span class="line">            return;                                                                 </span><br><span class="line">        &#125;                                                                           </span><br><span class="line">	/* 把注册的回调插入plugin的dyn_cb_arr_ht哈希表里 */</span><br><span class="line">    +-&gt; qemu_plugin_add_dyn_cb_arr(arr);                                            </span><br><span class="line">        /* 通过改变中间码插入回调函数 */</span><br><span class="line">    +-&gt; inject_mem_helper(begin_op, arr);                                           </span><br><span class="line">      /*                                                                          </span><br><span class="line">       * 在end后插入op，copy begin后的op到新op上，并做必要的修改，返回的begin_op</span><br><span class="line">       * 指向后一个节点，op指向最新的&quot;end&quot;。比如，这里了是新加了mov IR, 并修改mov</span><br><span class="line">       * 的输入为注册回调描述结构的地址:</span><br><span class="line">       *</span><br><span class="line">       * begin -&gt; mov -&gt; st -&gt; end -&gt; op(mov)</span><br><span class="line">       */</span><br><span class="line">      op = copy_const_ptr(&amp;begin_op, end_op, arr);                                </span><br><span class="line">                                                                                  </span><br><span class="line">      /* 用上面同样的方法修改store指令：begin -&gt; mov -&gt; st -&gt; end -&gt; mov -&gt; st */                                                                         </span><br><span class="line">      op = copy_st_ptr(&amp;begin_op, op);                                            </span><br><span class="line"></span><br><span class="line">      /*                                                                          </span><br><span class="line">       * 把中间码的桩都删去，只留下修改后的mov和st:</span><br><span class="line">       * begin -&gt; mov -&gt; st -&gt; end -&gt; mov -&gt; st                                 </span><br><span class="line">       * &lt;-------- remove -------&gt; </span><br><span class="line">       */                                                                         </span><br><span class="line">      rm_ops_range(orig_op, end_op);                                              </span><br></pre></td></tr></table></figure>
<p>经过上述操作，最终结果是把plugin中注册的回调函数保存到了CPUState的plugin_mem_cbs。</p>
<p>原来的中间码序列：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plugin_cb_start PLUGIN_GEN_FROM_INSN, PLUGIN_GEN_ENABLE_MEM_HELPER</span><br><span class="line">movi_64 ptr, 0</span><br><span class="line">st_i64 ptr CPUState, offset of plugin_mem_cbs</span><br><span class="line">plugin_cb_end</span><br></pre></td></tr></table></figure>
<p>变成了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">movi_64 ptr, arr地址</span><br><span class="line">st_i64 ptr CPUState, offset of plugin_mem_cbs</span><br></pre></td></tr></table></figure>

<p>在load/store的实现里(其中调用helper函数里)，qemu调用qemu_plugin_vcpu_mem_cb的到<br>CPUState中保存的memory相关回调，并执行。</p>
<p>如上我们分析了一个qemu处理plugin的特例情况，其它plugin插桩以及替换的原理也是一样<br>的，比如对于plugin_cb_start PLUGIN_GEN_FROM_TB, PLUGIN_GEN_CB_UDATA的情况，qemu<br>直接用call IR插入了一个空helper函数，后面的替换直接修改call IR里保存的函数地址以及<br>函数入参就好了。</p>
<p>如上我们大致根据plugin的执行流程分析其工作原理，下面在横向的维度上把plugin的基本<br>概念再展开下。</p>
<p>从plugin_gen_from的定义上看，qemu plugin的插桩点包括：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* TB翻译前 */                                                              </span><br><span class="line">PLUGIN_GEN_FROM_TB,                                                         </span><br><span class="line"></span><br><span class="line">/* guest指令翻译前 */                                                       </span><br><span class="line">PLUGIN_GEN_FROM_INSN,                                                       </span><br><span class="line"></span><br><span class="line">/* memory操作相关的点 */</span><br><span class="line">PLUGIN_GEN_FROM_MEM,                                                        </span><br><span class="line"></span><br><span class="line">/* guest指令翻译后 */                                                       </span><br><span class="line">PLUGIN_GEN_AFTER_INSN,                                                      </span><br></pre></td></tr></table></figure>
<p>指令和TB的插桩点在qemu翻译执行的主循环里，如上的分析中已经有涉及。PLUGIN_GEN_FROM_MEM<br>在load/store的公共实现代码里插桩时会用到，在qemu/tcg/tcg-op.c里load/store IR的实现<br>tcg_gen_qemu_ld/st_i32/i64会调用plugin_gen_mem_callbacks插入参数为PLUGIN_GEN_FROM_MEM<br>的plugin_cb_start/end，以及空的helper桩函数。</p>
<p>可以看到，qemu对应中间码使用插入helper桩函数再替换的方式支持plugin。对于helper函数<br>里需要支持plugin时，qemu把回调函数先保存到CPUState里，然后在helper里直接调用回调<br>函数，这个就是我们上面重点分析的例子里的情形。</p>
<h2 id="写一个自己的qemu-plugin"><a href="#写一个自己的qemu-plugin" class="headerlink" title="写一个自己的qemu plugin"></a>写一个自己的qemu plugin</h2><p>如上分析了qemu plugin的逻辑，我们再从plugin的角度看看qemu都提供的那些API出来，以<br>及他们的大概用法。我们还是从cache plugin入手，然后横向展开看看。</p>
<p>如上分析里，cache plugin首先使用qemu_plugin_register_vcpu_tb_trans_cb注册了在tb<br>翻译结束会调用的回调函数(vcpu_tb_trans)，这个回调函数可以得到tb里guest指令的句柄，<br>从而plugin里可以继续针对guest指令注册回调函数。</p>
<p>我们先看下第一层，也就是除了tb翻译完成可以注册回调，还有那些地方可以注册回调。qemu<br>里还提供了如下注册plugin的地方：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">QEMU_PLUGIN_EV_VCPU_INIT</span><br><span class="line">QEMU_PLUGIN_EV_VCPU_EXIT</span><br><span class="line">QEMU_PLUGIN_EV_VCPU_TB_TRANS</span><br><span class="line">QEMU_PLUGIN_EV_VCPU_IDLE</span><br><span class="line">QEMU_PLUGIN_EV_VCPU_RESUME</span><br><span class="line">QEMU_PLUGIN_EV_VCPU_SYSCALL</span><br><span class="line">QEMU_PLUGIN_EV_VCPU_SYSCALL_RET</span><br><span class="line">QEMU_PLUGIN_EV_FLUSH</span><br><span class="line">QEMU_PLUGIN_EV_ATEXIT</span><br></pre></td></tr></table></figure>
<p>如上的每个地方，qemu都提供了一个API来注册回调，比如QEMU_PLUGIN_EV_VCPU_INIT对应<br>的API就是qemu_plugin_register_vcpu_init_cb。如上回调点大概意思可以猜出来，但是要<br>知道确切意思还的去看qemu的代码，qemu并没有把自己执行的模型表述的很清楚。</p>
<p>cache plugin里针对tb里的每个guest指令注册了vcpu_mem_access和vcpu_insn_exec，我们<br>看看针对指令都可以怎么注册回调函数。针对guest指令可以注册内存读写相关的回调和指令<br>执行相关的回调，每种类型又分为cb和inline:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu_plugin_register_vcpu_mem_cb</span><br><span class="line">qemu_plugin_register_vcpu_mem_inline</span><br><span class="line"></span><br><span class="line">qemu_plugin_register_vcpu_insn_exec_cb</span><br><span class="line">qemu_plugin_register_vcpu_insn_exec_inline</span><br></pre></td></tr></table></figure>
<p>对于指令执行相关的回调，qemu会在每个指令执行前调用，对于内存读写相关的回调，qemu<br>会在访存指令的helper实现函数以及访存指令完成时调用。cb类型的回调是plugin里实现回调<br>函数，qemu主流程里调用plugin里定义的函数来实现信息记录的，而所谓inline并没有调用<br>helper函数记录信息，而是在plugin里定义操作指令和操作的目的地址，qemu主流程里每当<br>到了调用点就对目的地址做相关的操作，目前qemu定义的操作还只有add，可以看出，inline<br>是一种轻量级的记录方式，qemu内部实现上，只需要根据plugin提供的操作地址稍微调整下<br>主流程里的桩中间码就可以做到。</p>
<p>qemu针对tb执行也提供了可以注册回调的入口：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu_plugin_register_vcpu_tb_exec_cb</span><br><span class="line">qemu_plugin_register_vcpu_tb_exec_inline</span><br></pre></td></tr></table></figure>
<p>使用如上API注册回调，qemu会在执行tb前调用回调或者像上面分析中提到的那样更新注册<br>地址上的数据。</p>
<p>qemu plugin里需要获得guest指令或者tb的一些参数，为此qemu还对外提供了一组获取guest<br>指令或者tb的信息的辅助函数。这些辅助函数以及上述所提到的API在include/qemu/qemu-plugin.h<br>里均有定义。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu调试方法</title>
    <url>/qemu%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="基础介绍"><a href="#基础介绍" class="headerlink" title="基础介绍"></a>基础介绍</h2><p> qemu-riscv64 -d help可以看到支持的各种debug选项，我们一般用的有in_asm/exec/cpu<br> 以及singlestep等，其他的参数，比如，out_asm是翻译后得到的host上的汇编，op是qemu的<br> 中间码，这些都和qemu自己的调试有关。</p>
<p> 我们看下前面的几个：in_asm是输出guest的汇编，exec是记录每个翻译块执行的trace，<br> cpu是在执行一个翻译块之前打印当前的CPU各个寄存器的值，在没有配置singlestep的时候，<br> 上面的打印都是和翻译块(TB)相关的，singlestep可以看到每条guest指令模拟执行后的各种<br> 调试信息。上面的调试配置可以配合使用，使用-D file，可以输出到文件file，需要注意<br> 的是调试信息往往比较多。如果是用user mode调试小的程序，输出的log的大小还可以接受，<br> 如果，比如你用system mode起Linux内核的时候加上了上面的调试选项，运行的速度就会<br> 很慢，因为要输出的log太多了，qemu还有-dfilter可以只输出特定地址区间的log，这样就<br> 可以先不加-dfilter运行一遍，大概知道需要debug的地方，然后再加上-dfilter仔细的跟踪。<br> qemu里还可以用-d mmu去跟踪mmu翻译，用-d int去跟踪中断和异常。</p>
<h2 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h2><p> qemu上运行的代码：(test.c)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int __attribute__ ((noinline)) test_add(int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">	return a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	volatile int c = 0;</span><br><span class="line"></span><br><span class="line">	c = test_add(1, 2);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> riscv64-linux-gnu-gcc test.c –static<br> riscv64-linux-gnu-objdump -D a.out &gt; test.s</p>
<p> 我们这样运行下: qemu-riscv64 -d in_asm,exec,cpu -D ./log a.out<br> 然后看看test_add这个函数前后的各种调试信息。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[...]</span><br><span class="line">----------------</span><br><span class="line">IN: main</span><br><span class="line">0x0000000000010458:  1101              addi            sp,sp,-32</span><br><span class="line">0x000000000001045a:  ec06              sd              ra,24(sp)</span><br><span class="line">0x000000000001045c:  e822              sd              s0,16(sp)</span><br><span class="line">0x000000000001045e:  1000              addi            s0,sp,32</span><br><span class="line">0x0000000000010460:  fe042623          sw              zero,-20(s0)</span><br><span class="line">0x0000000000010464:  4589              addi            a1,zero,2</span><br><span class="line">0x0000000000010466:  4505              addi            a0,zero,1</span><br><span class="line">0x0000000000010468:  fc9ff0ef          jal             ra,-56          # 0x10430</span><br><span class="line"></span><br><span class="line">Trace 0: 0xffff98026780 [0000000000000000/0000000000010458/00006100/00000000] main</span><br><span class="line"> pc       0000000000010458</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 0000000000010610 x2/sp 0000004000800360 x3/gp 0000000000071028</span><br><span class="line"> x4/tp 0000000000072710 x5/t0 0000000000072000 x6/t1 2f2f2f2f2f2f2f2f x7/t2 0000000000072000</span><br><span class="line"> x8/s0 000000000001094a x9/s1 00000000000109da x10/a0 0000000000000001 x11/a1 00000040008004a8</span><br><span class="line"> x12/a2 00000040008004b8 x13/a3 0000000000000000 x14/a4 0000004000800388 x15/a5 0000000000010458</span><br><span class="line"> x16/a6 0000000000071140 x17/a7 4a5e000112702f5b x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 ffffffffffffffff x29/t4 000000000006ead0 x30/t5 0000000000000000 x31/t6 0000000000072000</span><br><span class="line">----------------</span><br><span class="line">IN: test_add</span><br><span class="line">0x0000000000010430:  1101              addi            sp,sp,-32</span><br><span class="line">0x0000000000010432:  ec22              sd              s0,24(sp)</span><br><span class="line">0x0000000000010434:  1000              addi            s0,sp,32</span><br><span class="line">0x0000000000010436:  87aa              mv              a5,a0</span><br><span class="line">0x0000000000010438:  872e              mv              a4,a1</span><br><span class="line">0x000000000001043a:  fef42623          sw              a5,-20(s0)</span><br><span class="line">0x000000000001043e:  87ba              mv              a5,a4</span><br><span class="line">0x0000000000010440:  fef42423          sw              a5,-24(s0)</span><br><span class="line">0x0000000000010444:  fec42703          lw              a4,-20(s0)</span><br><span class="line">0x0000000000010448:  fe842783          lw              a5,-24(s0)</span><br><span class="line">0x000000000001044c:  9fb9              addw            a5,a5,a4</span><br><span class="line">0x000000000001044e:  2781              sext.w          a5,a5</span><br><span class="line">0x0000000000010450:  853e              mv              a0,a5</span><br><span class="line">0x0000000000010452:  6462              ld              s0,24(sp)</span><br><span class="line">0x0000000000010454:  6105              addi            sp,sp,32</span><br><span class="line">0x0000000000010456:  8082              ret             </span><br><span class="line"></span><br><span class="line">Linking TBs 0xffff98026780 [0000000000010458] index 0 -&gt; 0xffff98026900 [0000000000010430]</span><br><span class="line">Trace 0: 0xffff98026900 [0000000000000000/0000000000010430/00006100/00000000] test_add</span><br><span class="line"> pc       0000000000010430</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 000000000001046c x2/sp 0000004000800340 x3/gp 0000000000071028</span><br><span class="line"> x4/tp 0000000000072710 x5/t0 0000000000072000 x6/t1 2f2f2f2f2f2f2f2f x7/t2 0000000000072000</span><br><span class="line"> x8/s0 0000004000800360 x9/s1 00000000000109da x10/a0 0000000000000001 x11/a1 0000000000000002</span><br><span class="line"> x12/a2 00000040008004b8 x13/a3 0000000000000000 x14/a4 0000004000800388 x15/a5 0000000000010458</span><br><span class="line"> x16/a6 0000000000071140 x17/a7 4a5e000112702f5b x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 ffffffffffffffff x29/t4 000000000006ead0 x30/t5 0000000000000000 x31/t6 0000000000072000</span><br><span class="line">----------------</span><br><span class="line">IN: main</span><br><span class="line">0x000000000001046c:  87aa              mv              a5,a0</span><br><span class="line">0x000000000001046e:  fef42623          sw              a5,-20(s0)</span><br><span class="line">0x0000000000010472:  4781              mv              a5,zero</span><br><span class="line">0x0000000000010474:  853e              mv              a0,a5</span><br><span class="line">0x0000000000010476:  60e2              ld              ra,24(sp)</span><br><span class="line">0x0000000000010478:  6442              ld              s0,16(sp)</span><br><span class="line">0x000000000001047a:  6105              addi            sp,sp,32</span><br><span class="line">0x000000000001047c:  8082              ret             </span><br><span class="line"></span><br><span class="line">Trace 0: 0xffff98026ac0 [0000000000000000/000000000001046c/00006100/00000000] main</span><br><span class="line"> pc       000000000001046c</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 000000000001046c x2/sp 0000004000800340 x3/gp 0000000000071028</span><br><span class="line"> x4/tp 0000000000072710 x5/t0 0000000000072000 x6/t1 2f2f2f2f2f2f2f2f x7/t2 0000000000072000</span><br><span class="line"> x8/s0 0000004000800360 x9/s1 00000000000109da x10/a0 0000000000000003 x11/a1 0000000000000002</span><br><span class="line"> x12/a2 00000040008004b8 x13/a3 0000000000000000 x14/a4 0000000000000001 x15/a5 0000000000000003</span><br><span class="line"> x16/a6 0000000000071140 x17/a7 4a5e000112702f5b x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 ffffffffffffffff x29/t4 000000000006ead0 x30/t5 0000000000000000 x31/t6 0000000000072000</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p> 如上，IN: xxx后面是guest的汇编，in_asm会显示这一部分。exec打印出来的是诸如，Linking TBs xxx<br> Trace 0：xxx的部分，这些都是qemu TB的信息，其中中括号里的第二个参数对调试guest<br> 代码有用，比如，Trace 0: 0xffff98026900 [0000000000000000/0000000000010430/00006100/00000000] test_add<br> 中的0000000000010430是这个TB对应的第一个guest指令的地址，中括号后面是符号名字。<br> exec可以用来跟踪程序执行到哪里了，运行时的跟踪。</p>
<p> qemu把若干条guest指令翻译到一个TB里，TB的划分一般是程序跳转的地方，可以看到main<br> 函数的前几条指令放到了一个TB里，test_add的指令都放到一个TB，main的后面几条指令<br> 放到了下一个TB里。</p>
<p> 需要注意的一个地方是，CPU寄存器打印表示的是进入当前TB翻译执行代码之前寄存器的数值。<br> 所以，可以看到test_add这一段log的a0是1、a1是2，表示进入test_add这段代码对应的TB<br> 之前CPU寄存器的值。逻辑上，test_add调用之前先把1放到a0、2放到a1，和log是一致的。</p>
<p> 使用singlestep可以看到一条guest指令执行后CPU的状态，我们只看CPU的状态，使用如下<br> 的命令：qemu-riscv64 -d in_asm,cpu -singlestep -D ./log a.out</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[...]</span><br><span class="line">----------------</span><br><span class="line">IN: main</span><br><span class="line">0x0000000000010468:  fc9ff0ef          jal             ra,-56          # 0x10430</span><br><span class="line"></span><br><span class="line"> pc       0000000000010468</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 0000000000010610 x2/sp 0000004000800340 x3/gp 0000000000071028</span><br><span class="line"> x4/tp 0000000000072710 x5/t0 0000000000072000 x6/t1 2f2f2f2f2f2f2f2f x7/t2 0000000000072000</span><br><span class="line"> x8/s0 0000004000800360 x9/s1 00000000000109da x10/a0 0000000000000001 x11/a1 0000000000000002</span><br><span class="line"> x12/a2 00000040008004b8 x13/a3 0000000000000000 x14/a4 0000004000800388 x15/a5 0000000000010458</span><br><span class="line"> x16/a6 0000000000071140 x17/a7 4a5e000112702f5b x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 ffffffffffffffff x29/t4 000000000006ead0 x30/t5 0000000000000000 x31/t6 0000000000072000</span><br><span class="line">----------------</span><br><span class="line">IN: test_add</span><br><span class="line">0x0000000000010430:  1101              addi            sp,sp,-32</span><br><span class="line"></span><br><span class="line"> pc       0000000000010430</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 000000000001046c x2/sp 0000004000800340 x3/gp 0000000000071028</span><br><span class="line"> x4/tp 0000000000072710 x5/t0 0000000000072000 x6/t1 2f2f2f2f2f2f2f2f x7/t2 0000000000072000</span><br><span class="line"> x8/s0 0000004000800360 x9/s1 00000000000109da x10/a0 0000000000000001 x11/a1 0000000000000002</span><br><span class="line"> x12/a2 00000040008004b8 x13/a3 0000000000000000 x14/a4 0000004000800388 x15/a5 0000000000010458</span><br><span class="line"> x16/a6 0000000000071140 x17/a7 4a5e000112702f5b x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 ffffffffffffffff x29/t4 000000000006ead0 x30/t5 0000000000000000 x31/t6 0000000000072000</span><br><span class="line">----------------</span><br><span class="line">IN: test_add</span><br><span class="line">0x0000000000010432:  ec22              sd              s0,24(sp)</span><br><span class="line"></span><br><span class="line"> pc       0000000000010432</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 000000000001046c x2/sp 0000004000800320 x3/gp 0000000000071028</span><br><span class="line"> x4/tp 0000000000072710 x5/t0 0000000000072000 x6/t1 2f2f2f2f2f2f2f2f x7/t2 0000000000072000</span><br><span class="line"> x8/s0 0000004000800360 x9/s1 00000000000109da x10/a0 0000000000000001 x11/a1 0000000000000002</span><br><span class="line"> x12/a2 00000040008004b8 x13/a3 0000000000000000 x14/a4 0000004000800388 x15/a5 0000000000010458</span><br><span class="line"> x16/a6 0000000000071140 x17/a7 4a5e000112702f5b x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 ffffffffffffffff x29/t4 000000000006ead0 x30/t5 0000000000000000 x31/t6 0000000000072000</span><br><span class="line">----------------</span><br><span class="line">IN: test_add</span><br><span class="line">0x0000000000010434:  1000              addi            s0,sp,32</span><br><span class="line"></span><br><span class="line"> pc       0000000000010434</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 000000000001046c x2/sp 0000004000800320 x3/gp 0000000000071028</span><br><span class="line"> x4/tp 0000000000072710 x5/t0 0000000000072000 x6/t1 2f2f2f2f2f2f2f2f x7/t2 0000000000072000</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p> 如上，因为显示的寄存器是执行本条指令之前的寄存器的数值，比如，test_add的第一条指令<br> addi sp,sp,-32对sp寄存器的更新要在test_add的第二个指令(sd s0,24(sp))的打印log里<br> 才能看到：x0/zero 0000000000000000 x1/ra 000000000001046c x2/sp 0000004000800320 x3/gp 0000000000071028。<br> 如果你想用qemu跟踪指令指令的流程，只要用脚本处理下如上的-singlestep的反汇编log就好，<br> 具体的处理方式可以参考下面代码分析中的介绍。</p>
<p> 我们加上-dfilter运行下qemu-riscv64 -d in_asm,exec,cpu -dfilter 0x10430+0x26 -D ./log a.out<br> dfilter的这种写法表示只输出pc在[0x10430, 0x10430+0x26]区间的log，这段区间是test_add<br> 函数的代码。全部log就只有：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">----------------</span><br><span class="line">IN: test_add</span><br><span class="line">0x0000000000010430:  1101              addi            sp,sp,-32</span><br><span class="line">0x0000000000010432:  ec22              sd              s0,24(sp)</span><br><span class="line">0x0000000000010434:  1000              addi            s0,sp,32</span><br><span class="line">0x0000000000010436:  87aa              mv              a5,a0</span><br><span class="line">0x0000000000010438:  872e              mv              a4,a1</span><br><span class="line">0x000000000001043a:  fef42623          sw              a5,-20(s0)</span><br><span class="line">0x000000000001043e:  87ba              mv              a5,a4</span><br><span class="line">0x0000000000010440:  fef42423          sw              a5,-24(s0)</span><br><span class="line">0x0000000000010444:  fec42703          lw              a4,-20(s0)</span><br><span class="line">0x0000000000010448:  fe842783          lw              a5,-24(s0)</span><br><span class="line">0x000000000001044c:  9fb9              addw            a5,a5,a4</span><br><span class="line">0x000000000001044e:  2781              sext.w          a5,a5</span><br><span class="line">0x0000000000010450:  853e              mv              a0,a5</span><br><span class="line">0x0000000000010452:  6462              ld              s0,24(sp)</span><br><span class="line">0x0000000000010454:  6105              addi            sp,sp,32</span><br><span class="line">0x0000000000010456:  8082              ret             </span><br><span class="line"></span><br><span class="line">Trace 0: 0xffffa00265c0 [0000000000000000/0000000000010430/0x6100] test_add</span><br><span class="line"> pc       0000000000010430</span><br><span class="line"> x0/zero 0000000000000000 x1/ra 000000000001046c x2/sp 0000004000800160 x3/gp 0000000000071028</span><br><span class="line"> x4/tp 0000000000072710 x5/t0 0000000000072000 x6/t1 2f2f2f2f2f2f2f2f x7/t2 0000000000072000</span><br><span class="line"> x8/s0 0000004000800180 x9/s1 00000000000109da x10/a0 0000000000000001 x11/a1 0000000000000002</span><br><span class="line"> x12/a2 00000040008002d8 x13/a3 0000000000000000 x14/a4 00000040008001a8 x15/a5 0000000000010458</span><br><span class="line"> x16/a6 0000000000071140 x17/a7 0112702f5b5a4001 x18/s2 0000000000000000 x19/s3 0000000000000000</span><br><span class="line"> x20/s4 0000000000000000 x21/s5 0000000000000000 x22/s6 0000000000000000 x23/s7 0000000000000000</span><br><span class="line"> x24/s8 0000000000000000 x25/s9 0000000000000000 x26/s10 0000000000000000 x27/s11 0000000000000000</span><br><span class="line"> x28/t3 ffffffffffffffff x29/t4 000000000006ead0 x30/t5 0000000000000000 x31/t6 0000000000072000</span><br></pre></td></tr></table></figure>
<p> dfilter参数还有其他的描述格式，从qemu的自测试代码可以看到具体的使用方式：qemu/tests/test-logging.c。<br> 还可以用: 基地址-区间大小、基地址..结尾地址，用逗号分开可以加多段不同的地址，比如：<br> -dfilter 0x1000+0x100,0x2100-0x100,0x3000..0x3100</p>
<p> 加上-d mmu可以打印出qemu上MMU相关的行为，打印大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_cpu_tlb_fill ad ffffffe001003fe0 rw 0 mmu_idx 1</span><br><span class="line">riscv_cpu_tlb_fill address=ffffffe001003fe0 ret 0 physical 0000000081203fe0 prot 3</span><br></pre></td></tr></table></figure>
<p> 上面表示MMU要查ffffffe001003fe0对应的物理地址，并且查到的结果是0000000081203fe0，<br> rw后面的数字表示访问类型，0表示数据load，1表示数据store，2表示指令fetch，ret后面<br> 的数字，0表示翻译成功，1表示翻译失败。</p>
<p> 加上-d int可以打印中断和异常的log，但是在5.1.50的版本上，这个打印还没有加上，在<br> 7.0.50上, 这个打印大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_cpu_do_interrupt: hart:0, async:0, cause:0000000000000009, epc:0xffffffe00020455e, tval:0x0000000000000000, desc=supervisor_ecall</span><br><span class="line">[...]</span><br><span class="line">riscv_cpu_do_interrupt: hart:0, async:1, cause:0000000000000005, epc:0xffffffe000201558, tval:0x0000000000000000, desc=s_timer</span><br></pre></td></tr></table></figure>
<p> hart表示异常或中断的cpu，async后面的数据，0表示异常，1表示中断，cause/epc/tval<br> 就是相关寄存器中的值，最后一个desc是一个描述异常或中断的字符串。</p>
<h2 id="qemu代码分析"><a href="#qemu代码分析" class="headerlink" title="qemu代码分析"></a>qemu代码分析</h2><p> in_asm的入口在：translator_loop的最后，可见guest汇编只在翻译的时候会打印，这些<br> 打印不反应执行的流程。另外，qemu对于同一段代码是可能会翻译到不同的TB里的，这个<br> 和翻译参数有关系。guest指令的反汇编是独立的逻辑，不在本文中描述。</p>
<p> exec和cpu的入口在：cpu_tb_exec的log_cpu_exec里，可见是在TB执行的时候打印。但是，<br> qemu为了提高执行效率，TB和TB之间可以直接跳过去，这个叫chained TB，而log_cpu_exec<br> 是在翻译-执行的循环里加的打印，所以，要跟踪程序执行流程，必须断开qemu里的chained<br> TB，所以参数变成了：-d in_asm,cpu,nochain -singlestep，如果只要得到程序的执行流程，<br> cpu可以去掉，就变成了-d in_asm,exec,nochain –singlestep, in_asm可以打出反汇编，<br> 但是，只是翻译的时候打一下，随后执行的时候不会打印，exec + nochain可以打印具体执行<br> 的流程，但是没有反汇编，所以，我们可以对得到的log在exec的log下插入对应的汇编代码，<br> 再把所有exec对应的汇编代码提取到一个文件里就得到了实际指令执行流程，可以写一个脚本<br> 搞定这个事情。</p>
<p> qemu里有两个singlestep相关的flag，一个是全局变量singlestep，一个是cpu结构体里的<br> singlestep_enabled。后者应该和gdb单步运行有关系，不是我们考虑东西。singlestep<br> 的相关逻辑在翻译执行主循环里，如果是singlestep就配置翻译flag(cflags)为CF_NO_GOTO_TB，<br> 并且一个TB只进行一条guest指令的翻译执行，CF_NO_GOTO_TB禁止qemu在TB之间跳转。<br> 这里有个点需要注意，GOTO_PTR也可能在TB之间跳转，这里之所没有连CF_NO_GOTO_PTR也<br> 配置上，是因为GOTO_PTR的help函数里调用了log_cpu_exec，所以，在GOTO_PTR的时候是<br> 可以打印trace的。</p>
<p> 可以看到，singlestep和nochain改变了qemu翻译执行的粒度，效率上是变差了很多的。<br> 实际在qemu system mode上用-d in_asm,cpu -singlestep跑，会慢的一塌糊涂，这个时候<br> 就可以用-dfilter指定输出log的指令地址区间。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv KVM虚拟化分析</title>
    <url>/riscv-KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="内核kvm基本框架"><a href="#内核kvm基本框架" class="headerlink" title="内核kvm基本框架"></a>内核kvm基本框架</h2><p> kvm的入口函数在体系构架相关的代码里，riscv在arch/riscv/kvm/main.c里，riscv_kvm_init<br> 直接调用到KVM的总入口函数kvm_init，kvm_init创建一个/dev/kvm的字符设备，随后所有<br> 的kvm相关的操作都依赖这个字符设备。</p>
<p> kvm_init的大概逻辑：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kvm_init</span><br><span class="line">      /*</span><br><span class="line">       * 以riscv为例, 主要是做一些基本的硬件检测，比较重要的是gstage mode和vmid</span><br><span class="line">       * 的检测。riscv里的两级地址翻译，第一级叫VS stage，第二级叫G stage，这里</span><br><span class="line">       * 检测的gstage mode就是第二级翻译的配置。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; kvm_arch_init</span><br><span class="line">  [...]</span><br><span class="line">      /* 注册/dev/kvm的字符设备 */</span><br><span class="line">  +-&gt; misc_register</span><br><span class="line">  +-&gt; kvm_preempt_ops.sched_in = kvm_sched_in;</span><br><span class="line">  +-&gt; kvm_preempt_ops.sched_out = kvm_sched_out;</span><br></pre></td></tr></table></figure>
<p> /dev/kvm这个字符设备只定义了对应的ioctl，这个ioctl支持的最主要的功能是创建一个虚拟机。<br> 我们看下KVM_CREATE_VM的逻辑:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kvm_dev_ioctl_create_vm</span><br><span class="line">  +-&gt; kvm_create_vm</span><br><span class="line">        /* 分配gstage的pgd，vmid，guest的timer */</span><br><span class="line">    +-&gt; kvm_arch_init_vm</span><br><span class="line">      /*</span><br><span class="line">       * 这个ioctl会创建一个匿名文件，ioctl返回值是文件的fd, 这个fd就代表新创建的虚拟机，</span><br><span class="line">       * 这个fd只实现了ioctl和release回调，release就是销毁虚拟机，ioctl用来配置虚拟机</span><br><span class="line">       * 的各种资源，比如创建虚拟机的CPU(KVM_CREATE_VCPU)、给虚拟机配置内存(KVM_SET_USER_MEMORY_REGION)</span><br><span class="line">       * 等等。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; file = anon_inode_getfile(&quot;kvm-vm&quot;, &amp;kvm_vm_fops, kvm, O_RDWR)</span><br></pre></td></tr></table></figure>
<p> 创建虚拟机的CPU的基本逻辑：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kvm_vm_ioctl_create_vcpu</span><br><span class="line">      /*</span><br><span class="line">       * riscv的实现在arch/riscv/kvm/vcpu.c</span><br><span class="line">       * 把HSTAUS_SPV, HSTATUS_SPVP, HSTATUS_VTW配置到虚拟机vcpu的软件结构里，</span><br><span class="line">       * 这里SPV比较有意思，虚拟机启动的时候，会先根据如上软件结构里的HSTATUS</span><br><span class="line">       * 更新hstatus寄存器，然后sret跳转到虚拟机启动的第一条指令，sret会根据SPV</span><br><span class="line">       * 寄存器的值配置机器的V状态，这里SPV是1，sret指令会先把V状态配置成1，然后</span><br><span class="line">       * 跳到虚拟机启动的第一条指令。这里描述的是虚拟机最开始启动时候的逻辑。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; kvm_arch_vcpu_create</span><br><span class="line">        /* 配置vcpu的timer，实现为配置一个hrtimer，在定时到时，注入时钟中断 */</span><br><span class="line">    +-&gt; kvm_riscv_vcpu_timer_init</span><br><span class="line">    +-&gt; kvm_riscv_rest_vcpu</span><br><span class="line">          /* 软件之前配置好的信息，在这个函数里写到硬件里 */</span><br><span class="line">      +-&gt; kvm_arch_vcpu_load</span><br><span class="line">        +-&gt; csr_write更新CSR寄存器</span><br><span class="line">        +-&gt; kvm_riscv_gstage_update_hgatp  更新hgatp</span><br><span class="line">        +-&gt; kvm_riscv_vcpu_timer_restore   更新htimedelta</span><br><span class="line">        /*</span><br><span class="line">         * 为每个vcpu创建一个匿名的fd，这个fd实现的回调函数有：release、ioctl和mmap，</span><br><span class="line">         * ioctl提供vcpu的控制接口，比如，运行vcpu(KVM_RUN)等等。</span><br><span class="line">         */</span><br><span class="line">  +-&gt; create_vcpu_fd</span><br></pre></td></tr></table></figure>
<p> 给虚拟机配置内存:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* kvm_userspace_mem是从用户态传进来的虚拟机内存的配置信息 */</span><br><span class="line">struct kvm_userspace_memory_region kvm_userspace_mem;</span><br><span class="line"></span><br><span class="line">kvm_vm_ioctl_set_memory_region(kvm, &amp;kvm_userspace_mem)</span><br><span class="line">  +-&gt; kvm_set_memory_region</span><br><span class="line">    +-&gt; __kvm_set_memory_region</span><br><span class="line">      +-&gt; kvm_prepare_memory_region</span><br><span class="line">            /* arch/riscv/kvm/mmu.c */</span><br><span class="line">        +-&gt; kvm_arch_prepare_memory_region</span><br><span class="line">              /*</span><br><span class="line">               * 虚拟机的物理地址是host的用户态分配的一段虚拟内存，这里面有三个</span><br><span class="line">               * 地址: 1. 这段虚拟地址的va；2. 这段虚拟地址对应的物理地址；3. 虚拟机</span><br><span class="line">               * 的物理地址(gpa)，这三个地址对应的实际内存是相同的，但是各自的数值</span><br><span class="line">               * 是不同的。实际上，第2级翻译是gpa-&gt;pa，但是host上申请到的va在host</span><br><span class="line">               * S mode上的翻译是va-&gt;pa(页表基地址是satp)，所以，我们就要把gpa-&gt;pa</span><br><span class="line">               * 的映射插到第2级翻译对应的页表里(hgatp)。</span><br><span class="line">               * </span><br><span class="line">               * 我们自然会联想第2级翻译缺页在哪里处理，这个逻辑单独在下面看。</span><br><span class="line">               */</span><br><span class="line">          +-&gt; gstage_ioremap</span><br><span class="line">                /* 配置第二级的页表 */</span><br><span class="line">            +-&gt; gstage_set_pte</span><br><span class="line">      +-&gt; kvm_create_memslot</span><br><span class="line">      +-&gt; kvm_commit_memory_region</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p> 我们先看下vcpu run的逻辑：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kvm_vcpu_ioctl</span><br><span class="line">  +-&gt; case KVM_RUN</span><br><span class="line">    +-&gt; kvm_arch_vcpu_ioctl_run</span><br><span class="line">          /* arch/riscv/kvm/vcpu.c */</span><br><span class="line">      +-&gt; kvm_riscv_vcpu_enter_exit</span><br><span class="line">            /* arch/riscv/kvm/vcpu_switch.S */</span><br><span class="line">        +-&gt; __kvm_riscv_switch_to</span><br></pre></td></tr></table></figure>
<p> __kvm_riscv_switch_to里把S mode的相关寄存器保存起来，换上VS状态的寄存器，然后sret<br> 跳到vcpu代码入口运行。vcpu的初始状态在如上vcpu create的逻辑中配置到vcpu的软件结构，<br> 通过这里的__kvm_riscv_switch_to配置到硬件CSR寄存器。</p>
<p>第2级翻译缺页的逻辑可以从vcpu_switch.S里的__kvm_riscv_switch_to入手看，这个函数<br>是vcpu运行的入口函数，在投入运行前，这个函数里把__kvm_switch_return这个函数的地址<br>配置给了stvec，当vcpu运行出现异常时，就会跳到__kvm_switch_return继续执行，这样就会<br>从上面的kvm_riscv_vcpu_enter_exit出来，继续执行kvm_riscv_vcpu_exit, 第2级缺页异常<br>在这个函数里处理：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kvm_riscv_vcpu_exit</span><br><span class="line">  +-&gt; gstage_page_fault</span><br><span class="line">        /*</span><br><span class="line">         * 这个函数里会用host va(不是gpa)，判断是不是有合法的vma存在，如果有合法</span><br><span class="line">         * 的vma存在，就可以分配内存，并且创建第二级页表，创建第2级map的时候使用</span><br><span class="line">         * gpa-&gt;pa</span><br><span class="line">         */</span><br><span class="line">    +-&gt; kvm_riscv_gstage_map</span><br><span class="line">      [...]</span><br></pre></td></tr></table></figure>

<p>如上是虚拟机进入以及运行的逻辑，在用户态看，就是进入一个ioctl，停在里面运行代码，<br>直到运行不下去了，ioctl就返回了，返回值以及ioctl的输出参数携带退出的原因和参数。<br>从kvm内部看，虚拟机退出是他执行指令的时候遇到了异常或者中断，异常或中断处理后从ioctl<br>返回到qemu线程的用户态。触发虚拟机退出的源头包括外设的MMIO访问，在构建虚拟机的地址空间<br>时，没有对外设的MMIO gpa对第二级映射，这样第二级翻译的时候就会触发缺页异常，kvm的<br>处理缺页的代码处理完缺页后就会退出虚拟机(vcpu run ioctl返回)。发生异常的指令的PC<br>保存在sepc里，qemu会再次通过vcpu run ioctl进来，然后通过sret从sepc处继续运行。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* arch/riscv/kvm/vcpu.c */</span><br><span class="line">kvm_arch_vcpu_ioctl_run</span><br><span class="line">      /* 这里一进来run vcpu就处理MMIO，可能是上次时MMIO原因退出的，这样当然要接着MMIO的上下文继续跑 */</span><br><span class="line">  +-&gt; if (run-&gt;exit_reason == KVM_EXIT_MMIO)</span><br><span class="line">              kvm_riscv_vcpu_mmio_return(vcpu, vcpu-&gt;run)</span><br><span class="line">      /* 投入运行虚拟机, 异常后也从这里退出来 */</span><br><span class="line">  +-&gt; kvm_riscv_vcpu_enter_exit</span><br><span class="line">      /* 处理异常*/</span><br><span class="line">  +-&gt; kvm_riscv_vcpu_exit</span><br><span class="line">    +-&gt; gstage_page_fault</span><br><span class="line">      +-&gt; emulate_load</span><br><span class="line">            /* 在这里配置退出条件 */</span><br><span class="line">        +-&gt; run-&gt;exit_reason = KVM_EXIT_MMIO</span><br></pre></td></tr></table></figure>

<p>随后独立考虑中断虚拟化。</p>
<h2 id="qemu-kvm的基本逻辑"><a href="#qemu-kvm的基本逻辑" class="headerlink" title="qemu kvm的基本逻辑"></a>qemu kvm的基本逻辑</h2><p> <a href="https://wangzhou.github.io/qemu-tcg%E7%BF%BB%E8%AF%91%E6%89%A7%E8%A1%8C%E6%A0%B8%E5%BF%83%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">qemu tcg翻译执行核心逻辑分析</a>已经介绍了虚拟机启动的相关逻辑，从qemu构架上看<br> kvm和tcg处于同一个层面上, 都是cpu模拟的一种加速器。</p>
<p> 虚拟机初始化逻辑:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* accel/kvm/kvm-all.c */</span><br><span class="line">kvm_init</span><br><span class="line">  +-&gt; qemu_open_old(&quot;/dev/kvm&quot;, O_RDWR)</span><br><span class="line">      /* 创建虚拟机 */</span><br><span class="line">  +-&gt; kvm_ioctl(s, KVM_CREATE_VM, type);</span><br><span class="line">      /* 虚拟机内存配置入口 */</span><br><span class="line">  +-&gt; kvm_memory_listener_register</span><br><span class="line">    +-&gt; kvm_region_add</span><br><span class="line">      +-&gt; kvm_set_phys_mem</span><br><span class="line">        +-&gt; kvm_set_user_memory_region</span><br><span class="line">          +-&gt; kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &amp;mem)</span><br></pre></td></tr></table></figure>
<p> kvm vcpu线程启动的逻辑：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_cpu_realize</span><br><span class="line">  +-&gt; qemu_init_vcpu(cs)</span><br><span class="line">        /* kvm对应的回调函数在：accel/kvm/kvm-accel-ops.c: kvm_vcpu_thread_fn */</span><br><span class="line">    +-&gt; cpus_accel-&gt;create_vcpu_thread(cpu)</span><br><span class="line">        (kvm_vcpu_thread_fn)</span><br><span class="line">      +-&gt; kvm_init_vcpu</span><br><span class="line">        +-&gt; kvm_get_vcpu</span><br><span class="line">              /* 创建vcpu */</span><br><span class="line">          +-&gt; kvm_vm_ioctl(s, KVM_CREATE_VCPU, (void *)vcpu_id)</span><br><span class="line">      +-&gt; kvm_cpu_exec(cpu)</span><br><span class="line">            /* 运行vcpu */</span><br><span class="line">        +-&gt; kvm_vcpu_ioctl(cpu, KVM_RUN, 0)</span><br></pre></td></tr></table></figure>
<h2 id="riscv-H扩展spec分析"><a href="#riscv-H扩展spec分析" class="headerlink" title="riscv H扩展spec分析"></a>riscv H扩展spec分析</h2><p> riscv H扩展的目的是在硬件层面创建出一个虚拟的机器出来，基于此可以支持各种类型的<br> 虚拟化，比如，可以在linux上支持KVM。先不考虑中断和外设，我们看看要创建一个虚拟机<br> 我们需要些什么，我们需要GPR寄存器、系统寄存器以及一个“物理”地址空间，在这个虚拟机<br> 里运行的程序认为这就是他们的全部世界。我们可以把host的GPR和host的系统寄存器给虚拟<br> 机里的程序用，对于每个虚拟机和host，当他们需要运行的时候，由一个更底层的程序把他们<br> 的GRP值和系统寄存器值换到物理GPR和系统寄存器上，这样每次虚拟机和虚拟机切换、虚拟机<br> 和host切换都要切全部寄存器。不同虚拟机不能直接使用host物理地址作为他们的“物理”地址<br> 空间，如果这样，就好小心划分host物理地址，避免虚拟机物理地址之间相互影响，我们会<br> 再加一个层翻译，这层翻译把虚拟机物理地址翻异成host物理地址，虚拟机自身看不到这层<br> 翻译，虚拟机正常做load/store访问(先假设load/store访问的是虚拟机物理地址)，load/store<br> 执行的时候会查tlb，可能做page walk，还可能报缺页异常，这些在虚拟机的世界里都不感知，<br> 查tlb和做page talk是硬件自己搞定的，处理缺页是更加底层的程序搞定的(hypvisor)。<br> 为了支持这层翻译以及相关的异常，就需要在给硬件加相关的寄存器，可以想象，我们要增加<br> 这层翻译对应的页表的基地址寄存器，还要增加对应的异常上下文寄存器，这些寄存器在虚拟<br> 机切换的时候都要切换成对应虚拟机的。</p>
<p> 只有host的时候，只要一层翻译就好，但是如果是运行在虚拟机里的系统，就需要两级翻译，<br> 运行在虚拟机里的系统自己不感知是运行在虚拟机上的，但是，硬件需要知道某个时刻是运行<br> 的是guest还是host的系统，这样硬件需要有一个状态表示，当前运行的是guest还是host的<br> 系统。</p>
<p> riscv的H扩展增加了CPU的状态，增加了一个隐式的V状态，当V=0的时候，CPU的U/M状态还和<br> 之前是一样的，S状态处在HS状态，当V=1的时候，CPU原来的U/S状态变成了VU/VS状态。<br> V状态在中断或异常时由硬件改变，还有一个改变的地方是sret/mret指令。具体的变化逻辑<br> 是: 1. 当在V状态trap进HS时，硬件会把V配置成0; 2. CPU trap进入M状态，硬件会把V配置成0;<br> 3. sret返回时, 恢复到之前的V状态；4. mret返回时, 恢复到之前的V状态。这里说的之前<br> 的V状态，riscv的hstatus寄存器上的SPV(Supervisor Previous Virtualization mode)表示<br> “之前的V状态”，上述的sret和mret从这个寄存器中得到之前的V状态。如前所述，kvm在启动<br> 虚拟机之前会配置hstatus的SPV为1，这样使用sret启动虚拟机后，V状体被置为1。</p>
<p> 增加了hypervisor和guest对应的两组寄存器，其中hypervisor对应的寄存器有: hstatus, hedeleg,<br> hideleg, hvip, hip, hie, hgeip, hgeie, henvcfg, henvcfgh, hounteren, htimedelta, htimedeltah,<br> htval, htinst, hgatp, guest对应的寄存器有：vsstatus, vsip, vsie, vstvec, vsscratch, vsepc,<br> vscause, vstval, vsatp。</p>
<p> 对于这些系统寄存器，我们可以大概分为两类，一类是配置hypvisor的行为的，一类是VS/VU<br> 的映射寄存器。我们一个一个寄存器看下。VS/VU的映射寄存器就是CPU在运行在V状态时使用<br> 的寄存器，这些寄存器基本上是S mode寄存器的翻版，riscv spec提到，当系统运行在V状态<br> 时，硬件的控制逻辑依赖这组vs开头的寄存器，这时对S mode相同寄存器的读写被映射到vs<br> 开头的这组寄存器上。</p>
<p> hedeleg/hideleg表示是否要把HS的中断继续委托到VS去处理，在进入V模式前，如果需要，<br> 就要提前配置好。具体的委托情况可以参考<a href="https://wangzhou.github.io/riscv%E4%B8%AD%E6%96%AD%E5%BC%82%E5%B8%B8%E5%A7%94%E6%89%98%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90/">这里</a><br> 这里需要注意的是，RV协议上提到，当H扩展实现时，VS的几个中断的mideleg对应域段硬件<br> 直接就配置成1了，也就是说默认被代理到HS处理。如果GEILEN非零，也就是有SGEI，那么<br> SGEI也会直接硬件默认代理到HS处理。</p>
<p> hgatp是第二级页表的基地址寄存器。</p>
<p> hvip用于给虚拟机VS mode注入中断，写VSEIP/VSTIP/VSSIP域段，会给VS mode注入相关中断，<br> riscv spec里没有说，注入的中断在什么状态下会的到响应？如果有多个VM实例，中断注入<br> 了哪个实例里?</p>
<p> hip/hie是hypvisor下中断相关的pending和enable控制。hip/hie包含hvip的各个域段，除了<br> 如上的域段，还有一个SGEIP域段。协议上这里写的比较绕，先是总述了hip/hie里各个域段<br> 在不同读写属性下对应的逻辑是怎么样的，然后分开bit去描述。细节的逻辑是，hip.SGEIP<br> 是只读的，只有在hgeip/hgeie表示的vCPU里有中断可以处理时才是1，所以这个域段表示这个<br> 物理CPU上的vCPU是否有外部中断需要处理; hip.VSEIP也是只读的，在hvip.VSEIP是1或者<br> hgeip有pending bit时，hip.VSEIP为1。</p>
<p> hgeip/hgeie是SGEI的pending和enable控制，如果hgeip/hgeie是一个64bit的寄存器，那么<br> 它的1-63bit可以表示1-63个vcpu的SGEI pending/enable bit，每个bit描述直通到该vCPU<br> 上的中断，所以，协议上说要配合中断控制器使用。hgeip是一个只读寄存器。</p>
<p> 所以，VSEIP是一个SEIP的对照中断，而SGEI是一个直通中断的汇集信号。</p>
<p> vcpu怎么响应这个直通的中断？我们把这个逻辑独立出来在这里描述:<br> <a href="https://wangzhou.github.io/riscv-AIA%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">https://wangzhou.github.io/riscv-AIA逻辑分析/</a></p>
<p> 罗列出riscv上所有的中断类型，S mode/M mode/VS mode的外部中断/时钟中断/软件中断<br> 这些一共下来就是9种中断类型，再加上supervisor guest external interrupt。</p>
<p> htval/htinst是HS异常时的参数寄存器。htval用来存放guest page fault的IPA，其他情况<br> 暂时时0，留给以后扩展。两级翻译的具体流程在独立的文档中描述。</p>
<p> 这些寄存器的qemu实现比较有意思，VS在实际运行的时候会把vs开头寄存器中配置的值copy<br> 到S mode的寄存器上，把HS mode的寄存器中原来的值，存在硬件里。qemu上VS实际运行依赖<br> 的还是S mode寄存器上的当前状态，qemu的实现如果和协议一样，qemu在VS时对S mode寄存器<br> 的改写应该同时写到vsxxx这组寄存器上，VS状态的实际控制应该依赖于vsxxx这组寄存器，<br> qemu目前的实现应该逻辑上也是对的。要让guest内核可以直接运行到KVM上，原来使用的<br> 寄存器名字也是不能改变的。</p>
<p> 在退出V状态的时候再把S mode的寄存器上的值保存会VS状态的寄存器上，同时把之前保存<br> 在硬件里的HS mode的寄存器的值写入S mode寄存器。HS工作的时候使用S mode寄存器，同时<br> 使用hypervisor寄存器里的配置信息。</p>
<p> 实际硬件的物理实现可能只需要做一个映射就好，并不需要qemu中类似的拷贝，如下是上面<br> 逻辑的示意图。</p>
<p> 虚拟机开始运行，HS切入VS的示意：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 配置vsxxx寄存器</span><br><span class="line">                   |</span><br><span class="line">                   |</span><br><span class="line">                   v</span><br><span class="line"> +-- vsstatus vsip ... vsatp ---- status  sip  ...  satp ---------+ &lt;---- 寄存器接口</span><br><span class="line"> |                                ^     \                         |</span><br><span class="line"> |                               /       \                        |</span><br><span class="line"> |    3. copy vsxxx to HS       /         \     2. save HS mode   |</span><br><span class="line"> |       register or map vsxxx /           \       register in    |</span><br><span class="line"> |       to HS register       /             \      hardware or    |</span><br><span class="line"> |                           /               \     stop mapping   |</span><br><span class="line"> |                          /                 v                   | &lt;---- 硬件</span><br><span class="line"> |   vsstatus vsip ... vsatp      status_hs  sip_hs ...  satp_hs  |</span><br><span class="line"> |                                                                |</span><br><span class="line"> +----------------------------------------------------------------+</span><br><span class="line"> sret的硬件逻辑完成步骤2和步骤3。</span><br></pre></td></tr></table></figure>

<p> 虚拟机停止运行，VS/VU切入HS：(初始状态是虚拟机跑在S mode寄存器上)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-- vsstatus vsip ... vsatp ---- status  sip  ...  satp ---------+ &lt;---- 寄存器接口</span><br><span class="line">|                                /     ^                         |</span><br><span class="line">|                               /       \                        |</span><br><span class="line">|    1. copy HS mode register  /         \    2. copy HS register|</span><br><span class="line">|       to vsxxx or stop      /           \      saved in hw to  |</span><br><span class="line">|       mapping              /             \     register or do  |</span><br><span class="line">|                           /               \    mapping         |</span><br><span class="line">|                          v                 \                   | &lt;---- 硬件</span><br><span class="line">|   vsstatus vsip ... vsatp      status_hs  sip_hs ...  satp_hs  |</span><br><span class="line">|                                                                |</span><br><span class="line">+----------------------------------------------------------------+</span><br><span class="line">中断或者异常的硬件处理逻辑完成步骤1和步骤2。</span><br></pre></td></tr></table></figure>
<p> 所以，从总体上看，不管是在HS还是VS，实际运行的时候使用的都是S mode的寄存器。当HS是处理<br> hypervisor的业务时，使用hypervisor相关寄存器里的定义。</p>
<p> 新增加的虚拟化相关的指令大概分两类，一类是和虚拟化相关的TLB指令，一类是虚拟化相关的访存指令。<br> 虚拟化扩展和TLB相关的指令有：hfence.vvma和hfence.gvma，虚拟化相关的访存指令有：hlv.xxx, hsv.xxx，<br> 这些指令提供在U/M/HS下的带两级地址翻译的访存功能，也就是虽然V状态没有使能，用这些指令依然可以<br> 得到gva两级翻译后的pa。</p>
<h2 id="qemu-riscv-H扩展基本逻辑"><a href="#qemu-riscv-H扩展基本逻辑" class="headerlink" title="qemu riscv H扩展基本逻辑"></a>qemu riscv H扩展基本逻辑</h2><p> qemu支持riscv H扩展的基本逻辑主要集中在中断和异常的处理逻辑，新增寄存器支持，以及<br> 新增虚拟化相关指令的支持。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv_cpu_do_interrupt</span><br><span class="line">      /* 从V状态进入HS，会把sxxx寄存器保存到vsxxx，把xxx_hs推到sxxx里 */</span><br><span class="line">  +-&gt; riscv_cpu_swap_hypervisor_regs(env)</span><br><span class="line">      /* 保存当前状态 */</span><br><span class="line">  +-&gt; env-&gt;hstatus = set_field(env-&gt;hstatus, HSTATUS_SPVP, env-&gt;priv);</span><br><span class="line">      /* 保存当前V状态 */</span><br><span class="line">  +-&gt; env-&gt;hstatus = set_field(env-&gt;hstatus, HSTATUS_SPV, riscv_cpu_virt_enabled(env));</span><br><span class="line">      /* 保存异常gpa地址 */</span><br><span class="line">  +-&gt; htval = env-&gt;guest_phys_fault_addr;</span><br><span class="line">      /* 后面可以看到cause, 异常pc, tval都是靠S mode寄存器报给软件的, 最后把模式切到S mode */</span><br><span class="line">  +-&gt; riscv_cpu_set_mode(env, PRV_S);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p> 在sret/mret指令里会处理V状态以及寄存器的倒换：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">helper_sret</span><br><span class="line">    /* 在H扩展打开的分支里会有如下的硬件操作 */</span><br><span class="line">  +-&gt; prev_priv = get_field(mstatus, MSTATUS_SPP);</span><br><span class="line">  +-&gt; prev_virt = get_field(hstatus, HSTATUS_SPV);</span><br><span class="line">  +-&gt; hstatus = set_field(hstatus, HSTATUS_SPV, 0);</span><br><span class="line">  +-&gt; mstatus = set_field(mstatus, MSTATUS_SPP, 0);</span><br><span class="line">  +-&gt; mstatus = set_field(mstatus, SSTATUS_SIE, get_field(mstatus, SSTATUS_SPIE));</span><br><span class="line">  +-&gt; mstatus = set_field(mstatus, SSTATUS_SPIE, 1);</span><br><span class="line">  +-&gt; env-&gt;mstatus = mstatus;</span><br><span class="line">  +-&gt; env-&gt;hstatus = hstatus;</span><br><span class="line">      /* 如果之前是V状态使能的，这里要做寄存器的倒换: 把S mode寄存器保存到xxx_hs，把vsxxx寄存器存到S mode寄存器里 */</span><br><span class="line">  +-&gt; riscv_cpu_swap_hypervisor_regs(env);</span><br><span class="line">      /* 使能V状态 */</span><br><span class="line">  +-&gt; riscv_cpu_set_virt_enabled(env, prev_virt);</span><br></pre></td></tr></table></figure>
<p> 新增了vsxxx以及hxxx寄存器的访问代码。新增加的虚拟化相关的指令大概分两类，一类是<br> 和虚拟化相关的TLB指令，一类是虚拟化相关的访存指令，可以直接查看他们的qemu实现,<br> TLB相关的指令依然是刷新全部TLB，访存相关的指令和普通访存指令的实现基本一样，不同<br> 的是在mem_idx上增加了TB_FLAGS_PRIV_HYP_ACCESS_MASK，表示要做两级地址翻译。</p>
<h2 id="运行情况跟踪"><a href="#运行情况跟踪" class="headerlink" title="运行情况跟踪"></a>运行情况跟踪</h2><ol>
<li><p>在第二层qemu的启动命令里加–trace “kvm_*”跟踪第二层qemu中kvm相关的配置，主要是<br>一些kvm相关的ioctl。</p>
</li>
<li><p>在第一层qemu的启动命令里加-d int，观察第一层qemu上虚拟化相关的各种异常，实际<br>上我们用第一层qemu模拟host机器，这里就是观察host机器在虚拟化下的各种异常。</p>
</li>
<li><p>在启动虚拟机的时候，kvm是把代码直接放到host机器上跑，tcg里-d参数的那些调试手段<br>都起不上用处了，但是，guest的主要代码是直接运行在host机器上的，我们这里使用两层<br>qemu，所以，guest的主要代码会跑在第一层qemu上，在第一层qemu加上-d选项是可以看到<br>guest中运行的指令的。</p>
<p>如下是在第一层qemu上增加-d cpu,in_asm，kvm启动guest内核的一段log，我们hack了下<br>cpu的打印，只保留了V状态、pc和hstatus，不然一层qemu的启动就会慢的要命，不过，<br>即使这样，两层qemu完全启动的全部log也有5G大小，我们把注释写到log内部。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">----------------</span><br><span class="line">IN: </span><br><span class="line">Priv: 1; Virt: 0</span><br><span class="line">0xffffffff800155d8:  02153c23          sd              ra,56(a0)      &lt;---- __kvm_riscv_switch_to对应的汇编</span><br><span class="line">0xffffffff800155dc:  04253023          sd              sp,64(a0)            这个是kvm为guest指令指令准备环境。</span><br><span class="line">0xffffffff800155e0:  04353423          sd              gp,72(a0)</span><br><span class="line">0xffffffff800155e4:  04453823          sd              tp,80(a0)</span><br><span class="line">0xffffffff800155e8:  f920              sd              s0,112(a0)</span><br><span class="line">0xffffffff800155ea:  fd24              sd              s1,120(a0)</span><br><span class="line">0xffffffff800155ec:  e54c              sd              a1,136(a0)</span><br><span class="line">0xffffffff800155ee:  e950              sd              a2,144(a0)</span><br><span class="line">0xffffffff800155f0:  ed54              sd              a3,152(a0)</span><br><span class="line">0xffffffff800155f2:  f158              sd              a4,160(a0)</span><br><span class="line">0xffffffff800155f4:  f55c              sd              a5,168(a0)</span><br><span class="line">0xffffffff800155f6:  0b053823          sd              a6,176(a0)</span><br><span class="line">0xffffffff800155fa:  0b153c23          sd              a7,184(a0)</span><br><span class="line">0xffffffff800155fe:  0d253023          sd              s2,192(a0)</span><br><span class="line">0xffffffff80015602:  0d353423          sd              s3,200(a0)</span><br><span class="line">0xffffffff80015606:  0d453823          sd              s4,208(a0)</span><br><span class="line">0xffffffff8001560a:  0d553c23          sd              s5,216(a0)</span><br><span class="line">0xffffffff8001560e:  0f653023          sd              s6,224(a0)</span><br><span class="line">0xffffffff80015612:  0f753423          sd              s7,232(a0)</span><br><span class="line">0xffffffff80015616:  0f853823          sd              s8,240(a0)</span><br><span class="line">0xffffffff8001561a:  0f953c23          sd              s9,248(a0)</span><br><span class="line">0xffffffff8001561e:  11a53023          sd              s10,256(a0)</span><br><span class="line">0xffffffff80015622:  11b53423          sd              s11,264(a0)</span><br><span class="line">0xffffffff80015626:  46853283          ld              t0,1128(a0)</span><br><span class="line">0xffffffff8001562a:  47053303          ld              t1,1136(a0)</span><br><span class="line">0xffffffff8001562e:  6d853383          ld              t2,1752(a0)</span><br><span class="line">0xffffffff80015632:  00000e97          auipc           t4,0            # 0xffffffff80015632</span><br><span class="line">0xffffffff80015636:  0bae8e93          addi            t4,t4,186</span><br><span class="line">0xffffffff8001563a:  46053f03          ld              t5,1120(a0)</span><br><span class="line">0xffffffff8001563e:  100292f3          csrrw           t0,sstatus,t0</span><br><span class="line"></span><br><span class="line"> V      =   0</span><br><span class="line"> pc       ffffffff800155d8</span><br><span class="line"> hstatus  0000000200000000</span><br><span class="line">----------------</span><br><span class="line">IN: </span><br><span class="line">Priv: 1; Virt: 0</span><br><span class="line">0xffffffff80015642:  60031373          csrrw           t1,0x600,t1    &lt;---- 更新hstatus, SPV配置为1，为sret切入V状态做准备。</span><br><span class="line"></span><br><span class="line"> V      =   0</span><br><span class="line"> pc       ffffffff80015642</span><br><span class="line"> hstatus  0000000200000000</span><br><span class="line">----------------</span><br><span class="line">IN: </span><br><span class="line">Priv: 1; Virt: 0</span><br><span class="line">0xffffffff80015646:  106393f3          csrrw           t2,scounteren,t2</span><br><span class="line"></span><br><span class="line"> V      =   0</span><br><span class="line"> pc       ffffffff80015646</span><br><span class="line"> hstatus  00000002002001c0                                            &lt;---- SPV已经为1</span><br><span class="line">----------------</span><br><span class="line">IN: </span><br><span class="line">Priv: 1; Virt: 0</span><br><span class="line">0xffffffff8001564a:  105e9ef3          csrrw           t4,stvec,t4</span><br><span class="line"></span><br><span class="line"> V      =   0</span><br><span class="line"> pc       ffffffff8001564a</span><br><span class="line"> hstatus  00000002002001c0</span><br><span class="line">----------------</span><br><span class="line">IN: </span><br><span class="line">Priv: 1; Virt: 0</span><br><span class="line">0xffffffff8001564e:  14051e73          csrrw           t3,sscratch,a0</span><br><span class="line"></span><br><span class="line"> V      =   0</span><br><span class="line"> pc       ffffffff8001564e</span><br><span class="line"> hstatus  00000002002001c0</span><br><span class="line">----------------</span><br><span class="line">IN: </span><br><span class="line">Priv: 1; Virt: 0</span><br><span class="line">0xffffffff80015652:  141f1073          csrrw           zero,sepc,t5</span><br><span class="line"></span><br><span class="line"> V      =   0</span><br><span class="line"> pc       ffffffff80015652</span><br><span class="line"> hstatus  00000002002001c0</span><br><span class="line">----------------</span><br><span class="line">IN: </span><br><span class="line">Priv: 1; Virt: 0</span><br><span class="line">0xffffffff80015656:  12553c23          sd              t0,312(a0)</span><br><span class="line">0xffffffff8001565a:  14653023          sd              t1,320(a0)</span><br><span class="line">0xffffffff8001565e:  02753023          sd              t2,32(a0)</span><br><span class="line">0xffffffff80015662:  01c53823          sd              t3,16(a0)</span><br><span class="line">0xffffffff80015666:  01d53c23          sd              t4,24(a0)</span><br><span class="line">0xffffffff8001566a:  36853083          ld              ra,872(a0)</span><br><span class="line">0xffffffff8001566e:  37053103          ld              sp,880(a0)</span><br><span class="line">0xffffffff80015672:  37853183          ld              gp,888(a0)</span><br><span class="line">0xffffffff80015676:  38053203          ld              tp,896(a0)</span><br><span class="line">0xffffffff8001567a:  38853283          ld              t0,904(a0)</span><br><span class="line">0xffffffff8001567e:  39053303          ld              t1,912(a0)</span><br><span class="line">0xffffffff80015682:  39853383          ld              t2,920(a0)</span><br><span class="line">0xffffffff80015686:  3a053403          ld              s0,928(a0)</span><br><span class="line">0xffffffff8001568a:  3a853483          ld              s1,936(a0)</span><br><span class="line">0xffffffff8001568e:  3b853583          ld              a1,952(a0)</span><br><span class="line">0xffffffff80015692:  3c053603          ld              a2,960(a0)</span><br><span class="line">0xffffffff80015696:  3c853683          ld              a3,968(a0)</span><br><span class="line">0xffffffff8001569a:  3d053703          ld              a4,976(a0)</span><br><span class="line">0xffffffff8001569e:  3d853783          ld              a5,984(a0)</span><br><span class="line">0xffffffff800156a2:  3e053803          ld              a6,992(a0)</span><br><span class="line">0xffffffff800156a6:  3e853883          ld              a7,1000(a0)</span><br><span class="line">0xffffffff800156aa:  3f053903          ld              s2,1008(a0)</span><br><span class="line">0xffffffff800156ae:  3f853983          ld              s3,1016(a0)</span><br><span class="line">0xffffffff800156b2:  40053a03          ld              s4,1024(a0)</span><br><span class="line">0xffffffff800156b6:  40853a83          ld              s5,1032(a0)</span><br><span class="line">0xffffffff800156ba:  41053b03          ld              s6,1040(a0)</span><br><span class="line">0xffffffff800156be:  41853b83          ld              s7,1048(a0)</span><br><span class="line">0xffffffff800156c2:  42053c03          ld              s8,1056(a0)</span><br><span class="line">0xffffffff800156c6:  42853c83          ld              s9,1064(a0)</span><br><span class="line">0xffffffff800156ca:  43053d03          ld              s10,1072(a0)</span><br><span class="line">0xffffffff800156ce:  43853d83          ld              s11,1080(a0)</span><br><span class="line">0xffffffff800156d2:  44053e03          ld              t3,1088(a0)</span><br><span class="line">0xffffffff800156d6:  44853e83          ld              t4,1096(a0)</span><br><span class="line">0xffffffff800156da:  45053f03          ld              t5,1104(a0)</span><br><span class="line">0xffffffff800156de:  45853f83          ld              t6,1112(a0)</span><br><span class="line">0xffffffff800156e2:  3b053503          ld              a0,944(a0)</span><br><span class="line">0xffffffff800156e6:  10200073          sret                          &lt;---- 配置V状态，跳到sepc，即guest内核首地址。</span><br><span class="line"></span><br><span class="line"> V      =   0</span><br><span class="line"> pc       ffffffff80015656</span><br><span class="line"> hstatus  00000002002001c0</span><br><span class="line">----------------</span><br><span class="line">IN: </span><br><span class="line">Priv: 1; Virt: 1</span><br><span class="line">0x0000000080000000:  5a4d              addi            s4,zero,-13   &lt;---- guest内核首地址，第二层qemu没有配置bios，直接跑内核。</span><br><span class="line">0x0000000080000002:  0ca0106f          j               4298            # 0x800010cc  &lt;--- 跳转到了第2个4K页面上，触发了第二级地址翻译异常。</span><br><span class="line">                                                                                          后面是退出到kvm里解决地址异常，退出到HS mode，</span><br><span class="line"> V      =   1                                                                             V状态清0。(todo: 需要确定下)</span><br><span class="line"> pc       0000000080000000</span><br><span class="line"> V      =   0</span><br><span class="line"> pc       ffffffff800156ec</span><br><span class="line"> hstatus  00000002002001c0</span><br><span class="line"> V      =   0</span><br><span class="line"> pc       ffffffff800156f0</span><br><span class="line"> hstatus  00000002002001c0</span><br><span class="line"> V      =   0</span><br><span class="line"> pc       ffffffff80015780</span><br><span class="line"> hstatus  00000002002001c0</span><br><span class="line"> V      =   0</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>虚拟化</tag>
        <tag>riscv</tag>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv特权级模型</title>
    <url>/riscv%E7%89%B9%E6%9D%83%E7%BA%A7%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="特权级"><a href="#特权级" class="headerlink" title="特权级"></a>特权级</h2><p> 这一节先梳理CPU运行的基本逻辑。CPU在PC处取指令，然后执行指令，指令改变CPU状态，<br> 这个状态周而复。指令改变CPU状态，被改变的有CPU的寄存器以及内存，被改变的寄存器<br> 包括PC，这样CPU就可以完成计算以及指令跳转。</p>
<p> 这个同步变化的模型可能被中断或者异常打断。实际上异常的整体运行情况也符合上面的<br> 模型，指令在执行的时候，会遇到各种各样的异常，当异常发生的时候，硬件通过一组寄存器<br> 向软件提供异常发生时的硬件状态，同时硬件改变PC的状态，使其指向一段预先放好的代码。</p>
<p> 中断异步的打断CPU正在执行的指令，通过一组寄存器向软件提供中断发生时的硬件状态，<br> 同时硬件改变PC的状态，使其指向一段预先放好的代码。</p>
<p> 当中断或异常发生的时候，硬件可以把CPU切换到一个正交的状态上，这些状态对于CPU的<br> 寄存器有着不同的访问权限。目前在riscv处理器上，有U, S, M等CPU模式(本文先不看虚拟化<br> 相关的CPU状体)，它们的编码分别是：0，1，3。</p>
<h2 id="寄存器以及访问方式"><a href="#寄存器以及访问方式" class="headerlink" title="寄存器以及访问方式"></a>寄存器以及访问方式</h2><p> CPU有通用寄存器，也有系统寄存器，其中系统寄存器也叫CSR，control system register。<br> 一般指令可以访问通用寄存器，但是只有特殊的指令才能访问系统寄存器，在riscv上，有<br> 一堆这样的csr指令。这些指令以csrxxx的方式命名，通过寄存器编号做各种读写操作。</p>
<p> 对于CSR的域段，riscv spec上定义了三个标准的读写属性，分别是：WPRI/WLRL/WARL，其中<br> W是write，R是read，P是preserve，I是ignore，L是legal。所以，每个域段含义是，WPRI<br> 对于写来说，不改变值，如果WPRI是reserve的域段，需要接死成0，软件读出这个字段需要<br> 忽略掉；对于WLRL的域段，必须写入一个合法的值，否则可能报非法指令异常，如果上一次<br> 写是一个合法值，那么下次读就是这个合法值，如果上一次写是一个非法值，那么下次读返回<br> 一个随机值，但是这个随机值和写入的非法值和写入非法值之前的值有关系；对于WARL，<br> 可以写入任何值，但是，读出的总是一个合法值，这个性质可以用来做特性检测，比如misa<br> 的extension域段，它表示extension的使能情况，软件是可以写这个bit把相关的特性关掉的，<br> 当一个特性被关掉时，我们可以尝试写相关的bit，看看能否写入，如果可以写入，那么这个<br> 特性是存在的，只是被关掉，如果写入不了，那么平台本来就不支持这个特性。</p>
<h2 id="M-mode寄存器"><a href="#M-mode寄存器" class="headerlink" title="M mode寄存器"></a>M mode寄存器</h2><p> M mode下的寄存器大概可以分为若干类，一类是描述机器的寄存器，一类是和中断异常以及<br> 内存相关的寄存器，一类是时间相关的寄存器，另外还有PMU相关的寄存器。</p>
<p> 描述机器的寄存器有：misa，mvendorid，marchid，mimpid，mhartid。从名字就可以看出，<br> 这些寄存器是机器相关特性的描述，其中最后一个是core的编号。misa描述当前硬件支持的特性，<br> riscv很多特性是可选的，这个寄存器就相当于一个能力寄存器。</p>
<p> 中断异常相关的寄存器有：mstatus, mepc, mtvec, mtval, medeleg, mideleg, mie, mip，mcause。<br> S mode的相关寄存器和这些寄存器基本一样，只是以s开头。这些寄存器我们可以用中断异常<br> 处理的流程把他们串起来，在下面S mode里描述。比较特殊的是medeleg/mideleg寄存器，默认<br> 情况下riscv的中断和异常都在M mode下处理，可以通过配置这两个寄存器把异常或中断委托<br> 在S mode下处理。</p>
<p> 内存管理相关的寄存器有: PMP相关寄存器。</p>
<p> 时间管理相关的寄存器：mtime，mtimecmp。CPU内时钟相关的寄存器。</p>
<p> 我们依次看看这些寄存器的具体定义，在看的时候，尽可能发掘一下这下寄存器定义背后的<br> 逻辑。</p>
<ul>
<li>mstatus</li>
</ul>
<p> status是机器相关的全局状态寄存器。riscv上，虽然各个mode的status寄存器是独立，但是<br> 各个mode的status的各个域段是正交的，基本上是每个域段在每个mode上都有一份。如下是<br> mstatus各个域段的说明：</p>
<p>  MIE   MPIE   MPP   SIE   SPIE   SPP   SXL   UXL   MBE   SBE   UBE   MPRV   MXR   SUM   TVM   TW   TSR   FS   VS   XS   SD</p>
<p> MIE/MPIE/MPP/SIE/SPIE/SPP在spec里是一起描述的。xIE表示全局中断的使能情况，xPIE<br> 表示特权级切换之前x mode全局中断的使能情况，xPP表示特权级切换到x mode之前CPU所处<br> 在的mode。下面具体讲下SIE/SPIE/SPP，MIE/MPIE/MPP的逻辑是一样的。</p>
<p> xIE控制中断的全局使能，这里比较有意思的逻辑是，如果CPU当前运行在x mode，比x mode<br> 特权级低的中断都关了，和更低特权级的中断使能没有关系，比x mode更高的中断都会上报<br> 和更高优先级的中断使能位没有关系。</p>
<p> 在中断或异常导致处理器切到S mode的，硬件会把SIE清0，这个就是关S mode的全局中断，<br> 这样SIE的原始信息就需要被另外保存到SPIE这个寄存器里，只有这样才不会丢信息。SPIE<br> 也用来提供sret返回时，用来恢复SIE的值，硬件在执行sret的时候会把SPIE的值写入SIE,<br> 同时把SPIE写1。</p>
<p> SPP的作用也是类似的，从哪个mode切到目前的mode也是一个重要信息，需要提供给软件。<br> 同样，SPP也用来给sret提供返回mode，硬件取SPP的值作为返回mode，同时更新SPP为系统<br> 支持的最低优先级。</p>
<p> 可以看到SPIE和SPP的语意只在切换进S mode时是有固定一个意义的，比如，sret返回新模式，<br> SPP会写为最低优先级，这个显然不能指示新模式的之前模式是最低优先级。</p>
<p> SXL/UXL描述的是S mode/U mode的寄存器宽度，riscv上这个可以是WARL也可以实现成只读，<br> riscv上搞的太灵活，但是实现的时候，搞成只读就好。</p>
<p> MBE/SBE/UBE是配置M mode、S mode以及U mode下数据load/store的大小端，指令总是小端的。<br> CPU对内存还有隐式的访问，比如page walk的时候访问页表，对于这种访问，CPU怎么理解<br> 对应数据的大小端，S mode页表的大小端也是SBE定义。</p>
<p> MPRV(modify privilege)改变特定特权级下load/store的行为，当MPRV是0时，当前特权级<br> load/store内存的规则不变，当MPRV是1时，使用MPP的值作为load/store内存的特权级，<br> sret/mret返回到非M mode是，顺带把MPRV清0。看起来MPRV只在M mode改变load/store的<br> 行为，难道从S mode陷入M mode(这时MPP是S mode)，如果把MPRV配置成1，M mode就可以用<br> S mode定义的页表去访问？查了下opensbi的代码，在处理misaligned load/store的时候<br> 还真有这样用的，貌似是从内核读引起misaligned load/store的对应指令到opensbi。<br> (相关代码在opensbi的lib/sbi/sbi_unpriv.c里)</p>
<p> MXR(Make executable readable)本质上用来抑制某些异常的产生，MXR是0时load操作只可以<br> 操作readable的地址，MXR是1，把这个约束放松到load操作也可以作用readable和executable<br> 的地址，这个就允许load text段的指令，这个配置在opensbi里是配合上面的MPRV一起用的，<br> 可以看到要在opensbi里load misaligned的load/store指令，就需要有这个配置。</p>
<p> MPRV/MXR是在基本逻辑上，对访存操作的一些限制做放松，从而方便一些M mode的操作。<br> 这个在riscv的spec里有明确的提及，其实就是我们上面举的例子，比如，没有MPRV，M mode<br> 也可以通过软件做page talk得到PA，有了MPRV硬件就可以帮助做这个工作。</p>
<p> SUM(permit supervisor user memory access)，控制S mode对U mode内存的访问。默认情况(SUM=0)<br> 不容许内核态直接访问用户态，SUM=1时，去掉这个限制。对于MPRV=1/MPP=S mode的情况，<br> SUM同样是适用的，MPRV实际上是在原来的隔离机制上打了洞，不过由于MPP只能是更低的<br> 特权级，本来高特权级就可以访问低特权级的资源，MRPV是实现的一个优化，但是，作为<br> 协议设计，只要开了这样的口，以后每个特性与之相关的时候，就都要考虑一下。</p>
<p> TVM(trap virtual memory)控制S mode的satp访问以及sfence.vma/sinval.vma指令是否有效。<br> 默认(TVM=0)是有效的，TVM=1可以关掉S mode如上的功能。目前，还没有看到opensbi里有<br> 使用这个bit。</p>
<p> TW(timeout wait)控制wfi指令的行为，默认(TW=0)情况，wfi可能一直等在当前指令运行<br> 的特权级，当TW=1时，wfi如果超时，会直接触发非法指令异常，如果这个超时时间是0，<br> 那么wfi立马就触发一个非法指令异常。spec上说，wfi的这个特性可以用在虚拟化上，看起来<br> 是用wfi陷入异常，然后可以换一个新的guest进来。目前，还没有看到opensbi里有使用这个bit。<br> 另外，目前的定义是，如果U mode的wfi超时，会有非法指令异常被触发。</p>
<p> TSR(trap sret)控制sret的行为，默认(TSR=0)下，S mode的sret指令是正常执行的，但是<br> 在TSR=1时，S mode执行sret会触发非法指令异常。spec的解释是，sret触发非法指令异常<br> 是为了在不支持H扩展的机器上模拟H扩展。目前，还没有看到opensbi里有使用这个bit。</p>
<p> FS/VS/XS/SD给是软件优化的hint，FS/VS/XS每个都是2bit，描述的是浮点扩展/向量扩展/<br> 所有扩展的状态，因为扩展有可能涉及到系统寄存器，那么在CPU上下文切换的时候就要做<br> 相关寄存器的保存和恢复，FS/VS/XS的状态有off/init/clean/dirty，这样增加了更多的<br> 状态，软件就可以依次做更加精细的管理和优化，SD只有一个bit，表示FS/VS/XS有没有<br> dirty。riscv的spec给出了这些状态的详细定义，这些不是关键特性，以后用的时候再看吧。</p>
<ul>
<li>mtvec</li>
</ul>
<p> mtvec用来存放中断异常时，PC的跳转地址。mtvec的最低两个bit用来描述不同的跳转方式，<br> 目前有两个，一个是直接跳转到mtvec base，一个是跳转到base + 中断号 * 4的地址，后面<br> 这种只是对中断起作用。</p>
<ul>
<li>medeleg/mideleg</li>
</ul>
<p> medeleg/mideleg可以把特定的异常和中断委托到S mode处理。riscv spec里提到，如果把<br> 一个异常委托到S mode，那么发生异常时，要更新的是mstatus里的SPIE/SIE/SPP，但是，<br> 异常在S mode模式处理，也看不到mstatus里的SPIE/SIE/SPP啊？从qemu的实现上看，mstatus<br> 和sstatus其实内部实现就是一个寄存器，只不过结合权限控制对外表现成两个寄存器，那么<br> 这里写mstatus的SPIE/SIE/SPP，在S mode读sstatus的SPIE/SIE/SPP，其实读写的都是相同<br> 的地方。</p>
<p> 软件可以把相应的位置写1，然后再读相关的位置，以此查看相关中断或者异常的代理寄存器<br> 有没有支持，medeleg/mideleg的域段属性是WARL，这个就是说，可以随便写入，但是总是<br> 返回合法值，当对应域段没有实现时，1是无法写入的，所以得到的还是0，反之读到的是1。</p>
<p> medeleg/mideleg不存在默认代理值，不能有只读1的bit存在。</p>
<p> 不能高特权级向低特权级做trap，比如，已经把非法指令异常委托到S mode了，但是在M mode<br> 的时候出现指令异常，那还是在M mode响应这个异常。同级trap是可以的。</p>
<p> 中断一旦被代理就必须在代理mode处理。从这里看中断还是和特权级关系比较近的，在定义<br> 中断的时候，其实已经明确定义了是哪个特权级处理的中断，所以这里的中断委托感觉还是<br> 还是比较假 :)</p>
<ul>
<li>mip/mie</li>
</ul>
<p> mip.MEIP/mip.MTIP是只读，由外部中断控制器或者timer配置和清除。mip.MSIP也是只读，<br> 一个核核写寄存器触发另外核的mip.MSIP。</p>
<p> mip.SEIP/mip.STIP可以通过外接中断控制器或者timer写1，也可以在M mode对他们写1，<br> 以此触发S mode的外部中断和timer中断。riscv spec提到mip.SSIP也可以由平台相关的中断<br> 控制器触发。这里又出现了和status寄存器一样的问题，mip的SEIP/STIP/SSIP域段的位置<br> 和sip域段上SEIP/STIP/SSIP的位置是一样的，所以，riscv spec有提到，如果中断被委托到<br> S mode, sip/sie和mip里对应域段的值是一样的。</p>
<p> 中断的优先级是，先从mode划分，M mode的最高，在同一级中从高到低依次是：external interrupt，<br> software interrupt, timer interrupt。</p>
<ul>
<li>mepc/mcause/mtval/mscratch</li>
</ul>
<p> 这些寄存器的功能相对比较简单，具体的描述在下面S mode里介绍。</p>
<h2 id="S-mode寄存器"><a href="#S-mode寄存器" class="headerlink" title="S mode寄存器"></a>S mode寄存器</h2><p> S mode存在和中断异常以及内存相关的寄存器。我们主要从S mode出发来整理梳理下。</p>
<p> 中断异常相关的寄存器有：sstatus, sepc, stvec, stval, sie, sip，scratch，scause。<br> 内存管理相关的寄存器有: satp</p>
<p> sstatus是S mode的status寄存器，具体的域段有SD/UXL/MXR/SUM/XS/FS/SPP/SPIE/UPIE/SIE/UIE，<br> 其中很多都在mstatus寄存器中已经有介绍。之前没有涉及到的，UIE/UPIE和用户态中断相关。</p>
<p> sie是riscv上定义的各种具体中断的使能状态，每种中断一个bit。sip是对应各种中断的<br> pending状态，每种中断一个bit。基本逻辑和M mode的是一样的，只不过控制的是S mode和<br> U mode的逻辑。</p>
<p> stvec是存放中断异常的跳转地址，当中断或异常发生时，硬件在做相应状态的改动后，就<br> 直接跳到stvec保存的地址，从这里取指令执行。基于这样的定义，软件可以把中断异常向量表<br> 的地址配置到这个寄存器里，中断或者异常的时候，硬件就会把PC跳到中断异常向量表。<br> 当然软件也可以把其他的地址配置到stvec，借用它完成跳转的功能。</p>
<p> sepc是S mode exception PC，就是硬件用来给软件报告发生异常或者中断时的PC的，当异常<br> 发生时，sepc就是异常指令对应的PC，当中断发生的时候，sepc是被中断的指令，比如有A、B<br> 两条指令，中断发生在AB之间、或者和B同时发生导致B没有执行，sepc保存B指令的PC。</p>
<p> scause报告中断或异常发生的原因，当这个寄存器的最高bit是1时表示中断，0表示发成的<br> 是异常。</p>
<p> stval报告中断或异常的参数，当发生非法指令异常时，这个寄存器里存放非法指令的指令编码，<br> 当发生访存异常时，这个寄存器存放的是被访问内存的地址。</p>
<p> scratch寄存器是留给软件使用的一个寄存器。Linux内核使用这个寄存器判断中断或者异常<br> 发生的时候CPU是在用户态还是内核态，当scratch是0时，表示在内核态，否则在用户态。</p>
<p> satp是存放页表的基地址，riscv内核态和用户态分时使用这个页表基地址寄存器，这个寄存器<br> 的最高bit表示是否启用页表。如果启用页表，硬件在执行访存指令的时候，在TLB没有命中<br> 时，就会通过satp做page table walk，以此来找虚拟地址对应的物理地址。</p>
<p> 到此为止，中断或异常发生时需要用到的寄存器都有了。我们下面通过具体的中断或者异常<br> 流程把整个过程串起来。</p>
<h2 id="中断或异常流程"><a href="#中断或异常流程" class="headerlink" title="中断或异常流程"></a>中断或异常流程</h2><p> 中断异常向量表的地址会提前配置到stvec。medeleg/mideleg寄存器需要提前配置好，把<br> 需要在S mode下处理的异常和中断对应的bit配置上。</p>
<p> 当中断或异常发生的时候，硬件把SIE的值copy到SPIE，当前处理器mode写入SPP，SIE清0。<br> sepc存入异常指令地址、或者中断指令地址，scause写入中断或者异常的原因，stval写入<br> 中断或异常的参数，然后通过stvec得到中断异常向量的地址。随后，硬件从中断异常向量<br> 地址取指令执行。(可以参考qemu代码：qemu/target/riscv/cpu.c riscv_cpu_do_interrupt函数)</p>
<p> 以Linux内核为例，riscv的中断异常处理流程，先保存中断或异常发生时的寄存器上下文，<br> 然后根据scause的信息找见具体的中断或异常处理函数执行。具体的软件流程分析可以参考<br> <a href="https://wangzhou.github.io/Linux%E5%86%85%E6%A0%B8riscv-entry-S%E5%88%86%E6%9E%90/">Linux内核riscv entry.S分析</a></p>
<p> 当异常或中断需要返回时，软件可以使用sret指令，sret指令在执行的时候会把sepc寄存器<br> 里保存的地址作为返回地址，使用SPP寄存器里的值作为CPU的mode，把SPIE的值写入SIE，<br> SPIE写1，SPP写入U mode编号。所以，在调用sret前，软件要配置好sepc、SPP、SPIE寄存器<br> 的值。</p>
<h2 id="特权级相关的指令"><a href="#特权级相关的指令" class="headerlink" title="特权级相关的指令"></a>特权级相关的指令</h2><p> 异常相关的指令：ecall、ebreak、sret、mret。ecall和ebreak比较相似，就是使用指令触发<br> ecall或者ebreak异常。ecall异常又可以分为ecall from U mode、ecall from S mode，分别<br> 表示ecall是在CPU U mode还是在S mode发起的。在Linux上，从U mode发起的ecall就是一个<br> 系统调用，软件把系统调用需要的参数先摆到系统寄存器上，然后触发ecall指令，硬件依照<br> 上述的异常流程改变CPU的状态，最终软件执行系统调用代码，参数从系统寄存器上获取。<br> Linux上，特性体系构架的系统调用ABI是软件约定的。sret、mret只是从S mode或者从M mode<br> 返回的不同，其他的逻辑是相同的。</p>
<p> 机器相关的指令：reset、wfi。reset复位整个riscv机器。wfi执行的时候会挂起CPU，直到<br> CPU收到中断，一般是用来降低功耗的。</p>
<p> 内存屏障相关的指令：sfence.vma。sfence.vma和其他体系构架下的TLB flush指令类似，<br> 用来清空TLB，这个指令可以带ASID或address参数，表示清空对应参数标记的TLB，当ASID<br> 或者address的寄存器是X0时，表示对应的参数是无效的。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>超标量处理器rename基本逻辑</title>
    <url>/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8rename%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<h2 id="为什么要寄存器重命名"><a href="#为什么要寄存器重命名" class="headerlink" title="为什么要寄存器重命名"></a>为什么要寄存器重命名</h2><p>指令序列里指令之间存在寄存器依赖的问题，大概分为WAW/WAR/RAW三种类型的寄存器依赖，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add r0, r1, r2       add r0, r1, r2        add r3, r0, r2</span><br><span class="line">...                  ...                   ...</span><br><span class="line">add r0, r3, r4       add r4, r0, r3        add r0, r5, r4</span><br><span class="line"></span><br><span class="line">    WAW                   RAW                  WAR</span><br></pre></td></tr></table></figure>
<p>如上是三种寄存器依赖的示意，如果编译器产生出这样的指令序列，上下两条add指令是不能<br>乱序执行的，因为如果乱序执行就会改变程序的逻辑。上述三种依赖中，只有RAW是真正的依赖，<br>其它两个依赖都可以重新选择第二条add指令的输出寄存器把依赖解除掉。比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add r0, r1, r2       add r3, r0, r2</span><br><span class="line">...                  ...</span><br><span class="line">add r6, r3, r4       add r7, r5, r4</span><br><span class="line"></span><br><span class="line">    WAW                  WAR</span><br></pre></td></tr></table></figure>
<p>我们可以把第二条add指令的输出分别变成r6和r7, 这样它们各自两条add指令相互之间就没有<br>依赖，就可以在处理器内部乱序执行(可以假设处理器内部有多个加法器，这样就可以真正的<br>并发执行)。编译器在分配寄存器的时候是可以做这些优化的，但是寄存器资源是有限的，<br>最后编译器生成的指令序列难免会有WAW/WAR依赖存起。</p>
<p>所谓寄存器重命名其中一个目的就是为了解决如上问题。处理器内部还有一组内部寄存器，<br>一般把程序员可见的寄存器成为架构寄存器，把处理器内部寄存器叫做物理寄存器，物理寄<br>存器的个数远大于构架寄存器，寄存器重命名需要把构架寄存器和物理寄存器对应起来，<br>对于WAW/WAR依赖的情况，就可以把后面指令的输出映射到不同的物理寄存器上。比如:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add r0, r1, r2   rename   add R0, R1, R2</span><br><span class="line">...              -----&gt;   ...           </span><br><span class="line">add r0, r3, r4            add R6, R3, R4</span><br><span class="line">                                        </span><br><span class="line">    WAW</span><br><span class="line">                                        </span><br><span class="line">add r3, r0, r2   rename   add R3, R0, R2</span><br><span class="line">...              -----&gt;   ...</span><br><span class="line">add r0, r5, r4            add R7, R5, R4</span><br><span class="line"></span><br><span class="line">    WAR</span><br></pre></td></tr></table></figure>
<p>这样CPU内部计算时，指令之间就可以乱序进行，在指令执行完再把物理寄存器里的指令输出<br>结果提交到架构寄存器。</p>
<p>超标量处理器内部使用物理寄存器的另一个原因是投机执行，其实在当前PC时刻，处理器内部<br>早就提前投机了很多指令，如果这些指令输出是寄存器，当指令投机执行得到输出值时，只能<br>先保存在物理寄存器，等到指令提交的时候，才能把物理寄存器的值提交到构架寄存器。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                            +-------+   +----+</span><br><span class="line">                  +-----+--&gt;|issue q|--&gt;| EX |\</span><br><span class="line">                  |issue|   +-------+   +----+ \</span><br><span class="line">                  |logic|                       \</span><br><span class="line">+----+   +----+   |     |   +-------+   +----+   \+-----+    +----+</span><br><span class="line">| IF |--&gt;| ID |--&gt;|     |--&gt;|issue q|--&gt;| EX |---&gt;| MEM |---&gt;| WB |</span><br><span class="line">+----+   +----+   |     |   +-------+   +----+   /+-----+    +----+</span><br><span class="line">                  |     |                       /</span><br><span class="line">                  |     |   +-------+   +----+ /</span><br><span class="line">                  +-----+--&gt;|issue q|--&gt;| EX |/</span><br><span class="line">                            +-------+   +----+</span><br><span class="line">          +--+--+--+--+--+--+--+--+--+--+--+--+--+--+</span><br><span class="line">     ROB  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |</span><br><span class="line">          +--+--+--+--+--+--+--+--+--+--+--+--+--+--+</span><br><span class="line">                 ^                 ^  \-------------/</span><br><span class="line">               issue             commit  retired</span><br></pre></td></tr></table></figure>

<h2 id="怎么样做重命名"><a href="#怎么样做重命名" class="headerlink" title="怎么样做重命名"></a>怎么样做重命名</h2><p>重命名需要把执行的指令流中使用的架构寄存器换成物理寄存器，直观上看需要解决的问题<br>有：1. 需要有一种转换的方法; 2. 需要考虑什么时候释放物理寄存器；3. 当投机执行失败<br>时，对于已经分配的物理寄存器以及相关资源，需要有办法回退到投机执行失败点之前。<br>我们下面一个一个问题考虑下。</p>
<p>指令其实就和软件上的函数类似，总有指令的输入和输出，指令使用输入参数做一定操作后，<br>通过输出寄存器输出结果(部分指令比如store没有输出寄存器)。所以，我们只要针对输出<br>寄存器做重命名，即给指令的输出寄存器分配物理寄存器，并把分配导致的构架寄存器和物<br>理寄存器的映射关系保存起来，输入寄存器通过查表就可以得到对应的物理寄存器。</p>
<p>但是，对于一个执行指令流，不同位置的相同构架寄存器对应的物理寄存器必然是不一样的，<br>其实重命名的作用就是造成这种不同，看起来构架寄存器和物理寄存器的映射表(map)似乎不<br>是一一映射的。实际上，map是一个动态的映射，在指令执行流的每个指令上是不断变动的，<br>map的存在是为了在顺序做rename的时候，为后续指令的输入寄存器提供映射到的物理寄存器:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A: ld a0, [a1]        map: a0-&gt;p0, a1-&gt;p1, ...</span><br><span class="line">B: add a2, a0, a3     map: a0-&gt;p0, a1-&gt;p1, a2-&gt;p2, a3-&gt;p3, ...</span><br><span class="line">   ...</span><br><span class="line">C: mov a0, a4         map: a0-&gt;p4, a1-&gt;p1, a2-&gt;p2, a3-&gt;p3, ...</span><br><span class="line">D: add a5, a0, a6     map: a0-&gt;p4, a1-&gt;p1, a2-&gt;p2, a3-&gt;p3, a4-&gt;p5, a5-&gt;p6, a6-&gt;p7...</span><br></pre></td></tr></table></figure>
<p>顺着指令执行流，map不断变化。对应单条指令，map表示这一时刻，系统上，构架寄存器和<br>物理寄存器的映射关系。</p>
<p>《超标量处理器设计》这本书上介绍了三种rename的实现方式，分为使用ROB实现，构架寄存<br>器扩展实现，以及完全使用物理寄存器实现，我们这里只看下最后一种。顾名思义完全使用<br>物理寄存器的实现方式下，构架寄存器和物理寄存器的物理实现只有一组寄存器，我们这里<br>就叫这组寄存器是物理寄存器，而所谓构架寄存器，只是物理寄存器的映射(别名)。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     add a5, a0, a6    mov a0, a4   add a2, a0, a3  ld a0, [a1]</span><br><span class="line">            v              v              v              v</span><br><span class="line">     +-------------------------------------------------------------------------+</span><br><span class="line">ROB  |      D       |      C       |      B       |      A       |             |</span><br><span class="line">     +-------------------------------------------------------------------------+</span><br><span class="line">                                                         ^                  </span><br><span class="line">                                                       commit        retired</span><br><span class="line"></span><br><span class="line">     add a5, a0, a6    mov a0, a4   add a2, a0, a3  ld a0, [a1]</span><br><span class="line">            v              v              v              v</span><br><span class="line">     +-------------------------------------------------------------------------+</span><br><span class="line">ROB  |      D       |      C       |      B       |      A       |             |</span><br><span class="line">     +-------------------------------------------------------------------------+</span><br><span class="line"></span><br><span class="line">                           ^               </span><br><span class="line">                         commit        retired    a_to_p map: a0-&gt;p0, a2-&gt;p2 ...</span><br><span class="line"></span><br><span class="line">     add a5, a0, a6    mov a0, a4   add a2, a0, a3  ld a0, [a1]</span><br><span class="line">            v              v              v              v</span><br><span class="line">     +-------------------------------------------------------------------------+</span><br><span class="line">ROB  |      D       |      C       |      B       |      A       |             |</span><br><span class="line">     +-------------------------------------------------------------------------+</span><br><span class="line"></span><br><span class="line">         retired    a_to_p map: a0-&gt;p4, a2-&gt;p2, a5-&gt;p6 ...</span><br></pre></td></tr></table></figure>
<p>上面第一幅图A/B/C/D四条指令都处于投机执行阶段，对应的p0-p7是一定不会映射到架构寄<br>存器上的。第二幅图中，A/B指令被提交，硬件用另一个映射表(a_to_p map)表示架构寄存器<br>实际对应的物理寄存器，可见只要在指令提交的时候，更新输出寄存器在a_to_p map上的映射<br>就好，根据a_to_p map，访问a0/a2架构寄存器实际访问的是物理寄存器p0/p2。第三幅图里，<br>C/D指令提交(我们假设一拍提交两条指令)，C指令的输出寄存器a0被重新映射为p4，同时p0<br>被释放。</p>
<p>如上已经提到了物理寄存器释放的问题。理论上讲，当一个物理寄存器不再有后续指令使用<br>时，这个物理寄存器就可以释放了，实践上一般把这个条件放松为，当物理寄存器对应的架构<br>寄存器更新时，就可以把之前的物理寄存器释放掉。</p>
<p>我们具体考虑硬件实现时的“数据结构”，上面已经知道我们需要两个映射表，一个记录构架<br>寄存器和物理寄存器的映射关系(map)，另一个记录构架寄存器访问时实际访问的物理寄存器<br>(a_to_p map)，前者是处理器内部寄存器重命名时使用，后者是在处理器外部访问构架寄存<br>器时使用。指令的构架寄存器被rename成物理寄存器后，相关的信息应该保存在指令流中每个<br>指令对应的ROB内(注意，ROB对应的是指令流，而不是二进制文件中的每条指令，比如一个<br>几条指令组成的循环，执行的时候会在ROB里被“展开”)。为了释放物理寄存器，每个ROB里<br>还要保存对应指令的输出架构寄存器映射的上一个物理寄存器，这样才能在这个指令退休时<br>释放相关物理寄存器，相关的信息在rename的时候拿到并保存到对应的ROB里，这样在map里<br>就应该增加相关信息的维护，就是map里不但记录构架寄存器到物理寄存器的映射，它还记录<br>构架寄存器上次是映射到哪个物理寄存器上的。</p>
<p>如下图，map是value有两个值，下面只写出了我们专注的值，可以看到C指令的p4,p0表示a0<br>上次是map到p0的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A: ld a0, [a1]        map: a0-&gt;p0,p?, a1-&gt;p1, ...</span><br><span class="line">B: add a2, a0, a3     map: a2-&gt;p2,p?, a0-&gt;p0,p?, a1-&gt;p1, a3-&gt;p3 ...</span><br><span class="line">   ...</span><br><span class="line">C: mov a0, a4         map: a0-&gt;p4,p0, a1-&gt;p1, a2-&gt;p2,p?, a3-&gt;p3, a4-&gt;p5 ...</span><br><span class="line">D: add a5, a0, a6     map: a0-&gt;p4,p0, a1-&gt;p1, a2-&gt;p2,p?, a3-&gt;p3, a4-&gt;p5, a5-&gt;p6,p?, a6-&gt;p7...</span><br></pre></td></tr></table></figure>

<p>ROB里相关的数据结构示意如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     add a5, a0, a6    mov a0, a4   add a2, a0, a3  ld a0, [a1]</span><br><span class="line">            v              v              v              v</span><br><span class="line">     +-------------------------------------------------------------------------+</span><br><span class="line">ROB  |      D       |      C       |      B       |      A       |             |</span><br><span class="line">     |              |              |              |              |             |</span><br><span class="line">     |  a5-&gt;p6,p?   |   a0-&gt;p4,p0  |   a2-&gt;p2,p?  |   a0-&gt;p0,p?  |             |</span><br><span class="line">     |  a0-&gt;p4      |   a4-&gt;p5     |   a0-&gt;p0     |   a1-&gt;p1     |             |</span><br><span class="line">     |  a6-&gt;p7      |              |   a3-&gt;p3     |              |             |</span><br><span class="line">     +-------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<h2 id="超标量处理器要考虑的问题"><a href="#超标量处理器要考虑的问题" class="headerlink" title="超标量处理器要考虑的问题"></a>超标量处理器要考虑的问题</h2><p>我们下考虑标量处理器的情况，就是一拍rename一条指令。考虑一个两输入一输出的指令，<br>硬件在这一拍需要做的事有：1. 根据输入构架寄存器，在map表里找到对应的物理寄存器，<br>然后更新对应指令ROB里的映射信息; 2. 对于输出寄存器，需要在物理寄存器空闲列表(free list)<br>里找一个空闲的物理寄存器，然后把输出构架寄存器到物理寄存器的映射写入map，同时还要<br>更新该指令对应ROB里的输出寄存器的映射信息; 3. 硬件需要从map表里读到输出寄存器上次<br>映射的物理寄存器，并把这个信息更新到该指令对应ROB的映射信息里。</p>
<p>如上的分析中，对应map表，需要3个读口和一个写口，ROB也需要出相关的接口用于映射信息<br>更新。这里的读口和写口可以理解为，硬件读写特定信息时的专门接口，因为硬件上各个读写<br>操作是并行执行的，所以对于特定信息的特定操作需要专门接口。作为软件人员，我们先姑且<br>这样理解硬件，目前并不确定如上操作2和操作3中对同一位置的读写操作是否可以在一拍内<br>完成。</p>
<p>可以看到如上都是直接对map表的读写，但是，在超标量处理下，一拍需要rename多条指令，<br>比如一拍rename四条指令，我们考虑这个时候硬件实现的方式，可以看出来上面的处理方式<br>已经无法使用，我们从RAW/WAW/WAR三种依赖的rename以及构架寄存器的前次物理寄存器的映<br>射更新逻辑分析每种情况对应的处理办法。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1: add r0, r1, r2   rename    add p0, p4, p5</span><br><span class="line">2: add r0, r3, r0   ------&gt;   add p1, p6, p0</span><br><span class="line">3: sub r5, r4, r6             sub p2, p7, p8</span><br><span class="line">4: sub r0, r8, r9             sub p3, p9, p10</span><br></pre></td></tr></table></figure>
<p>超标量处理器需要在一拍内把如上的四条指令重命名为右边所示的情况, 并且更新map表的<br>相关内容。如果这四条指令相互之间没有依赖，如上的分析结果还是成立的，只不过之前是<br>对一条指令操作，现在是并行对四条指令操作。</p>
<p>但是如果这四条指令之间有依赖，情况就会不一样。如上，第一条指令和第二条指令存起RAW<br>和WAW的依赖，我们先看RAW依赖。因为第一条指令的输出物理寄存器是这一拍才从free list<br>里拿到的，所以第二条指令的输入寄存器r0所对应的物理寄存器显然应该使用直接从free list<br>里拿到的p0，而不是从map表里读取。</p>
<p>对于输出寄存器的rename，只要一拍同时在free list里取出四个空闲的物理寄存器，把四个<br>输出构架寄存器和物理寄存器做映射，结果分别写入四条指令对应的ROB里。但是，map里的<br>信息要怎么更新？如果没有WAW依赖，直接把四个不同的架构寄存器和物理寄存器的映射写入<br>map表就好。但是，如果存在WAW依赖，这四条指令里会有相同的输出架构寄存器，而map表里<br>每个架构寄存器一个时刻只对应一个物理寄存器。map表的作用是在rename阶段为后续指令的<br>输入寄存器提供对应物理寄存器的查找，所以map表里的内容是针对每条指令动态变化的，如<br>果WAW依赖之间有指令的输出来自WAW依赖指令的输出，那么这就又构成了一个RAW的依赖，正<br>如上面提到的，RAW依赖中的后一条指令的输入寄存器重命名不应该从map表里读，而是直接<br>使用被依赖的指令从free list里拿到的物理寄存器，所以，当存在WAW依赖时，只要直接把<br>最后WAW依赖里最后一条指令的rename信息写入map表就好。以分组的视角看这个问题，把一<br>拍执行的四条指令看成一组指令，这种情况下，map表的作用是以组为粒度给后续组中的指令<br>提供输入寄存器重命名的信息。</p>
<p>WAR依赖并不影响rename的逻辑, 对于第一条指令中的输入寄存器，如果没有RAW，就从map表<br>里读rename信息，如果存在RAW，就按照上面RAW的方式处理，对于后一条指令里的输出寄存器，<br>也是看和其它指令之间有没有WAW，完全按照上面的逻辑处理就可以。</p>
<p>在rename的时候，还要从map表里读到输出构架寄存器上次映射的物理寄存器，然后把这个信<br>息写入对应指令的ROB。没有WAW依赖时，这个信息还是读map表获取，WAW依赖存在时，直接<br>按照依赖关系拿到这个信息。</p>
<h2 id="重命名恢复"><a href="#重命名恢复" class="headerlink" title="重命名恢复"></a>重命名恢复</h2><p>超标量处理存在投机执行，投机执行的指令按照上面讲的进行rename，rename过程中会占据<br>各种资源，当处理器可以确认投机失败时，就需要丢弃投机执行的指令，并且释放相关的资<br>源，我们看下rename相关的资源要如何释放。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">             out of order execute</span><br><span class="line">            /---------------------\</span><br><span class="line">     +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+</span><br><span class="line">ROB  |  |  |  |  |  |j |  |  |  |  |  |  |  |  |  |  |  |</span><br><span class="line">     +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+</span><br><span class="line">     -&gt;--/           |              \---------/ \--------&gt;</span><br><span class="line">  input_insn         |               completed   retired</span><br><span class="line">                     v</span><br><span class="line">                 save map to checkpoint</span><br></pre></td></tr></table></figure>
<p>上面是ROB的一个示意图，硬件取指令后从input_insn的位置不断把指令放入ROB，j表示一条<br>跳转指令，放进ROB的指令都已经完成了rename，处理器内部可能有很多指令在并发乱序执行，<br>有的指令可能已经执行完处于完成状态(completed)，有的指令已经提交处于退休状态(retired)。<br>当一个时刻处理器发现j这个跳转指令分支预测做错了，就是input_insn和j指令之间投机执<br>行的指令取错了，处理器应该停止继续fetch指令，并丢弃j指令之前的指令，map表应该恢复<br>到j指令rename时的状态，然后从正确的跳转地址继续取指令执行。</p>
<p>如上所示的是用checkpoint做map恢复的方式，这种方式很直白，对于每个跳转指令，在其<br>rename的时候都把当时的map表保存到checkpoint里，遇到需要回退的时候就直接用保存起来<br>的checkpoint恢复map。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     +-------------------------------------------------------------------------+</span><br><span class="line">ROB  |      D       |      C       |      B       |      jump    |             |</span><br><span class="line">     |  a5-&gt;p6,p1   |   a0-&gt;p4,p3  |   a2-&gt;p2,p5  |   a0-&gt;p0,p7  |             |</span><br><span class="line">     +-------------------------------------------------------------------------+</span><br><span class="line">        -&gt;--/</span><br><span class="line">     input_insn</span><br><span class="line"></span><br><span class="line">        map3 &lt;---------  map2  &lt;-------  map1  &lt;---------  map</span><br><span class="line">             ---------&gt;        -------&gt;        ---------&gt;</span><br></pre></td></tr></table></figure>
<p>如上所示还有一种walk的方法，就是按照ROB里保存的“前一个映射的物理寄存器”这个信息<br>反向的一步一步回退到jump时的map。当前的map表是map3, 从jump指令到D指令，每个指令<br>rename的过程，就是修改map表上，当前指令输出架构寄存器到物理寄存器的映射的过程，<br>只要根据ROB里的信息，把map3一步一步改回map即可。比如，D指令ROB a5-&gt;p6,p1表示D指令<br>rename的结果是把a5映射到物理寄存器p6，但是a5之前是被映射到p1上的，那么，就在map3<br>的基础上把a5改映射到p1，同理把a0映射到p3，a2映射到p5，就得到了jump rename后的map表。</p>
<p>书中还介绍了利用a_to_p map表还原map表的方法。当检测到分支预测失败的时候，继续执行<br>分支指令前的指令，直到到出错的分支指令，这时a_to_p map表的值和分支指令对应的map<br>表的值是完全一样的，直接把a_to_map的值拷贝到map就可以完成恢复。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>opensbi逻辑分析</title>
    <url>/opensbi%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> riscv构架定义了一套SBI标准(supervisor binary interface)，顾名思义，就是特权态软件<br> 和运行机器的一套二进制接口，这个标准其实是把机器行为抽象了，特权态软件通过这套<br> 二进制标准向底层机器请求服务，这个特权态软件包括host上的内核，也包括跑在guest上<br> 的内核。</p>
<p> riscv SBI标准的链接是：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3Jpc2N2LW5vbi1pc2EvcmlzY3Ytc2JpLWRvYw==">https://github.com/riscv-non-isa/riscv-sbi-doc<i class="fa fa-external-link-alt"></i></span></p>
<p> opensbi是这个标准的一个实现。opensbi的代码组织上，把sbi公共代码抽象出来形成两个<br> 库：libsbi, libsbiutils，不同平台调用这两个库的代码，和平台相关代码构建出具体平台<br> 上的bios二进制。</p>
<p> 对于qemu的virt平台，相关的代码在：opensbi/platform/generic, 编译生成的二进制在<br> opensbi/build/platform/generic/firmware/。</p>
<p> 编译会生成三种类型的固件，分别是：fw_jump.elf, fw_load.elf, fw_dynamic.elf。<br> 这三种固件的区别是：fw_jump.elf跳到指定地址运行下一个阶段的代码，fw_load.elf会<br> 把下一个阶段的二进制，通常是Linux内核，一起打包到fw_load.elf固件里，fw_dynamic.elf<br> 定义了上一个程序以及下一个程序和fw_dynamic.elf的接口，如下的代码分析可以清楚的<br> 看到这一点。</p>
<h2 id="编译运行"><a href="#编译运行" class="headerlink" title="编译运行"></a>编译运行</h2><p> 我们可以这样编译出qemu virt平台的opensbi二进制:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">make all PLATFORM=generic CROSS_COMPILE=riscv64-linux-gnu- PLATFORM_RISCV_XLEN=64</span><br></pre></td></tr></table></figure>
<p> 如果系统上已经有riscv的qemu，我们可以这样运行下编译好的固件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">~/repos/qemu/build/qemu-system-riscv64 -M virt -m 256M -nographic -bios /home/sherlock/repos/opensbi/build/platform/generic/firmware/fw_payload.elf</span><br></pre></td></tr></table></figure>

<p> 大概的运行效果大概是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OpenSBI v1.1-2-gcaa5eea</span><br><span class="line">   ____                    _____ ____ _____</span><br><span class="line">  / __ \                  / ____|  _ \_   _|</span><br><span class="line"> | |  | |_ __   ___ _ __ | (___ | |_) || |</span><br><span class="line"> | |  | | &#x27;_ \ / _ \ &#x27;_ \ \___ \|  _ &lt; | |</span><br><span class="line"> | |__| | |_) |  __/ | | |____) | |_) || |_</span><br><span class="line">  \____/| .__/ \___|_| |_|_____/|____/_____|</span><br><span class="line">        | |</span><br><span class="line">        |_|</span><br><span class="line"></span><br><span class="line">Platform Name             : riscv-virtio,qemu</span><br><span class="line">Platform Features         : medeleg</span><br><span class="line">Platform HART Count       : 1</span><br><span class="line">Platform IPI Device       : aclint-mswi</span><br><span class="line">Platform Timer Device     : aclint-mtimer @ 10000000Hz</span><br><span class="line">Platform Console Device   : uart8250</span><br><span class="line">Platform HSM Device       : ---</span><br><span class="line">Platform Reboot Device    : sifive_test</span><br><span class="line">Platform Shutdown Device  : sifive_test</span><br><span class="line">Firmware Base             : 0x80000000</span><br><span class="line">Firmware Size             : 284 KB</span><br><span class="line">Runtime SBI Version       : 1.0</span><br><span class="line"></span><br><span class="line">Domain0 Name              : root</span><br><span class="line">Domain0 Boot HART         : 0</span><br><span class="line">Domain0 HARTs             : 0*</span><br><span class="line">Domain0 Region00          : 0x0000000002000000-0x000000000200ffff (I)</span><br><span class="line">Domain0 Region01          : 0x0000000080000000-0x000000008007ffff ()</span><br><span class="line">Domain0 Region02          : 0x0000000000000000-0xffffffffffffffff (R,W,X)</span><br><span class="line">Domain0 Next Address      : 0x0000000080200000</span><br><span class="line">Domain0 Next Arg1         : 0x0000000082200000</span><br><span class="line">Domain0 Next Mode         : S-mode</span><br><span class="line">Domain0 SysReset          : yes</span><br><span class="line"></span><br><span class="line">Boot HART ID              : 0</span><br><span class="line">Boot HART Domain          : root</span><br><span class="line">Boot HART Priv Version    : v1.12</span><br><span class="line">Boot HART Base ISA        : rv64imafdch</span><br><span class="line">Boot HART ISA Extensions  : time,sstc</span><br><span class="line">Boot HART PMP Count       : 16</span><br><span class="line">Boot HART PMP Granularity : 4</span><br><span class="line">Boot HART PMP Address Bits: 54</span><br><span class="line">Boot HART MHPM Count      : 16</span><br><span class="line">Boot HART MIDELEG         : 0x0000000000001666</span><br><span class="line">Boot HART MEDELEG         : 0x0000000000f0b509</span><br><span class="line"></span><br><span class="line">Test payload running</span><br><span class="line">QEMU: Terminated</span><br></pre></td></tr></table></figure>

<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p> 我们把代码分析直接以注释的形式写到代码里，固件的代码是从fw_base.S开始执行的，我们<br> 也从这个文件开始注释，需要其他文件的时候，我们直接嵌入其他文件的注释，我们加的<br> 注释以“n:”开头。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * SPDX-License-Identifier: BSD-2-Clause</span><br><span class="line"> *</span><br><span class="line"> * Copyright (c) 2019 Western Digital Corporation or its affiliates.</span><br><span class="line"> *</span><br><span class="line"> * Authors:</span><br><span class="line"> *   Anup Patel &lt;anup.patel@wdc.com&gt;</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">#include &lt;sbi/riscv_asm.h&gt;</span><br><span class="line">#include &lt;sbi/riscv_encoding.h&gt;</span><br><span class="line">#include &lt;sbi/riscv_elf.h&gt;</span><br><span class="line">#include &lt;sbi/sbi_platform.h&gt;</span><br><span class="line">#include &lt;sbi/sbi_scratch.h&gt;</span><br><span class="line">#include &lt;sbi/sbi_trap.h&gt;</span><br><span class="line"></span><br><span class="line">#define BOOT_STATUS_RELOCATE_DONE	1</span><br><span class="line">#define BOOT_STATUS_BOOT_HART_DONE	2</span><br><span class="line"></span><br><span class="line">.macro	MOV_3R __d0, __s0, __d1, __s1, __d2, __s2</span><br><span class="line">	add	\__d0, \__s0, zero</span><br><span class="line">	add	\__d1, \__s1, zero</span><br><span class="line">	add	\__d2, \__s2, zero</span><br><span class="line">.endm</span><br><span class="line"></span><br><span class="line">.macro	MOV_5R __d0, __s0, __d1, __s1, __d2, __s2, __d3, __s3, __d4, __s4</span><br><span class="line">	add	\__d0, \__s0, zero</span><br><span class="line">	add	\__d1, \__s1, zero</span><br><span class="line">	add	\__d2, \__s2, zero</span><br><span class="line">	add	\__d3, \__s3, zero</span><br><span class="line">	add	\__d4, \__s4, zero</span><br><span class="line">.endm</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * If __start_reg &lt;= __check_reg and __check_reg &lt; __end_reg then</span><br><span class="line"> *   jump to __pass</span><br><span class="line"> */</span><br><span class="line">.macro BRANGE __start_reg, __end_reg, __check_reg, __jump_lable</span><br><span class="line">	blt	\__check_reg, \__start_reg, 999f</span><br><span class="line">	bge	\__check_reg, \__end_reg, 999f</span><br><span class="line">	j	\__jump_lable</span><br><span class="line">999:</span><br><span class="line">.endm</span><br><span class="line"></span><br><span class="line">	.section .entry, &quot;ax&quot;, %progbits</span><br><span class="line">	.align 3</span><br><span class="line">	.globl _start</span><br><span class="line">	.globl _start_warm</span><br><span class="line">_start:</span><br><span class="line">	/* n: 把a0,a1,a2的值先保存到s0,s1,s2, 下面的fw_boot_hart要用a0,a1,a2 */</span><br><span class="line">	/* Find preferred boot HART id */</span><br><span class="line">	MOV_3R	s0, a0, s1, a1, s2, a2</span><br><span class="line">	/*</span><br><span class="line">	 * n: 调用这个函数获取boot hart的id，对于fw_dynamic, fw_jump, fw_payload</span><br><span class="line">	 *    三种类型的bios, boot hart是不一样的，fw_jump和fw_payload这里直接返回</span><br><span class="line">	 *    -1, fw_dynamic这种bios情况有点不一样，启动fw_dynamic的上一级程序会把</span><br><span class="line">	 *    参数放到struct fw_dynamic_info里，用a2传给opensbi，根据具体代码可以</span><br><span class="line">	 *    看出，根据fw_dynamic_info version不同，boot hart是不一样的，version1</span><br><span class="line">	 *    是固定的boot hart, version2返回-1，没有定义boot hart。</span><br><span class="line">	 */</span><br><span class="line">	call	fw_boot_hart</span><br><span class="line">	/* n: boot hart id放到a6里 */</span><br><span class="line">	add	a6, a0, zero</span><br><span class="line">	/* n: 这里恢复a0,a1,a2的值 */</span><br><span class="line">	MOV_3R	a0, s0, a1, s1, a2, s2</span><br><span class="line">	li	a7, -1</span><br><span class="line">	/* n: 如果boot hart id是-1，使用lottery的方式选择boot hart */</span><br><span class="line">	beq	a6, a7, _try_lottery</span><br><span class="line">	/*</span><br><span class="line">	 * n: 如果是固定的boot hart, a6是上一级定义的boot hart，a0是当前hart id,</span><br><span class="line">	 *    这里可以参考qemu的代码：hw/riscv/boot.c, reset_vec定义的地方，第三条</span><br><span class="line">	 *    指令获取当前hart的hart id值。两者不相等，说明不是指定boot hart，跳到</span><br><span class="line">	 *    重定位结束的位置。</span><br><span class="line">	 */</span><br><span class="line">	/* Jump to relocation wait loop if we are not boot hart */</span><br><span class="line">	bne	a0, a6, _wait_relocate_copy_done</span><br><span class="line">_try_lottery:</span><br><span class="line">	/*</span><br><span class="line">	 * n: 多个核执行到了这里，只要启动核去做二进制的重定位就好。其他核到重定位</span><br><span class="line">	 *    结束的位置就好。</span><br><span class="line">	 * </span><br><span class="line">	 *    lla和la有什么区别?</span><br><span class="line">	 */</span><br><span class="line">	/* Jump to relocation wait loop if we don&#x27;t get relocation lottery */</span><br><span class="line">	lla	a6, _relocate_lottery</span><br><span class="line">	li	a7, 1</span><br><span class="line">	/*</span><br><span class="line">	 * n: a6指向的地址上的值(_relocate_lottery的地址)做原子加1，_relocate_lottery</span><br><span class="line">	 *    的老值写入a6。</span><br><span class="line">	 */</span><br><span class="line">	amoadd.w a6, a7, (a6)</span><br><span class="line">	/*</span><br><span class="line">	 * n: _relocate_lottery不等于0，就跳到boot hart做完重定位的地方。如果多个</span><br><span class="line">	 *    核一起启动执行，只有最先执行上面原子加指令的核的a6(_relocate_lottery</span><br><span class="line">	 *    初始值)是0，所以，后续执行到这里的核都是从核，直接跳到重定位完成的</span><br><span class="line">	 *    地址。</span><br><span class="line">	 */</span><br><span class="line">	bnez	a6, _wait_relocate_copy_done</span><br><span class="line"></span><br><span class="line">	/* n: t0是_load_start这个符号的地址，这个地址存的是_fw_start的值 */</span><br><span class="line">	/* Save load address */</span><br><span class="line">	lla	t0, _load_start</span><br><span class="line">	/*</span><br><span class="line">	 * n: t1是_fw_start这个符号的地址，三种fw中，_fw_start这个符号都在二进制</span><br><span class="line">	 *    的起始位置，所以，这里取_fw_start的运行时的地址，其实就是二进制实际</span><br><span class="line">	 *。  被加载到的地址。</span><br><span class="line">	 */</span><br><span class="line">	lla	t1, _fw_start</span><br><span class="line">	/*</span><br><span class="line">	 * n: 保存实际二进制实际加载位置到_load_start, qemu virt平台上的链接地址</span><br><span class="line">	 *    和实际加载地址是一样的，都是内存0x80000000这个地址，但是，如果bios</span><br><span class="line">	 *    是加载到其他地方，比如flash里，链接地址和实际加载位置就是不一样的。</span><br><span class="line">	 *    这个时候就需要做relocate，把二进制从实际加载位置拷贝到链接地址。</span><br><span class="line">	 */</span><br><span class="line">	REG_S	t1, 0(t0)</span><br><span class="line"></span><br><span class="line">#ifdef FW_PIC</span><br><span class="line">	/*</span><br><span class="line">	 * n: opensbi编译生成最终bios的二进制以及sbi相关的库，最终二进制和sbi库</span><br><span class="line">	 *    可以是动态链接的，那么这就需要对于动态链接的符号做下重定位。</span><br><span class="line">	 *</span><br><span class="line">	 *    这块的内容相对独立，可以先跳过。</span><br><span class="line">	 */</span><br><span class="line">	/* relocate the global table content */</span><br><span class="line">	lla	t0, _link_start</span><br><span class="line">	REG_L	t0, 0(t0)</span><br><span class="line">	/* n: 实际加载地址 - 链接地址 */</span><br><span class="line">	/* t1 shall has the address of _fw_start */</span><br><span class="line">	sub	t2, t1, t0</span><br><span class="line">	lla	t3, _runtime_offset</span><br><span class="line">	REG_S	t2, (t3)</span><br><span class="line">	/* n: 对于PIC的情况，只看这一个段就好了？*/</span><br><span class="line">	lla	t0, __rel_dyn_start</span><br><span class="line">	lla	t1, __rel_dyn_end</span><br><span class="line">	/* n: 这两个符号的地址想等，表示没有这个段 */</span><br><span class="line">	beq	t0, t1, _relocate_done</span><br><span class="line">	j	5f</span><br><span class="line">2:</span><br><span class="line">	REG_L	t5, -(REGBYTES*2)(t0)	/* t5 &lt;-- relocation info:type */</span><br><span class="line">	li	t3, R_RISCV_RELATIVE	/* reloc type R_RISCV_RELATIVE */</span><br><span class="line">	bne	t5, t3, 3f</span><br><span class="line">	REG_L	t3, -(REGBYTES*3)(t0)</span><br><span class="line">	REG_L	t5, -(REGBYTES)(t0)	/* t5 &lt;-- addend */</span><br><span class="line">	add	t5, t5, t2</span><br><span class="line">	add	t3, t3, t2</span><br><span class="line">	REG_S	t5, 0(t3)		/* store runtime address to the GOT entry */</span><br><span class="line">	j	5f</span><br><span class="line"></span><br><span class="line">3:</span><br><span class="line">	lla	t4, __dyn_sym_start</span><br><span class="line"></span><br><span class="line">4:</span><br><span class="line">	REG_L	t5, -(REGBYTES*2)(t0)	/* t5 &lt;-- relocation info:type */</span><br><span class="line">	srli	t6, t5, SYM_INDEX	/* t6 &lt;--- sym table index */</span><br><span class="line">	andi	t5, t5, 0xFF		/* t5 &lt;--- relocation type */</span><br><span class="line">	li	t3, RELOC_TYPE</span><br><span class="line">	bne	t5, t3, 5f</span><br><span class="line"></span><br><span class="line">	/* address R_RISCV_64 or R_RISCV_32 cases*/</span><br><span class="line">	REG_L	t3, -(REGBYTES*3)(t0)</span><br><span class="line">	li	t5, SYM_SIZE</span><br><span class="line">	mul	t6, t6, t5</span><br><span class="line">	add	s5, t4, t6</span><br><span class="line">	REG_L	t6, -(REGBYTES)(t0)	/* t0 &lt;-- addend */</span><br><span class="line">	REG_L	t5, REGBYTES(s5)</span><br><span class="line">	add	t5, t5, t6</span><br><span class="line">	add	t5, t5, t2		/* t5 &lt;-- location to fix up in RAM */</span><br><span class="line">	add	t3, t3, t2		/* t3 &lt;-- location to fix up in RAM */</span><br><span class="line">	REG_S	t5, 0(t3)		/* store runtime address to the variable */</span><br><span class="line"></span><br><span class="line">5:</span><br><span class="line">	/* n: 循环处理.rel.dyn这个段里的信息 */</span><br><span class="line">	addi	t0, t0, (REGBYTES*3)</span><br><span class="line">	ble	t0, t1, 2b</span><br><span class="line">	j	_relocate_done</span><br><span class="line">_wait_relocate_copy_done:</span><br><span class="line">	j	_wait_for_boot_hart</span><br><span class="line">#else</span><br><span class="line">	/* Relocate if load address != link address */</span><br><span class="line">_relocate:</span><br><span class="line">	lla	t0, _link_start</span><br><span class="line">	REG_L	t0, 0(t0)</span><br><span class="line">	lla	t1, _link_end</span><br><span class="line">	REG_L	t1, 0(t1)</span><br><span class="line">	/* n: 加载地址在上面已经得到，这里把加载地址取出，放到t2 */</span><br><span class="line">	lla	t2, _load_start</span><br><span class="line">	REG_L	t2, 0(t2)</span><br><span class="line">	sub	t3, t1, t0</span><br><span class="line">	add	t3, t3, t2</span><br><span class="line">	/* n: 如果link address和load address相等，不需要relocate了 */</span><br><span class="line">	beq	t0, t2, _relocate_done</span><br><span class="line">	/* n: 否则，开始做relocate */</span><br><span class="line">	lla	t4, _relocate_done</span><br><span class="line">	/* n: t4是_relocate_done - _load_start */</span><br><span class="line">	sub	t4, t4, t2</span><br><span class="line">	/* n: t4现在是link下，_relocate_done应该在的地址 */</span><br><span class="line">	add	t4, t4, t0</span><br><span class="line">	/* n: _load_start比_link_start小 */</span><br><span class="line">	blt	t2, t0, _relocate_copy_to_upper</span><br><span class="line">	/*</span><br><span class="line">	 * n: _load_start比_link_start大, 往低地址拷贝数据, 可以看到t2和t0是拷贝</span><br><span class="line">	 *    时的指针。 所以，_link_end(t1)要比_load_start(t2)小，不然拷贝的时候会</span><br><span class="line">	 *    出现重叠的情况。</span><br><span class="line">	 *</span><br><span class="line">	 *     _link_start &lt;--- t0</span><br><span class="line">	 *</span><br><span class="line">	 *</span><br><span class="line">	 *     _link_end   &lt;--- t1</span><br><span class="line">	 *</span><br><span class="line">	 *     _load_start &lt;--- t2</span><br><span class="line">	 *</span><br><span class="line">	 *</span><br><span class="line">	 *     _load_end   &lt;--- t3</span><br><span class="line">	 */</span><br><span class="line">_relocate_copy_to_lower:</span><br><span class="line">	ble	t1, t2, _relocate_copy_to_lower_loop</span><br><span class="line">	/* n: 出现重叠的情况，_relocate_lottery这个位置有什么特殊的意义？*/</span><br><span class="line">	lla	t3, _relocate_lottery</span><br><span class="line">	/* n: t2在[t1, t3)内就跳到_start_hang, 异常处理逻辑没有看懂? */</span><br><span class="line">	BRANGE	t2, t1, t3, _start_hang</span><br><span class="line">	lla	t3, _boot_status</span><br><span class="line">	BRANGE	t2, t1, t3, _start_hang</span><br><span class="line">	lla	t3, _relocate</span><br><span class="line">	lla	t5, _relocate_done</span><br><span class="line">	BRANGE	t2, t1, t3, _start_hang</span><br><span class="line">	BRANGE	t2, t1, t5, _start_hang</span><br><span class="line">	BRANGE  t3, t5, t2, _start_hang</span><br><span class="line">_relocate_copy_to_lower_loop:</span><br><span class="line">	/* n: load address向link address拷贝 */</span><br><span class="line">	REG_L	t3, 0(t2)</span><br><span class="line">	REG_S	t3, 0(t0)</span><br><span class="line">	add	t0, t0, __SIZEOF_POINTER__</span><br><span class="line">	add	t2, t2, __SIZEOF_POINTER__</span><br><span class="line">	blt	t0, t1, _relocate_copy_to_lower_loop</span><br><span class="line">	/* n: 循环拷贝完数据就跳到_relocate_done */</span><br><span class="line">	jr	t4</span><br><span class="line">_relocate_copy_to_upper:</span><br><span class="line">	/*</span><br><span class="line">	 * n: _load_start比_link_start小, 往高地址拷贝数据, 可以看到t2和t0是拷贝</span><br><span class="line">	 *    时的指针。 所以，_load_end(t3)要比_link_start(t0)小，不然拷贝的时候会</span><br><span class="line">	 *    出现重叠的情况。</span><br><span class="line">	 *</span><br><span class="line">	 *     _load_start &lt;--- t2</span><br><span class="line">	 *</span><br><span class="line">	 *</span><br><span class="line">	 *     _load_end   &lt;--- t3</span><br><span class="line">	 *</span><br><span class="line">	 *     _link_start &lt;--- t0</span><br><span class="line">	 *</span><br><span class="line">	 *</span><br><span class="line">	 *     _link_end   &lt;--- t1</span><br><span class="line">	 */</span><br><span class="line">	ble	t3, t0, _relocate_copy_to_upper_loop</span><br><span class="line">	/* n: 异常处理 */</span><br><span class="line">	lla	t2, _relocate_lottery</span><br><span class="line">	BRANGE	t0, t3, t2, _start_hang</span><br><span class="line">	lla	t2, _boot_status</span><br><span class="line">	BRANGE	t0, t3, t2, _start_hang</span><br><span class="line">	lla	t2, _relocate</span><br><span class="line">	lla	t5, _relocate_done</span><br><span class="line">	BRANGE	t0, t3, t2, _start_hang</span><br><span class="line">	BRANGE	t0, t3, t5, _start_hang</span><br><span class="line">	BRANGE	t2, t5, t0, _start_hang</span><br><span class="line">_relocate_copy_to_upper_loop:</span><br><span class="line">	/* n: 倒着拷贝 */</span><br><span class="line">	add	t3, t3, -__SIZEOF_POINTER__</span><br><span class="line">	add	t1, t1, -__SIZEOF_POINTER__</span><br><span class="line">	REG_L	t2, 0(t3)</span><br><span class="line">	REG_S	t2, 0(t1)</span><br><span class="line">	blt	t0, t1, _relocate_copy_to_upper_loop</span><br><span class="line">	jr	t4</span><br><span class="line">_wait_relocate_copy_done:</span><br><span class="line">	lla	t0, _fw_start</span><br><span class="line">	lla	t1, _link_start</span><br><span class="line">	REG_L	t1, 0(t1)</span><br><span class="line">	/*</span><br><span class="line">	 * n: 没有动态链接的情况下，非启动核看到加载地址和链接地址一样时，可以</span><br><span class="line">	 *    直接跳到等待的地址，实际上relocate已经完成。对于没有relocate完成的</span><br><span class="line">	 *    情况，下面先计算出relocate完成后wait_for_boot_hart的地址，但是没有</span><br><span class="line">	 *    relocate完之前还不能跳过去，先把计算出的地址存到t3，然后在一个循环</span><br><span class="line">	 *    里反复检测_boot_status的值</span><br><span class="line">	 */</span><br><span class="line">	beq	t0, t1, _wait_for_boot_hart</span><br><span class="line">	lla	t2, _boot_status</span><br><span class="line">	lla	t3, _wait_for_boot_hart</span><br><span class="line">	/* n: 计算wait_for_boot_hart - _fw_start */</span><br><span class="line">	sub	t3, t3, t0</span><br><span class="line">	/* n: 计算出链接后wait_for_boot_hart的地址(t3) */</span><br><span class="line">	add	t3, t3, t1</span><br><span class="line">1:</span><br><span class="line">	/* waitting for relocate copy done (_boot_status == 1) */</span><br><span class="line">	li	t4, BOOT_STATUS_RELOCATE_DONE</span><br><span class="line">	REG_L	t5, 0(t2)</span><br><span class="line">	/* Reduce the bus traffic so that boot hart may proceed faster */</span><br><span class="line">	nop</span><br><span class="line">	nop</span><br><span class="line">	nop</span><br><span class="line">	/*</span><br><span class="line">	 * n: t5表示当前hart的状态，t4是relocate完成状态, hart一开始_boot_status</span><br><span class="line">	 *    是0，relocate done的状态是1，当relocate拷贝完时，_boot_statue的值</span><br><span class="line">	 *    变成1，跳出这个等待。</span><br><span class="line">	 *</span><br><span class="line">	 *    主核在完成relocate后会把_boot_statue赋1。</span><br><span class="line">	 */</span><br><span class="line">	bgt     t4, t5, 1b</span><br><span class="line">	/* n: 跳到wait_for_boot_hart。从核的视角，它先等待主核做完relocate， */</span><br><span class="line">	jr	t3</span><br><span class="line">#endif</span><br><span class="line">_relocate_done:</span><br><span class="line">	/*</span><br><span class="line">	 * n: 要从多核的角度去看这个代码，主核，也就是做relocate的核，做完relocate</span><br><span class="line">	 *    后会跳到这里，它配置_boot_status这个全局变量为BOOT_STATUS_RELOCATE_DONE，</span><br><span class="line">	 *    但是从核会在上面_wait_relocate_copy_done这里循环等待，直到主核完成</span><br><span class="line">	 *    relocate。</span><br><span class="line">	 */</span><br><span class="line">	/*</span><br><span class="line">	 * Mark relocate copy done</span><br><span class="line">	 * Use _boot_status copy relative to the load address</span><br><span class="line">	 */</span><br><span class="line">	lla	t0, _boot_status</span><br><span class="line">	/* n: 计算_boot_status的地址，为啥PIC的时候的逻辑不一样？*/</span><br><span class="line">#ifndef FW_PIC</span><br><span class="line">	lla	t1, _link_start</span><br><span class="line">	REG_L	t1, 0(t1)</span><br><span class="line">	lla	t2, _load_start</span><br><span class="line">	REG_L	t2, 0(t2)</span><br><span class="line">	sub	t0, t0, t1</span><br><span class="line">	add	t0, t0, t2</span><br><span class="line">#endif</span><br><span class="line">	li	t1, BOOT_STATUS_RELOCATE_DONE</span><br><span class="line">	REG_S	t1, 0(t0)</span><br><span class="line">	fence	rw, rw</span><br><span class="line"></span><br><span class="line">	/* At this point we are running from link address */</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * n: 下面的逻辑就比较直白了，如下是主核的启动逻辑, 从核会跳到wait_for_boot_hart，</span><br><span class="line">	 *    等主核把如下的这些公共逻辑执行一下，等到主核完全启动后，从核从</span><br><span class="line">	 *    _start_warm继续跑。</span><br><span class="line">	 *</span><br><span class="line">	 *    为啥要等主核完全启动？</span><br><span class="line">	 */</span><br><span class="line">	/* Reset all registers for boot HART */</span><br><span class="line">	li	ra, 0</span><br><span class="line">	/* n: ra, a0, a1, a2不清，其他都清0 */</span><br><span class="line">	call	_reset_regs</span><br><span class="line"></span><br><span class="line">	/* n: 循环清0 BSS段 */</span><br><span class="line">	/* Zero-out BSS */</span><br><span class="line">	lla	s4, _bss_start</span><br><span class="line">	lla	s5, _bss_end</span><br><span class="line">_bss_zero:</span><br><span class="line">	REG_S	zero, (s4)</span><br><span class="line">	add	s4, s4, __SIZEOF_POINTER__</span><br><span class="line">	blt	s4, s5, _bss_zero</span><br><span class="line"></span><br><span class="line">	/* Setup temporary trap handler */</span><br><span class="line">	lla	s4, _start_hang</span><br><span class="line">	csrw	CSR_MTVEC, s4</span><br><span class="line"></span><br><span class="line">	/* Setup temporary stack */</span><br><span class="line">	lla	s4, _fw_end</span><br><span class="line">	/* n: s5设置成8KB */</span><br><span class="line">	li	s5, (SBI_SCRATCH_SIZE * 2)</span><br><span class="line">	/* n: 从opensbi二进制加载结束位置开始的8K设置为栈，栈向低地址生长 */</span><br><span class="line">	add	sp, s4, s5</span><br><span class="line"></span><br><span class="line">	/* n: 保存a0-a4, fw_save_info会用到 */</span><br><span class="line">	/* Allow main firmware to save info */</span><br><span class="line">	MOV_5R	s0, a0, s1, a1, s2, a2, s3, a3, s4, a4</span><br><span class="line">	/*</span><br><span class="line">	 * n: 把上一个阶段的启动信息，放到这个阶段和下一个阶段通信的数据结构里，</span><br><span class="line">	 *    目前，只有fw_dynamic会通过一个数据结构向内核传递启动参数。</span><br><span class="line">	 */</span><br><span class="line">	call	fw_save_info</span><br><span class="line">	MOV_5R	a0, s0, a1, s1, a2, s2, a3, s3, a4, s4</span><br><span class="line"></span><br><span class="line">	/* n: 看起来opensbi支持配置dtb进来 */</span><br><span class="line">#ifdef FW_FDT_PATH</span><br><span class="line">	/* Override previous arg1 */</span><br><span class="line">	lla	a1, fw_fdt_bin</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * Initialize platform</span><br><span class="line">	 * Note: The a0 to a4 registers passed to the</span><br><span class="line">	 * firmware are parameters to this function.</span><br><span class="line">	 */</span><br><span class="line">	MOV_5R	s0, a0, s1, a1, s2, a2, s3, a3, s4, a4</span><br><span class="line">	/* n: 返回值是dtb的地址, generic platform也就是qemu virt有这个函数，貌似没有干什么 */</span><br><span class="line">	call	fw_platform_init</span><br><span class="line">	add	t0, a0, zero</span><br><span class="line">	MOV_5R	a0, s0, a1, s1, a2, s2, a3, s3, a4, s4</span><br><span class="line">	add	a1, t0, zero</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * n: platform是平台相关的数据结构，qemu virt的定义在:</span><br><span class="line">	 *    platform/generic/platform.c, 这个是opensbi里针对qemu virt平台定义</span><br><span class="line">	 *    的配置。最大的核数是128，栈大小是8KB。</span><br><span class="line">	 */</span><br><span class="line">	/* Preload HART details</span><br><span class="line">	 * s7 -&gt; HART Count</span><br><span class="line">	 * s8 -&gt; HART Stack Size</span><br><span class="line">	 */</span><br><span class="line">	lla	a4, platform</span><br><span class="line">#if __riscv_xlen &gt; 32</span><br><span class="line">	lwu	s7, SBI_PLATFORM_HART_COUNT_OFFSET(a4)</span><br><span class="line">	lwu	s8, SBI_PLATFORM_HART_STACK_SIZE_OFFSET(a4)</span><br><span class="line">#else</span><br><span class="line">	lw	s7, SBI_PLATFORM_HART_COUNT_OFFSET(a4)</span><br><span class="line">	lw	s8, SBI_PLATFORM_HART_STACK_SIZE_OFFSET(a4)</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">	/* Setup scratch space for all the HARTs */</span><br><span class="line">	lla	tp, _fw_end</span><br><span class="line">	/* n: 算出全部核需要的栈内存大小 */</span><br><span class="line">	mul	a5, s7, s8</span><br><span class="line">	/* n: tp,t3都指向全部栈内存的起始基地址 */</span><br><span class="line">	add	tp, tp, a5</span><br><span class="line">	/* Keep a copy of tp */</span><br><span class="line">	add	t3, tp, zero</span><br><span class="line">	/* Counter */</span><br><span class="line">	li	t2, 1</span><br><span class="line">	/* hartid 0 is mandated by ISA */</span><br><span class="line">	li	t1, 0</span><br><span class="line">_scratch_init:</span><br><span class="line">	/*</span><br><span class="line">	 * The following registers hold values that are computed before</span><br><span class="line">	 * entering this block, and should remain unchanged.</span><br><span class="line">	 *</span><br><span class="line">	 * t3 -&gt; the firmware end address &lt;--- 程序会从二进制加载的结尾分配栈内存, t3指向最后的结尾。</span><br><span class="line">	 * s7 -&gt; HART count</span><br><span class="line">	 * s8 -&gt; HART stack size</span><br><span class="line">	 */</span><br><span class="line">	/* n: tp拿到全部栈内存的基地址 */</span><br><span class="line">	add	tp, t3, zero</span><br><span class="line">	/*</span><br><span class="line">	 * n: t1是首次是0，所以，首次计算a5是0，这个t1是hart的编号，s8是每个核的栈大小，</span><br><span class="line">	 *    所以，a5是每个hart的栈的偏移。</span><br><span class="line">	 */</span><br><span class="line">	mul	a5, s8, t1</span><br><span class="line">	/*</span><br><span class="line">	 * n: 可以看到每个hart的栈是全部栈内存的基地址开始，随hart id增加，依次</span><br><span class="line">	 *    向小地址分配的，每个核的scratch内存是4KB，是从栈的中间开始的。    </span><br><span class="line">         *    tp这里是scratch的开始地址。整个分布的示意图如下。</span><br><span class="line">	 */</span><br><span class="line">	sub	tp, tp, a5</span><br><span class="line">	li	a5, SBI_SCRATCH_SIZE</span><br><span class="line">	/*</span><br><span class="line">	 * n: tp这里是scratch开始地址，scratch的基本排布大概类似：</span><br><span class="line">	 *</span><br><span class="line">	 *    _fw_start </span><br><span class="line">	 *</span><br><span class="line">	 *</span><br><span class="line">	 *    _fw_end</span><br><span class="line">	 *                                          |</span><br><span class="line">	 *                                          |  地址增大</span><br><span class="line">	 *                                          |</span><br><span class="line">	 *                                          v</span><br><span class="line">	 *    stack                                      </span><br><span class="line">	 *    ^                                      </span><br><span class="line">	 *    | 8KB                                     </span><br><span class="line">	 *    |                                      </span><br><span class="line">	 *    |  scratch start(hart 0)     &lt;--- tp</span><br><span class="line">	 *    |    ^</span><br><span class="line">	 *    |    | 4KB</span><br><span class="line">	 *    |    v</span><br><span class="line">	 *    v  scratch end               &lt;--- t3</span><br><span class="line">	 */    </span><br><span class="line">	sub	tp, tp, a5</span><br><span class="line"></span><br><span class="line">	/* Initialize scratch space */</span><br><span class="line">	/* Store fw_start and fw_size in scratch space */</span><br><span class="line">	lla	a4, _fw_start</span><br><span class="line">	/*</span><br><span class="line">         * n: t3是包含栈内存的fw的结尾地址，所以，a5是包含栈内存的fw的大小，这个值</span><br><span class="line">         *    要保存到scratch内存。</span><br><span class="line">         */</span><br><span class="line">	sub	a5, t3, a4</span><br><span class="line">	/* n: 把各种信息存到scratch内存里 */</span><br><span class="line">	REG_S	a4, SBI_SCRATCH_FW_START_OFFSET(tp)</span><br><span class="line">	REG_S	a5, SBI_SCRATCH_FW_SIZE_OFFSET(tp)</span><br><span class="line">	/* Store next arg1 in scratch space */</span><br><span class="line">	MOV_3R	s0, a0, s1, a1, s2, a2</span><br><span class="line">	/*</span><br><span class="line">	 * n: 从opensbi的上一个bootloader拿opensbi启动下一个系统(一般是内核)。可以</span><br><span class="line">	 *    看到，三种fw的函数个不相同，fw_dynamic对上和对下都有固定的参数传递</span><br><span class="line">	 *    数据结构；fw_jump和fw_payload的处理一样，可以是编译配置的参数，也可以</span><br><span class="line">	 *    是从a1传递的参数。a0是如下函数的返回值。</span><br><span class="line">         *</span><br><span class="line">         *    可以看到_dynamic_next_arg1相关的内存和scratch内存是两个不同的概念。</span><br><span class="line">         *    下面就是填scratch的各个域段。</span><br><span class="line">	 */</span><br><span class="line">	call	fw_next_arg1</span><br><span class="line">	REG_S	a0, SBI_SCRATCH_NEXT_ARG1_OFFSET(tp)</span><br><span class="line">	MOV_3R	a0, s0, a1, s1, a2, s2</span><br><span class="line">	/* Store next address in scratch space */</span><br><span class="line">	MOV_3R	s0, a0, s1, a1, s2, a2</span><br><span class="line">	/* n：如下的逻辑都和上面的一样，只是配置的信息不一样, 这里是配置下个系统的初始地址 */</span><br><span class="line">	call	fw_next_addr</span><br><span class="line">	REG_S	a0, SBI_SCRATCH_NEXT_ADDR_OFFSET(tp)</span><br><span class="line">	MOV_3R	a0, s0, a1, s1, a2, s2</span><br><span class="line">	/* Store next mode in scratch space */</span><br><span class="line">	MOV_3R	s0, a0, s1, a1, s2, a2</span><br><span class="line">	/* n: 配置下个系统的CPU mode, 三种fw都是直接在opensbi代码里配死成S mode */</span><br><span class="line">	call	fw_next_mode</span><br><span class="line">	REG_S	a0, SBI_SCRATCH_NEXT_MODE_OFFSET(tp)</span><br><span class="line">	MOV_3R	a0, s0, a1, s1, a2, s2</span><br><span class="line">	/* Store warm_boot address in scratch space */</span><br><span class="line">	lla	a4, _start_warm</span><br><span class="line">	REG_S	a4, SBI_SCRATCH_WARMBOOT_ADDR_OFFSET(tp)</span><br><span class="line">	/* Store platform address in scratch space */</span><br><span class="line">	lla	a4, platform</span><br><span class="line">	REG_S	a4, SBI_SCRATCH_PLATFORM_ADDR_OFFSET(tp)</span><br><span class="line">	/* n: _hartid_to_scratch函数的功能？*/</span><br><span class="line">	/* Store hartid-to-scratch function address in scratch space */</span><br><span class="line">	lla	a4, _hartid_to_scratch</span><br><span class="line">	REG_S	a4, SBI_SCRATCH_HARTID_TO_SCRATCH_OFFSET(tp)</span><br><span class="line">	/* n: 可以看到_trap_exit就是恢复异常现场，然后调用mret回到异常点 */</span><br><span class="line">	/* Store trap-exit function address in scratch space */</span><br><span class="line">	lla	a4, _trap_exit</span><br><span class="line">	REG_S	a4, SBI_SCRATCH_TRAP_EXIT_OFFSET(tp)</span><br><span class="line">	/* Clear tmp0 in scratch space */</span><br><span class="line">	REG_S	zero, SBI_SCRATCH_TMP0_OFFSET(tp)</span><br><span class="line">	/* Store firmware options in scratch space */</span><br><span class="line">	MOV_3R	s0, a0, s1, a1, s2, a2</span><br><span class="line">#ifdef FW_OPTIONS</span><br><span class="line">	li	a0, FW_OPTIONS</span><br><span class="line">#else</span><br><span class="line">	call	fw_options</span><br><span class="line">#endif</span><br><span class="line">	REG_S	a0, SBI_SCRATCH_OPTIONS_OFFSET(tp)</span><br><span class="line">	MOV_3R	a0, s0, a1, s1, a2, s2</span><br><span class="line">	/* Move to next scratch space */</span><br><span class="line">	add	t1, t1, t2</span><br><span class="line">	/* n: 循环处理所有核的scratch内存 */</span><br><span class="line">	blt	t1, s7, _scratch_init</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * Relocate Flatened Device Tree (FDT)</span><br><span class="line">	 * source FDT address = previous arg1    &lt;--- previous arg1和next arg1一样么？</span><br><span class="line">	 * destination FDT address = next arg1</span><br><span class="line">	 *</span><br><span class="line">	 * Note: We will preserve a0 and a1 passed by</span><br><span class="line">	 * previous booting stage.</span><br><span class="line">	 */</span><br><span class="line">	/* n: 貌似a1是0表示device tree relocate搞定了, 跳到_fdt_reloc_done做warm boot? */</span><br><span class="line">	beqz	a1, _fdt_reloc_done</span><br><span class="line">	/* Mask values in a4 */</span><br><span class="line">	li	a4, 0xff</span><br><span class="line">	/* t1 = destination FDT start address */</span><br><span class="line">	MOV_3R	s0, a0, s1, a1, s2, a2</span><br><span class="line">	call	fw_next_arg1</span><br><span class="line">	add	t1, a0, zero</span><br><span class="line">	MOV_3R	a0, s0, a1, s1, a2, s2</span><br><span class="line">	/* n: 哪里配置是0的？*/</span><br><span class="line">	beqz	t1, _fdt_reloc_done</span><br><span class="line">	beq	t1, a1, _fdt_reloc_done</span><br><span class="line">	/* t0 = source FDT start address */</span><br><span class="line">	/* n: 开始dts relocate */</span><br><span class="line">	add	t0, a1, zero</span><br><span class="line">	/* t2 = source FDT size in big-endian */</span><br><span class="line">#if __riscv_xlen == 64</span><br><span class="line">	lwu	t2, 4(t0)</span><br><span class="line">#else</span><br><span class="line">	lw	t2, 4(t0)</span><br><span class="line">#endif</span><br><span class="line">	/* n: 把大端的数据转成小端的？*/</span><br><span class="line">	/* t3 = bit[15:8] of FDT size */</span><br><span class="line">	add	t3, t2, zero</span><br><span class="line">	srli	t3, t3, 16</span><br><span class="line">	and	t3, t3, a4</span><br><span class="line">	slli	t3, t3, 8</span><br><span class="line">	/* t4 = bit[23:16] of FDT size */</span><br><span class="line">	add	t4, t2, zero</span><br><span class="line">	srli	t4, t4, 8</span><br><span class="line">	and	t4, t4, a4</span><br><span class="line">	slli	t4, t4, 16</span><br><span class="line">	/* t5 = bit[31:24] of FDT size */</span><br><span class="line">	add	t5, t2, zero</span><br><span class="line">	and	t5, t5, a4</span><br><span class="line">	slli	t5, t5, 24</span><br><span class="line">	/* t2 = bit[7:0] of FDT size */</span><br><span class="line">	srli	t2, t2, 24</span><br><span class="line">	and	t2, t2, a4</span><br><span class="line">	/* t2 = FDT size in little-endian */</span><br><span class="line">	or	t2, t2, t3</span><br><span class="line">	or	t2, t2, t4</span><br><span class="line">	or	t2, t2, t5</span><br><span class="line">	/* n: 为啥dtb的地址会不一样？*/</span><br><span class="line">	/* t2 = destination FDT end address */</span><br><span class="line">	add	t2, t1, t2</span><br><span class="line">	/* FDT copy loop */</span><br><span class="line">	ble	t2, t1, _fdt_reloc_done</span><br><span class="line">_fdt_reloc_again:</span><br><span class="line">	REG_L	t3, 0(t0)</span><br><span class="line">	REG_S	t3, 0(t1)</span><br><span class="line">	add	t0, t0, __SIZEOF_POINTER__</span><br><span class="line">	add	t1, t1, __SIZEOF_POINTER__</span><br><span class="line">	blt	t1, t2, _fdt_reloc_again</span><br><span class="line">_fdt_reloc_done:</span><br><span class="line"></span><br><span class="line">	/* mark boot hart done */</span><br><span class="line">	li	t0, BOOT_STATUS_BOOT_HART_DONE</span><br><span class="line">	lla	t1, _boot_status</span><br><span class="line">	REG_S	t0, 0(t1)</span><br><span class="line">	fence	rw, rw</span><br><span class="line">	j	_start_warm</span><br><span class="line"></span><br><span class="line">	/* n: 非启动核都等在这里 */</span><br><span class="line">	/* waiting for boot hart to be done (_boot_status == 2) */</span><br><span class="line">_wait_for_boot_hart:</span><br><span class="line">	li	t0, BOOT_STATUS_BOOT_HART_DONE</span><br><span class="line">	lla	t1, _boot_status</span><br><span class="line">	REG_L	t1, 0(t1)</span><br><span class="line">	/* Reduce the bus traffic so that boot hart may proceed faster */</span><br><span class="line">	nop</span><br><span class="line">	nop</span><br><span class="line">	nop</span><br><span class="line">	/*</span><br><span class="line">	 * n: 非启动核都等在这里，不断的检测_boot_status，启动核完成启动后会把</span><br><span class="line">	 *    _boot_status配置成BOOT_STATUS_BOOT_HART_DONE?</span><br><span class="line">	 */</span><br><span class="line">	bne	t0, t1, _wait_for_boot_hart</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * n: 主从核启动都要跑这个，无非是从核先在_wait_for_boot_hart那里等待，</span><br><span class="line">	 *    直到主核继续执行完一些公共的处理逻辑，比如上面的bss清理，scratch内存</span><br><span class="line">	 *    初始化，dtb重定位等。</span><br><span class="line">	 */</span><br><span class="line">_start_warm:</span><br><span class="line">	/* Reset all registers for non-boot HARTs */</span><br><span class="line">	li	ra, 0</span><br><span class="line">	call	_reset_regs</span><br><span class="line"></span><br><span class="line">	/* Disable and clear all interrupts */</span><br><span class="line">	csrw	CSR_MIE, zero</span><br><span class="line">	csrw	CSR_MIP, zero</span><br><span class="line"></span><br><span class="line">	/* Find HART count and HART stack size */</span><br><span class="line">	lla	a4, platform</span><br><span class="line">#if __riscv_xlen == 64</span><br><span class="line">	lwu	s7, SBI_PLATFORM_HART_COUNT_OFFSET(a4)</span><br><span class="line">	lwu	s8, SBI_PLATFORM_HART_STACK_SIZE_OFFSET(a4)</span><br><span class="line">#else</span><br><span class="line">	lw	s7, SBI_PLATFORM_HART_COUNT_OFFSET(a4)</span><br><span class="line">	lw	s8, SBI_PLATFORM_HART_STACK_SIZE_OFFSET(a4)</span><br><span class="line">#endif</span><br><span class="line">	/*</span><br><span class="line">	 * n: 似乎是从sbi_platform这个数据结构里取出hart_index2id这个表里定义的</span><br><span class="line">	 *    数组index到hard id的映射，当没有这个数组，也就是hart_index2id域段</span><br><span class="line">	 *    是0的时候，表示一一映射。</span><br><span class="line">	 */</span><br><span class="line">	REG_L	s9, SBI_PLATFORM_HART_INDEX2ID_OFFSET(a4)</span><br><span class="line"></span><br><span class="line">	/* n: 获取当前核的hart id */</span><br><span class="line">	/* Find HART id */</span><br><span class="line">	csrr	s6, CSR_MHARTID</span><br><span class="line"></span><br><span class="line">	/* n: hart_index2id是NULL，s6中已经是得到hart_id */</span><br><span class="line">	/* Find HART index */</span><br><span class="line">	beqz	s9, 3f</span><br><span class="line">	li	a4, 0</span><br><span class="line">1:</span><br><span class="line">	/* n: hart_index2id不是NULL, 第一次执行时，a5是hart_index2id[0]的值 */</span><br><span class="line">#if __riscv_xlen == 64</span><br><span class="line">	lwu	a5, (s9)</span><br><span class="line">#else</span><br><span class="line">	lw	a5, (s9)</span><br><span class="line">#endif</span><br><span class="line">	/* n: 为啥要在hart_index2id这个表里搜索当前hart id? */</span><br><span class="line">	beq	a5, s6, 2f</span><br><span class="line">	/* n: s9指向hart_index2id数组的下一个位置 */</span><br><span class="line">	add	s9, s9, 4</span><br><span class="line">	add	a4, a4, 1</span><br><span class="line">	blt	a4, s7, 1b</span><br><span class="line">	/* n: 遍历hart id已经异常 */</span><br><span class="line">	li	a4, -1</span><br><span class="line">2:	add	s6, a4, zero</span><br><span class="line">	/* n: 判断是否当前hart id有异常 */</span><br><span class="line">3:	bge	s6, s7, _start_hang</span><br><span class="line"></span><br><span class="line">	/* n: tp指向的是scratch内存的基地址 */</span><br><span class="line">	/* Find the scratch space based on HART index */</span><br><span class="line">	lla	tp, _fw_end</span><br><span class="line">	mul	a5, s7, s8</span><br><span class="line">	add	tp, tp, a5</span><br><span class="line">	mul	a5, s8, s6</span><br><span class="line">	sub	tp, tp, a5</span><br><span class="line">	li	a5, SBI_SCRATCH_SIZE</span><br><span class="line">	sub	tp, tp, a5</span><br><span class="line"></span><br><span class="line">	/* update the mscratch */</span><br><span class="line">	csrw	CSR_MSCRATCH, tp</span><br><span class="line"></span><br><span class="line">	/* n: 这里比较奇怪，上面分配了8K, 这里只用了4KB? */</span><br><span class="line">	/* Setup stack */</span><br><span class="line">	add	sp, tp, zero</span><br><span class="line"></span><br><span class="line">	/* n: 配上M mode的异常处理向量 */</span><br><span class="line">	/* Setup trap handler */</span><br><span class="line">	lla	a4, _trap_handler</span><br><span class="line">#if __riscv_xlen == 32</span><br><span class="line">	csrr	a5, CSR_MISA</span><br><span class="line">	srli	a5, a5, (&#x27;H&#x27; - &#x27;A&#x27;)</span><br><span class="line">	/* n: rv32? 如果支持虚拟化扩展，中断异常配置不同的向量？*/</span><br><span class="line">	andi	a5, a5, 0x1</span><br><span class="line">	beq	a5, zero, _skip_trap_handler_rv32_hyp</span><br><span class="line">	lla	a4, _trap_handler_rv32_hyp</span><br><span class="line">_skip_trap_handler_rv32_hyp:</span><br><span class="line">#endif</span><br><span class="line">	csrw	CSR_MTVEC, a4</span><br><span class="line"></span><br><span class="line">#if __riscv_xlen == 32</span><br><span class="line">	/* n: 这里的逻辑还是不清楚，对于虚拟化扩展，异常退出的逻辑也不一样？*/</span><br><span class="line">	/* Override trap exit for H-extension */</span><br><span class="line">	csrr	a5, CSR_MISA</span><br><span class="line">	srli	a5, a5, (&#x27;H&#x27; - &#x27;A&#x27;)</span><br><span class="line">	andi	a5, a5, 0x1</span><br><span class="line">	beq	a5, zero, _skip_trap_exit_rv32_hyp</span><br><span class="line">	lla	a4, _trap_exit_rv32_hyp</span><br><span class="line">	csrr	a5, CSR_MSCRATCH</span><br><span class="line">	REG_S	a4, SBI_SCRATCH_TRAP_EXIT_OFFSET(a5)</span><br><span class="line">_skip_trap_exit_rv32_hyp:</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">	/* Initialize SBI runtime */</span><br><span class="line">	csrr	a0, CSR_MSCRATCH</span><br><span class="line">	call	sbi_init</span><br><span class="line"></span><br><span class="line">	/* We don&#x27;t expect to reach here hence just hang */</span><br><span class="line">	j	_start_hang</span><br><span class="line"></span><br><span class="line">	.data</span><br><span class="line">	.align 3</span><br><span class="line">#ifdef FW_PIC</span><br><span class="line">_runtime_offset:</span><br><span class="line">	RISCV_PTR	0</span><br><span class="line">#endif</span><br><span class="line">	/* n: 定义一个dword的空间，整体上看，这里相当于定义了一些全局变量 */</span><br><span class="line">_relocate_lottery:</span><br><span class="line">	RISCV_PTR	0</span><br><span class="line">_boot_status:</span><br><span class="line">	RISCV_PTR	0</span><br><span class="line">_load_start:</span><br><span class="line">	RISCV_PTR	_fw_start</span><br><span class="line">_link_start:</span><br><span class="line">	RISCV_PTR	FW_TEXT_START</span><br><span class="line">_link_end:</span><br><span class="line">	RISCV_PTR	_fw_reloc_end</span><br><span class="line"></span><br><span class="line">	.section .entry, &quot;ax&quot;, %progbits</span><br><span class="line">	.align 3</span><br><span class="line">	.globl _hartid_to_scratch</span><br><span class="line">_hartid_to_scratch:</span><br><span class="line">	/*</span><br><span class="line">	 * a0 -&gt; HART ID (passed by caller)</span><br><span class="line">	 * a1 -&gt; HART Index (passed by caller)</span><br><span class="line">	 * t0 -&gt; HART Stack Size</span><br><span class="line">	 * t1 -&gt; HART Stack End</span><br><span class="line">	 * t2 -&gt; Temporary</span><br><span class="line">	 */</span><br><span class="line">	lla	t2, platform</span><br><span class="line">#if __riscv_xlen == 64</span><br><span class="line">	lwu	t0, SBI_PLATFORM_HART_STACK_SIZE_OFFSET(t2)</span><br><span class="line">	lwu	t2, SBI_PLATFORM_HART_COUNT_OFFSET(t2)</span><br><span class="line">#else</span><br><span class="line">	lw	t0, SBI_PLATFORM_HART_STACK_SIZE_OFFSET(t2)</span><br><span class="line">	lw	t2, SBI_PLATFORM_HART_COUNT_OFFSET(t2)</span><br><span class="line">#endif</span><br><span class="line">	sub	t2, t2, a1</span><br><span class="line">	mul	t2, t2, t0</span><br><span class="line">	lla	t1, _fw_end</span><br><span class="line">	add	t1, t1, t2</span><br><span class="line">	li	t2, SBI_SCRATCH_SIZE</span><br><span class="line">	sub	a0, t1, t2</span><br><span class="line">	ret</span><br><span class="line"></span><br><span class="line">	.section .entry, &quot;ax&quot;, %progbits</span><br><span class="line">	.align 3</span><br><span class="line">	.globl _start_hang</span><br><span class="line">_start_hang:</span><br><span class="line">	wfi</span><br><span class="line">	j	_start_hang</span><br><span class="line"></span><br><span class="line">	.section .entry, &quot;ax&quot;, %progbits</span><br><span class="line">	.align 3</span><br><span class="line">	.weak fw_platform_init</span><br><span class="line">fw_platform_init:</span><br><span class="line">	add	a0, a1, zero</span><br><span class="line">	ret</span><br><span class="line"></span><br><span class="line">	/* Map implicit memcpy() added by compiler to sbi_memcpy() */</span><br><span class="line">	.section .text</span><br><span class="line">	.align 3</span><br><span class="line">	.globl memcpy</span><br><span class="line">memcpy:</span><br><span class="line">	tail	sbi_memcpy</span><br><span class="line"></span><br><span class="line">	/* Map implicit memset() added by compiler to sbi_memset() */</span><br><span class="line">	.section .text</span><br><span class="line">	.align 3</span><br><span class="line">	.globl memset</span><br><span class="line">memset:</span><br><span class="line">	tail	sbi_memset</span><br><span class="line"></span><br><span class="line">	/* Map implicit memmove() added by compiler to sbi_memmove() */</span><br><span class="line">	.section .text</span><br><span class="line">	.align 3</span><br><span class="line">	.globl memmove</span><br><span class="line">memmove:</span><br><span class="line">	tail	sbi_memmove</span><br><span class="line"></span><br><span class="line">	/* Map implicit memcmp() added by compiler to sbi_memcmp() */</span><br><span class="line">	.section .text</span><br><span class="line">	.align 3</span><br><span class="line">	.globl memcmp</span><br><span class="line">memcmp:</span><br><span class="line">	tail	sbi_memcmp</span><br><span class="line"></span><br><span class="line">.macro	TRAP_SAVE_AND_SETUP_SP_T0</span><br><span class="line">	/* Swap TP and MSCRATCH */</span><br><span class="line">	csrrw	tp, CSR_MSCRATCH, tp</span><br><span class="line"></span><br><span class="line">	/* Save T0 in scratch space */</span><br><span class="line">	REG_S	t0, SBI_SCRATCH_TMP0_OFFSET(tp)</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * Set T0 to appropriate exception stack</span><br><span class="line">	 *</span><br><span class="line">	 * Came_From_M_Mode = ((MSTATUS.MPP &lt; PRV_M) ? 1 : 0) - 1;</span><br><span class="line">	 * Exception_Stack = TP ^ (Came_From_M_Mode &amp; (SP ^ TP))</span><br><span class="line">	 *</span><br><span class="line">	 * Came_From_M_Mode = 0    ==&gt;    Exception_Stack = TP</span><br><span class="line">	 * Came_From_M_Mode = -1   ==&gt;    Exception_Stack = SP</span><br><span class="line">	 */</span><br><span class="line">	csrr	t0, CSR_MSTATUS</span><br><span class="line">	srl	t0, t0, MSTATUS_MPP_SHIFT</span><br><span class="line">	and	t0, t0, PRV_M</span><br><span class="line">	slti	t0, t0, PRV_M</span><br><span class="line">	add	t0, t0, -1</span><br><span class="line">	xor	sp, sp, tp</span><br><span class="line">	and	t0, t0, sp</span><br><span class="line">	xor	sp, sp, tp</span><br><span class="line">	xor	t0, tp, t0</span><br><span class="line"></span><br><span class="line">	/* Save original SP on exception stack */</span><br><span class="line">	REG_S	sp, (SBI_TRAP_REGS_OFFSET(sp) - SBI_TRAP_REGS_SIZE)(t0)</span><br><span class="line"></span><br><span class="line">	/* Set SP to exception stack and make room for trap registers */</span><br><span class="line">	add	sp, t0, -(SBI_TRAP_REGS_SIZE)</span><br><span class="line"></span><br><span class="line">	/* Restore T0 from scratch space */</span><br><span class="line">	REG_L	t0, SBI_SCRATCH_TMP0_OFFSET(tp)</span><br><span class="line"></span><br><span class="line">	/* Save T0 on stack */</span><br><span class="line">	REG_S	t0, SBI_TRAP_REGS_OFFSET(t0)(sp)</span><br><span class="line"></span><br><span class="line">	/* Swap TP and MSCRATCH */</span><br><span class="line">	csrrw	tp, CSR_MSCRATCH, tp</span><br><span class="line">.endm</span><br><span class="line"></span><br><span class="line">.macro	TRAP_SAVE_MEPC_MSTATUS have_mstatush</span><br><span class="line">	/* Save MEPC and MSTATUS CSRs */</span><br><span class="line">	csrr	t0, CSR_MEPC</span><br><span class="line">	REG_S	t0, SBI_TRAP_REGS_OFFSET(mepc)(sp)</span><br><span class="line">	csrr	t0, CSR_MSTATUS</span><br><span class="line">	REG_S	t0, SBI_TRAP_REGS_OFFSET(mstatus)(sp)</span><br><span class="line">	.if \have_mstatush</span><br><span class="line">	csrr	t0, CSR_MSTATUSH</span><br><span class="line">	REG_S	t0, SBI_TRAP_REGS_OFFSET(mstatusH)(sp)</span><br><span class="line">	.else</span><br><span class="line">	REG_S	zero, SBI_TRAP_REGS_OFFSET(mstatusH)(sp)</span><br><span class="line">	.endif</span><br><span class="line">.endm</span><br><span class="line"></span><br><span class="line">.macro	TRAP_SAVE_GENERAL_REGS_EXCEPT_SP_T0</span><br><span class="line">	/* Save all general regisers except SP and T0 */</span><br><span class="line">	REG_S	zero, SBI_TRAP_REGS_OFFSET(zero)(sp)</span><br><span class="line">	REG_S	ra, SBI_TRAP_REGS_OFFSET(ra)(sp)</span><br><span class="line">	REG_S	gp, SBI_TRAP_REGS_OFFSET(gp)(sp)</span><br><span class="line">	REG_S	tp, SBI_TRAP_REGS_OFFSET(tp)(sp)</span><br><span class="line">	REG_S	t1, SBI_TRAP_REGS_OFFSET(t1)(sp)</span><br><span class="line">	REG_S	t2, SBI_TRAP_REGS_OFFSET(t2)(sp)</span><br><span class="line">	REG_S	s0, SBI_TRAP_REGS_OFFSET(s0)(sp)</span><br><span class="line">	REG_S	s1, SBI_TRAP_REGS_OFFSET(s1)(sp)</span><br><span class="line">	REG_S	a0, SBI_TRAP_REGS_OFFSET(a0)(sp)</span><br><span class="line">	REG_S	a1, SBI_TRAP_REGS_OFFSET(a1)(sp)</span><br><span class="line">	REG_S	a2, SBI_TRAP_REGS_OFFSET(a2)(sp)</span><br><span class="line">	REG_S	a3, SBI_TRAP_REGS_OFFSET(a3)(sp)</span><br><span class="line">	REG_S	a4, SBI_TRAP_REGS_OFFSET(a4)(sp)</span><br><span class="line">	REG_S	a5, SBI_TRAP_REGS_OFFSET(a5)(sp)</span><br><span class="line">	REG_S	a6, SBI_TRAP_REGS_OFFSET(a6)(sp)</span><br><span class="line">	REG_S	a7, SBI_TRAP_REGS_OFFSET(a7)(sp)</span><br><span class="line">	REG_S	s2, SBI_TRAP_REGS_OFFSET(s2)(sp)</span><br><span class="line">	REG_S	s3, SBI_TRAP_REGS_OFFSET(s3)(sp)</span><br><span class="line">	REG_S	s4, SBI_TRAP_REGS_OFFSET(s4)(sp)</span><br><span class="line">	REG_S	s5, SBI_TRAP_REGS_OFFSET(s5)(sp)</span><br><span class="line">	REG_S	s6, SBI_TRAP_REGS_OFFSET(s6)(sp)</span><br><span class="line">	REG_S	s7, SBI_TRAP_REGS_OFFSET(s7)(sp)</span><br><span class="line">	REG_S	s8, SBI_TRAP_REGS_OFFSET(s8)(sp)</span><br><span class="line">	REG_S	s9, SBI_TRAP_REGS_OFFSET(s9)(sp)</span><br><span class="line">	REG_S	s10, SBI_TRAP_REGS_OFFSET(s10)(sp)</span><br><span class="line">	REG_S	s11, SBI_TRAP_REGS_OFFSET(s11)(sp)</span><br><span class="line">	REG_S	t3, SBI_TRAP_REGS_OFFSET(t3)(sp)</span><br><span class="line">	REG_S	t4, SBI_TRAP_REGS_OFFSET(t4)(sp)</span><br><span class="line">	REG_S	t5, SBI_TRAP_REGS_OFFSET(t5)(sp)</span><br><span class="line">	REG_S	t6, SBI_TRAP_REGS_OFFSET(t6)(sp)</span><br><span class="line">.endm</span><br><span class="line"></span><br><span class="line">.macro	TRAP_CALL_C_ROUTINE</span><br><span class="line">	/* Call C routine */</span><br><span class="line">	add	a0, sp, zero</span><br><span class="line">	call	sbi_trap_handler</span><br><span class="line">.endm</span><br><span class="line"></span><br><span class="line">.macro	TRAP_RESTORE_GENERAL_REGS_EXCEPT_A0_T0</span><br><span class="line">	/* Restore all general regisers except A0 and T0 */</span><br><span class="line">	REG_L	ra, SBI_TRAP_REGS_OFFSET(ra)(a0)</span><br><span class="line">	REG_L	sp, SBI_TRAP_REGS_OFFSET(sp)(a0)</span><br><span class="line">	REG_L	gp, SBI_TRAP_REGS_OFFSET(gp)(a0)</span><br><span class="line">	REG_L	tp, SBI_TRAP_REGS_OFFSET(tp)(a0)</span><br><span class="line">	REG_L	t1, SBI_TRAP_REGS_OFFSET(t1)(a0)</span><br><span class="line">	REG_L	t2, SBI_TRAP_REGS_OFFSET(t2)(a0)</span><br><span class="line">	REG_L	s0, SBI_TRAP_REGS_OFFSET(s0)(a0)</span><br><span class="line">	REG_L	s1, SBI_TRAP_REGS_OFFSET(s1)(a0)</span><br><span class="line">	REG_L	a1, SBI_TRAP_REGS_OFFSET(a1)(a0)</span><br><span class="line">	REG_L	a2, SBI_TRAP_REGS_OFFSET(a2)(a0)</span><br><span class="line">	REG_L	a3, SBI_TRAP_REGS_OFFSET(a3)(a0)</span><br><span class="line">	REG_L	a4, SBI_TRAP_REGS_OFFSET(a4)(a0)</span><br><span class="line">	REG_L	a5, SBI_TRAP_REGS_OFFSET(a5)(a0)</span><br><span class="line">	REG_L	a6, SBI_TRAP_REGS_OFFSET(a6)(a0)</span><br><span class="line">	REG_L	a7, SBI_TRAP_REGS_OFFSET(a7)(a0)</span><br><span class="line">	REG_L	s2, SBI_TRAP_REGS_OFFSET(s2)(a0)</span><br><span class="line">	REG_L	s3, SBI_TRAP_REGS_OFFSET(s3)(a0)</span><br><span class="line">	REG_L	s4, SBI_TRAP_REGS_OFFSET(s4)(a0)</span><br><span class="line">	REG_L	s5, SBI_TRAP_REGS_OFFSET(s5)(a0)</span><br><span class="line">	REG_L	s6, SBI_TRAP_REGS_OFFSET(s6)(a0)</span><br><span class="line">	REG_L	s7, SBI_TRAP_REGS_OFFSET(s7)(a0)</span><br><span class="line">	REG_L	s8, SBI_TRAP_REGS_OFFSET(s8)(a0)</span><br><span class="line">	REG_L	s9, SBI_TRAP_REGS_OFFSET(s9)(a0)</span><br><span class="line">	REG_L	s10, SBI_TRAP_REGS_OFFSET(s10)(a0)</span><br><span class="line">	REG_L	s11, SBI_TRAP_REGS_OFFSET(s11)(a0)</span><br><span class="line">	REG_L	t3, SBI_TRAP_REGS_OFFSET(t3)(a0)</span><br><span class="line">	REG_L	t4, SBI_TRAP_REGS_OFFSET(t4)(a0)</span><br><span class="line">	REG_L	t5, SBI_TRAP_REGS_OFFSET(t5)(a0)</span><br><span class="line">	REG_L	t6, SBI_TRAP_REGS_OFFSET(t6)(a0)</span><br><span class="line">.endm</span><br><span class="line"></span><br><span class="line">.macro	TRAP_RESTORE_MEPC_MSTATUS have_mstatush</span><br><span class="line">	/* Restore MEPC and MSTATUS CSRs */</span><br><span class="line">	REG_L	t0, SBI_TRAP_REGS_OFFSET(mepc)(a0)</span><br><span class="line">	csrw	CSR_MEPC, t0</span><br><span class="line">	REG_L	t0, SBI_TRAP_REGS_OFFSET(mstatus)(a0)</span><br><span class="line">	csrw	CSR_MSTATUS, t0</span><br><span class="line">	.if \have_mstatush</span><br><span class="line">	REG_L	t0, SBI_TRAP_REGS_OFFSET(mstatusH)(a0)</span><br><span class="line">	csrw	CSR_MSTATUSH, t0</span><br><span class="line">	.endif</span><br><span class="line">.endm</span><br><span class="line"></span><br><span class="line">.macro TRAP_RESTORE_A0_T0</span><br><span class="line">	/* Restore T0 */</span><br><span class="line">	REG_L	t0, SBI_TRAP_REGS_OFFSET(t0)(a0)</span><br><span class="line"></span><br><span class="line">	/* Restore A0 */</span><br><span class="line">	REG_L	a0, SBI_TRAP_REGS_OFFSET(a0)(a0)</span><br><span class="line">.endm</span><br><span class="line"></span><br><span class="line">	.section .entry, &quot;ax&quot;, %progbits</span><br><span class="line">	.align 3</span><br><span class="line">	.globl _trap_handler</span><br><span class="line">	.globl _trap_exit</span><br><span class="line">_trap_handler:</span><br><span class="line">	TRAP_SAVE_AND_SETUP_SP_T0</span><br><span class="line"></span><br><span class="line">	TRAP_SAVE_MEPC_MSTATUS 0</span><br><span class="line"></span><br><span class="line">	TRAP_SAVE_GENERAL_REGS_EXCEPT_SP_T0</span><br><span class="line"></span><br><span class="line">	TRAP_CALL_C_ROUTINE</span><br><span class="line"></span><br><span class="line">	/* n: opensbi/lib/sbi/sbi_trap.c里的sbi_trap_exit会调用这里 */</span><br><span class="line">_trap_exit:</span><br><span class="line">	TRAP_RESTORE_GENERAL_REGS_EXCEPT_A0_T0</span><br><span class="line"></span><br><span class="line">	TRAP_RESTORE_MEPC_MSTATUS 0</span><br><span class="line"></span><br><span class="line">	TRAP_RESTORE_A0_T0</span><br><span class="line"></span><br><span class="line">	mret</span><br><span class="line"></span><br><span class="line">#if __riscv_xlen == 32</span><br><span class="line">	.section .entry, &quot;ax&quot;, %progbits</span><br><span class="line">	.align 3</span><br><span class="line">	.globl _trap_handler_rv32_hyp</span><br><span class="line">	.globl _trap_exit_rv32_hyp</span><br><span class="line">_trap_handler_rv32_hyp:</span><br><span class="line">	TRAP_SAVE_AND_SETUP_SP_T0</span><br><span class="line"></span><br><span class="line">	TRAP_SAVE_MEPC_MSTATUS 1</span><br><span class="line"></span><br><span class="line">	TRAP_SAVE_GENERAL_REGS_EXCEPT_SP_T0</span><br><span class="line"></span><br><span class="line">	TRAP_CALL_C_ROUTINE</span><br><span class="line"></span><br><span class="line">_trap_exit_rv32_hyp:</span><br><span class="line">	TRAP_RESTORE_GENERAL_REGS_EXCEPT_A0_T0</span><br><span class="line"></span><br><span class="line">	TRAP_RESTORE_MEPC_MSTATUS 1</span><br><span class="line"></span><br><span class="line">	TRAP_RESTORE_A0_T0</span><br><span class="line"></span><br><span class="line">	mret</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">	.section .entry, &quot;ax&quot;, %progbits</span><br><span class="line">	.align 3</span><br><span class="line">	.globl _reset_regs</span><br><span class="line">_reset_regs:</span><br><span class="line"></span><br><span class="line">	/* flush the instruction cache */</span><br><span class="line">	fence.i</span><br><span class="line">	/* Reset all registers except ra, a0, a1 and a2 */</span><br><span class="line">	li sp, 0</span><br><span class="line">	li gp, 0</span><br><span class="line">	li tp, 0</span><br><span class="line">	li t0, 0</span><br><span class="line">	li t1, 0</span><br><span class="line">	li t2, 0</span><br><span class="line">	li s0, 0</span><br><span class="line">	li s1, 0</span><br><span class="line">	li a3, 0</span><br><span class="line">	li a4, 0</span><br><span class="line">	li a5, 0</span><br><span class="line">	li a6, 0</span><br><span class="line">	li a7, 0</span><br><span class="line">	li s2, 0</span><br><span class="line">	li s3, 0</span><br><span class="line">	li s4, 0</span><br><span class="line">	li s5, 0</span><br><span class="line">	li s6, 0</span><br><span class="line">	li s7, 0</span><br><span class="line">	li s8, 0</span><br><span class="line">	li s9, 0</span><br><span class="line">	li s10, 0</span><br><span class="line">	li s11, 0</span><br><span class="line">	li t3, 0</span><br><span class="line">	li t4, 0</span><br><span class="line">	li t5, 0</span><br><span class="line">	li t6, 0</span><br><span class="line">	csrw CSR_MSCRATCH, 0</span><br><span class="line"></span><br><span class="line">	ret</span><br><span class="line"></span><br><span class="line">#ifdef FW_FDT_PATH</span><br><span class="line">	.section .rodata</span><br><span class="line">	.align 4</span><br><span class="line">	.globl fw_fdt_bin</span><br><span class="line">fw_fdt_bin:</span><br><span class="line">	.incbin FW_FDT_PATH</span><br><span class="line">#ifdef FW_FDT_PADDING</span><br><span class="line">	.fill FW_FDT_PADDING, 1, 0</span><br><span class="line">#endif</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>opensbi</tag>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu tcg中间码优化和后端翻译</title>
    <url>/qemu-tcg%E4%B8%AD%E9%97%B4%E7%A0%81%E4%BC%98%E5%8C%96%E5%92%8C%E5%90%8E%E7%AB%AF%E7%BF%BB%E8%AF%91/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p> qemu tcg的基本翻译思路是把guest指令先翻译成中间码(IR)，然后再把IR翻译成host指令。<br> guest-&gt;IR-&gt;host这种三段式实现的好处是把前端翻译，优化和后端翻译拆开了，降低了开发<br> 的难度，比如，要模拟一个新构架的CPU，只要实现guest-&gt;IR这一步就好，后续在X86或者在<br> ARM64的host的机器上跑，重新编译下qemu就好，并不用去熟悉host CPU的构架。</p>
<p> guest翻译成IR的逻辑在<a href="%5Bhttps://wangzhou.github.io/qemu-tcg%E7%BF%BB%E8%AF%91%E6%89%A7%E8%A1%8C%E6%A0%B8%E5%BF%83%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/">这里</a>已经有介绍，这一步主要靠程序员手写代码生成IR，本文主<br> 要讲中间码的优化和后端翻译，我们可以认为这两部分属于翻译IR到host指令，为了看清楚<br> IR到host指令的翻译，我们首先要明确前端翻译得到的IR是怎么样的。</p>
<p> IR指令的理解是比较直白的，qemu定义了一套IR的指令，具体的定义在tcg/README里说明,<br> 在一个tb里，qemu前端翻译得到的IR被串联到一个链表里，中间码优化和后端翻译都靠这个<br> 链表得到IR，中间码优化时，需要改动IR时(比如，删掉不可达的IR)，对这个链表做操作就<br> 好。</p>
<p> 中间码不只是定义了对应的指令，也有寄存器的定义，它形成了一个独立的逻辑空间，在IR<br> 这一层，可以认为都在中间码相关的寄存器上做计算的。IR这一层定义了几个寄存器类型，<br> 它们分别是：global, local temp, normal temp, fixed, const, ebb</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">typedef enum TCGTempKind &#123;                                                      </span><br><span class="line">    /* Temp is dead at the end of all basic blocks. */                          </span><br><span class="line">    TEMP_NORMAL,                                                                </span><br><span class="line">    /* Temp is live across conditional branch, but dead otherwise. */           </span><br><span class="line">    TEMP_EBB,                                                                   </span><br><span class="line">    /* Temp is saved across basic blocks but dead at the end of TBs. */         </span><br><span class="line">    TEMP_LOCAL,                                                                 </span><br><span class="line">    /* Temp is saved across both basic blocks and translation blocks. */        </span><br><span class="line">    TEMP_GLOBAL,                                                                </span><br><span class="line">    /* Temp is in a fixed register. */                                          </span><br><span class="line">    TEMP_FIXED,                                                                 </span><br><span class="line">    /* Temp is a fixed constant. */                                             </span><br><span class="line">    TEMP_CONST,                                                                 </span><br><span class="line">&#125; TCGTempKind;                                                                  </span><br></pre></td></tr></table></figure>
<p> 一般guest的gpr也被定义为IR这一层的global寄存器，中间码做计算的时候，会用到一些<br> 临时变量，这些临时变量就保存在local temp或者是normal temp这样的寄存器里，计算的<br> 时候要用到一些常量时，需要定义一个TCG寄存器，创建一个常量并把它赋给TCG寄存器。</p>
<p> global、local temp、normal temp和const这些TCG寄存器我们在写前端翻译的时候会经常<br> 用到，fixed和ebb直接用到的情况不多。</p>
<p> TCG寄存器是怎么定义和被使用的，以及它们本质上是什么？我们基于riscv看下这个问题。<br> riscv下global寄存器一般如下定义：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* target/riscv/translate.c */</span><br><span class="line">riscv_translate_init</span><br><span class="line">  [...]</span><br><span class="line">  +-&gt; cpu_gpr[i] = tcg_global_mem_new(cpu_env, </span><br><span class="line">  		    offsetof(CPURISCVState, gpr[i]), riscv_int_regnames[i]);            </span><br><span class="line">        /* 在TCGContext里分配对应的空间，并且设定这个寄存器是TEMP_GLOBAL */</span><br><span class="line">    +-&gt; tcg_global_mem_new_internal(..., reg, offset, name);  </span><br><span class="line">  [...]</span><br><span class="line">  +-&gt; cpu_pc = tcg_global_mem_new(cpu_env, offsetof(CPURISCVState, pc), &quot;pc&quot;);    </span><br><span class="line">  [...]</span><br></pre></td></tr></table></figure>
<p> 我们只挑了gpr和pc的这几行代码出来，这里分配了对应的TCG寄存器，返回值是这些寄存器<br> 存储地址相对tcg_ctx的偏移。注意这里得到的是global寄存器的描述结构，类型是TCGTemp，<br> 而global寄存器实际存储在CPURISCVState内具体定义的地方，TCGTemp内通过mem_base和<br> mem_offset指向具体存储地址。</p>
<p> 实际上，所有TCG寄存器的分配都是在TCGContext里分配了对应的存储空间，并且配置上相关<br> 参数，这些参数和IR一起交给后端做IR优化和后端翻译，后端使用TCGContext的地址和具体<br> 寄存器的偏移可以找见具体的TCG寄存器。</p>
<p> local temp和normal temp的说明在<a href="https://wangzhou.github.io/qemu%E4%B8%ADbasic-block%E4%BB%A5%E5%8F%8Atcg%E4%B8%AD%E5%90%84%E7%A7%8D%E5%8F%98%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91">这里</a>有说明。简而言之，normal temp只在一个BB中有效，<br> local temp在一个TB中有效。</p>
<p> fixed要结合host寄存器分配来看，首先IR中分配的这些寄存器都是虚拟的寄存器，IR翻译<br> 到host指令都要给虚拟寄存器分配对应的host物理寄存器，当一个TCG寄存器有TEMP_FIXED<br> 标记表示在后端翻译时把这个虚拟寄存器固定映射到一个host物理寄存器上，一般fixed寄<br> 存器都是翻译执行时经常要用到的参数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/tcg.c */</span><br><span class="line">tcg_context_init</span><br><span class="line">  /*</span><br><span class="line">   * 在tcg后端公共逻辑里，定义一个TCG寄存器，并把它绑定到host的TCG_AREG0这个寄存器上</span><br><span class="line">   * 每个host都会把具体的实际物理寄存器映射到TCG_AREG0。</span><br><span class="line">   */</span><br><span class="line">  +-&gt; ts = tcg_global_reg_new_internal(s, TCG_TYPE_PTR, TCG_AREG0, &quot;env&quot;);        </span><br><span class="line">  +-&gt; cpu_env = temp_tcgv_ptr(ts);                                                </span><br></pre></td></tr></table></figure>
<p> 如上的cpu_env依然是cpu_env寄存器存储地址针对tcg_ctx的偏移，前端翻译代码里会大量<br> 的用到cpu_env这个值，所以这里把它定义成fixed寄存器，提示后端翻译把cpu_env的值固定<br> 的放到一个host寄存器里。具体看，tcg_global_reg_new_internal里会把被绑定的host物理<br> 寄存器放到reserved_regs集合，这样，后端翻译后续就不会分配这个物理寄存器，cpu_env<br> 保存的是guest CPU软件结构体的指针，那么这个指针又是怎么传递给reserved TCG_AREG0<br> host物理寄存器？可以看到一个tb执行的时候这个指针作为第一个入参传递给tb里生成的<br> host指令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cpu_exec</span><br><span class="line">  +-&gt; cpu_loop_exec_tb</span><br><span class="line">    +-&gt; cpu_tb_exec(cpu, ...)</span><br><span class="line">      +-&gt; tcg_qemu_tb_exec(env, ...)</span><br></pre></td></tr></table></figure>
<p> 在tb头里，会有一条指令把这个入参送给TCG_AREG0(ARM64的x19)，我们看看ARM64作为后端<br> 时，这个代码生成的逻辑：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/aarch64/tcg-target.c.inc */</span><br><span class="line">tcg_target_qemu_prologue</span><br><span class="line">  [...]</span><br><span class="line">  +-&gt; tcg_set_frame(s, TCG_REG_SP, TCG_STATIC_CALL_ARGS_SIZE, CPU_TEMP_BUF_NLONGS * sizeof(long));</span><br><span class="line">    +-&gt; tcg_global_reg_new_internal(s, TCG_TYPE_PTR, reg, &quot;_frame&quot;);          </span><br><span class="line">  [...]</span><br><span class="line">  +-&gt; tcg_out_mov(s, TCG_TYPE_PTR, TCG_AREG0, tcg_target_call_iarg_regs[0]);      </span><br></pre></td></tr></table></figure>
<p> 如上，公共代码里还会把host的栈寄存器也reserv出来做特定的用途，这里sp就是host上<br> sp自己的语意，因为host调用翻译好的host指令，就是一个host上的函数调用。</p>
<p> ebb类型的TCG寄存器表示这个寄存器可以跨越条件跳转，但是跨越之后状态为dead，这种<br> 变量类型和indirect的global寄存器有关系，在liveness_pass_2中会为indirect的global<br> 寄存器新创建ebb类型的TCG寄存器，具体用法还待分析。</p>
<p> (todo: 具体用法)</p>
<h2 id="中间码优化"><a href="#中间码优化" class="headerlink" title="中间码优化"></a>中间码优化</h2><p> 前端翻译得到的IR可能会有优化的空间存在，所以qemu在进行后端翻译之前会先做中间码<br> 优化，优化以一个TB为单位，优化的输入就是一个TB对应的IR和用到的TCG寄存器。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/tcg.c */</span><br><span class="line">tcg_gen_code</span><br><span class="line">  +-&gt; tcg_optimize(s)</span><br><span class="line">    +-&gt; done = fold_add(&amp;ctx, op);                                          </span><br><span class="line"></span><br><span class="line">  +-&gt; reachable_code_pass(s);                                                     </span><br></pre></td></tr></table></figure>
<p> tcg_optimize是做着一些常量的检查，进而做指令优化(折叠常量表达式), 我们取其中的<br> 一个case，比如fold_add具体看下，大概知道下这里是在干什么。可以看到这个case检测<br> add_32/64这个IR的两个操作数是不是常量，如果是常量，那么在这里直接把常量相加后的<br> 结果放到一个常量类型TCG寄存器，然后把之前的add_32/64改成一条mov指令。</p>
<p> 从名字就可以看出reachable_code_pass应该做的是一些死代码的删除，这里检测到运行不到<br> 的IR就直接从IR链表里把他们删掉。</p>
<p> 中间码优化的输出还是IR链表和相关的TCG寄存器，可见我们也可以把这两个函数注释掉，<br> 从而把中间码优化关掉。可以看出，中间码优化和编译器IR优化的逻辑是类似的。</p>
<p> 中间码优化的具体case本文就不继续展开了，后续有需要再写吧。</p>
<h2 id="寄存器活性分析"><a href="#寄存器活性分析" class="headerlink" title="寄存器活性分析"></a>寄存器活性分析</h2><p> qemu最终还是要把IR和TCG寄存器翻译成host指令和host寄存器，才能在host机器上运行,<br> 这一节和下一节就是要解决这个问题。直观来看，IR和host指令大概是可以对应上的，这里<br> 要解决的关键问题就是怎么把虚拟级的TCG寄存器映射到host物理寄存器上。</p>
<p> 我们先看下具体的两条riscv指令是怎么翻译成host指令的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">riscv guest汇编:</span><br><span class="line"></span><br><span class="line">  0x0000000000010172:  1101              addi            sp,sp,-32</span><br><span class="line">  0x0000000000010174:  ec06              sd              ra,24(sp)</span><br><span class="line"></span><br><span class="line">中间码：</span><br><span class="line"></span><br><span class="line">  ---- 0000000000010172 0000000000000000</span><br><span class="line">  add_i64 x2/sp,x2/sp,$0xffffffffffffffe0</span><br><span class="line">  </span><br><span class="line">  ---- 0000000000010174 0000000000000000</span><br><span class="line">  add_i64 tmp4,x2/sp,$0x18</span><br><span class="line">  qemu_st_i64 x1/ra,tmp4,leq,0</span><br><span class="line"></span><br><span class="line">ARM64 host汇编：</span><br><span class="line"></span><br><span class="line">    -- guest addr 0x0000000000010172 + tb prologue</span><br><span class="line">  0xffff9c000140:  b85f8274  ldur     w20, [x19, #-8]</span><br><span class="line">  0xffff9c000144:  7100029f  cmp      w20, #0</span><br><span class="line">  0xffff9c000148:  5400064b  b.lt     #0xffff9c000210</span><br><span class="line">  0xffff9c00014c:  f9400a74  ldr      x20, [x19, #0x10]</span><br><span class="line">  0xffff9c000150:  d1008294  sub      x20, x20, #0x20</span><br><span class="line">  0xffff9c000154:  f9000a74  str      x20, [x19, #0x10]</span><br><span class="line">    -- guest addr 0x0000000000010174</span><br><span class="line">  0xffff9c000158:  91006295  add      x21, x20, #0x18</span><br><span class="line">  0xffff9c00015c:  f9400676  ldr      x22, [x19, #8]</span><br><span class="line">  0xffff9c000160:  f83f6ab6  str      x22, [x21, xzr]</span><br></pre></td></tr></table></figure>
<p> riscv的addi被翻译成中间码add_i64, 注意中间码中的x2/sp是TEMP_GLOBAL类型的TCG寄存器，<br> riscv的sd指令被翻译成两条中间码，第一个中间码计算store的地址，并存在tmp4里，第二个<br> 中间码把ra寄存器的值保存到tmp4指向的地址。</p>
<p> 我们看下实际翻译出来的ARM64指令，第一条指令合并了一点tb头的指令，addi对应的host<br> 指令是从0xffff9c00014c这里开始的，从上面知道x19就是cpu_env的指针，0x10是riscv sp<br> 对应的TCG寄存器在cpu_env的偏移，所以“ldr x20, [x19, #0x10]”就是把保存在内存里的<br> guest CPU sp的值load到host寄存器x20上，下面sub指令对应的就是riscv的addi指令，然后<br> 紧接着一个str指令把sp的值更新回cpu_env，注意x20还是sp的值，所以, host还是可以使用<br> x20中保存的sp计算指令“sd ra,24(sp)”中sd要保存值的地址，翻译到host上的“add x21, x20, #0x18”，<br> x21保存sd要保存值的地址，后面的“ldr x22, [x19, #8]”同样把riscv的ra load到host寄存器<br> x22上，最后host使用“str x22, [x21, xzr]”完成“sd ra,24(sp)”的模拟。</p>
<p> 需要注意的是，如上的log是用qemu的user mode下得到的，user mode没有地址翻译，所以<br> store的模拟才会如此直接。如上的host寄存器对应的TCG寄存器类型: x19是fixed，x20/x22<br> 是global，x21是temp。寄存器如何分配、分配出的host寄存器什么时候可以重复利用、host<br> 寄存器上的值什么时候需要保存回cpu_env，这些都是活性分析和后端翻译要考虑的问题。</p>
<p> 寄存器活性分析代码主体逻辑如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_gen_code</span><br><span class="line">  +-&gt; liveness_pass_1(s);                                                         </span><br><span class="line">  /*</span><br><span class="line">   * nb_indirects的值在创建global TCG寄存器的时候更新: tcg_global_mem_new_internal，</span><br><span class="line">   * 这个函数会检测base入参的TCG类型，注意不是自己的TCG类型，如果base的类型是global</span><br><span class="line">   * 才会增加nb_indirects的计数。一般调用这个函数为guest gpr创建global TCG寄存器</span><br><span class="line">   * 都是用cpu_env作为base入参，所以nb_indirects的值都不会增加。</span><br><span class="line">   *</span><br><span class="line">   * 也就是qemu认为，对于global虚拟寄存器的访问，如果是通过一个fix寄存器作为指针</span><br><span class="line">   * 访问，就叫direct，但是如果不是，就叫indirect。针对indirect的访问需要进行额外</span><br><span class="line">   * 的liveness_pass_2优化。</span><br><span class="line">   *</span><br><span class="line">   * 目前还有没有想到需要liveness_pass_2优化的例子。</span><br><span class="line">   */</span><br><span class="line">  +-&gt; if (s-&gt;nb_indirects &gt; 0) &#123;</span><br><span class="line">          if (liveness_pass_2(s)) &#123;                                               </span><br><span class="line">              liveness_pass_1(s);                                                 </span><br><span class="line">          &#125;                                                                       </span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>

<p> 如上，我们目前先分析liveness_pass_1的逻辑，IR和TCG寄存器的数据结构大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_ctx:</span><br><span class="line">        +--------+---------+---------+---------+---------+</span><br><span class="line">        | temp0  |  temp1  |  temp2  |  temp3  |  temp4  |</span><br><span class="line">        +--------+---------+---------+---------+---------+</span><br><span class="line">           ^        ^                   ^           ^</span><br><span class="line">TB ops:    |        +----------------+  |           |</span><br><span class="line">           +-------------+-------+   |  +--------+  |                 </span><br><span class="line">                         |    +--+---+-----------+--+                ^</span><br><span class="line">                         |    |  |   |           |                   |</span><br><span class="line">        +----------------+----+--+---+-----------+----------------+  |</span><br><span class="line">        | +-----+      +-+--+ |  | +-+--+      +-+--+             |  |</span><br><span class="line">        | |insn0|      |arg0| |  | |arg1|      |arg2|        life |  |  parse insn</span><br><span class="line">        | +-----+      +----+ |  | +----+      +----+             |  |</span><br><span class="line">        +---------------------+--+--------------------------------+  |</span><br><span class="line">        +---------------------+--+--------------------------------+  |</span><br><span class="line">        | +-----+      +----+ |  | +----+                         |  |</span><br><span class="line">        | |insn1|      |arg0+-+  +-+arg1|                    life |  |</span><br><span class="line">        | +-----+      +----+      +----+                         |  |</span><br><span class="line">        +---------------------------------------------------------+  |</span><br><span class="line">          ...                                                        |</span><br></pre></td></tr></table></figure>
<p> 如上所示，前端翻译生成的IR组成一个IR链表，每个IR节点里有它自己的寄存器定义和life，<br> 这个life标记当前IR中每个寄存器的状态。IR中的每个TCG变量指向tcg_ctx中TCG变量的实际<br> 保存地址，活性分析对于TB中的IR，按照<strong>逆序</strong>逐个分析对应的IR和IR的TCG寄存器的状态，<br> 分析过程把TCG寄存器的状态动态的更新到tcg_ctx的TCG寄存器对象中，位置相对在上面的<br> IR的TCG寄存器状态受下面IR的TCG寄存器状态的影响，而下面的TCG寄存器状态在分析的时候<br> 已经更新到tcg_ctx的TCG寄存器对象中，每条IR的寄存器分析完后的静态状态保存在op-&gt;life<br> 里。</p>
<p> TCG寄存器的状态有两种，分别是TS_DEAD和TS_MEM，TS_DEAD的寄存器表示这个寄存器不会<br> 被后续(顺序)的指令使用，TS_MEM的表示寄存器需要向内存同步。</p>
<p> TCG寄存器在遍历开始的初始值是：global变量是TS_DEAD | TS_MEM, 其它是TS_DEAD。所有<br> global变量，比如gpr，都要刷回内存，其它的变量都是临时变量(cpu_env，sp也不需要刷会<br> 内存)，先都配置成dead，如果后续检测到寄存器之间存在依赖，再配置成live(我们把dead<br> 这个状态被去掉，认为TCG寄存器变成live状态)。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IR0: add temp2, temp4, temp5</span><br><span class="line">IR1: add temp1, temp2, temp3</span><br></pre></td></tr></table></figure>
<p> 用如上的两个IR举个例子，qemu从IR1开始分析，IR1的temp1/temp2/temp3都会配置成dead，<br> 因为后续没有指令，也就没有指令会使用，分析IR0时，qemu会检测到temp2在IR1里会使用，<br> 就会把IR0对应的temp2配置成live。可以看到寄存器dead/live的状态是针对具体指令的，<br> 寄存器dead/live状态会直接影响到后续host寄存器的分配，分配寄存器的时候，当一个虚拟<br> 寄存器是dead时，它后续不会被用，qemu就可以把这个虚拟寄存器对应的host寄存器重新分配<br> 使用，反之不行。</p>
<p> liveness_pass_1的逻辑大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">liveness_pass_1</span><br><span class="line">  /* 遍历开始，更新TCG寄存器为初始状态 */</span><br><span class="line">  la_func_end(s, nb_globals, nb_temps);                                       </span><br><span class="line">  /*</span><br><span class="line">   * 逆序遍历TB的中间码链表，除了几种类型的中间码要特殊处理下，剩余的都在默认</span><br><span class="line">   * 处理分支里(default)。需要单独处理的中间码有：call、insn_start、discard、</span><br><span class="line">   * 多输出的中间码(add2/sub2/mulu2/muls2_i32/i64)。</span><br><span class="line">   *</span><br><span class="line">   * 我们先关注default流程，然后再看需要单独处理的中间码。</span><br><span class="line">   */</span><br><span class="line">  QTAILQ_FOREACH_REVERSE_SAFE(op, &amp;s-&gt;ops, link, op_prev)</span><br><span class="line">  /* </span><br><span class="line">   * 如下是switch中的default的逻辑。</span><br><span class="line">   *</span><br><span class="line">   * 对于不是side_effect的指令，只要有输出参数不是dead，就不能去掉这条指令，否则</span><br><span class="line">   * 所有输出参数都dead了，这个指令就可以去掉了。</span><br><span class="line">   */</span><br><span class="line">  do_remove:</span><br><span class="line">    +-&gt; tcg_op_remove(s, op);                                               </span><br><span class="line"></span><br><span class="line">  /* 寄存器活性分析核心逻辑在这里 */</span><br><span class="line">  do_not_remove:</span><br><span class="line">    /*</span><br><span class="line">     * 首先处理IR的输出寄存器，根据TCG寄存器状态更新IR的life，更新完后把TCG寄存</span><br><span class="line">     * 器状态配置为dead，对于输出寄存器，必然不会对之前IR的寄存器有依赖。</span><br><span class="line">     */</span><br><span class="line">    for (i = 0; i &lt; nb_oargs; i++) &#123;</span><br><span class="line">        ts = arg_temp(op-&gt;args[i]);</span><br><span class="line"></span><br><span class="line">        op-&gt;output_pref[i] = *la_temp_pref(ts);</span><br><span class="line"></span><br><span class="line">        /* Output args are dead. */</span><br><span class="line">        if (ts-&gt;state &amp; TS_DEAD) &#123;</span><br><span class="line">            arg_life |= DEAD_ARG &lt;&lt; i;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (ts-&gt;state &amp; TS_MEM) &#123;</span><br><span class="line">            arg_life |= SYNC_ARG &lt;&lt; i;</span><br><span class="line">        &#125;</span><br><span class="line"> /*</span><br><span class="line">  * 这里对所有输出都配置dead，是因为显然当前这个寄存器在程序运行时不会依</span><br><span class="line">  * 赖前序指令中的这个寄存器。</span><br><span class="line">  */</span><br><span class="line">        ts-&gt;state = TS_DEAD;</span><br><span class="line">        la_reset_pref(ts);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * 处理TB结束、BB结束、条件跳转以及有side effect的指令。TCG_OPF_BB_EXIT是</span><br><span class="line">     * 离开TB，所以temp dead，global dead和sync。</span><br><span class="line">     */</span><br><span class="line">    if (def-&gt;flags &amp; TCG_OPF_BB_EXIT) &#123;</span><br><span class="line">        la_func_end(s, nb_globals, nb_temps);</span><br><span class="line">    /*</span><br><span class="line">     * 把条件branch指令单拿出来看，条件branch指令只是结束一个BB的一种情况, 结束</span><br><span class="line">     * 一个BB还有goto_tb和exit_tb，开始一个BB还有set_label。     </span><br><span class="line">     *                                                               </span><br><span class="line">     * insn5                         |  BB0                          </span><br><span class="line">     * insn6                         |                               </span><br><span class="line">     * brcond t0, t1, cond, label  --+                               </span><br><span class="line">     * insn1                       --+                               </span><br><span class="line">     * insn2                         |  BB1                          </span><br><span class="line">     * insn3                         |                               </span><br><span class="line">     * insn4     --------------------+                               </span><br><span class="line">     * set_label --------------------+                               </span><br><span class="line">     * insn7                         |  BB2                          </span><br><span class="line">     * insn8                         |                               </span><br><span class="line">     *                                                               </span><br><span class="line">     * 从下到上解析到brcond的时候，所有global和local要sync，但是不一定dead, 普通</span><br><span class="line">     * temp要都dead, 也就是说在程序执行时，后续指令不会再使用temp寄存器，这个和</span><br><span class="line">     * qemu规定的normal temp不能垮BB使用是一致的。对于所有的EBB和const，TCG寄存</span><br><span class="line">     * 器状态不改变。</span><br><span class="line">     */                                                                 </span><br><span class="line">    &#125; else if (def-&gt;flags &amp; TCG_OPF_COND_BRANCH) &#123;</span><br><span class="line">        la_bb_sync(s, nb_globals, nb_temps);</span><br><span class="line">    /*</span><br><span class="line">     * BB_END时，也就是br(直接跳转)、goto_tb、exit_tb以及set_label之前，在上面</span><br><span class="line">     * brcond的基础上EBB/const要dead，fixed要sync。但是原因是？</span><br><span class="line">     */</span><br><span class="line">    &#125; else if (def-&gt;flags &amp; TCG_OPF_BB_END) &#123;</span><br><span class="line">        la_bb_end(s, nb_globals, nb_temps);</span><br><span class="line">    /*</span><br><span class="line">     * 看起来只有ld/store是有effect的指令，根据qemu注释，load/store可能触发异常，</span><br><span class="line">     * global寄存器作为guest CPU的上下文信息，要刷回表示表示guest CPU的内存中。</span><br><span class="line">     */</span><br><span class="line">    &#125; else if (def-&gt;flags &amp; TCG_OPF_SIDE_EFFECTS) &#123;</span><br><span class="line">        la_global_sync(s, nb_globals);</span><br><span class="line">        /* 这里没有搞明白？*/</span><br><span class="line">        if (def-&gt;flags &amp; TCG_OPF_CALL_CLOBBER) &#123;</span><br><span class="line">            la_cross_call(s, nb_temps);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * 处理输入寄存器状态。对于输入寄存器，如果之前已经dead，对于本条指令，这个</span><br><span class="line">     * 寄存器是dead，因为后面没有人用了，所以配置当前IR的life。但是再往上遍历，</span><br><span class="line">     * 因为这个寄存器在这里使用了，就要激活TCG寄存器，这个就是下面一段代码做的事。</span><br><span class="line">     */</span><br><span class="line">    for (i = nb_oargs; i &lt; nb_oargs + nb_iargs; i++) &#123;                  </span><br><span class="line">        ts = arg_temp(op-&gt;args[i]);                                     </span><br><span class="line">        if (ts-&gt;state &amp; TS_DEAD) &#123;                                      </span><br><span class="line">            arg_life |= DEAD_ARG &lt;&lt; i;                                  </span><br><span class="line">        &#125;                                                               </span><br><span class="line">    &#125;                                                                   </span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * 激活输入TCG寄存器。当有一条指令的input用了一个寄存器，那么这个寄存器当然</span><br><span class="line">     * 要live了，这里配置成live，是给产生这个input的指令看的，当后续逆序解析到</span><br><span class="line">     * 这条指令的时候，对应的寄存器就不能是dead。</span><br><span class="line">     */</span><br><span class="line">    for (i = nb_oargs; i &lt; nb_oargs + nb_iargs; i++) &#123;                  </span><br><span class="line">        ts = arg_temp(op-&gt;args[i]);                                     </span><br><span class="line">        if (ts-&gt;state &amp; TS_DEAD) &#123;                                      </span><br><span class="line">            /* 得到可以使用的host寄存器的集合 */</span><br><span class="line">            *la_temp_pref(ts) = tcg_target_available_regs[ts-&gt;type];    </span><br><span class="line">            ts-&gt;state &amp;= ~TS_DEAD;                                      </span><br><span class="line">        &#125;                                                               </span><br><span class="line">    &#125;                                                                   </span><br><span class="line"></span><br><span class="line">    /* todo: 寄存器传递？*/</span><br></pre></td></tr></table></figure>

<p>下面是单独case处理的中间码的相关TCG寄存器的分析，insn_start直接跳过，因为insn_start<br>只是一个hint，discard表示这个指令标记的寄存器后面没有再使用了，所以直接配置对应<br>的TCG寄存器为dead，剩下是call和一堆二输出的中间码。</p>
<p>二输出中间码和对应的单输出的中间码的逻辑是一致的，只不过每个输入输出值是由两个TCG<br>寄存器组成，一个存放低32或64bit，一个存放高32或64bit。二输出的中间码又分为加减和<br>乘法两类，如果两个输出TCG都是dead，对应的IR可以删除，二输出的加减IR，如果只有高位<br>输出寄存器是dead，IR可以转化成单输出加减IR，二输出的乘法IR，输出其中之一是dead时，<br>IR可以分别转化为不同的单输出乘法IR(todo: 没有搞清这里的逻辑)。</p>
<p>call中间码用来支持helper函数，对于没有副作用的函数，如果输出都dead, 就可以删去掉<br>这个call中间码，也就没有对应的helper函数调用了。call的一般处理逻辑和上面的逻辑是<br>基本一致的，从IR的角度看，call可以看成是一条用户自定义的顺序执行的IR，并不会改变<br>BB的划分，另外qemu提供了一组call_flag，程序员可以用call_flag描述helper函数的一些<br>共同特征，比如有没有副作用、有没有对global寄存器的读写等，在处理call相关的寄存器<br>时，qemu根据这些call_flag做寄存器的同步。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">case INDEX_op_call:</span><br><span class="line">  /* 处理没有副作用的情况，这里主要是判断能不能删掉call */</span><br><span class="line">  if (call_flags &amp; TCG_CALL_NO_SIDE_EFFECTS)</span><br><span class="line">      [...]</span><br><span class="line"></span><br><span class="line">  /* 处理输出寄存器和上面的分析基本一致，但是会把op-&gt;output_pref[i]清0 */</span><br><span class="line">  [...]</span><br><span class="line">  op-&gt;output_pref[i] = 0;</span><br><span class="line"></span><br><span class="line">  /*</span><br><span class="line">   * 处理call_flag相关的global寄存器同步，helper函数里有对global寄存器的读写。</span><br><span class="line">   * kill和下面sync的区别是，kill是sync+dead。有对global的写，为啥要dead global? </span><br><span class="line">   * 这里和一个IR的输出是一样的逻辑，被写的global不可能是：在上面一条IR的输出，</span><br><span class="line">   * 然后这里的输入。</span><br><span class="line">   */</span><br><span class="line">  if (!(call_flags &amp; (TCG_CALL_NO_WRITE_GLOBALS | TCG_CALL_NO_READ_GLOBALS)))</span><br><span class="line">      la_global_kill(s, nb_globals);</span><br><span class="line">  /*</span><br><span class="line">   * helper函数里有对全局变量的读。因为helper函数有直接读global(读cpu_env上的变量), </span><br><span class="line">   * 所以调用之前必须把host寄存器上的值刷回内存。qemu这里是全部刷回内存了，其实</span><br><span class="line">   * 是没有必要的，只刷回helper函数里要用的就可以，但是这个信息比较难拿到。所以，</span><br><span class="line">   * 后续的后端翻译，要在call之前，要先处理对应的寄存器，而且其它的IR也要这样。</span><br><span class="line">   */</span><br><span class="line">  else if (!(call_flags &amp; TCG_CALL_NO_READ_GLOBALS))</span><br><span class="line">      la_global_sync(s, nb_globals);</span><br><span class="line"></span><br><span class="line">  /* 这里没有搞懂？*/</span><br><span class="line">  la_cross_call(s, nb_temps);</span><br><span class="line"></span><br><span class="line">  /*</span><br><span class="line">   * 处理输入寄存器和上面的分析基本一致，state_ptr上略有不同，call的输入参数可能</span><br><span class="line">   * 会很多，这里只能对host参数传递寄存器个数范围内的虚拟寄存器指定state_ptr。</span><br><span class="line">   * 另外特殊的地方是，call会直接分配输入寄存器。</span><br><span class="line">   */</span><br><span class="line">  *la_temp_pref(ts) = (i &lt; nb_call_regs ? 0 : tcg_target_available_regs[ts-&gt;type]);</span><br><span class="line">  tcg_regset_set_reg(*la_temp_pref(ts), tcg_target_call_iarg_regs[i]);</span><br></pre></td></tr></table></figure>

<h2 id="后端翻译"><a href="#后端翻译" class="headerlink" title="后端翻译"></a>后端翻译</h2><p> 进入后端翻译的主流程前在tcg_reg_alloc_start函数中先根据TCG寄存器类型(kind)得到<br> TCG寄存器的val_type域段的值，这个域段是一个动态值，指示的是TCG寄存器值对应的存储<br> 状态，比如TEMP_VAL_REG表示当前TCG寄存器的值保存在host寄存器上，TEMP_VAL_MEM表示<br> 当前TCG寄存器的值保存在内存里(cpu_env的TCG寄存器对应域段)，TEMP_VAL_CONST表示常量，<br> TEMP_VAL_DEAD表示一个寄存器不需要从TCG load到host寄存器使用。所以，具体的映射初始<br> 值是：fixed -&gt; TEMP_VAL_REG，global/local temp -&gt; TEMP_VAL_MEM，const -&gt; TEMP_VAL_CONST，<br> normal temp/ebb -&gt; TEMP_VAL_DEAD。</p>
<p> 正序遍历TB的IR链表，逐个翻译每个中间码和TCG寄存器，这个是后端翻译的主流程。可以<br> 看到，这里针对几个特殊的中间码做特殊处理，主流程在tcg_reg_alloc_op里。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">QTAILQ_FOREACH(op, &amp;s-&gt;ops, link) &#123;</span><br><span class="line">    case ...</span><br><span class="line">    default:</span><br><span class="line">        tcg_reg_alloc_op(s, op);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 分配过程会涉及到IR定义里的args_ct域段，这个域段描述IR翻译成特定host指令的限制，<br> 我们先看IR定义里的args_ct如何初始化，明确其中的含义。IR指令的定义在qemu公共代码<br> 里初始化:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* tcg/tcg-common.c */</span><br><span class="line">TCGOpDef tcg_op_defs[] = &#123;</span><br><span class="line">#define DEF(s, oargs, iargs, cargs, flags) \</span><br><span class="line">         &#123; #s, oargs, iargs, cargs, iargs + oargs + cargs, flags &#125;,</span><br><span class="line">#include &quot;tcg/tcg-opc.h&quot;</span><br><span class="line">#undef DEF</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p> 其中的参数分别是指令名字、输出参数个数、输入参数个数、指令控制参数个数(比如brcond<br> 里的cond)、指令flag(描述指令附加的一些属性)，注意这里只静态定义了每个IR的公共部分，<br> 并没有定义args_ct，args_ct和host指令的特点有关系，所以自然定义在具体host代码里。<br> args_ct初始化的代码路径是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_context_init</span><br><span class="line">      /* 遍历每个IR，得到host定义的针对每个IR的约束的定义 */</span><br><span class="line">  +-&gt; process_op_defs(s);</span><br><span class="line">        /*</span><br><span class="line">         * 如果host是riscv，tcg_target_op_def就是定义在tcg/riscv/tcg-target.c.inc，</span><br><span class="line">         * 可以看到con_set的值是一个枚举值，对应枚举元素的定义类似：c_o1_i2_r_r_rI</span><br><span class="line">         * 这个枚举类型定义在tcg/tcg.c，枚举元素include host上的具体定义：</span><br><span class="line">         * </span><br><span class="line">         * typedef enum &#123;</span><br><span class="line">         * #include &quot;tcg-target-con-set.h&quot;  &lt;- 如果rv是host，就是tcg/riscv/tcg-target-con-set.h</span><br><span class="line">         * &#125; TCGConstraintSetIndex;</span><br><span class="line">         *</span><br><span class="line">         * 继续从constraint_sets得到这个宏对应的字符串，这里重定义了名字相同的参数</span><br><span class="line">         * 宏，使得名字相同的宏对应的代码不一样：</span><br><span class="line">         * </span><br><span class="line">         * static const TCGTargetOpDef constraint_sets[] = &#123;</span><br><span class="line">         * #include &quot;tcg-target-con-set.h&quot;</span><br><span class="line">         * &#125;;</span><br><span class="line">         *</span><br><span class="line">         * 比如，还是如上的枚举元素，这里得到的tdefs包含一个字符串数组，其中的</span><br><span class="line">         * 每个字符串是：r，r，rI，一个参数可能有多个属性的叠加，比如这里的最后</span><br><span class="line">         * 一个参数就有r和I。</span><br><span class="line">         *</span><br><span class="line">         * 这代码写的也是风骚！具体字符的解析下面分析。</span><br><span class="line">         */</span><br><span class="line">    +-&gt; con_set = tcg_target_op_def(op);</span><br><span class="line">    +-&gt; tdefs = &amp;constraint_sets[con_set];</span><br><span class="line">          /* 解析一个IR中每个输入输出参数的限制，更新到args_ct域段 */</span><br><span class="line">      +-&gt; for (i = 0; i &lt; nb_args; i++) &#123;</span><br><span class="line">              while (*ct_str != &#x27;\0&#x27;) &#123;</span><br><span class="line">              /* 数字的含义没有看懂，似乎表示alias */</span><br><span class="line">              case &#x27;0&#x27; ... &#x27;9&#x27;</span><br><span class="line">              /* 很少用到，是需要分一个新寄存器的意思？*/</span><br><span class="line">              case &#x27;&amp;&#x27;</span><br><span class="line">              /* 表示需要一个常数，但是rv上是i */</span><br><span class="line">              case &#x27;i&#x27;:</span><br><span class="line">              &#125;</span><br><span class="line">              /*</span><br><span class="line">               * 特定host还可以自定寄存器的限制塞到这里，比如，rv在这里塞了如下</span><br><span class="line">               * 的case。从这里也可以看出，args_ct里reg表示寄存器的约束，ct表示</span><br><span class="line">               * 常量的约束。</span><br><span class="line">               *</span><br><span class="line">               * 特定host的约束在host代码里具体定义，比如，下面的ALL_GENERAL_REGS</span><br><span class="line">               * 就定义在tcg/riscv/tcg-target.c.inc，是MAKE_64BIT_MASK(0, 32)</span><br><span class="line">               *</span><br><span class="line">               * qemu里分配寄存器的公共代码的入参中就包含了这里定义的具体约束。</span><br><span class="line">               */</span><br><span class="line">              case &#x27;r&#x27;: def-&gt;args_ct[i].regs |= ALL_GENERAL_REGS; ct_str++; break;</span><br><span class="line">              ...</span><br><span class="line">              case &#x27;I&#x27;: def-&gt;args_ct[i].ct |= TCG_CT_CONST_S12; ct_str++; break;</span><br><span class="line">              ...</span><br><span class="line">          &#125;</span><br><span class="line">          /* 根据特定优先级分配为输出输入参数排序 */</span><br><span class="line">      +-&gt; sort_constraints(def, 0, def-&gt;nb_oargs);</span><br><span class="line">          sort_constraints(def, def-&gt;nb_oargs, def-&gt;nb_iargs);</span><br></pre></td></tr></table></figure>
<p> 单个IR生成host指令以及分配host寄存器的过程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcg_reg_alloc_op(s, op)</span><br><span class="line">      /*</span><br><span class="line">       * 整个翻到host指令的过程，关键是分配寄存器，在一个TB里分配寄存器，那么就</span><br><span class="line">       * 要有中间变量把分配的和还没有分配的寄存器记录下来。</span><br><span class="line">       *</span><br><span class="line">       * reserved_regs表示被保留起来的host上寄存器，TB块里不能用, 所以这里直接</span><br><span class="line">       * 标记为已分配。注意对于reserved_regs这里有一个独立的逻辑，拿riscv后端举</span><br><span class="line">       * 例，除了sp/tp/gp/zero寄存器，riscv还会把reserve出几个gpr临时使用，公共</span><br><span class="line">       * 的寄存器分配已经基本完成了host物理寄存器的分配，但是在具体创建host指令</span><br><span class="line">       * 的时候有的IR可能还需要插入新host指令，这样就需要分配新的host物理寄存器，</span><br><span class="line">       * 提前reserve出的host gpr就是用来做这个。</span><br><span class="line">       *</span><br><span class="line">       * 这个函数只是翻译一个IR，几个核心的数据结构的含义是：new_args[]、con_args[]</span><br><span class="line">       * 指的是当前这个IR翻译到host指令时，分配得到的host物理寄存器。s中的reserved_regs、</span><br><span class="line">       * reg_to_temp[]指的是tb翻译上下文host寄存器的分配情况，所以，当一个host</span><br><span class="line">       * 物理寄存器被分配或释放时需要更新s中的reg_to_temp[]对应元素。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; i_allocated_regs = s-&gt;reserved_regs;</span><br><span class="line">      o_allocated_regs = s-&gt;reserved_regs;</span><br><span class="line"></span><br><span class="line">      /* 处理输入参数 */</span><br><span class="line">  +-&gt; for (k = 0; k &lt; nb_iargs; k++) &#123;</span><br><span class="line">      /*</span><br><span class="line">       * 处理输入参数限制, 参数优先级排序还不清楚? 如果是常数，并且根据host</span><br><span class="line">       * 指令可以表示常量的限制，判断是否可以直接把常量编码到指令里，如果可以，</span><br><span class="line">       * 就把信息记录在const_args和new_args，这两者都是后续生成host指令的参数。</span><br><span class="line">       * 如果无法编码到指令里，就要分配寄存器，使用寄存器保存参数，参与计算。</span><br><span class="line">       *</span><br><span class="line">       * 看一个实际的例子，host是riscv时，翻译add_i64这个中间码。add_i64的两个</span><br><span class="line">       * 输入参数，如果后一个参数是立即数，就有可能被翻译成riscv上的addi，否则</span><br><span class="line">       * 就需要翻译成add，但是addi的立即数只有12bit位宽，如果装不下的话，还是需要</span><br><span class="line">       * 把立即数搬到host寄存器上，翻译成add指令。参照如上分析，这个立即数位宽</span><br><span class="line">       * 的限制被生成到add_i64中间码的arg_ct的ct域段，具体在riscv tcg_target_op_def</span><br><span class="line">       * case INDEX_op_add_i64的C_O1_I2(r, r, rI)，rI表示add_i64翻译到riscv上时，</span><br><span class="line">       * 最后一个参数可以被翻译到host寄存器或者立即数编码到指令。当qemu前端翻译</span><br><span class="line">       * 使用add_i64这个中间码并且第二个入参定义成CONST时，如下的代码会得到这个</span><br><span class="line">       * 信息并使用tcg_target_const_match做检测，如果检查成功就做个标记，同时结束</span><br><span class="line">       * 当前寄存器的分配，后面就会根据这个标记生成addi，把第二个虚拟寄存器直接</span><br><span class="line">       * 编码到host指令里。如果检测不成功，就继续分配host物理寄存器，所以可见const</span><br><span class="line">       * 虚拟寄存器可能直接编码到指令或者占用一个host物理寄存器。</span><br><span class="line">       */</span><br><span class="line">      if (ts-&gt;val_type == TEMP_VAL_CONST</span><br><span class="line">          &amp;&amp; tcg_target_const_match(ts-&gt;val, ts-&gt;type, arg_ct-&gt;ct)) &#123;</span><br><span class="line">          const_args[i] = 1;</span><br><span class="line">          new_args[i] = ts-&gt;val;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      /* ialias这个没有看懂？*/</span><br><span class="line">      if (arg_ct-&gt;ialias) &#123;&#125;</span><br><span class="line"></span><br><span class="line">      /*</span><br><span class="line">       * 给输入参数分配host寄存器，并且从cpu_env中load输入参数到host寄存器，为后</span><br><span class="line">       * 续计算做准备。arg_ct-&gt;regs是host指令在host寄存器分配上的限制。</span><br><span class="line">       *</span><br><span class="line">       * 从这个函数的逻辑就可以看出TCG寄存器val_type的语意，它表示TCG寄存器当前</span><br><span class="line">       * 的存储状态。TEMP_VAL_REG表示已经在host寄存器里，所以直接返回。</span><br><span class="line">       *</span><br><span class="line">       * TEMP_VAL_CONST表示是一个常量(并且当前保存在TCG寄存器)，这里就要分配一个</span><br><span class="line">       * host寄存器，并且生成一条host movi指令把这个常量送到host寄存器上，并且</span><br><span class="line">       * 配置ts-&gt;mem_coherent = 0，这个表示TCG寄存器和host寄存器不同步。</span><br><span class="line">       *</span><br><span class="line">       * TEMP_VAL_MEM表示TCG寄存器的值在内存里，这里要分配host寄存器，并把对应</span><br><span class="line">       * 的值load进host寄存器，同时配置ts-&gt;mem_coherent = 1。从host分配物理寄存器</span><br><span class="line">       * 就会有分不到的情况，遇到这种情况就需要把host寄存器先换到内存，并且标记</span><br><span class="line">       * 这个虚拟寄存器的值在内存上，tcg_reg_free完成这个动作。</span><br><span class="line">       * </span><br><span class="line">       * TEMP_VAL_DEAD表示一个寄存器不需要从TCG load到host寄存器使用。tcg_reg_alloc_start</span><br><span class="line">       * 把TEMP_NORMAL/TEMP_EBB转换成TEMP_VAL_DEAD，像normal temp和ebb这种中间</span><br><span class="line">       * 计算产生的数据，显然始终产生于一个左值，用于存放临时变量(生命周期只在</span><br><span class="line">       * 一个BB内)，不需要刷回内存，更不需要从没存load进host寄存器。</span><br><span class="line">       */</span><br><span class="line">      temp_load(s, ts, arg_ct-&gt;regs, i_allocated_regs, i_preferred_regs);</span><br><span class="line">            /* 如上分析，处理各种val_type的情况 */</span><br><span class="line">        +-&gt; switch (ts-&gt;val_type) &#123;&#125;</span><br><span class="line"></span><br><span class="line">            /*</span><br><span class="line">             * 返回用掉的寄存器，注意这里一定会分到host物理寄存器，没有时也会像</span><br><span class="line">             * 上面说的那样换一个host物理寄存器出来。</span><br><span class="line">             */</span><br><span class="line">        +-&gt; ts-&gt;reg = reg;</span><br><span class="line">            /* TEMP_VAL_REG表示这个虚拟机寄存器的值已经在host寄存器里了 */</span><br><span class="line">        +-&gt; ts-&gt;val_type = TEMP_VAL_REG;</span><br><span class="line">            /* 表示当前物理寄存器对应的虚拟寄存器 */</span><br><span class="line">        +-&gt; s-&gt;reg_to_temp[reg] = ts;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      /*</span><br><span class="line">       * 处理dead输入寄存器。上面是考虑如何往出分配host物理寄存器，这里是考虑怎</span><br><span class="line">       * 么回收host物理寄存器。只需要考虑已经dead的寄存器，就是后续不会再用的寄</span><br><span class="line">       * 存器，对于global、local temp，要刷回cpu_env内存，对于normal temp、ebb</span><br><span class="line">       * 只需要dead就好，它们本来就是临时变量。</span><br><span class="line">       *</span><br><span class="line">       * 需要注意的是，temp_free_or_dead这里使用的是dead，也就是对于normal temp</span><br><span class="line">       * 和ebb是dead，另外针对normal temp和ebb的free操作体现在host寄存器不够用的</span><br><span class="line">       * 时候，qemu可以把这些临时变量也换到内存里。(这些临时变量何时销毁？)</span><br><span class="line">       *</span><br><span class="line">       * 这里并没有插入刷到内存的具体操作，只是更新虚拟寄存器的存储状态(val_type)，</span><br><span class="line">       * 以及tb翻译上下文中host物理寄存器的翻译状态(s-&gt;reg_to_temp): 如果当前已</span><br><span class="line">       * 经分配物理寄存器，但是dead了，那么这个物理寄存器就可以给其它虚拟寄存器</span><br><span class="line">       * 用，所以，清理掉reg_to_temp中reg到temp的指向。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; for (i = nb_oargs; i &lt; nb_oargs + nb_iargs; i++) &#123;</span><br><span class="line">        if (IS_DEAD_ARG(i)) &#123;</span><br><span class="line">          temp_dead(s, arg_temp(op-&gt;args[i]));</span><br><span class="line">            temp_free_or_head()</span><br><span class="line">            [...]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      /*</span><br><span class="line">       * 检查条件跳转、BB结尾以及side effect的情况, 处理call_clobber。检查只是做</span><br><span class="line">       * assert，没有逻辑处理。</span><br><span class="line">       */</span><br><span class="line">  +-&gt; if (def-&gt;flags &amp; TCG_OPF_COND_BRANCH)</span><br><span class="line">        [...]</span><br><span class="line">      else if (def-&gt;flags &amp; TCG_OPF_BB_END)</span><br><span class="line">        [...]</span><br><span class="line">      else &#123;</span><br><span class="line">        /*</span><br><span class="line">         * 已经有分支单独处理call，这里就只处理st/ld了，这里会free clobber寄存器,</span><br><span class="line">         * 原因是什么？</span><br><span class="line">         */</span><br><span class="line">        if (def-&gt;flags &amp; TCG_OPF_CALL_CLOBBER)</span><br><span class="line">          [...]</span><br><span class="line">        /* 只是做检查，没有真实sync。确保内存里记录的虚拟寄存器值是对的 */</span><br><span class="line">        if (def-&gt;flags &amp; TCG_OPF_SIDE_EFFECTS)</span><br><span class="line">          [...]</span><br><span class="line"></span><br><span class="line">        /* 处理输出参数 */</span><br><span class="line">  +---&gt; for(k = 0; k &lt; nb_oargs; k++) &#123;</span><br><span class="line">          /* arg_ct这段没有看懂？如下是最后一个分支 */</span><br><span class="line">          reg = tcg_reg_alloc(s, arg_ct-&gt;regs, o_allocated_regs,</span><br><span class="line">                              op-&gt;output_pref[k], ts-&gt;indirect_base);</span><br><span class="line"></span><br><span class="line">          tcg_regset_set_reg(o_allocated_regs, reg);</span><br><span class="line">          /* 没有理解这里？*/</span><br><span class="line">          if (ts-&gt;val_type == TEMP_VAL_REG) &#123;</span><br><span class="line">              s-&gt;reg_to_temp[ts-&gt;reg] = NULL;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          /* 分了一个物理寄存器，所以这个虚拟寄存器的值现在保存在物理寄存器上 */</span><br><span class="line">          ts-&gt;val_type = TEMP_VAL_REG;</span><br><span class="line">          ts-&gt;reg = reg;</span><br><span class="line"></span><br><span class="line">          /* TCG寄存器对应的物理寄存器和内存值当前不一致性 */</span><br><span class="line">          ts-&gt;mem_coherent = 0;</span><br><span class="line">          s-&gt;reg_to_temp[reg] = ts;</span><br><span class="line">          new_args[i] = reg;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      /* 根据参数构建指令，各个不同host实现自己的回调 */</span><br><span class="line">  +-&gt; tcg_out_op(s, op-&gt;opc, new_args, const_args);</span><br><span class="line"></span><br><span class="line">      /* 把global输出寄存器刷回cpu_env */</span><br><span class="line">  +-&gt; for(i = 0; i &lt; nb_oargs; i++) &#123;</span><br><span class="line">        /*</span><br><span class="line">         * 核心是处理sync，实现寄存器活性分析里已经确定的需要sync的寄存器，TEMP_VAL_REG</span><br><span class="line">         * 即当前值在host寄存器上时，才要刷回内存。为什么TEMP_VAL_CONST有时要先load在store？</span><br><span class="line">         */</span><br><span class="line">        if (NEED_SYNC_ARG(i))</span><br><span class="line">            temp_sync(s, ts, o_allocated_regs, 0, IS_DEAD_ARG(i));</span><br><span class="line"></span><br><span class="line">        /* 处理只dead的输出寄存器，只处理寄存器分配层面的逻辑 */</span><br><span class="line">        else if (IS_DEAD_ARG(i))</span><br><span class="line">            temp_dead(s, ts);</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>

<p>下面看剩余IR的翻译，主要是mov/call/set_label。</p>
<p>qemu实现set_label的思路是，先把label记录在TCGContext的labels链表里，后端翻译br/brcond<br>时，把跳转指令的地址和labels建立联系，把跳到一个label的跳转指令地址都记录在label<br>的relocs链表里，注意一个label可能从不同的地方跳进来，在tcg_gen_code的结尾处调用<br>tcg_resolve_relocsg更新所有label对应的跳转指令的目的地址。</p>
<p>call其实就是要在TB function上下文里实现helper函数的调用。可以想象qemu的后端翻译<br>要做的有：helper函数入参准备，这个包括入参寄存器不够用的时候，用栈传递函数入参，<br>保存caller save寄存器，跳转到helper函数执行，从栈内恢复caller save寄存器。从qemu<br>具体的实现上看，caller save寄存器并没有被保存到栈上，而至直接刷回了对应cpu_env里，<br>这样只要改变下寄存器存储位置的标识就好，也不需要恢复caller save寄存器。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>超标量处理器中访存指令的处理逻辑</title>
    <url>/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E4%B8%AD%E8%AE%BF%E5%AD%98%E6%8C%87%E4%BB%A4%E7%9A%84%E5%A4%84%E7%90%86%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>load/store指令被取入处理器内部后，进过各级流水线的处理，最终会到达LSU部件(load store unit)，<br>之前的各级流水只是对load/store指令做一些通用的处理，比如，decode，rename等。</p>
<p>load/store指令到达LSU后，理论上看如果没有地址上的依赖，计算出访问地址以及数据寄存器<br>ready后应该立即投入执行。展开看看load/store存在的地址上的依赖，地址依赖和指令间的<br>寄存器依赖比较类似，不同的是RAW/WAR/WAW都是真依赖，比如，WAR对相同地址先读后写，<br>后面的store先执行逻辑一定错了。</p>
<p>所以这里的问题就是，严格按照地址依赖关系把load/store投入执行，处理器性能就会低，<br>load/store在没有计算出访问地址之前投机执行，就需要冲突检测逻辑以及出现冲突时flush<br>流水线。现在的CPU一般会按顺序执行store，提前投机执行load，因为load是把数据取入CPU，<br>处于依赖链条的前端，load取到数据后，后续依赖它的指令就可以继续投机执行。</p>
<p>我们依照顺序执行store、提前投机执行load的基本逻辑，看看应该如何处理如上的各种冲突。<br>首先因为store是顺序执行的，就不会有WAW的冲突了。</p>
<p>针对WAR违例的处理。如下x1/x3的寄存器里的值相同，处理器先投机执行了store，因为处理<br>器里的指令要顺序提交，所以指令B写出的值需要先保存在处理器内部buffer里，一般load指<br>令在取入数据时会先搜索如上的处理器内部buffer，但是这里因为store指令比load新，所以<br>load不能用处理器内部buffer(store buffer)里的值，load需要从cache里取数据进来。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  A:  load x0, (x1)	</span><br><span class="line">      ...</span><br><span class="line">  B:  store x2, (x3)</span><br><span class="line"></span><br><span class="line">+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+  old</span><br><span class="line">|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  </span><br><span class="line">+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+</span><br><span class="line">       ^              ^           ^        ^  \-------------/</span><br><span class="line">    allocate          B           A      commit  retired</span><br></pre></td></tr></table></figure>
<p>这么看，处理器只要对LSU里的load/store做编号，load取数据的时候不用store buffer里<br>比它年轻(标号比load大的store)的store指令的输出就好。</p>
<p>针对RAW违例的处理。如下x1/x3的寄存器里的值相同，B处的load指令先投机执行了，B后依赖<br>B的指令也可以投机执行，当A处的store指令执行时，需要检查是否有访问相同地址的更年轻<br>的load指令已经执行，如果有，那么就检测到了RAW违例。发生RAW违例时，load指令以及比<br>它年轻的指令的运算都可能使用错误的数据，所以都要被flush掉。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  A:  store x2, (x3)</span><br><span class="line">      ...</span><br><span class="line">  B:  load x0, (x1)	</span><br><span class="line">  C:  add x4, x0, x5</span><br><span class="line"></span><br><span class="line">flush  &lt;--------------+</span><br><span class="line">+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+  old</span><br><span class="line">|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  </span><br><span class="line">+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+</span><br><span class="line">       ^           ^  ^           ^        ^  \-------------/</span><br><span class="line">    allocate       C  B           A      commit  retired</span><br></pre></td></tr></table></figure>
<p>allocate到B的指令可以立即做flush也可以等到B提交的时候再做flush，其中的不同在于，<br>flush指令不只是释放ROB的资源，被flush指令在CPU里各个部件上占用的资源都要释放，立即<br>做flush可以及时释放错误占用的硬件资源，但是需要增加额外的检测电路，从CPU的各个部件<br>里把flush指令挑出来，等到B提交时，比B老的指令都已经退休，流水线里都是需要被flush<br>掉的指令(检测到RAW违例要停止fetch指令)，flush逻辑实现会比较简单, 但是因为处理器一<br>直在等B到提交状态，性能就会比较差。</p>
<p>从上面的分析得到，我们想象的LSU里至少需要有：load queue、store queue、store buffer，<br>其中load queue是一个无序的容器，store queue和store buffer是FIFO。</p>
<p>对于一个store指令，其投机执行时得到的要素必须是先缓存到CPU内部buffer里的，我们就<br>先叫这个内部buffer为store buffer，所谓store指令投机执行的要素是指store的地址和数据。<br>当store指令处于待提交状态时，store可以把数据写到store地址对应的存储单元上，就是把<br>数据写入cache或者内存，理论上只有这个写操作生效，store指令才可以retired，处理器才<br>能继续提交后续的指令，但是写cache或内存相比处理器执行指令是一个慢速操作，处理器不<br>能一直等着这个动作完成，所以这里还要有一个缓存，CPU把commit的store按顺序放入这个<br>缓存，这个缓存里的store操作本质上已经是构架状态，必须成功完成，CPU认为放入这个缓存<br>的store指令已经retire，于是CPU继续顺序提交ROB上的后续待提交指令，可以把后面这个缓存<br>成为store buffer 2。</p>
<p>需要注意的是如上提到的两个store buffer逻辑上是独立的，实现中可以合并在一个buffer。<br>这个在《超标量处理器设计》中有提及，就是给store buffer上的每个位置设置三种状态:<br>(1) no-complete store指令进入store buffer，地址和数据还在计算中；(2) complete store<br>指令的地址和数据都准备好了；(3) retired store已经被提交处于retired状态。需要注意的<br>是1/2还是处理器内部状态，处于1/2的store是可能被flush掉的，3已经是CPU外部状态，而且<br>如上所述，必须成功写入存储器。</p>
<p>这里的新问题就是怎么保存3一定可以成功写入存储器，纵然CPU在把一个store变成3状态时，<br>可以确保TLB命中(包括做Page Table Walk之后重新fill TLB的情况)，但是当处于3状态的store<br>在store buffer里排队时，VA-&gt;PA的映射关系也可能发生变换，随后的store访问就会出错。<br>这么看起来，当页表发生变换后，使用TLB无效化指令做同步时，TLB无效化指令要flush这里<br>store buffer里处于3状态的对应store操作。</p>
<p>LSU和MMU的关系。load/store通过LSU进行访存操作，当系统里开启虚拟地址时，load/store<br>指令使用的地址是虚拟地址，这时就需要先通过MMU把虚拟地址翻译为物理地址，随后使用物理<br>地址访问内存。这个过程中TLB作为地址翻译的cache，可以大大加速地址翻译效率。当MMU无法<br>把虚拟地址翻译为物理地址时，对应的load/store需要报异常。</p>
<p>LSU和cache的关系。cache作为存储系统的最前端直接和LSU做交互，当store的值缓存在store<br>buffer里时，外部不可见(逻辑上看，如上状态3的store操作已经外部可感知)，当写入cache<br>时，外部可见，CPU无法撤销。</p>
<p>多核系统下的LSU。Paul很出名的文章《Memory Barriers: a Hardware View for Software Hackers》<br>里指出之所以存在barrier是CPU微架构上引入store buffer和invalid queue导致的，我们看看<br>能不能把这里的store buffer和LSU里的store buffer的概念整合到一起理解。</p>
<p>其实，我们这里讨论的store buffer和文章里讨论的store buffer逻辑上是一致的，commit<br>store进store buffer的原因都是防止核被挂住，在多核情况下，一个store操作还需要和其它<br>核做通信，如果一直等其它核的响应，虽然可以保证数据的一致性，但是牺牲了本核的性能。</p>
<p>注意这里的关键点是store buffer里处于3状态的store操作如何写存储器，如果对于不同地址<br>的store操作依然是顺序的，就不会出问题，如果对于不同地址的store是可以乱序的，就会<br>出现文章里错误执行的行为。只不过文章里，对于可以直接写cache的操作就直接写cache了，<br>这个和所有commit store都进store buffer，store buffer里可以乱序是一样的行为。</p>
<p>考虑把invalid queue的逻辑也一起整合进来。CPU里的一个core(B)收到另一个core(A)发给<br>它的一个invalid cache请求，core B自己还有一堆事要做，于是core B把这个invalid cache<br>请求放到自己的invalid queue里，回响应给core A，随后core B再去做invalid queue中的请求。<br>但是，正如上面所提到的，core B会投机执行它自己核上的load指令，如果投机执行正确，<br>Core B完全可能提交的是invalid cache之前就拿到的cache上的数据，这样load提交的结果<br>就错了。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>超标量处理器指令发射的基本逻辑</title>
    <url>/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E6%8C%87%E4%BB%A4%E5%8F%91%E5%B0%84%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p>超标量处理器中，指令在完成rename后通过分发逻辑把指令分发(dispatch)到各个不同的执行<br>单元处理，指令之间存在寄存器上的依赖，所以指令被dispatch到执行单元后不能马上指令，<br>它们会被缓存在执行单元的buffer中，直到指令的依赖满足才会被发到执行单元上去执行。</p>
<p>如下是一个简单的示意图，其中issue q就是我们这里说的执行单元的buffer。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                            +-------+   +----+</span><br><span class="line">                  +-----+--&gt;|issue q|--&gt;| EX |\</span><br><span class="line">                  |issue|   +-------+   +----+ \</span><br><span class="line">                  |logic|                       \</span><br><span class="line">+----+   +----+   |     |   +-------+   +----+   \+-----+    +----+</span><br><span class="line">| IF |--&gt;| ID |--&gt;|     |--&gt;|issue q|--&gt;| EX |---&gt;| MEM |---&gt;| WB |</span><br><span class="line">+----+   +----+   |     |   +-------+   +----+   /+-----+    +----+</span><br><span class="line">                  |     |                       /</span><br><span class="line">                  |     |   +-------+   +----+ /</span><br><span class="line">                  +-----+--&gt;|issue q|--&gt;| EX |/</span><br><span class="line">                            +-------+   +----+</span><br></pre></td></tr></table></figure>
<p>考虑这里面硬件需要解决的问题：1. issue q里需要有资源维护指令的状态，指令操作数<br>没有准备好时，指令不能发射，指令操作数准备好后，发射指令到执行单元执行; 2. 对于<br>issue q需要有仲裁逻辑挑出准备就续的指令发射执行(同一时刻可能有多条指令准备就绪);<br>3. EX/WB需要把指令执行的完毕这个信息反馈给依赖它的指令；4. 当拍数可控时，反馈信号<br>可以不等到指令执行完毕，这样可以使得前一条指令和后面依赖指令背靠背执行，流水线里<br>没有气泡，诸如此类的优化存在于很多地方。</p>
<h2 id="issue-queue结构"><a href="#issue-queue结构" class="headerlink" title="issue queue结构"></a>issue queue结构</h2><p>《超标量处理器设计》上按照不同维度描述了issue queue的可能组织形式。</p>
<p>按照执行单元和issue queue的对应关系，分成了集中式和分布式的issue queue，现在的CPU<br>里一般是两者的折中，比如几个执行单元共用一个issue queue。集中式issue queue的好处<br>是资源可以被有效利用，分布式issue queue里即使有空位，不同执行单元之间的指令也不能<br>混用issue queue。集中式issue queue的缺点是实现比较复杂。</p>
<p>按照指令输入数据的获取方式，issue queue可以分成捕获式和非捕获式，捕获式的issue queue,<br>指令在进入其中的时候，就会把物理寄存器的值保存入issue queue，非捕获式的issue queue,<br>在指令发射到执行单元时，才从物理寄存器堆中读取指令输入值，两者在issue queue里都要<br>有标记位记录指令输入的状态，当一个指令的输入都准备好时，指令可以被仲裁和反射。</p>
<p>按照指令在issue queue里的移动方式，issue queue被分成了压缩和非压缩两种，对于压缩<br>式的issue queue，当有一个指令被发射出去后，后续指令依次占据之前指令的位置，对于非<br>压缩式的issue queue，指令在其中占据固定的位置，指令进入issue queue时，需要找见一个<br>空位，指令离开issue queue时，直接离开就好。可以想象，压缩式的issue queue，指令在<br>其中频繁移动，功耗就会比较大。需要注意的是，虽然这里叫queue，但是其中的指令都是<br>操作数准备好就发射的，也就是在没有依赖关系的时候是乱序发射的。</p>
<p>可以看到，各个执行单元和各个issue queue的反馈逻辑是一个网状结构，一个issue queue<br>里等待发射的指令可能依赖别的执行单元的执行结果。</p>
<h2 id="仲裁的逻辑"><a href="#仲裁的逻辑" class="headerlink" title="仲裁的逻辑"></a>仲裁的逻辑</h2><p>一般，仲裁逻辑都是取出年龄最老的指令发射执行，因为最老的指令处于依赖链条的前端，<br>执行最老的指令可以使得后续指令尽量提前投机执行。</p>
<h2 id="唤醒的逻辑和优化"><a href="#唤醒的逻辑和优化" class="headerlink" title="唤醒的逻辑和优化"></a>唤醒的逻辑和优化</h2><p>可以在写回阶段或者通过各种旁路网络做指令唤醒，所谓唤醒，就是标记后续依赖指令的输入<br>数据是有效状态。基本功能的逻辑很直白，但是硬件在实现的时候需要仔细计算唤醒的具体<br>时间点，使得前后指令可以很好的衔接起来，这里要解决的问题是：1. 不能出错；2. 性能<br>要高。</p>
<p>所以，这里的指导原则就是尽量早点唤醒，因为后续的指令可能是多拍完成的，用到依赖指令<br>结果的可能是后面几拍，早一点唤醒可以使得后续指令中没有依赖关系的几拍先运行，这样<br>依赖数据生成和实际使用无缝衔接在了一起。</p>
<p>但是有些指令的执行拍数不是一个确定值，比如load指令，大部分情况下load会直接命中cache，<br>但是也有少量cache不命中的情况。对于这种情况，可以投机的去做唤醒，这样做的基础是大<br>部分情况会命中cache，load拍数可控，投机的结果是对的，对于少量的投机错误的情况，因为<br>过早唤醒依赖load的指令拿到了错误的数据，load后的指令要被flush掉或者根据正确load到的<br>数据重新执行(replay)。</p>
<h2 id="replay"><a href="#replay" class="headerlink" title="replay"></a>replay</h2><p>如上已经提到，对于输入数据错误的指令，可以不必flush掉，而是重新从issue queue里使用<br>正确的数据重新发射指令执行，对于分支预测出错，还是要把错误指令flush掉的。指令replay<br>可以省去重新fetch指令以及decode/rename/dispatch的步骤，因为这个对于数据输入错误的<br>指令本来就是不必要的步骤，但是，要想做replay，就需要在issue queue里继续保留发射执行<br>的指令，这样会挤占issue queue位置，可能造成issue queue变满，反压前端的rename步骤。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>超标量处理器分支预测基本逻辑</title>
    <url>/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E5%88%86%E6%94%AF%E9%A2%84%E6%B5%8B%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<h2 id="分支预测基本逻辑"><a href="#分支预测基本逻辑" class="headerlink" title="分支预测基本逻辑"></a>分支预测基本逻辑</h2><p>超标量处理器遇到分支指令的时候会预测下分支指令的目的地址，然后从预测的地址fetch<br>指令执行，这些投机执行的指令处于分支预测的路径上，所以处理器只能把它们的结果缓存<br>在内部，如果后面发现预测的分支是对的，就把投机执行的指令的结果提交，如果后面发现<br>是分支预测错了就需要把投机执行的执行从流水线里抹去。</p>
<p>所以可以看到硬件上的BP部件一定是顶在流水线最前端的，比如对于一个一拍fetch 4条指令<br>的超标量处理器，第二拍它又可以去fetch 4条指令，但是从哪个地址上去fetch呢，如果第<br>一拍fetch的四条指令里就有分支指令，那么分支预测的结果最好第二拍就可以拿到，否则<br>第二拍的fetch就会空转(stall)，处理器的性能就会下降。</p>
<p>我们先忽略直接跳转的分支指令，其它分支指令可以分为条件跳转和间接跳转，条件跳转通<br>过动态计算判断条件决定是否发生跳转，一般跳转目的地址的偏移会直接编码到指令里，比<br>如RISCV中的beq等指令，间接跳转的跳转目的地址存放在一个寄存器里，每次跳转寄存器里<br>保存的目的地址都可能是不一样的，CPU做分支预测主要面对的就是如上这两种情况。</p>
<p>首先，分支可以做预测，一定是要有规律的，一个没有规律的分支是没有办法做预测的。分<br>支预测器通过一定的电路就可以把相应的规律捕捉住，这样下次fetch到同样PC上的跳转指令<br>就可以直接预测跳转的地址。</p>
<p>对于条件跳转，我们可以这样建模，对于一个PC上的条件跳转指令，发生一次跳转记1，没有<br>发生跳转(就是继续顺序执行指令)记0，那么跳转的历史就是0和1组成的一个序列，对于一个<br>这样的序列，比如，0001000100010001… 分支预测电路要做的工作是找出其中的规律，比如<br>这里的规律就是0001在做反复的重复。</p>
<p>严格上来讲，跳转情况是过去所有指令执行的结果，当只看分支指令的变化得到的预测正确<br>率低的时候，就要考虑把更多的信息一起加进来进行预测，所以CPU里还会把一段时间的跳转<br>指令的历史保存起来，用这个全局历史信息做分支预测，和这个相比，如上只考虑跳转指令<br>本身历史情况的预测称为依据局部历史信息的分支预测。</p>
<p>对于间接跳转，一个PC上的跳转指令的目的地址是有限几个，如果用数字标记每个目的地址，<br>间接跳转形成的序列大概是这样的：134213421342，那么其中的规律是就是1342反复重复。<br>因为一个间接跳转可能的目的地址是有限的，一般可以把它们保存在一个表(BTB Branch Target Buffer)，<br>需要做预测的时候，查表即可。</p>
<p>一般，CPU体系架构的手册都会建议尽量少用间接跳转指令，但是在函数跳转时，还是会用到<br>间接跳转指令，针对函数跳转和返回，CPU里一般采用类似栈的方式保存函数返回指令的跳转<br>目的地址，这种方式一般叫做RAS(return address stack)，CPU在函数跳转时，把返回地址<br>保存到内部的这个栈里，执行函数返回指令时从栈中弹出返回地址。</p>
<h2 id="各种分支预测方法"><a href="#各种分支预测方法" class="headerlink" title="各种分支预测方法"></a>各种分支预测方法</h2><p>局部历史信息条件跳转预测方法。我们直接直接看书中介绍的方法，必要的时候再看其中的<br>细节和辅助设计。</p>
<p>首先一个分支指令先对应一个BHR(Branch History Register)，这个寄存器的最大bit数决定<br>了可以完美预测的跳转序列的”循环节”大小，BHR里的每一个值都对应一个两位饱和计数器，<br>预测是根据当前BHR的值对应的两位饱和计数器的值来完成的。</p>
<p>如下是一个大概的示意图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">BHR:           PHT(Pattern History Table)</span><br><span class="line">+------+       +------+                             </span><br><span class="line">| 0001 | ----&gt; | 0000 | X</span><br><span class="line">+------+       |      | +-------------------------------+</span><br><span class="line">               |      | |     +-+   +-+   +-+   +-+     |</span><br><span class="line">               | 0001 | |  +--|0|--&gt;|0|--&gt;|1|--&gt;|1|--+  | -&gt; 0</span><br><span class="line">               |      | |  |  | |   | |   | |   | |  |  |</span><br><span class="line">               |      | |  +-&gt;|0|--&gt;|1|--&gt;|0|--&gt;|1|&lt;-+  |</span><br><span class="line">               |      | |     +-+   +-+   +-+   +-+     |</span><br><span class="line">               |      | +-------------------------------+</span><br><span class="line">               | 0010 | ...                               -&gt; 0</span><br><span class="line">               |      |                 </span><br><span class="line">               | ...  | X                 </span><br><span class="line">               |      |                 </span><br><span class="line">               | 0100 | ...                               -&gt; 0</span><br><span class="line">               |      |                 </span><br><span class="line">               | ...  | X</span><br><span class="line">               |      |                 </span><br><span class="line">               | 1000 | ...                               -&gt; 1</span><br><span class="line">               |      |                 </span><br><span class="line">               | ...  | X</span><br><span class="line">               +------+                 </span><br></pre></td></tr></table></figure>
<p>还是拿如上00010001…这样的条件跳转序列分析，这个序列里，只有四个状态，每个状态<br>的下一次跳转情况都是确定的，也就是当分支预测器检测到这个分支指令的历史是四个状态<br>其中之一时，本次的跳转情况就可以确定了。这么看起来如果可以直接确定，那么都不用两位<br>饱和计数器了，估计这里的两位饱和计数器是防止有零星的意外变化？</p>
<p>上面可以看出来，BHR的位数和跳转序列规律位数相同时，已经可以做完美预测。当BHR位数<br>比跳转序列规律位数大时，都可以做完美预测，比如，如果这里BHR是5 bit，那么如上序列<br>的pattern就是：00010/00100/01000/10001，不过就是PHT中没有用到的entry也增多了。</p>
<p>深入看下如上的两位饱和计数器，这个计数器有4个状态，其中两个状态的预测结果是分支跳<br>转，两个状态的预测结果是分支不跳转，分支跳转或不跳转的两个状态分强和弱状态。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   strong   weak  weak  strong</span><br><span class="line">      no taken      taken</span><br><span class="line">+-------------------------------+</span><br><span class="line">|     +-+   +-+   +-+   +-+     |</span><br><span class="line">|  +--|0|--&gt;|0|--&gt;|1|--&gt;|1|--+  |</span><br><span class="line">|  |  | |   | |   | |   | |  |  |</span><br><span class="line">|  +-&gt;|0|--&gt;|1|--&gt;|0|--&gt;|1|&lt;-+  |</span><br><span class="line">|     +-+   +-+   +-+   +-+     |</span><br><span class="line">+-------------------------------+</span><br></pre></td></tr></table></figure>
<p>在如上的状态定义下，各个状态的跳转关系是，当处于strong taken态，如果实际又是taken，<br>那么保持strong taken不变，如果实际是no taken，也就是预测失败了，状态移动到weak taken，<br>也就是说一直预测是taken，第一次出现no taken，我们还是认为下次是taken，当处于weak<br>taken时，如果实际是taken，状体转移到strong taken, 如果实际是no taken，也就是又预测<br>错了，状态转移到weak no taken，预测器在两次都预测失败的情况下改变了预测方向。</p>
<p>todo: 实际硬件实现上的考虑?</p>
<p>全局历史信息条件跳转预测方法。如上BHR是一个跳转指令taken/not taken的历史，如果BHR<br>里存放的是程序动态执行路径上之前跳转指令的历史，那么预测就是根据之前的”全局”跳转<br>信息的，当然受到物理实现的限制，BHR有一定的取值位数，并不是理想的全局信息。</p>
<p>间接跳转预测方法。间接跳转和直接跳转都可以用BTB(Branch Target Buffer)保存跳转的<br>目的地址，直接跳转在decode阶段就可以得到物理地址，但是超标量处理器一拍可能fetch多<br>条指令，需要在fetch阶段就得到跳转的目的地址，所以直接跳转也要使用到BTB。</p>
<p>具体看下函数跳转和返回时的跳转预测，函数跳转可能用直接跳转也可能用间接跳转实现，<br>两者都可以用BTB做跳转预测，一个函数跳转的目的地址是一定的。但是，函数返回一般用<br>return指令+link寄存器的方式实现，这是一个典型的同一位置根据寄存器内值不同，跳转的<br>目的地址不同的间接跳转的场景，如果函数返回地址的预测也用BTB，必然存在一个搜索行为，<br>性能就会比较差。</p>
<p>RAS的预测方法在函数跳转点处把函数返回地址先保存起来，遇到函数return指令直接pop<br>出RAS中预先保存的函数返回地址。这个方法的关键是要识别函数进入点的跳转指令，因为<br>函数进入可以用直接跳转或者间接跳转实现，而且并不是所有的直接跳转和间接跳转都是函数<br>调用点，一旦匹配错误，所有的预测结果都可能是不准的了。而且，这个方法还要识别return<br>指令，比如在riscv里，return是用jalr指令实现，而jalr可以支持各种间接跳转的场景。</p>
<p>todo: 怎么做这个识别？</p>
<h2 id="分支预测失败恢复"><a href="#分支预测失败恢复" class="headerlink" title="分支预测失败恢复"></a>分支预测失败恢复</h2><p>分支预测失败时需要flush掉错误路径上投机执行的指令，关于CPU中flush的一个整体理解<br>可以参考<a href="https://wangzhou.github.io/CPU%E5%BE%AE%E6%9E%B6%E6%9E%84%E9%87%8C%E7%9A%84Flush%E6%A6%82%E5%BF%B5/">这里</a>。然后把处理器里各个部件的状态恢复到分支指令对应的点上。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>CPU中cache和MMU的基本逻辑</title>
    <url>/CPU%E4%B8%ADcache%E5%92%8CMMU%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<h2 id="处理器角度的存储"><a href="#处理器角度的存储" class="headerlink" title="处理器角度的存储"></a>处理器角度的存储</h2><p>从CPU核角度看，数据存在于cache和内存两个地方，其中cache又分为L1/L2/L3等不同级别,<br>数据的索引用地址描述，对cache可以用虚拟地址或者物理地址做索引，而内存只能用物理<br>地址做索引。</p>
<p>也就是对于内存的访问，不管是用页表还是用TLB，总是需要先翻译得到物理地址，使用物理<br>地址做访问。内存的访问逻辑比较简单，我们下面重点看cache的访问。</p>
<p>cache的访问需要快，最好是可以直接使用虚拟地址访问cache，当然先做地址翻译，然后用<br>物理地址访问cache是一定没有错的。但是，用虚拟地址访问cache就会出现同名和重名的问题，<br>同名是相同虚拟地址在不同时间被映射到不同物理地址上，重名是同一时间，一个物理地址<br>有两个虚拟地址映射。</p>
<p>同名问题比较简单，说白了就是虚拟地址和物理地址的映射关系变了，这个问题有两个解决办法：<br>(1) 可以在变化映射关系的时候把原来虚拟地址对应的cache清理掉；(2) 可以引入新的变量<br>表明不同的虚拟地址到物理地址的映射所属的主体。具体实现上，第一个解决办法需要引入<br>cache清理的命令，第二个解决办法就是引入ASID，使用虚拟地址+ASID的cache索引方式。</p>
<p>重名问题比较绕，说白了就是一个数据被cache到了不同的cache line上，同步问题需要处理。<br>直接的处理方式是消灭问题本身，使用虚拟地址但是一个物理地址上的数据还是只能被cache<br>到一个cache line上，就没有这个问题了，直接的想法就是使用页内偏移做cache索引，因为<br>一页内，虚拟地址和物理地址是一样的，这样做带来的问题就是cache的容量做不大，理解<br>这个问题前需要看下cache的映射模式。</p>
<p>《超标量处理器》中把cache的映射模式总结的很清晰: 直接相连是一个地址上的数据只能放<br>到cache的一个固定位置，全相连是一个地址上的数据可以放到cache的全部cache line上，<br>组相连是把cache分成一个个组(set)，一个地址上的数据只可以放到cache的一个组里，但是<br>一个组里有多个cache line，数据可以放到特定组的不同cache line上。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                          way</span><br><span class="line">      +------------+------------+------------+-------------+</span><br><span class="line">set0  | cache line | cache line | cache line | cache line  |</span><br><span class="line">      +------------+------------+------------+-------------+</span><br><span class="line">      +------------+------------+------------+-------------+</span><br><span class="line">set1  | cache line | cache line | cache line | cache line  |</span><br><span class="line">      +------------+------------+------------+-------------+</span><br><span class="line"></span><br><span class="line">      +------------+------------+------------+-------------+</span><br><span class="line">setN  | cache line | cache line | cache line | cache line  |</span><br><span class="line">      +------------+------------+------------+-------------+</span><br></pre></td></tr></table></figure>
<p>如上是一个4路(way)组相连的cache，如果一个cache line 64Byte，整个cache是32KB，那么<br>很容易计算出这片cache有32KB/64B/4=128个set，需要7个bit的宽度去描述，加上6个bit描述<br>cache line里各个Byte的地址，已经超过了12bit的页内偏移。所以，32KB的cache要至少做成<br>8路组相连cache，才能使用虚拟地址作为index索引cache。那么我们可以得到一个4KB页面下，<br>特定cache大小最小路数的表出来：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  8KB   2 way</span><br><span class="line"> 16KB   4 way</span><br><span class="line"> 32KB   8 way</span><br><span class="line"> 64KB  16 way</span><br><span class="line">128KB  32 way</span><br></pre></td></tr></table></figure>
<p>如果不这样做，也可以硬件搞定重名问题，代价是增加硬件的复杂度。(todo)</p>
<p>cache的替换算法。相同index的地址数远大于一个set里的way的数目，cache替换算法决定<br>一个set里，谁应该被替换出去为新的访问腾出cache line的位置。</p>
<p>cache的操作命令有invalid和clean，invalid cache是无效化对应地址的cache，clean cache<br>的语意其实是和dirty cache对应的，clean cache把cache上的数据写回到内存，对于数据<br>只写到cache还没有写回内存的情况，就需要用clean cache把数据同步回内存。</p>
<h2 id="地址翻译"><a href="#地址翻译" class="headerlink" title="地址翻译"></a>地址翻译</h2><p>这里我们总结下地址翻译里，容易忽略的几个基本逻辑。</p>
<p>TLB是页表项的cache，这个定义里包含着这些逻辑：页表项的所有内容必须要TLB中有记录，<br>否则的话，在TLB命中的场景下，页表项里的属性无法体现出来。页表项里既有只读也有可写<br>的属性，比如access bit和dirty bit就是可以被硬件动态改变的，硬件对页表项的写操作<br>应该也作用到TLB上，同时TLB和页表项之间应该有同步逻辑。</p>
<p>这里我们想象下页表项里的access bit和dirty bit的基本逻辑。首先，我们定义在页表项里<br>有access bit和dirty bit这两个bit，当CPU访问一个page时，对应的access bit会被硬件<br>配置成1，当CPU写一个page时，对应的dirty bit会被硬件配置成1。access/dirty bit的定义<br>描述是基于页表项的，所以不管怎么说，满足条件时页表项上的对应bit是一定要更新的。<br>当TLB里有access/dirty bit相关的信息时，硬件可以优化更新页表项上对应bit的行为，如果<br>TLB里相关的bit已经是1，硬件就可以不去更新页表项的对应bit，因为TLB和页表项里的内容<br>是一样的。可以看出以上逻辑成立的基础是TLB和页表项需要保持同步，这个同步可以软件来<br>做也可以硬件来做，最终会表现为软硬件接口的不同。如果软件来做，那么软件在更新页表<br>项上的access/dirty bit时，需要flush掉对应的TLB，软硬件接口定义上也要明确指出这个<br>问题。</p>
<p>页表的操作主体有：通过store/load修改和读取；通过MMU硬件隐式的访问和修改。store/load<br>会和cache系统交互，把页表保存到cache或内存上，所以MMU对页表的隐式操作也会和cache<br>系统交互。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>glibc协程的使用方法</title>
    <url>/glibc%E5%8D%8F%E7%A8%8B%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>有很多实现协程的方式，glibc里实现的协程叫做ucontext。我们可以先从相关的头文件里<br>看下ucontext的基本数据结构和API。</p>
<p>glibc代码的glibc/stdlib/ucontext.h是ucontext的几个API，ucontext数据结构是构架相关<br>的，定义在sys/ucontext.h，比如对于riscv的定义在：sysdeps/unix/sysv/linux/riscv/sys/ucontext.h。</p>
<p>具体看API之间，先看下协程(coroutine)的基本概念。协程可以在用户态实现执行上下文的<br>切换，可以看成一个轻量级的线程，但是，线程是系统层面实现的机制，没办法自己控制调度，<br>线程本身的调度就是内核调度，线程自己看不到调度，而协程API可以控制程序执行流直接<br>跳到一个上下文上执行。</p>
<p>在用户态一个上下文的定义就寄存器集合+栈(严格讲还要加上内存)，协程在两个用户态上下<br>文上切换也就是要：1. 定义用户态上下文；2. 给出相关的用户态上下文获取、修改、配置以及<br>切换的API。</p>
<p>ucontext_t描述一个用户态上下文，显示是体系结构相关的，程序员可以直接操作里面的数据。<br>riscv下这个结构大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">typedef struct ucontext_t</span><br><span class="line">  &#123;</span><br><span class="line">    unsigned long int  __uc_flags;</span><br><span class="line">    struct ucontext_t *uc_link;</span><br><span class="line">    /* 其中有stack的基地址(ss_sp)，大小(ss_size)和flag(ss_flags) */</span><br><span class="line">    stack_t            uc_stack;</span><br><span class="line">    sigset_t           uc_sigmask;</span><br><span class="line">    /* There&#x27;s some padding here to allow sigset_t to be expanded in the</span><br><span class="line">       future.  Though this is unlikely, other architectures put uc_sigmask</span><br><span class="line">       at the end of this structure and explicitly state it can be</span><br><span class="line">       expanded, so we didn&#x27;t want to box ourselves in here.  */</span><br><span class="line">    char               __glibc_reserved[1024 / 8 - sizeof (sigset_t)];</span><br><span class="line">    /* We can&#x27;t put uc_sigmask at the end of this structure because we need</span><br><span class="line">       to be able to expand sigcontext in the future.  For example, the</span><br><span class="line">       vector ISA extension will almost certainly add ISA state.  We want</span><br><span class="line">       to ensure all user-visible ISA state can be saved and restored via a</span><br><span class="line">       ucontext, so we&#x27;re putting this at the end in order to allow for</span><br><span class="line">       infinite extensibility.  Since we know this will be extended and we</span><br><span class="line">       assume sigset_t won&#x27;t be extended an extreme amount, we&#x27;re</span><br><span class="line">       prioritizing this.  */</span><br><span class="line">    /* 其中定义riscv的通用寄存器和浮点寄存器 */</span><br><span class="line">    mcontext_t uc_mcontext;</span><br><span class="line">  &#125; ucontext_t;</span><br></pre></td></tr></table></figure>

<p>如下函数就是所有ucontext_t相关的API，头文件的注释写的比较清楚了，我们后面只做补充解释。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* Get user context and store it in variable pointed to by UCP.  */</span><br><span class="line">extern int getcontext (ucontext_t *__ucp) __THROWNL;</span><br><span class="line"></span><br><span class="line">/* Set user context from information of variable pointed to by UCP.  */</span><br><span class="line">extern int setcontext (const ucontext_t *__ucp) __THROWNL;</span><br><span class="line"></span><br><span class="line">/* Save current context in context variable pointed to by OUCP and set</span><br><span class="line">   context from variable pointed to by UCP.  */</span><br><span class="line">extern int swapcontext (ucontext_t *__restrict __oucp,</span><br><span class="line">			const ucontext_t *__restrict __ucp)</span><br><span class="line">  __THROWNL __INDIRECT_RETURN;</span><br><span class="line"></span><br><span class="line">/* Manipulate user context UCP to continue with calling functions FUNC</span><br><span class="line">   and the ARGC-1 parameters following ARGC when the context is used</span><br><span class="line">   the next time in `setcontext&#x27; or `swapcontext&#x27;.</span><br><span class="line"></span><br><span class="line">   We cannot say anything about the parameters FUNC takes; `void&#x27;</span><br><span class="line">   is as good as any other choice.  */</span><br><span class="line">extern void makecontext (ucontext_t *__ucp, void (*__func) (void),</span><br><span class="line">			 int __argc, ...) __THROW;</span><br></pre></td></tr></table></figure>
<p>makecontext是对__ucp的配置，当下次触发__ucp运行时，使用__ucp的上下文，但是要执行<br>__func，__argc是__func的入参个数，后面依次是每个入参。这里__func的定义是没有参数，<br>但是后面确给了入参信息，使用的时候可以定义带入参的函数，传给makecontext时强制转换<br>成void (* __func)(void)。</p>
<p>程序员可以直接通过makecontext构造一个新的执行上下文，而不仅仅是通过getcontext获取<br>当前程序的上下文:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">typedef void (* task_t) (void);</span><br><span class="line">ucontext_t nc;</span><br><span class="line"></span><br><span class="line">void new_context(ctx);</span><br><span class="line">void *stack = malloc(4096);</span><br><span class="line"></span><br><span class="line">nc.uc_stack.ss_sp = stack;</span><br><span class="line">nc.uc_stack.ss_size = 4096;</span><br><span class="line">nc.uc_link = NULL;</span><br><span class="line"></span><br><span class="line">makecontext(&amp;uc, (task_t)new_context, 1, ctx);</span><br></pre></td></tr></table></figure>

<p>程序在必要的时候可以直接使用swapcontext执行makecontext构造的上下文：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ucontext_t curr;</span><br><span class="line"></span><br><span class="line">swapcontext(&amp;curr, &amp;nc);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>协程</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核spinlock实现分析</title>
    <url>/Linux%E5%86%85%E6%A0%B8spinlock%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p>spinlock的基本行为就是各个CPU core去互斥的处理一个数据，加锁后只有获取锁的core<br>可以处理数据，解锁后，CPU core又可以去获取锁。整个过程关闭调度。</p>
<p>spinlock最直白的实现方法是多核间用原子的读修改写指令抢一个标记，这个标记原始值<br>是0，0表示没有core占有这个锁，当一个core原子的检测到这个标记是0，并修改成1时，这个<br>core占有锁，其它core做这个检测时，这个标记是1，读修改写的原子指令不生效。</p>
<p>这个原子指令大概是这样：CAS(int <em>val, int old, int new)，如果和old和</em>val相等，才<br>把new写入val的地址，把*val的老值保存到new里。</p>
<p>用最直白的逻辑写出的锁实现类似:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct self_spinlock &#123;</span><br><span class="line">	__u32 lock;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">static inline void self_spinlock(struct self_spinlock *lock)</span><br><span class="line">&#123;</span><br><span class="line">	while (__atomic_test_and_set(&amp;lock-&gt;lock, __ATOMIC_ACQUIRE))</span><br><span class="line">		while (__atomic_load_n(&amp;lock-&gt;lock, __ATOMIC_RELAXED))</span><br><span class="line">			;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static inline void self_unspinlock(struct self_spinlock *lock)</span><br><span class="line">&#123;</span><br><span class="line">	__atomic_clear(&amp;lock-&gt;lock, __ATOMIC_RELEASE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样的锁有两个问题：1. 锁的请求顺序和实际获得锁的顺序不一致，因为上面本质上还是<br>多个core在无序的争抢标记位；2. 多核之间cache会相互影响。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+------+    +------+    +------+    +------+</span><br><span class="line">| CPU0 |    | CPU1 |    | CPU2 |    | CPU3 |</span><br><span class="line">+------+    +------+    +------+    +------+</span><br><span class="line">   v           v           v           v</span><br><span class="line">+------+    +------+    +------+    +------+</span><br><span class="line">|cache0|    |cache1|    |cache2|    |cache3|</span><br><span class="line">+------+    +------+    +------+    +------+</span><br><span class="line">         \      \          /     /</span><br><span class="line">          \  +----------------+ /</span><br><span class="line">           \ | Flag in memory |/</span><br><span class="line">             +----------------+</span><br></pre></td></tr></table></figure>
<p>对于第二个问题，我们展开看下，在有cache的系统里，系统大概的样子如上，如果CPU0占有<br>锁，cach0为1(cache0/1/2/3是Flag在各个core上的cache)，CPU1/2/3的cache也会在各个core<br>读Flag时被设置为1，CPU0释放锁的时候，cache0被写成0，同时CPU1/2/3上对应的cach被无效<br>化，随后哪个core抢先把Flag写成1，对应的cache就是1。后面重复之前的逻辑。可以看出，<br>本来在unlock core和lock core之间的通行行为被扩展到了所有参与竞争锁的core，不但锁的<br>请求顺序和实际获得锁的顺序不一致，而且做了很多无用功。</p>
<h2 id="ticket-spinlock"><a href="#ticket-spinlock" class="headerlink" title="ticket spinlock"></a>ticket spinlock</h2><p>tick spinlock的实现在ARMv6的内核代码里还有保留，具体的路径在linux/arch/arm/include/asm/spinlock.h。<br>这里只把它核心的逻辑提取出来。</p>
<p>ticket spinlock锁本身的数据结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct __raw_tickets &#123;</span><br><span class="line">	u16 next;</span><br><span class="line">	u16 owner;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>获取锁的行为就是原子的增加next值，然后作为自己的ticket，拿着自己的ticket一直和owner<br>做对比，看看是不是轮到自己拿锁，释放锁的行为就是增加owner的值。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                        +------+    +------+    +------+    +------+</span><br><span class="line">                        | CPU0 |    | CPU1 |    | CPU2 |    | CPU3 |</span><br><span class="line">                        +------+    +------+    +------+    +------+</span><br><span class="line"></span><br><span class="line">        lock               ----------------------------------------&gt; </span><br><span class="line">  local next:              0           1       +---2---+   +---3---+</span><br><span class="line">                                               |       |   |       |</span><br><span class="line">                                     owner++   ^       v   ^       v</span><br><span class="line">      unlock                         -----&gt;    |       |   |       |</span><br><span class="line">owner in __raw_tickets:    0           1       +-owner-+   +-owner-+</span><br></pre></td></tr></table></figure>
<p>如上是一个ticket spinlock的示意图，CPU2/3现在在等待锁，CPU1在释放锁。试图获取锁<br>的CPU原子的对next加1并在本地保存一个加1后的本地next，作为自己的ticket，释放锁的<br>CPU把锁里的owner值加1，试图获取锁的CPU，一直在拿自己的ticket和owner做比较，如果<br>ticket和owner相等自己就拿到了锁。</p>
<p>可以看出，ticket spinlock解决了上面的问题1，但是没有解决问题2，因为竞争锁的core<br>必须时刻拿着自己的ticket和owner做对比，实际上谁得到锁这个信息只要依次传递就好。</p>
<h2 id="MCS-spinlock"><a href="#MCS-spinlock" class="headerlink" title="MCS spinlock"></a>MCS spinlock</h2><p>基于以上的认识，后来人们又提出了MCS锁，这个名字是用提出这种锁算法的两个人的名字<br>命名的。</p>
<p>其实从上面ticket spinlock的图上我们已经可以大概看出来要怎么做，就是把每次lock搞<br>一个锁的副本出来，然后把这些副本用链表链接起来，加锁时还是spin在自己的副本上，解锁<br>时顺着链表依次释放锁。</p>
<p>大概的示意图如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                                      ------------+</span><br><span class="line">                                      | lock tail |</span><br><span class="line">                                    / +-----------+</span><br><span class="line">+------+    +------+    +------+   /+------+</span><br><span class="line">| CPU0 |    | CPU1 |    | CPU2 |  / | CPU3 |</span><br><span class="line">+------+    +------+    +------+ /  +------+</span><br><span class="line">                              1 /        +-----+</span><br><span class="line">                               /         |     | 3 spin on owner</span><br><span class="line">+------+    +------+    +------+    +------+   |</span><br><span class="line">| owner|    | owner|    | owner| 2  | owner|&lt;--+</span><br><span class="line">| next |---&gt;| next |---&gt;| next |---&gt;| next |</span><br><span class="line">+------+    +------+    +------+    +------+</span><br></pre></td></tr></table></figure>
<p>如上所示，加锁就是找见当前锁链表结尾(步骤1)，把要加的锁节点挂在上面(步骤2)，然后<br>就spin在自己的owner标记上等待(步骤3)。</p>
<p>解锁就是把当前锁节点的下一个节点的owner配置成1，这样，spin的core检测到owner为1，<br>就知道现在自己拥有锁了。</p>
<p>MCS锁可以解决上面的两个问题，但是占用的内存比较多了。我们考虑MCS的实际实现问题，<br>一个MCS锁需要一个lock tail以及每个core上的mcs node结构，占用内存比较大，实际上<br>在现在的Linux内核中只有MCS node的定义，并没有MCS锁的实现，只所以MCS的定义还在，<br>是因为qspinlock里要复用MCS node的定义。</p>
<h2 id="qspinlock"><a href="#qspinlock" class="headerlink" title="qspinlock"></a>qspinlock</h2><p>当前的内核里，各个体系构架下的spinlock基本上都使用了qspinlock的实现。我们重点分析<br>下qspinlock的基本逻辑和代码实现。</p>
<p>spinlock_t的定义在linux/include/linux/spinlock_types.h，封装的数据结结构是struct raw_spinlock，<br>进一步到arch_spinlock_t, 对于支持qspinlock的构架，arch_spinlock_t的定义就是struct qspinlock。</p>
<p>我们只看小端下qspinlock的定义，基于此分析lock和unlock的逻辑细节。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">typedef struct qspinlock &#123;</span><br><span class="line">	union &#123;</span><br><span class="line">		atomic_t val;</span><br><span class="line">		struct &#123;</span><br><span class="line">			u8	locked;</span><br><span class="line">			u8	pending;</span><br><span class="line">		&#125;;</span><br><span class="line">		struct &#123;</span><br><span class="line">			u16	locked_pending;</span><br><span class="line">			u16	tail;</span><br><span class="line">		&#125;;</span><br><span class="line">	&#125;;</span><br><span class="line">&#125; arch_spinlock_t;</span><br></pre></td></tr></table></figure>

<p>kernel/locking/qspinlock.c中一个CPU上静态分配一个size是4的struct qnode数组，每个<br>qnode元素是struct mcs_spinlock的封装，每个core上的一个qnode对应一种内核上下文，<br>所以4个qnode分别对应task, hardirq, softirq, NMI内核上下文。</p>
<p>我们再整理下qspinlock的相关数据结构。如下是qspinlock的位域结构, val状态按照如下描述<br>(tail, pending, locked):</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(tail,          pending, locked)</span><br><span class="line">&lt;----16bit----&gt;&lt;----16bit-----&gt; </span><br></pre></td></tr></table></figure>
<p>每个qspinlock都会有一个这样的结构，所有qspinlock复用每个core上的mcs node，mcs node<br>封装成qnode后静态定义如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);</span><br></pre></td></tr></table></figure>
<p>也就是每个core的per-CPU数据区上，都有一个size是4的qnode的数组。</p>
<p>我们先看qspinlock的基本逻辑，具体代码的分析以代码注释的形式放在最后。qspinlock和<br>MCS核心的不同在于，qspinlock把锁占用与否的信息放到了qspinlock的locked域段，锁的<br>排队信息放到了pending和mcs链表。</p>
<p>如果是第一个core进来，就直接把locked set 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-------+------------+----------+</span><br><span class="line">| tail  |  pending 0 | locked 1 |</span><br><span class="line">+-------+------------+----------+</span><br></pre></td></tr></table></figure>

<p>如果已经有一个core占有锁，就把pending set 1，并在locked上spin等待：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                             +------+</span><br><span class="line">                             |      |</span><br><span class="line">+-------+------------+----------+   |  spin check locked</span><br><span class="line">| tail  |  pending 1 | locked 1 |&lt;--+</span><br><span class="line">+-------+------------+----------+</span><br></pre></td></tr></table></figure>

<p>如果已经有一个core占有锁并且一个core在排队，就在mcs node上排队，并在pending/locked<br>上spin等待，直到pending也释放锁，才占有锁。 我们把第二个排队节点，也就是这里的mcs node0<br>的处理展开看下，这一节描述的是mcs node0抢锁的逻辑。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">              +-----------+---------+</span><br><span class="line">              |           |         |</span><br><span class="line">+-------+------------+----------+   |</span><br><span class="line">| tail  |  pending 1 | locked 1 |&lt;--+</span><br><span class="line">+-------+------------+----------+</span><br><span class="line">    |</span><br><span class="line">    v</span><br><span class="line">+-----------+</span><br><span class="line">| mcs node0 |</span><br><span class="line">+-----------+</span><br></pre></td></tr></table></figure>

<p>如果系统里有两个节点在排队，就是如上第三个core还在mcs node0排队的情况，这时如果<br>又有core想占有锁，那么它们都在各自的mcs node上spin，并逐个加入mcs node的链表：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                    +-----------+---------+</span><br><span class="line">                    |           |         |</span><br><span class="line">      +-------+------------+----------+   |</span><br><span class="line">      | tail  |  pending 1 | locked 1 |&lt;--+</span><br><span class="line">      +-------+------------+----------+</span><br><span class="line">              \          +------+</span><br><span class="line">               v         |      |</span><br><span class="line">+-----------+   +-----------+   |</span><br><span class="line">| mcs node0 |--&gt;| mcs node1 |&lt;--+</span><br><span class="line">+-----------+   +-----------+</span><br><span class="line">    cpu0            cpu3</span><br></pre></td></tr></table></figure>
<p>如上，cpu3也想占有锁，那么cpu3上的mcs node被加到mcs node链表上spin等待。这时，系统<br>里cpu0也在等待锁，如上它等的是pending/locked为0，还有两个cpu和这个锁有关系，那就<br>当前占有锁的cpu，和在pending位置的cpu，其中pending位置的cpu在locked上spin等待。</p>
<p>当cpu0成功占有锁时，cpu0会解开cpu3的spin，这时cpu3的状态成为了之前cpu0的状体，cpu3<br>开始做pending/locked的spin等待。</p>
<p>qspinlock的核心逻辑大概就是这样，代码里的其它细节处理在具体代码分析里看吧。可以<br>看到一个core上相同种类上下文的两把锁是复用对应mcs node的，比如，cpu3上一段代码流程<br>里先后上两把不同的锁lock1/lock2，lock1如果在mcs node1上排队，lock2上锁的代码还没有<br>执行到，等到cpu3得到了lock1，cpu3上的mcs node1的位置已经释放了，因为cpu3得到lock1<br>这个信息以经被记录到lock1 qspinlock的locked域段。</p>
<p>如下是qspinlock加锁函数的代码分析，我们忽略了和虚拟化有关的PV相关的代码:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* linux/include/asm-generic/qspinlock.h */</span><br><span class="line">static __always_inline void queued_spin_lock(struct qspinlock *lock)</span><br><span class="line">&#123;</span><br><span class="line">	int val = 0;</span><br><span class="line"></span><br><span class="line">	if (likely(atomic_try_cmpxchg_acquire(&amp;lock-&gt;val, &amp;val, _Q_LOCKED_VAL)))</span><br><span class="line">		return;</span><br><span class="line"></span><br><span class="line">	queued_spin_lock_slowpath(lock, val);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果val是0，对lock-&gt;val原子写入_Q_LOCKED_VAL(0x1)，否则把lock-&gt;val的值写入val，<br>有写入lock-&gt;val函数返回true。可见对于没有写入的slowpath拿到lock-&gt;val当前状态的值，<br>基于此在slowpath里做处理。如果写入成功val的状态变成(0, 0, 1)。这里acquire的语意是<br>后续操作不能越过当前操作先执行。</p>
<p>这里release的语意是之前的指令不能乱序到当前操作之后执行。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static __always_inline void queued_spin_unlock(struct qspinlock *lock)</span><br><span class="line">&#123;</span><br><span class="line">	/*</span><br><span class="line">	 * unlock() needs release semantics:</span><br><span class="line">	 */</span><br><span class="line">	smp_store_release(&amp;lock-&gt;locked, 0);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>qspinlock的加锁逻辑的代码细节分析直接写到代码注释里了, 在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L2xpbnV4L2Jsb2IvYmMyM2E4YmQ5MzgyM2VhZTBjYzkyZGMxZjk2Njg1NThjZDRkMTRmMy9rZXJuZWwvbG9ja2luZy9xc3BpbmxvY2suYyNMMzE3">这个位置<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>转载-香山开源高性能RISC-V处理器设计与实现</title>
    <url>/%E8%BD%AC%E8%BD%BD-%E9%A6%99%E5%B1%B1%E5%BC%80%E6%BA%90%E9%AB%98%E6%80%A7%E8%83%BDRISC-V%E5%A4%84%E7%90%86%E5%99%A8%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<div class="pdfobject-container" data-target="xiangshan.pdf" data-height="500px"></div>
]]></content>
      <categories>
        <category>reprint</category>
      </categories>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>ARM构架下原子操作相关指令总结</title>
    <url>/ARM%E6%9E%84%E6%9E%B6%E4%B8%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="LDXR-STXR"><a href="#LDXR-STXR" class="headerlink" title="LDXR/STXR"></a>LDXR/STXR</h2><p>ARM提供load-reserved/store-conditional的方式实现原子操作，其中相关的一组指令有：<br>LDXR/STXR/LDXRB/STXRB/LDXRH/STXRH/LDXP/STXP，这些指令基本功能一样，只是操作的地址<br>位宽不一样，XR是寄存器位宽，XRB是8bit，XRH是16bit，XP是两个寄存器位宽。load-acquire/<br>store-release这种barrier属性也可以加到这个指令上，其中前者加入A，后者加入L，比如<br>LDXR/STXR加了load-acquire/store-release属性的指令是LDAXR/STLXR。</p>
<p>ARM里叫这种指令是load-exclusive/store-exclusive的指令，其实和RISCV上的<br>load-reserved/store-conditional指令是基本一样的逻辑，学术界最开始提出这种指令时，<br>用的也是LR/SC(load-reserved/store-conditional)这样的叫法。</p>
<p>ARM的这组指令的用法和RISCV的LR/SC指令的逻辑是一样的，基本逻辑我们可以大概参考RISCV<br>LR/SC的逻辑，<a href="https://wangzhou.github.io/riscv%E5%8E%9F%E5%AD%90%E6%8C%87%E4%BB%A4%E5%88%86%E6%9E%90/">这里</a>是一个RISCV LR/SC的细节分析。</p>
<p>ARM上使用LDXR/STXR的例子可以查看内核的这个位置：linux/arch/arm64/include/asm/atomic_ll_sc.h<br>和其它体系结构类似，ARM上的各种原子操作都可以用这两个指令实现，其中就包括了普通<br>的原子运算和CAS(compare and swap)的原子操作。</p>
<p>ARM上LDXR和WFE结合还有一个巧妙的用法，LDXR会设置一个exclusive的标记，当有其它store<br>操作把这个标记去掉时，ARM规定系统会触发event register的set操作，这个操作是解开WFE<br>的其中一个信号。这样，一个core反复读一个地址直到读到特定值的条件读操作可以使用<br>LDXR+WFE的方式实现，内核里的核心实现代码如下，我们把注释直接写在下面的代码里。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* linux/arch/arm64/include/asm/cmpxchg.h */</span><br><span class="line">#define __CMPWAIT_CASE(w, sfx, sz)					\</span><br><span class="line">static inline void __cmpwait_case_##sz(volatile void *ptr,		\</span><br><span class="line">				       unsigned long val)		\</span><br><span class="line">&#123;									\</span><br><span class="line">	unsigned long tmp;						\</span><br><span class="line">									\</span><br><span class="line">	asm volatile(							\</span><br><span class="line"></span><br><span class="line">	/* sevl和紧接着的wfe去掉之前可能已经set的event register */</span><br><span class="line"></span><br><span class="line">	&quot;	sevl\n&quot;							\</span><br><span class="line">	&quot;	wfe\n&quot;							\</span><br><span class="line"></span><br><span class="line">	/* 读检测地址上的数据，并把exclusive标记配置上 */</span><br><span class="line"></span><br><span class="line">	&quot;	ldxr&quot; #sfx &quot;\t%&quot; #w &quot;[tmp], %[v]\n&quot;			\</span><br><span class="line"></span><br><span class="line">	/* 检测下读到的值，如果是等待的值，就跳出循环 */</span><br><span class="line"></span><br><span class="line">	&quot;	eor	%&quot; #w &quot;[tmp], %&quot; #w &quot;[tmp], %&quot; #w &quot;[val]\n&quot;	\</span><br><span class="line">	&quot;	cbnz	%&quot; #w &quot;[tmp], 1f\n&quot;				\</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * 否则睡眠等待, 当其它core写被检测的地址时，会触发这个地址的global monitor</span><br><span class="line">	 * 被清理，从何触发WFE wakeup event唤醒WFE。注意，这里并不需要stxr之类的</span><br><span class="line">	 * 条件写指令，只要普通store指令就可以触发global monitor被清理。</span><br><span class="line">	 */</span><br><span class="line"></span><br><span class="line">	&quot;	wfe\n&quot;							\</span><br><span class="line">	&quot;1:&quot;								\</span><br><span class="line">	: [tmp] &quot;=&amp;r&quot; (tmp), [v] &quot;+Q&quot; (*(u##sz *)ptr)			\</span><br><span class="line">	: [val] &quot;r&quot; (val));						\</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在循环中反复调用如上的检测逻辑即可以完成条件读的功能，条件读在qspinlock的实现里<br>有大量的使用。</p>
<h2 id="LSE-Large-System-Extensions-中的原子指令"><a href="#LSE-Large-System-Extensions-中的原子指令" class="headerlink" title="LSE(Large System Extensions)中的原子指令"></a>LSE(Large System Extensions)中的原子指令</h2><p>ARMv8.1的LSE里新增加了单指令的原子操作，大概分为：1. 对内存里的值做原子运算的指令；<br>2. 把一个值和内存里的值做交换的指令；3. CAS原子指令。</p>
<p>第一类指令数量庞大，命名逻辑是LD/ST + 运算种类，LD前缀表示有返回值，返回值是内存<br>里的旧值，ST表示没有返回值。比如LDADD x1, x2, [x3]表示，把x2和x3地址上值相加的和<br>写入x3地址，x3地址上的旧值写入x1，STADD x1, [x3]只有原子加这个操作，没有返回旧值。</p>
<p>第二类指令有: SWP/SWPB/SWPH，把输入寄存器里的值写入内存，并把内存上的旧值写到输出<br>寄存器。</p>
<p>第三类指令有：CAS/CASB/CASH/CASP，这些就是经典的CAS原子指令，比如CAS Xs, Xt, [Xn]<br>的功能是比较Xs和Xn内存上的值，如果相等就把Xt的值写入到Xn内存上。需要注意的是如上<br>的CASP指令，其中的P是pair的意思，虽然这个指令的寄存器还是Xs/Xt/Xn，但是它语意上<br>扩展到了128bit，逻辑是比较Xs Xs+1和Xn为基地值的128bit的值，如果相等就把Xs Xs+1的<br>值写入Xn为基地址的128bit内存上。这里用64bit寄存器举例介绍了，32bit寄存器的CASP指令<br>是一样的逻辑。</p>
<p>Linux内核里ARM上LSE实现的原子操作的代码在这里: linux/arch/arm64/include/asm/atomic_lse.h<br>可以看出基本上就是对相应LSE原子指令的封装。</p>
<h2 id="Single-copy-64B-load-store"><a href="#Single-copy-64B-load-store" class="headerlink" title="Single copy 64B load/store"></a>Single copy 64B load/store</h2><p>ARMv8.7引入的这组指令支持原子的对MMIO读写64Byte数据，相关的指令有：LD64B/ST64B/<br>ST64BV/ST64BV0，这些指令只能操作Non-cacheable或者device的地址空间，带V后缀的表示<br>有返回值，带0的表示可以工作在EL0。举一个例子看下具体使用的逻辑，比如ST64B Xt [Xn]<br>表示把Xt-X(t+7)这八个64bit寄存器里的值写入Xn为基地值的64Byte地址空间。</p>
<p>这组指令主要用来原子给设备的MMIO空间发请求，地址用来唯一的标识设备，64Byte的数据<br>是请求参数。因为地址是设备的MMIO空间，所以这个指令的功能其实是设备自定义的，广义<br>上看，设备的side effect都是这个指令的功能。</p>
<h2 id="WFI-WFE-SEV-SEVL"><a href="#WFI-WFE-SEV-SEVL" class="headerlink" title="WFI/WFE/SEV/SEVL"></a>WFI/WFE/SEV/SEVL</h2><p>如上提到了WFE指令，这里把相关的指令都展开看下。 ARM里一开始只有WFI，顾名思义WFI<br>会使core进入低功耗模式，直到core上有中断，才唤醒core继续运行。</p>
<p>WFE(wait for event)是睡眠core等待event发生，这里的event的定义就有很多了，WFE里还<br>定义了一个event register的概念，这个是一个虚拟register，只能由特定的指令或硬件行为<br>set。spec里规定set event register的行为有：SEV/SEVL指令，前者set全部PE，后者set<br>当前PE；异常返回；清理PE的global monitor; Generic Timer event stream。event register<br>只能由WFE指令清理掉。</p>
<p>event register如果没有set，WFE正常进入低功耗状态，event register如果已经被set，<br>WFE指令并不能触发core进入低功耗，而且event register会被清理掉。可以看到LDXR/STXR<br>这一节提到使用sevl + wfe清理掉之前可能set的event register，这里不管之前event register<br>有没有set，这样操作总能把event register清理掉。</p>
<p>唤醒WFE的evnet有: SEV指令; 当前core收到并可以处理中断；asynchronous external request<br>debug event; timer event stream; global monitor被清理掉。其中最后一种event就是我们<br>上面在LDXR/STXR一节中提到的情况。</p>
<p>如上，SEV(send Event)和SEVL(send Event Local)，都可以set event register，只不过<br>作用core的范围不一样。SEV还可以一把唤醒所有core上WFE，如果一个core已经WFE了，语意<br>上就不可能再执行SEVL唤醒，所以可以看到spec里也没有相关的定义。</p>
<h2 id="PRFM"><a href="#PRFM" class="headerlink" title="PRFM"></a>PRFM</h2><p>CASP指令的示例代码里提到了PRFM，这里把预取相关的指令都展开看下。PRFM完成对地址上<br>数据的预取，具体硬件可以把它实现成提前加载数据到cache，这个指令可以带不同的参数。</p>
<h2 id="customized-instruction"><a href="#customized-instruction" class="headerlink" title="customized instruction"></a>customized instruction</h2><p>ARM支持其它厂商可以自定义指令，相关的预留指令空间的定义可以查看ARM用户手册的这个<br>位置：Reserved encoding space for IMPLEMENTATION DEFINED instructions</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>ARM64</tag>
      </tags>
  </entry>
  <entry>
    <title>RCU的原理和使用</title>
    <url>/RCU%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p>RCU主要解决多个CPU core读写一个数据时的同步问题，一般情况下，我们都是加锁做同步，<br>RCU这种锁使得读写core可以最大的并行，对于读多写少的场景可以大幅提升性能。</p>
<p>RCU在需要更改数据的时候，不直接更新到现有数据上，而是先拷贝一份数据，在拷贝的数据<br>上先把数据更新好，然后再原子的更新数据，当数据是用指针访问时，只要用一个原子操作<br>更新掉指针就好。更改数据按照这样的逻辑，可以不断的进行，这样系统里就会出现多个数据<br>的版本，但是每个版本上的数据是一致的，只是不同版本的数据之间不一致。读数据时，就<br>直接读当前最新的数据就好，读到的数据使用完成，当没有其它的core还是使用时，就可以把<br>这个版本的数据释放掉，因为这个版本的数据可能被新的写操作更新了，系统里最新的数据<br>已经不是这个版本了。</p>
<p>下图是一个RCU的示意图:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-------&gt; time                               data can be released</span><br><span class="line">                                             /</span><br><span class="line">  point_to_data(ptd)            ptd1        /            ptd2</span><br><span class="line">        |                        |         /              |    </span><br><span class="line">        v                        |        /               |     data1 can be released</span><br><span class="line">     +------+                    |       /                |        /</span><br><span class="line">     | data |----------------------------                 |       /</span><br><span class="line">     +------+                    v                        |      /</span><br><span class="line">        |                    +-------+                    |     /</span><br><span class="line">        |                    | data1 |--------------------------</span><br><span class="line">        |                    +-------+                    v</span><br><span class="line">        |                        |                    +-------+</span><br><span class="line">        |                        |                    | data2 |</span><br><span class="line">        |                        |                    +-------+</span><br><span class="line">reader1:|--data-----             |                        |</span><br><span class="line">        |                        |                        |</span><br><span class="line">reader2:|   --data-----------------------                 |</span><br><span class="line">        |                        |                        |</span><br><span class="line">reader3:|                        | ---data1---------------------</span><br><span class="line">        |                        |                        |</span><br><span class="line">reader4:|                        |   ----data1-------     |</span><br><span class="line">        |                        |                        |</span><br><span class="line">reader5:|                        |                        |  data2</span><br></pre></td></tr></table></figure>
<p>其中data是要同步的对象。每次更新data，都是在一个新拷贝上做更新，对于之前的旧值，<br>如果还有reader在用就不能释放，直到没有reader还在用老值时，就可以把老值释放掉。<br>reader总是使用当前最新的版本。所以，可以看出来RCU实现最核心的地方是老值的释放逻辑。</p>
<h2 id="Linux内核中的RCU-API"><a href="#Linux内核中的RCU-API" class="headerlink" title="Linux内核中的RCU API"></a>Linux内核中的RCU API</h2><p>我们先看下使用RCU保护一个全局变量的具体使用方法。比如，我们要保护如下全局变量：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct data &#123;</span><br><span class="line">       int value;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>程序里用一个指针struct data __rcu *p 访问这个全局变量，那么这个变量的访问、修改<br>代码大概是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* 访问数据 */</span><br><span class="line">rcu_read_lock();</span><br><span class="line">struct data *d = rcu_dereference(p);</span><br><span class="line">rcu_read_unlock();</span><br><span class="line"></span><br><span class="line">/* 修改数据 */</span><br><span class="line">struct data *new = kmalloc(sizeof(*p), GFP_KERNEL);</span><br><span class="line">new-&gt;value = 10;</span><br><span class="line">rcu_assign_pointer(p, new);</span><br><span class="line">synchronize_rcu();</span><br></pre></td></tr></table></figure>

<p>我们从朴素的逻辑出发看看这个问题，首先rcu_dereference和rcu_assign_pointer之间保持<br>互斥，就可以保证对指针p的更新和引用是互斥的。考虑synchronize_rcu的实现，这个函数<br>需要在老值不再使用的时候把他们释放掉，最直观的思路就是把data的所有版本的指针都记录<br>起来，对于其中的每个值使用引用计数的办法记录是否还有用户使用，可以在lock的时候引用<br>计数加1，在unlock的时候引用计数减1，在synchronize_rcu里定期扫描全部指针的引用计数，<br>如果为0，表示没有reader访问了，就可以把对应这个版本的data释放掉。</p>
<p>显然内核里不是按如上的逻辑实现的，这样实现每个RCU锁使用的资源太多了。经典的内核RCU<br>实现中，lock/unlock只是做关内核抢占/开内核抢占的操作，synchronize_rcu中使用cpu_mask<br>记录每个CPU的调度情况，当每个CPU都调度过一次后，之前做RCU read的CPU一定已经离开<br>关抢占的临界区，这样可以认为所有对新版本数据的读者都已经离开临界区了。</p>
<p>有几个问题还没有想明白：对一个RCU保护数据的读操作可能发生在synchronize_rcu之后，<br>在这个间隙内，可以先发生一次调度，再进行RCU read，synchronize_rcu不会错误的检测到<br>已经调度过，就认为没有RCU reader么？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CPU0                            CPU1</span><br><span class="line"></span><br><span class="line">rcu_assign_pointer(p, new);</span><br><span class="line">synchronize_rcu</span><br><span class="line">                                schedule</span><br><span class="line">                                RCU read p</span><br></pre></td></tr></table></figure>
<p>RCU检测所有CPU都调度一次，有的CPU并没有做RCU read，那么理论上都不需要检测，如何<br>做到这个？有些CPU下线了或者进入低功耗状态，应该在做检测的CPU集合中去掉这些CPU。</p>
<p>内核里提供了基于RCU的基础数据结构，比如，内核里RCU保护的链表的实现逻辑的代码位置在:<br>linux/include/linux/rculist.h。</p>
<h2 id="RCU用户态库"><a href="#RCU用户态库" class="headerlink" title="RCU用户态库"></a>RCU用户态库</h2><p>有各种RCU用户态库支持在用户态使用RCU，比如，liburcu。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>riscv浮点指令整理</title>
    <url>/riscv%E6%B5%AE%E7%82%B9%E6%8C%87%E4%BB%A4%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h2 id="基本逻辑"><a href="#基本逻辑" class="headerlink" title="基本逻辑"></a>基本逻辑</h2><p>riscv当前的版本提供了多个浮点指令支持的扩展，它们分别是单精度浮点(F)、双精度浮点(D)、<br>四倍精度浮点(Q)以及目前只是一个占位符的十进制浮点(L)。</p>
<p>riscv在提供浮点指令的同时也增加了32个64bit的浮点指令寄存器(f0-f31)，以及一个浮点<br>特性相关的控制寄存器(fcsr)。</p>
<p>riscv浮点指令再从功能上细分，大概可以分为：计算相关，IO相关，转换相关。riscv的浮点<br>指令汇编命名的规则是: f + 功能 + .精度定义，比如，fadd.s就是单精度浮点加法。</p>
<h2 id="具体指令"><a href="#具体指令" class="headerlink" title="具体指令"></a>具体指令</h2><p>列一个表格总结下浮点相关的指令，必要的地方直接给出说明：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--------------------------------------------------------------------</span><br><span class="line"> 计算：                                                             </span><br><span class="line">                                                                    </span><br><span class="line">     基础计算：                                                     </span><br><span class="line">                                                                    </span><br><span class="line">           fadd.s    fadd.d                                         </span><br><span class="line">           fsub.s    fsub.d                                         </span><br><span class="line">           fmul.s    fmul.d                                         </span><br><span class="line">           fdiv.s    fdiv.d                                         </span><br><span class="line">           fsqr.s    fsqr.d                                         </span><br><span class="line">           fmin.s    fmin.d                                         </span><br><span class="line">           fmax.s    fmax.d                                         </span><br><span class="line">          fmadd.s   fmadd.d    乘加，两数相乘再加上一个数           </span><br><span class="line">         fnmsub.s  fnmsub.d    相乘的结果取反后再减去一个数         </span><br><span class="line">                                                                    </span><br><span class="line">      符号注入：                                                    </span><br><span class="line">                                                                    </span><br><span class="line">          fsgnj.s   fsgnj.d    符号注入都是取rs1绝对值, 取rs2的符号 </span><br><span class="line">         fsgnjn.s  fsgnjn.d    取rs2的符号再取反                    </span><br><span class="line">         fsgnjx.s  fsgnjx.d    取rs2/rs1的符号的异或作为符号        </span><br><span class="line">                                                                    </span><br><span class="line">      比较：                                                        </span><br><span class="line">                                                                    </span><br><span class="line">            feq.s     feq.d                                         </span><br><span class="line">            flt.s     flt.d                                         </span><br><span class="line">            fle.s     fle.d                                         </span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line"> 转换：                                                             </span><br><span class="line">                                                                    </span><br><span class="line">     寄存器数据移动：                                               </span><br><span class="line">                                                                    </span><br><span class="line">            fmv.sx              转换指令的两个寄存器都是后一个是源，</span><br><span class="line">            fmv.xs              前一个是目的，比如sx是，s &lt;- x。    </span><br><span class="line">                                                                    </span><br><span class="line">     数据格式转换：                                                 </span><br><span class="line">                                                                    </span><br><span class="line">            fcvt.sw             s: 单精度，d: 双精度，              </span><br><span class="line">            fcvt.dwu            w: word(32bit), wu: unsigned word   </span><br><span class="line">            fcvt.wus                                                </span><br><span class="line">            fcvt.wud                                                </span><br><span class="line">            fcvt.sd                                                 </span><br><span class="line">            fcvt.ds                                                 </span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line">  IO和分类：                                                        </span><br><span class="line">            flw                                                     </span><br><span class="line">            fsd                                                     </span><br><span class="line">            fclass.s fclass.d   返回一个浮点数的分类                </span><br><span class="line">--------------------------------------------------------------------</span><br></pre></td></tr></table></figure>

<h2 id="qemu实现"><a href="#qemu实现" class="headerlink" title="qemu实现"></a>qemu实现</h2><p>运算相关的浮点指令，qemu的实现都是使用helper函数，在helper函数里先尝试用硬件实现<br>直接计算，精度不符合要求的话再调用qemu里的软浮点函数完成运算。这里需要注意的是，<br>浮点运算里可以配置不同的rm值，确定数据的取舍类型，qemu会根据硬件的具体配置，传入<br>不同的rm值。</p>
]]></content>
      <tags>
        <tag>riscv</tag>
      </tags>
  </entry>
  <entry>
    <title>汇编代码里直接嵌入二进制</title>
    <url>/%E6%B1%87%E7%BC%96%E4%BB%A3%E7%A0%81%E9%87%8C%E7%9B%B4%E6%8E%A5%E5%B5%8C%E5%85%A5%E4%BA%8C%E8%BF%9B%E5%88%B6/</url>
    <content><![CDATA[<p>demo的主程序如下：(asm_binary.c)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">extern int add2(int);</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	int a = 1, ret = 0;</span><br><span class="line"></span><br><span class="line">	ret = add2(a);</span><br><span class="line">	printf(&quot;ret is %d\n&quot;, ret);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>add2这个函数的实现如下：(add.S)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// sf op S        sh imm12        rn    rd</span><br><span class="line">// 1  0  0 100010 0  xxxxxxxxxxxx xxxxx xxxxx</span><br><span class="line">//</span><br><span class="line">//                   imm12: 2     x10   x12</span><br><span class="line">// 1  0  0 100010 0  000000000010 01010 01100  ---+</span><br><span class="line">//                                                |</span><br><span class="line">// 10010001000000000000100101001100         &lt;-----+</span><br><span class="line">.text</span><br><span class="line">.global add2</span><br><span class="line">add2:</span><br><span class="line">	mov x10, x0</span><br><span class="line">.word   0b10010001000000000000100101001100</span><br><span class="line">        mov x0, x12</span><br><span class="line">	ret</span><br></pre></td></tr></table></figure>
<p>汇编指令和二进制指令被封装到一个叫add2的函数里，这个函数接受一个输入，返回输入+2<br>的值。这里嵌入的二进制是立即数add这个指令的内容，这个指令把rn的值和imm12域段表示<br>的立即数相加后的值写入rd寄存器。</p>
<p>这里需要注意的有以下两点：1. C代码和汇编代码如何衔接；2. 汇编代码和二进制指令如何衔接。</p>
<p>这里因为是函数调用形式，所以C代码和汇编代码直接的接口是满足ARM64 call convention<br>ABI接口的，这里编译器会把函数的入参先放到x0上，所以add2里可以直接使用x0，它的值就是<br>add2的入参。</p>
<p>继续看第二个问题。体系架构的汇编手册一般都会支持直接插入二进制的语法，比如这里就是<br>用.word表示一个32bit的二进制编码，编译器和汇编器看到这样的写法，不会关心这32bit的<br>内容，直接把它放到最后的二进制里。寄存器如何排布，这里其实可以直接只用x0作为rn寄存器，<br>把x0的值再传给x10，是为了说明什么样的寄存器是可以用的，在我们这种场景下，caller<br>save的寄存器都可以直接使用(ARM64下，x10/x12都是caller save的寄存器)。</p>
<p>所谓caller save寄存器，就是程序在过程调用时由主调方保存的寄存器，因为被调方可能<br>使用caller save寄存器，主调方必须在发起调用前主动保存自己随后还需要使用的寄存器,<br>所以主调方也只要保存自己在使用的caller save寄存器就好。</p>
<p>还是如上的例子，如果我们在add2里再调用一个函数，而且在add2里使用了x10，就需要在<br>调用新函数之前保存x10的值。</p>
<p>gcc asm_binary.c add.S做编译，使用qemu-aarch64 a.out运行这个demo，可以看见打印的<br>返回值为3。</p>
]]></content>
      <tags>
        <tag>编译链接</tag>
      </tags>
  </entry>
</search>
