<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>APUE学习笔记(第一章)</title>
    <url>/2021/07/17/APUE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%80%E7%AB%A0/</url>
    <content><![CDATA[<ol>
<li><p>每个进程有一个工作目录，所有相对目录都从这个工作目录开始解释。可以用chdir改<br>变这个工作目录。</p>
</li>
<li><p>用户态程序出错处理：<br>GNU c库中会定义一个errno的全局变量(ubuntu下在/usr/include/errno.h中)。函数<br>发生错误的时候可以把错误值写入这个errno，用以指名是什么错误。比如read()错误<br>时返回-1，我们去查errno的值，知道发生了什么错误。</p>
<p>#include &lt;string.h&gt;<br>char *strerror(int errnum); 输入errno，输出和errno相关的字符串的指针。</p>
<p>#include &lt;stdio.h&gt;<br>void perror(const char *msg);<br>输入是一个自定义的字符串，输出输入的字符串 + “：” + errno对应的字符串。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title>APUE学习笔记(第三章)</title>
    <url>/2021/07/17/APUE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%89%E7%AB%A0/</url>
    <content><![CDATA[<ol>
<li><p>文件相关的几个数据结构：文件描述符，文件表，i node.<br>每个进程有一个文件描述符表(用户态中), 用来指代这个进程打开的文件，文件描述符<br>是0，1，2…这样的数字，对应的是一个个打开的文件。read(), write(), lseek()<br>中指代文件的入参用文件描述符表。</p>
<p>文件表是一个内核数据结构，每个进程打开的每个文件在内核里都有对应的一个结构<br>来描述，所有的这样的结构组成文件表。每个这样的结构中包括，文件状态标志，文件<br>偏移量，i node指针。</p>
<p>open(path, oflag, …); open函数的第二个入参，可以设置只读，只写等参数。这<br>些参数最终被写到上面的文件状态标志中。当再调用read, write时，文件状态标志<br>将起到判定的作用。</p>
<p>这三个数据结构的关系见APUE上的图。</p>
</li>
<li><p>缓冲概念：从硬盘上读数据，文件系统会做缓冲，这个在内核中。用户态的I/O也会坐<br>缓冲，这个存在用户态的buffer中。数据写入硬盘，会经过用户态缓冲，内核文件系统<br>缓冲，添加到写队列等步骤。</p>
</li>
<li><p>多个进程写一个文件：如果是都在文件末尾写入，则lseek到文件末尾，再write写入，<br>会出现竞争。用open(..,O_APPEND,..)可以避免。O_APPEND会设置文件状态标志，write<br>时会查看文件状态标志，如果有O_APPEND则在文件末尾写入。</p>
</li>
<li><p>./a.out 6&gt;test<br>在文件描述符6上打开文件test. 编译下面的代码，运行上面的命令。可以开到文件<br>test中写入“12345”。以此类推：ls 2&gt;&gt;log, 在文件描述符2上打开文件，文件描述<br>符2一般是错误错误输出，所以这样的命令把ls命令的错误输出写入log文件。<br>command 6&lt;&gt;log, 是运行command的时候，在文件描述符6上，以读写的方式打开文件log。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">#include &lt;errno.h&gt;</span><br><span class="line"></span><br><span class="line">char buffer[5] = &#123;&#x27;1&#x27;,&#x27;2&#x27;,&#x27;3&#x27;,&#x27;4&#x27;,&#x27;5&#x27;&#125;;</span><br><span class="line"></span><br><span class="line">int main(void)</span><br><span class="line">&#123;</span><br><span class="line">	int ret;</span><br><span class="line">	int file;</span><br><span class="line"></span><br><span class="line">	/* 把下面的5改成其他大于5的数，写入文件时会出错</span><br><span class="line">                * 貌似是要输出几个buffer中的数据，这里的count就是多少，</span><br><span class="line">                * 否则会出错</span><br><span class="line">                */		</span><br><span class="line">	ret = write(6, buffer, 5);</span><br><span class="line">	if (ret == -1) &#123;</span><br><span class="line">		perror(&quot;read read_test&quot;);</span><br><span class="line">		return -1;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>fcntl函数, 提供对文件描述符标志和文件状态标志的查询和修改。<br>不清楚文中所说的文件描述符是什么? 下面得到的结果均是0</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fd = fcntl(0, F_GETFD);	</span><br><span class="line">printf(&quot;fd: %d\n&quot;, fd);</span><br><span class="line">fd = fcntl(1, F_GETFD);	</span><br><span class="line">printf(&quot;fd: %d\n&quot;, fd);</span><br><span class="line">fd = fcntl(2, F_GETFD);	</span><br><span class="line">printf(&quot;fd: %d\n&quot;, fd);</span><br></pre></td></tr></table></figure>
<p>文件状态标志位和上面的描述一致。fcntl可以复制文件描述符：fcntl(fd, F_DUPFD, fd_min)，<br>大于或等于fd_min的文件描述符将指向fd所指向的文件表项，相当于把fd复制到了fd_min</p>
</li>
</ol>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title>ARM64 qemu native build</title>
    <url>/2021/07/11/ARM64-qemu-native-build/</url>
    <content><![CDATA[<ol start="0">
<li><p>cd qemu; mkdir build; cd build;</p>
</li>
<li><p>install pkg-config</p>
</li>
<li><p>install zlib1g-dev zlib1g</p>
</li>
<li><p>ERROR: glib-2.22 gthread-2.0 is required to compile QEMU<br>install libglib2.0-dev</p>
</li>
<li><p>ERROR: pixman &gt;= 0.21.8 not present. Your options: …<br>install libpixman-1-dev</p>
</li>
<li><p>ERROR: DTC (libfdt) version &gt;= 1.4.0 not present. Your options: …<br>install libfdt-dev</p>
</li>
<li><p>./configure –target-list=aarch64-softmmu –enable-fdt –enable-kvm –disable-werror</p>
</li>
<li><p>make</p>
</li>
<li><p>make install # better uninstall previous qemu version if had one.</p>
</li>
<li><p>qemu-system-aarch64 –version</p>
<p> root@linaro-developer:~/qemu/build# qemu-system-aarch64 –version<br> QEMU emulator version 2.5.50, Copyright (c) 2003-2008 Fabrice Bellard</p>
</li>
</ol>
<p>In aarch64 ubuntu-16.04, there is doc: cnblogs.com/from-zero/p/14327440.html</p>
]]></content>
      <tags>
        <tag>qemu</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>APUE学习笔记(第四章)</title>
    <url>/2021/07/17/APUE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%9B%9B%E7%AB%A0/</url>
    <content><![CDATA[<p>这一章将文件系统的相关操作，比如文件，目录，软硬链接等等。要深入的了解这些需要<br>了解linux内核的相关知识。</p>
<ol>
<li><p>用户态和文件相关的基本数据结构是：struct stat<br>可以用int stat(const char *pathname, struct stat *buf); 读出stat的内容。<br>struct stat中的内容主要是对应文件的属性，一般从对应文件的inode节点得到这些<br>内容。inode节点是文件系统的内容，下面的部分会提到。</p>
</li>
<li><p>linux有多种文件类型。目录也是一种文件，其内容是目录中的文件名。另外符号链接，<br>设备文件，管道也是文件。</p>
</li>
<li><p>各种ID: 每个进程相关的ID</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+--------------+-----------+------------+</span><br><span class="line">|  实际用户ID  | 实际组ID  |            |</span><br><span class="line">+--------------+-----------+------------+</span><br><span class="line">|  有效用户ID  | 有效组ID  | 附加组ID   |</span><br><span class="line">+--------------+-----------+------------+</span><br><span class="line">|  ...         | ...       |  ...       |</span><br><span class="line">+--------------+-----------+------------+</span><br></pre></td></tr></table></figure>
<p> 一般决定文件访问权限的是第二行的有较<em><strong>，一般实际</strong></em> = 有效***</p>
<p> 除了和进程相关的ID, 每个文件有它自己的所有者和所有组。执行文件操作时，会<br> 比较进程的有效***和文件的所有者和所有组，来判定是否可以执行。</p>
<p> 这里有个特殊的文件属性位，当设置该位的时候，执行该文件的进程会自动把自己<br> 的有效用户ID设置成文件的所有这ID。文件的这个位一般用来提升执行权限，完成<br> 一些功能。比如/usr/bin/passwd</p>
<p> test@Latitude-D630:~$ ls -l <code>which passwd</code><br> -rwsr-xr-x 1 root root 45420  2月 17  2014 /usr/bin/passwd</p>
<p> 一般的用户可以用passwd去改自己的密码，但是修改密码要写/etc/shadow文件，</p>
<p> test@Latitude-D630:~$ ls -l /etc/shadow<br> -rw-r—– 1 root shadow 1513 12月 21 23:59 /etc/shadow</p>
<p> 可以看到/etc/shadow的所有者是root，且其他人不可以写。可以看到<br> /usr/bin/passwd的可执行位是’s’, 这表示文件/usr/bin/passwd的特殊文件属性位<br> 被设定，这样当普通用户执行passwd命令时当前进程把自己的有效用户ID提升成<br> root, 执行完后再恢复自己的ID。给文件加该属性要用chmod u+s file。</p>
</li>
<li><p>umask.<br>文件和目录创建时的默认属性一般是666和777。umask值可以去掉对应的读写执行<br>权限，比如现在的umask值是：022，那么同组用户和其他用户没有了写权限。<br>现在新建一个文件，得到的属性是644。使用umask命令可以查看当前shell的umask值，<br>使用umask *** 可以修改当前的umask值。</p>
</li>
<li><p>文件系统[1]</p>
</li>
</ol>
<p>  有关文件系统的进一步介绍，可以参考[1]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+----------------+------------------+--------------------+</span><br><span class="line">|      分区      |       分区       |      分区          |</span><br><span class="line">+----------------+------------------+--------------------+</span><br><span class="line"></span><br><span class="line">         boot block</span><br><span class="line">         +--------+----------+-----------+-------------+--------------+</span><br><span class="line">文件系统 | 自举块 |  超级块  |  柱面1    |    柱面2    |     ...      |</span><br><span class="line">         +--------+----------+-----------+-------------+--------------+</span><br><span class="line"></span><br><span class="line">+-------------+-------------+------------+-----------+---------+----------+</span><br><span class="line">| 超级块副本  |  配置信息   |  i节点图   |  块位图   |  i节点  |  数据块  |</span><br><span class="line">+-------------+-------------+------------+-----------+---------+----------+</span><br></pre></td></tr></table></figure>
<p>  上图根据APUE插图得到，一个磁盘可以划分几个分区，每个分区上可以部署不同的<br>  文件系统，文件系统的每个柱面中又有超级块副本，…，i节点，数据块等内容</p>
<p>  主要关注i节点和数据块，i节点存放文件的属性，数据块存放文件的内容。</p>
<p>  文件的根文件系统和磁盘上的根文件系统有说明区别？<br>  只是存储数据的介质不同，在挂载文件系统的时候加入的数据读写的回调函数不同。</p>
<p>  在ubuntu的根文件系统中有/boot/initrd.img-XXX, /boot/initrd.img-XXX是一个最小<br>  根文件系统, 它基本作用是在内核起来之时挂载的第一个文件系统，在这个文件系统中<br>  有必要的工具加载挂载磁盘文件系统需要的驱动。之后内核就可以挂载磁盘中真正的<br>  根文件系统了。这样内核里一开始就不用放相关的驱动了，initrd.img文件系统里，<br>  其实是使用的udev的机制，动态的识别系统外设，然后动态的加载所需要的内核驱动。<br>  之前内核驱动都已经以模块的形式编译。有关initrd.img的内容可以参考内核文档[2]</p>
<p>  制作一个文件上的文件系统, 相关的命令：<br>  dd mkfs.ext4 mount cp umount</p>
<ol start="6">
<li><p>符号链接和硬链接</p>
<p>创建一个B指向A的软连接, 可以建立指向文件和目录的软连接。软连接和硬连接的实质<br>区别是，硬链接只有一个inode节点，软链接的文件/目录本身的inode和软链接本身的<br>inode不同。</p>
<p>ln -s A B  /* A &lt;– B */</p>
</li>
<li><p>文件的时间有：文件最后访问时间，文件最后修改时间，文件inode节点最后修改时间。</p>
</li>
</ol>
<p>reference:<br>[1] Linux C编程一站式学习<br>[2] linux/Documentation/initrd.txt</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title>APUE学习笔记(第九章)</title>
    <url>/2021/07/11/APUE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B9%9D%E7%AB%A0/</url>
    <content><![CDATA[<p>最早的终端是电传打字机输入，纸带输出, 后来发展为键盘输入，显示器输出。终端在<br>linux下的文件是/dev/tty*, 虚拟终端/dev/tty1<del>6。之所以叫虚拟终端是因为/dev/tty1</del>6<br>公用一个物理的键盘和显示器。</p>
<p>终端无法输出系统启动的信息，所以需要用控制台输出系统启动信息用来调试系统。<br>控制台在linux下的文件是/dev/console, 一般控制台的物理接入是串口。</p>
<p>串口在linux下的文件是/dev/ttyS*</p>
<p>之前的终端一般由串口接入，由网络也可以接入终端，这个一般叫伪终端。像ubuntu上使用<br>terminal程序接入的系统的shell所对应的终端是伪终端。从这个伪终端上看，就好像是<br>通过一个终端接入系统的。在linux下伪终端一般表示为pts*</p>
<p>伪终端主设备和伪终端从设备是伪终端相关的两个重要概念。</p>
<pre><code>                   (伪终端主设备)
</code></pre>
<p>telnet client          telnet server————-+<br>      |                     |                    |<br>networking driver    networking driver           |<br>      |                     |                    |<br>networking card      networking card             |    pts/0(伪终端从设备) pts/1(伪终端从设备) …<br>      |                     |<br>      +—–internet ——-+</p>
<p>下面是所做的一些测试(ubuntu 14.04)</p>
<hr>
<p>who可以查看都现在系统中的登录情况</p>
<p>sherlock@T440:~/notes$ who<br>sherlock tty2         2015-12-14 14:57<br>sherlock tty1         2015-12-14 14:55<br>sherlock :0           2015-12-14 14:44 (:0)<br>sherlock pts/1        2015-12-14 14:48 (:0)<br>sherlock pts/13       2015-12-14 14:56 (:0)</p>
<hr>
<p>虚拟终端：tty1, tty2… tty6<br>echo “test” &gt; /dev/tty1<br>打开虚拟终端tty1（ctrl + alt + f1）, 可以看见在屏幕上输出了”test”</p>
<hr>
<p>在ubuntu terminal中打开的终端是伪终端，在/dev/pts/*是他们的设备文件<br>echo “test1” &gt; /dev/pts/0<br>在伪终端上有”test1”输出</p>
<p>进程组, 进程组ID, 作业控制，前后台进程都是和进程相关的概念, 用ps命令可以查看这些<br>信息<br>(ctrl + z 挂起作业中的前台进程组中的所有进程)</p>
<hr>
<p>e.g. ps -alxf (desktop ubuntu 14.04)</p>
<p>PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND<br>…<br>2113  6613  2296  2296 ?           -1 Sl    1000   0:17          _ gnome-terminal<br>6613  6620  2296  2296 ?           -1 S     1000   0:00          |   _ gnome-pty-helper<br>6613  6621  6621  6621 pts/0     7501 Ss    1000   0:00          |   _ bash<br>6621  7501  7501  6621 pts/0     7501 R+    1000   0:00          |   |   _ ps -ajxf<br>6613  6648  6648  6648 pts/18    7485 Ss    1000   0:00          |   _ bash<br>6648  7485  7485  6648 pts/18    7485 S+    1000   0:00          |   |   _ vi APUE_note_5.md<br>6613  7435  7435  7435 pts/20    7497 Ss    1000   0:00          |   _ bash<br>7435  7497  7497  7435 pts/20    7497 S+       0   0:00          |       _ sudo grep -r wang<br>7497  7498  7497  7435 pts/20    7497 D+       0   0:01          |           _ grep -r wang</p>
<p>如上图所示, 首先打开了Terminal的程序。Terminal下打开了三个bash, 分别对应三个<br>伪终端：pts/0, pts/18, pts/20. 每个bash和它的自进程都在一个session里(SID=session ID).<br>每个session里的情况又各有不同，比如，SID=6621 它由两个进程组组成(PGID=6621, 7501),<br>而SID=7435的session, 它由两个进程组组成，其中的第二个进程组又有两个进程组成</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>APUE</tag>
        <tag>Linux用户态编程</tag>
      </tags>
  </entry>
  <entry>
    <title>Add CentOS yum repo in Redhat system</title>
    <url>/2021/07/05/Add-CentOS-yum-repo-in-Redhat-system/</url>
    <content><![CDATA[<p>In Redhat system, you should buy it to use. However, CentOS is<br>free and almost same with related Redhat system. So after we<br>add the CentOS yum repo, then we can directly use Redhat system.<br>(If you already have a CentOS, please use it directly)</p>
<p>We can add the repo configuration file in /etc/yum.repos.d/ like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># CentOS-Base.repo</span><br><span class="line">#</span><br><span class="line"># The mirror system uses the connecting IP address of the client and the</span><br><span class="line"># update status of each mirror to pick mirrors that are updated to and</span><br><span class="line"># geographically close to the client.  You should use this for CentOS updates</span><br><span class="line"># unless you are manually picking other mirrors.</span><br><span class="line">#</span><br><span class="line"># If the mirrorlist= does not work for you, as a fall back you can try the </span><br><span class="line"># remarked out baseurl= line instead.</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">[base]</span><br><span class="line">name=CentOS-$releasever - Base</span><br><span class="line">baseurl=http://mirror.centos.org/altarch/$releasever/os/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</span><br><span class="line">       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7-aarch64</span><br><span class="line"></span><br><span class="line">#released updates </span><br><span class="line">[updates]</span><br><span class="line">name=CentOS-$releasever - Updates</span><br><span class="line">baseurl=http://mirror.centos.org/altarch/$releasever/updates/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</span><br><span class="line">       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7-aarch64</span><br><span class="line"></span><br><span class="line">#additional packages that may be useful</span><br><span class="line">[extras]</span><br><span class="line">name=CentOS-$releasever - Extras</span><br><span class="line">baseurl=http://mirror.centos.org/altarch/$releasever/extras/$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</span><br><span class="line">       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7-aarch64</span><br></pre></td></tr></table></figure>
<p>but it also goes wrong.</p>
<p>here we should replace the variable “$releasever” and “$basearch” to the<br>specific one which we want to use. you can search in <span class="exturl" data-url="aHR0cDovL21pcnJvci5jZW50b3Mub3JnL2FsdGFyY2gv">http://mirror.centos.org/altarch/<i class="fa fa-external-link-alt"></i></span><br>to see which repo you will use.</p>
<p>In my case, I will use:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[base]</span><br><span class="line">name=CentOS-base</span><br><span class="line">baseurl=http://mirror.centos.org/altarch/7.4.1708/os/aarch64/</span><br></pre></td></tr></table></figure>
<p>and I also remove gpgcheck related lines.</p>
<p>It works fine with me :)</p>
]]></content>
      <tags>
        <tag>包管理</tag>
        <tag>RPM/YUM</tag>
      </tags>
  </entry>
  <entry>
    <title>Build your own rootfs by buildroot</title>
    <url>/2021/06/20/Build-your-own-rootfs-by-buildroot/</url>
    <content><![CDATA[<ol>
<li><p>git clone git://git.buildroot.net/buildroot</p>
</li>
<li><p>cd buildroot</p>
</li>
<li><p>make defconfig</p>
</li>
<li><p>make menuconfig to choose special tools, e.g. numactl</p>
</li>
<li><p>make menuconfig to choose BR2_TARGET_ROOTFS_CPIO=y and<br>Compression method to gzip.</p>
</li>
<li><p>make menuconfig to choose your arch, e.g. my arch is BR2_aarch64<br>Note: if you choose a wrong arch, after changing to right arch config,</p>
<pre><code>  you should make clean, than make, otherwise you may meet something
</code></pre>
<p>  wrong like: “No working init found”.</p>
</li>
<li><p>wait and you will find built minirootfs in buildroot/output/images/<br>you can use rootfs.cpio.gz as rootfs here.</p>
</li>
</ol>
<p>Note: 编译glib(BR2_PACKAGE_LIBGLIB2), 依赖BR2_TOOLCHAIN_BUILDROOT_WCHAR, BR2_USE_WCHAR</p>
<p>如上是之前编译buildroot的一个笔记，其实buildroot在交叉编译构建文件系统的情况还是<br>非常好用的，因为buildroot里包含了很多基本库，如果你的app依赖了第三方的库，在交叉编译<br>的时候，一般你要先交叉编译依赖的库，然后再交叉编译app的时候链接之前交叉编译出来的<br>库，一两个这样的依赖库还好，要是有比较多的依赖库就会比较麻烦。如果使用buildroot，<br>只要在buildroot配置的时候打开依赖库的配置就好。</p>
<p>自己的app如何集成在buildroot编译生成的小系统里。一个好用的方法是使用buildroot的<br>override功能。它的基本逻辑是，现在buildroot的编译配置体系里加如你想编译的包的配置，<br>如下的patch中，我们加了一个叫devmmu的包的配置，这个包的配置基本是空的，然后我们<br>在buildroot的根目录下发放一个local.mk的文件，并在里指明devmmu这个包的源码目录，<br>相关的写法一定要按照<package_name>_OVERRIDE_SRCDIR = <package_path>的写法。<br>全部配置好后，在buildroot make menuconfig的时候把devmmu选上，make编译的时候就会<br>去指定的目录里找devmmu的代码，并编译安装到生成的小文件系统里。如果后面改了devmmu<br>的源码，使用make devmmu-rebuild all可以只便宜安装devmmu。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff --git a/local.mk b/local.mk</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000000..2775c16f6b</span><br><span class="line">--- /dev/null</span><br><span class="line">+++ b/local.mk</span><br><span class="line">@@ -0,0 +1 @@</span><br><span class="line">+DEVMMU_OVERRIDE_SRCDIR = /home/wangzhou/devmmu-user/</span><br><span class="line">diff --git a/package/Config.in b/package/Config.in</span><br><span class="line">index 82b28d2835..bcb1649da7 100644</span><br><span class="line">--- a/package/Config.in</span><br><span class="line">+++ b/package/Config.in</span><br><span class="line">@@ -2524,4 +2524,6 @@ menu &quot;Text editors and viewers&quot;</span><br><span class="line"> 	source &quot;package/vim/Config.in&quot;</span><br><span class="line"> endmenu</span><br><span class="line"> </span><br><span class="line">+source &quot;package/devmmu/Config.in&quot;</span><br><span class="line">+</span><br><span class="line"> endmenu</span><br><span class="line">diff --git a/package/devmmu/Config.in b/package/devmmu/Config.in</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000000..4c7f9edcc4</span><br><span class="line">--- /dev/null</span><br><span class="line">+++ b/package/devmmu/Config.in</span><br><span class="line">@@ -0,0 +1,4 @@</span><br><span class="line">+config BR2_PACKAGE_DEVMMU</span><br><span class="line">+	bool &quot;devmmu&quot;</span><br><span class="line">+	help</span><br><span class="line">+		DevMMU testsuit</span><br><span class="line">diff --git a/package/devmmu/devmmu.mk b/package/devmmu/devmmu.mk</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000000..c02eedf5c3</span><br><span class="line">--- /dev/null</span><br><span class="line">+++ b/package/devmmu/devmmu.mk</span><br><span class="line">@@ -0,0 +1,12 @@</span><br><span class="line">+################################################################################</span><br><span class="line">+#</span><br><span class="line">+# devmmu</span><br><span class="line">+#</span><br><span class="line">+################################################################################</span><br><span class="line">+</span><br><span class="line">+DEVMMU_VERSION = 0.1</span><br><span class="line">+DEVMMU_SOURCE=</span><br><span class="line">+DEVMMU_INSTALL_STAGING=NO</span><br><span class="line">+DEVMMU_DEPENDENCIES=</span><br><span class="line">+</span><br><span class="line">+$(eval $(autotools-package))</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>rootfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Compile DPDK in ARMv8 machine</title>
    <url>/2021/06/29/Compile-DPDK-in-ARMv8-machine/</url>
    <content><![CDATA[<p>Hardware: D05 board.<br>System: Linux 157 4.11.0-45.el7.aarch64 aarch64 GNU/Linux<br>        CentOS Linux release 7.4.1708 (AltArch) </p>
<ol start="0">
<li><p>download dpdk codes<br> git clone git://dpdk.org/dpdk</p>
</li>
<li><p>install kernel header files<br> yum install kernel-headers.aarch64</p>
</li>
<li><p>install libpcap<br> yum install libpcap-devel.aarch64</p>
</li>
<li><p>install numa header files<br> yum install numactl-devel.aarch64</p>
</li>
<li><p>config and make<br> make config T=arm64-armv8a-linuxapp-gcc<br> make</p>
<p> Note: above will put compiled files to dpdk/build</p>
<p> “make install T=arm64-armv8a-linuxapp-gcc” will create a directory<br> named as arm64-armv8a-linuxapp-gcc under dpdk and put compiled files<br> in it.</p>
</li>
<li><p>build the dpdk helloworld example:</p>
<p> export RTE_SDK=~/repos/dpdk<br> export RTE_TARGET=arm64-armv8a-linuxapp-gcc<br> cd examples/helloworld/<br> make</p>
<p> note: should use “make install T=arm64-armv8a-linuxapp-gcc” in step4</p>
<pre><code>   above to avoid compile error in step5.
</code></pre>
</li>
<li><p>test helloworld in above build:</p>
<p> (1) enable hugepage</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo 2 &gt; /sys/kernel/mm/hugepages/hugepages-524288kB/nr_hugepages</span><br><span class="line">grep -i huge /proc/meminfo</span><br><span class="line">AnonHugePages:         0 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">HugePages_Total:       2</span><br><span class="line">HugePages_Free:        1</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:     524288 kB</span><br></pre></td></tr></table></figure>
<p> (2) run ./helloworld -c 3 -n 2 in examples/helloworld</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">EAL: Detected 64 lcore(s)</span><br><span class="line">EAL: Detected 4 NUMA nodes</span><br><span class="line">EAL: Multi-process socket /var/run/dpdk/rte/mp_socket</span><br><span class="line">EAL: No free hugepages reported in hugepages-524288kB</span><br><span class="line">EAL: No free hugepages reported in hugepages-524288kB</span><br><span class="line">EAL: 10 hugepages of size 2097152 reserved, but no mounted hugetlbfs found for that size</span><br><span class="line">EAL: Probing VFIO support...</span><br><span class="line">EAL: PCI device 0002:e9:00.0 on NUMA socket -1</span><br><span class="line">EAL:   Invalid NUMA socket, default to 0</span><br><span class="line">EAL:   probe driver: 8086:10fb net_ixgbe</span><br><span class="line">EAL: PCI device 0002:e9:00.1 on NUMA socket -1</span><br><span class="line">EAL:   Invalid NUMA socket, default to 0</span><br><span class="line">EAL:   probe driver: 8086:10fb net_ixgbe</span><br><span class="line">hello from core 1</span><br><span class="line">hello from core 0</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <tags>
        <tag>ARMv8</tag>
        <tag>DPDK</tag>
      </tags>
  </entry>
  <entry>
    <title>ARM64内存屏障</title>
    <url>/2021/07/05/ARM64%E5%86%85%E5%AD%98%E5%B1%8F%E9%9A%9C/</url>
    <content><![CDATA[<p> 要理解内存屏障，要知道memory的一些特性，这里的memory不是只指内存。而是从CPU角度<br> 看到的存储空间。如果CPU对一系列的指令执行是严格串行的，我们是不用外加上面内存<br> 屏障的。比如：</p>
<pre><code> str x1 [x2]   // A
str x2 [x3]   // B
</code></pre>
<p> A指令完全执行完，B指令才执行。这样根本不需要内存屏障指令的介入。</p>
<p> 但是现在处理器和内存系统之间的速度差距已经非常大，如果要上面的A执行完成才执行B<br> cpu要消耗大量的时间干等在那里。为了缓解CPU和内存系统间的速度差距，同时也为了不断<br> 提升CPU的效率，现代CPU存在很多指令执行上的技术，导致的结果是指令执行的结果和之前<br> 的逻辑已经不一样了。</p>
<p> CPU上存在着多种指令执行的技术，其中导致需要加上内存屏障执行的是CPU上的指令乱序<br> 执行。</p>
<p> 为了解释清楚CPU执行乱序执行需要引入两个基本的概念，一个是内存类型(memory type),<br> 另外一个是master/slave.</p>
<p> 内存类型定义的CPU和内存相互作用时的一些性质，内存类型只有normal和device两种,<br> normal基本上可以对应DDR，device基本上可以对应设备的MMIO. normal的内存可以配置成<br> cacheable和non-cacheable, 其中cacheable的内存属性又可以进一步配置shareability的属性。<br> device的内存的属性使用GRE表述，G是gather，R是re-order, E是write early aknowledge,<br> 每种属性可以是nX(e.g. nG), 就是不支持的意思。关于normal和device下面的诸多属性,<br> 可以查阅本文开始时提到的ARM编程手册13章中的内容。</p>
<p> master和slave是芯片里面的概念，master是一个动作的发起者，slave是一个动作的接收<br> 者。一般的一个芯片里，master有各个core，以及外设的DMA控制器。slave有内存和MMIO.</p>
<p> 基于以上的概念，我们看内存屏障为啥有存在的意义。master(CPU, 不确定DMA是不是这样)<br> 对normal内存的操作，存在很多提升效率的处理(多发射，乱序执行，执行预取…).<br> 我们回到乱序执行上来。ARMv8的CPU对指令乱序执行的几条规则是这样的:</p>
<ol>
<li><p>在单一的core上，指令完成的顺序是串行的。注意，master收到slave的(有时不一定是<br>slave, 比如，device内存的E属性)完成信息为一个执行完成。</p>
</li>
<li><p>在单一的core上下发的指令，slave上完成的顺序是无法保证串行的。</p>
</li>
<li><p>多个master上看到的操作是不保序的。</p>
</li>
</ol>
<p> 这样就会导致:</p>
<p> 在一个core上:</p>
<pre><code> 等待A条件出现
 A条件出现后执行B动作
</code></pre>
<p>  如果B动作先被执行，则可能出错。所以要在之间加上内存屏障：</p>
<pre><code> 等待A条件出现
 内存屏障
 A条件出现后执行B动作
</code></pre>
<p>  在多个core上:</p>
<pre><code> core 1           core 2

 DMA              check flag
 set flag         get date from DMA range
  
</code></pre>
<p>  core1的逻辑是先用DMA搬数据，搬完数据后设立一个标记位。core2不断的在检测标记<br>  位，当检测到标记位的时候，core2就可以使用DMA搬好的数据了。</p>
<p>  但是，core1的DMA和set flag两个操作可能是乱序执行的，可能在core2看来flags已经<br>  置位了，但是其实DMA的数据还没有搬完, 这时，如果core2去使用数据，就有可能出错。<br>  正确的做法是在DMA和set flag执行加上内存屏障，确保DMA完成了，再去set flag.<br>  注意，这里其实就是上面的第三点。另外，这里和cache一致性没有关系, 如果，我们<br>  单独看dma，或者单独看flag，两者各自都是硬件保证cache一致性的。</p>
<p>  具体来说ARMv8上的内存屏障执行有ISB，DMB，DSB。ISB和指令相关，后面两个和内存<br>  访问相关。具体的区别可以查文章开头提到的书。</p>
<p>  具体驱动代码, 可以参考下arm64下几组read/write的实现，代码在arch/arm64/include/asm/io.h.<br>  可以看到同一个write函数有一下三种类型:</p>
<pre><code>__raw_writeb
writeb_relaxed(v,c)
writeb(v,c)    
</code></pre>
<p>  第一个是str指令的封装，第二个用于I/O内存也是str的封装，第三个可以用于normal的<br>  内存读写，其实是真是write之前加了一个DSB的内存屏障指令。</p>
<p>  未完待续…</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>ARM64</tag>
        <tag>内存屏障</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言嵌入ARM64汇编</title>
    <url>/2021/07/05/C%E8%AF%AD%E8%A8%80%E5%B5%8C%E5%85%A5ARM64%E6%B1%87%E7%BC%96/</url>
    <content><![CDATA[<ol start="0">
<li><p>使用场景</p>
<p>嵌入汇编一般用在c语言无法使用的地方，比如要操作系统寄存器了，比如要直接调用<br>汇编指令完成功能了(内存屏障，刷cache), 还有就是要优化性能。Linux内核里和硬件<br>直接接触的体系架构相关的代码有很多是直接用汇编写的。</p>
</li>
<li><p>嵌入汇编的语法</p>
<p>asm volatile(code : output operand list : input operand list: clobber list);</p>
<p>其中volatile告诉编译器不要去优化这里的汇编代码。防止你辛苦写的汇编代码一下被<br>编译器优化的面目全非。</p>
<p>有时你会看到这样的嵌入汇编代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">asm(&quot;mov %[result], %[value], ror #1&quot; : [result] &quot;=r&quot; (y) : [value] &quot;r&quot; (x) :)</span><br></pre></td></tr></table></figure>
<p>但是一般的，比如在linux内核里你看到的是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">asm(&quot;mov %0, %1, ror #1&quot; : &quot;=r&quot; (result) : &quot;r&quot; (value));</span><br></pre></td></tr></table></figure>
<p>唯一不同的是，code里和operand里参数的对应关系的写法。我们这里用第二种方法，<br>它的对应关系是，从output operand list到input operand list, 参数的命名从0开始，<br>依次是%0，%1…</p>
<p>operand list的一般格式是：”常量” (参数)。参数是和C语言里变量的接口，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static inline void __raw_writel(u32 val, volatile void __iomem *addr)</span><br><span class="line">&#123;</span><br><span class="line">	asm volatile(&quot;str %w0, [%1]&quot; : : &quot;rZ&quot; (val), &quot;r&quot; (addr));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的val和addr就是C代码里的变量。常量的意义可以参考:<br><span class="exturl" data-url="aHR0cHM6Ly9nY2MuZ251Lm9yZy9vbmxpbmVkb2NzL2djYy9Db25zdHJhaW50cy5odG1s">https://gcc.gnu.org/onlinedocs/gcc/Constraints.html<i class="fa fa-external-link-alt"></i></span>, 简单讲这个常量可以是r,<br>Q, m等，也可以再次被=，+， &amp;等修饰。r, m, Q讲的是这个变量可以被编译器编译在<br>寄存器里，内存里，或者是通过指针再间接寻址下，=标识这个变量只读，+表示这个<br>变量可读可写。这里code里的w表示这个变量的位宽，w表示是32bit的，x表示是64bit<br>的，这里再次注意，我们现在讨论的是aarch64下的汇编。</p>
<p>可以随便写一个程序，aarch64-linux-gnu-gcc -S xx.c编译下，看看生成的汇编代码<br>是怎么样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void test()</span><br><span class="line">&#123;</span><br><span class="line">        int y = 0;</span><br><span class="line">	int x = 1;</span><br><span class="line"></span><br><span class="line">	asm volatile(&quot;add %w0, %w1, 1&quot; : &quot;=r&quot; (y) : &quot;r&quot; (x) :);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面是编译成的汇编代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	.arch armv8-a</span><br><span class="line">	.file	&quot;embedded_asm.c&quot;</span><br><span class="line">	.text</span><br><span class="line">	.align	2</span><br><span class="line">	.global	test</span><br><span class="line">	.type	test, %function</span><br><span class="line">test:</span><br><span class="line">	sub	sp, sp, #16</span><br><span class="line">	str	wzr, [sp, 12]</span><br><span class="line">	mov	w0, 1</span><br><span class="line">	str	w0, [sp, 8]</span><br><span class="line">	ldr	w0, [sp, 8]</span><br><span class="line">#APP</span><br><span class="line">// 6 &quot;embedded_asm.c&quot; 1</span><br><span class="line">	add w0, w0, 1</span><br><span class="line">// 0 &quot;&quot; 2</span><br><span class="line">#NO_APP</span><br><span class="line">	str	w0, [sp, 12]</span><br><span class="line">	nop</span><br><span class="line">	add	sp, sp, 16</span><br><span class="line">	ret</span><br><span class="line">	.size	test, .-test</span><br><span class="line">	.ident	&quot;GCC: (Linaro GCC 6.3-2017.05) 6.3.1 20170404&quot;</span><br><span class="line">	.section	.note.GNU-stack,&quot;&quot;,@progbits</span><br></pre></td></tr></table></figure>
<p>把r改成m后是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test:</span><br><span class="line">	sub	sp, sp, #16</span><br><span class="line">	str	wzr, [sp, 12]</span><br><span class="line">	mov	w0, 1</span><br><span class="line">	str	w0, [sp, 8]</span><br><span class="line">#APP</span><br><span class="line">// 6 &quot;embedded_asm.c&quot; 1</span><br><span class="line">	add [sp, 12], [sp, 8], 1</span><br><span class="line">// 0 &quot;&quot; 2</span><br><span class="line">#NO_APP</span><br><span class="line">	nop</span><br><span class="line">	add	sp, sp, 16</span><br><span class="line">	ret</span><br></pre></td></tr></table></figure>
<p>把r改成Q后是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   test:</span><br><span class="line">	sub	sp, sp, #16</span><br><span class="line">	str	wzr, [sp, 12]</span><br><span class="line">	mov	w0, 1</span><br><span class="line">	str	w0, [sp, 8]</span><br><span class="line">	add	x0, sp, 12</span><br><span class="line">	add	x1, sp, 8</span><br><span class="line">#APP</span><br><span class="line">// 6 &quot;embedded_asm.c&quot; 1</span><br><span class="line">	add [x0], [x1], 1</span><br><span class="line">// 0 &quot;&quot; 2</span><br><span class="line">#NO_APP</span><br><span class="line">	nop</span><br><span class="line">	add	sp, sp, 16</span><br><span class="line">	ret</span><br></pre></td></tr></table></figure>

<p>clobber的解释gcc上说是：告知编译器嵌入的汇编代码除了影响output operand里的<br>变量外，还对哪些寄存器或者是对内存会产生影响。把他们可以列到clobber list里面<br>来。寄存器直接列名字，两个特殊的clobber参数是”memory”和”cc”, cc表示会影响到<br>flags register，memory就会影响到input operant参数指向的内存。</p>
</li>
<li><p>举例</p>
<p>…</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>ARM64</tag>
        <tag>汇编</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言bit操作</title>
    <url>/2021/07/05/C%E8%AF%AD%E8%A8%80bit%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p>#include &lt;stdio.h&gt;</p>
<p>int main()<br>{<br>        /* test normal &lt;&lt; */<br>        int i = 1;<br>        int j = i &lt;&lt; 5;<br>        long k;<br>        printf(“test normal &lt;&lt;\n”);<br>        printf(“j = i &lt;&lt; 5 –&gt; i: %x, j: %x\n”, i, j);<br>        printf(“\n”);</p>
<pre><code>    /* test normal &lt;&lt;, shift &gt; type size */
    i = 1;
    /* this is not right in gcc, j will be 1, not 0x100000000 */
    j = i &lt;&lt; 32;
    printf(&quot;test normal &lt;&lt;, shift &gt; type size\n&quot;);
    printf(&quot;j = i &lt;&lt; 32 --&gt; i: %lx, j: %lx\n&quot;, i, j);
    printf(&quot;\n&quot;);

     /* test normal &lt;&lt;, shift &gt; type size */
    i = 1;
    /* this is right in gcc */
    k = (long)i &lt;&lt; 32;
    printf(&quot;test normal &lt;&lt;, shift &gt; type size\n&quot;);
    printf(&quot;size of long %d\n&quot;, sizeof(long));
    printf(&quot;k = i &lt;&lt; 32 --&gt; i: %lx, k: %lx\n&quot;, i, k);
    printf(&quot;\n&quot;);       

    /* test normal &gt;&gt; */
    i = 0xffe00000;
    /* as i is a int, it will remove to right with bit 1 in top bit */
    j = i &gt;&gt; 1;
    printf(&quot;test normal &lt;&lt;\n&quot;);
    printf(&quot;j = i &gt;&gt; 1 --&gt; i: %x, j: %x\n&quot;, i, j);
    printf(&quot;\n&quot;);

     /* test |, type of two operands is different */
    int l = 0x10000000;
    long m = 0;
    long g = l | m;
    printf(&quot;test |, type of two operands is different\n&quot;);
    printf(&quot;g = l | m --&gt; l: %lx, m: %llx, g: %llx\n&quot;, l, m, g);
    printf(&quot;\n&quot;);

printf(&quot;0x%08x\n&quot;, 2);

    return 0;
</code></pre>
<p>}</p>
]]></content>
      <tags>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>Dump PCIe设备BAR寄存器</title>
    <url>/2021/06/27/Dump-PCIe%E8%AE%BE%E5%A4%87BAR%E5%AF%84%E5%AD%98%E5%99%A8/</url>
    <content><![CDATA[<p>下载后编译即可，可能需要安装readline和curses库。<br>gcc pci_debug.c -lreadline -lcurses -o pci_debug</p>
<p>这个小工具的用法很简单，看help就可以懂:</p>
<p>pci_debug -s <BDF><br>可以进入一个命令行交互界面。<br>(这个工具有一个bug，就是不支持设备有domain号, 对于有domain号的设备可以打上如<br> 下的补丁)</p>
<p>输入命令：d addr len 查看地址addr开始的len个单位的数值，这里的单位default数值是<br>          32bit。</p>
<pre><code>  q退出工具。

      e改变大小端。
  ...
</code></pre>
<p>这个工具需要sudo或者root权限。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff --git a/pci_debug.c b/pci_debug.c</span><br><span class="line">index f746840..30f4d45 100644</span><br><span class="line">--- a/pci_debug.c</span><br><span class="line">+++ b/pci_debug.c</span><br><span class="line">@@ -177,9 +177,9 @@ int main(int argc, char *argv[])</span><br><span class="line"> 	 */</span><br><span class="line"> </span><br><span class="line"> 	/* Extract the PCI parameters from the slot string */</span><br><span class="line">-	status = sscanf(slot, &quot;%2x:%2x.%1x&quot;,</span><br><span class="line">-			&amp;dev-&gt;bus, &amp;dev-&gt;slot, &amp;dev-&gt;function);</span><br><span class="line">-	if (status != 3) &#123;</span><br><span class="line">+	status = sscanf(slot, &quot;%4x:%2x:%2x.%1x&quot;,</span><br><span class="line">+			&amp;dev-&gt;domain, &amp;dev-&gt;bus, &amp;dev-&gt;slot, &amp;dev-&gt;function);</span><br><span class="line">+	if (status != 4) &#123;</span><br><span class="line"> 		printf(&quot;Error parsing slot information!\n&quot;);</span><br><span class="line"> 		show_usage();</span><br><span class="line"> 		return -1;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言volatile笔记</title>
    <url>/2021/07/05/C%E8%AF%AD%E8%A8%80volatile%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>C语言里volatile用来修饰一个变量，告诉编译器怎么去编译一个变量。编译器编译一个变量，<br>看起来应该是很直白的：一个变量在内存里存放，读的话就读对应的内存，写这个变量的<br>话就向这个内存写值就可以了，为什么还要volatile这个词来修饰下？</p>
<p>在这之前我们要搞清楚两个问题.</p>
<p>第一，代码要编译成汇编指令才可以执行，编译成的汇编指令可能不是从内存里取这个值，<br>当然也可能写内存的时候不会立即生效。注意我这里只提到了内存，没有说cache，这里我<br>假设代码运行的机器是可以在硬件层面保证内存/cache的数据一致性的，就是说cache在软<br>件层面是透明的。其实现在大部分机器也确实是这样的。</p>
<p>第二，读写一个值的时候，不要认为它对应的物理存储一定是内存(DDR), 它还可以是设备<br>的一段IO寄存器(MMIO), 比如，我把一个物理设备的一段IO寄存器直接mmap出来，你在<br>用户态用mmap的到一个地址，你访问这个地址，其实访问的是这个设备的寄存器。IO寄存器<br>的属性和内存的完全不一样，你写下去一个值是为了触发一个操作，你再写下去一个相同的<br>值是为了再触发一次这个操作, 第二个操作是不能代替第一个操作的；硬件还可以通过IO<br>寄存器给你返回状态，也就是一个“内存”里的值，可能自己会改变 :)</p>
<p>还有一种情况和上面的情况相似，就是多个独立的执行流改变一个变量的情况。当一个写<br>操作已经改变了内存里的值的时候，如果读代码是去读一个缓存的值，这样就会出错。再次<br>强调这里不包括cache和内存不一致的问题，也就是说，我们假设如果写已经更新了内存，<br>读的时候，即使读的是对应的cache，硬件也会识别到cache里的数据和内存的数据是不一样<br>的，硬件会自动把内存里的新数据填到cache里，之后读是可以读到正确的值。那什么时候<br>会出问题呢: 当读操作把数据缓存在寄存器里的时候。</p>
<p>基于以上的认识，我们看一段代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* for test 1 */</span><br><span class="line">int test;</span><br><span class="line"></span><br><span class="line">/* for test 2 */</span><br><span class="line">volatile int test_v;</span><br><span class="line"></span><br><span class="line">/* for test 3 */</span><br><span class="line">int test_3, tmp1, tmp2, tmp3;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void volatile_test_1()</span><br><span class="line">&#123;</span><br><span class="line">	test = 111;	</span><br><span class="line">	test = 222;</span><br><span class="line">	test = 333;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void volatile_test_2()</span><br><span class="line">&#123;</span><br><span class="line">	test_v = 111;	</span><br><span class="line">	test_v = 222;</span><br><span class="line">	test_v = 333;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void volatile_test_3_read()</span><br><span class="line">&#123;</span><br><span class="line">	test = test_3;	</span><br><span class="line">	tmp1 = test_3;</span><br><span class="line">	tmp2 = test_3;</span><br><span class="line">	tmp3 = test_3;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void volatile_test_3_write()</span><br><span class="line">&#123;</span><br><span class="line">	test_3 = 111;	</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对这段代码普通的情况下，编译成的汇编代码是：(使用x86-arm64 gcc交叉编译器)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	.arch armv8-a</span><br><span class="line">	.file	&quot;test.c&quot;</span><br><span class="line">	.comm	test,4,4</span><br><span class="line">	.comm	test_v,4,4</span><br><span class="line">	.comm	test_3,4,4</span><br><span class="line">	.comm	tmp1,4,4</span><br><span class="line">	.comm	tmp2,4,4</span><br><span class="line">	.comm	tmp3,4,4</span><br><span class="line">	.text</span><br><span class="line">	.align	2</span><br><span class="line">	.global	volatile_test_1</span><br><span class="line">	.type	volatile_test_1, %function</span><br><span class="line">volatile_test_1:</span><br><span class="line">	adrp	x0, test</span><br><span class="line">	add	x0, x0, :lo12:test</span><br><span class="line">	mov	w1, 111</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test</span><br><span class="line">	add	x0, x0, :lo12:test</span><br><span class="line">	mov	w1, 222</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test</span><br><span class="line">	add	x0, x0, :lo12:test</span><br><span class="line">	mov	w1, 333</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	nop</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_1, .-volatile_test_1</span><br><span class="line">	.align	2</span><br><span class="line">	.global	volatile_test_2</span><br><span class="line">	.type	volatile_test_2, %function</span><br><span class="line">volatile_test_2:</span><br><span class="line">	adrp	x0, test_v</span><br><span class="line">	add	x0, x0, :lo12:test_v</span><br><span class="line">	mov	w1, 111</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test_v</span><br><span class="line">	add	x0, x0, :lo12:test_v</span><br><span class="line">	mov	w1, 222</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test_v</span><br><span class="line">	add	x0, x0, :lo12:test_v</span><br><span class="line">	mov	w1, 333</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	nop</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_2, .-volatile_test_2</span><br><span class="line">	.align	2</span><br><span class="line">	.global	volatile_test_3_read</span><br><span class="line">	.type	volatile_test_3_read, %function</span><br><span class="line">volatile_test_3_read:</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	add	x0, x0, :lo12:test_3</span><br><span class="line">	ldr	w1, [x0]</span><br><span class="line">	adrp	x0, test</span><br><span class="line">	add	x0, x0, :lo12:test</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	add	x0, x0, :lo12:test_3</span><br><span class="line">	ldr	w1, [x0]</span><br><span class="line">	adrp	x0, tmp1</span><br><span class="line">	add	x0, x0, :lo12:tmp1</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	add	x0, x0, :lo12:test_3</span><br><span class="line">	ldr	w1, [x0]</span><br><span class="line">	adrp	x0, tmp2</span><br><span class="line">	add	x0, x0, :lo12:tmp2</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	add	x0, x0, :lo12:test_3</span><br><span class="line">	ldr	w1, [x0]</span><br><span class="line">	adrp	x0, tmp3</span><br><span class="line">	add	x0, x0, :lo12:tmp3</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	nop</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_3_read, .-volatile_test_3_read</span><br><span class="line">	.align	2</span><br><span class="line">	.global	volatile_test_3_write</span><br><span class="line">	.type	volatile_test_3_write, %function</span><br><span class="line">volatile_test_3_write:</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	add	x0, x0, :lo12:test_3</span><br><span class="line">	mov	w1, 111</span><br><span class="line">	str	w1, [x0]</span><br><span class="line">	nop</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_3_write, .-volatile_test_3_write</span><br><span class="line">	.ident	&quot;GCC: (Linaro GCC 6.3-2017.05) 6.3.1 20170404&quot;</span><br><span class="line">	.section	.note.GNU-stack,&quot;&quot;,@progbits</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>但是加上-O3的选项，叫编译器帮忙给我优化下编译结果，最后的代码是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	.arch armv8-a</span><br><span class="line">	.file	&quot;test.c&quot;</span><br><span class="line">	.text</span><br><span class="line">	.align	2</span><br><span class="line">	.p2align 3,,7</span><br><span class="line">	.global	volatile_test_1</span><br><span class="line">	.type	volatile_test_1, %function</span><br><span class="line">volatile_test_1:</span><br><span class="line">	adrp	x0, test</span><br><span class="line">	mov	w1, 333</span><br><span class="line">	str	w1, [x0, #:lo12:test]</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_1, .-volatile_test_1</span><br><span class="line">	.align	2</span><br><span class="line">	.p2align 3,,7</span><br><span class="line">	.global	volatile_test_2</span><br><span class="line">	.type	volatile_test_2, %function</span><br><span class="line">volatile_test_2:</span><br><span class="line">	adrp	x0, test_v</span><br><span class="line">	mov	w3, 111</span><br><span class="line">	mov	w2, 222</span><br><span class="line">	mov	w1, 333</span><br><span class="line">	str	w3, [x0, #:lo12:test_v]</span><br><span class="line">	str	w2, [x0, #:lo12:test_v]</span><br><span class="line">	str	w1, [x0, #:lo12:test_v]</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_2, .-volatile_test_2</span><br><span class="line">	.align	2</span><br><span class="line">	.p2align 3,,7</span><br><span class="line">	.global	volatile_test_3_read</span><br><span class="line">	.type	volatile_test_3_read, %function</span><br><span class="line">volatile_test_3_read:</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	adrp	x4, test</span><br><span class="line">	adrp	x3, tmp1</span><br><span class="line">	adrp	x2, tmp2</span><br><span class="line">	adrp	x1, tmp3</span><br><span class="line">	ldr	w0, [x0, #:lo12:test_3]</span><br><span class="line">	str	w0, [x4, #:lo12:test]</span><br><span class="line">	str	w0, [x3, #:lo12:tmp1]</span><br><span class="line">	str	w0, [x2, #:lo12:tmp2]</span><br><span class="line">	str	w0, [x1, #:lo12:tmp3]</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_3_read, .-volatile_test_3_read</span><br><span class="line">	.align	2</span><br><span class="line">	.p2align 3,,7</span><br><span class="line">	.global	volatile_test_3_write</span><br><span class="line">	.type	volatile_test_3_write, %function</span><br><span class="line">volatile_test_3_write:</span><br><span class="line">	adrp	x0, test_3</span><br><span class="line">	mov	w1, 111</span><br><span class="line">	str	w1, [x0, #:lo12:test_3]</span><br><span class="line">	ret</span><br><span class="line">	.size	volatile_test_3_write, .-volatile_test_3_write</span><br><span class="line">	.comm	tmp3,4,4</span><br><span class="line">	.comm	tmp2,4,4</span><br><span class="line">	.comm	tmp1,4,4</span><br><span class="line">	.comm	test_3,4,4</span><br><span class="line">	.comm	test_v,4,4</span><br><span class="line">	.comm	test,4,4</span><br><span class="line">	.ident	&quot;GCC: (Linaro GCC 6.3-2017.05) 6.3.1 20170404&quot;</span><br><span class="line">	.section	.note.GNU-stack,&quot;&quot;,@progbits</span><br></pre></td></tr></table></figure>

<p>可以看出普通编译的时候都是没有问题的。问题出现在编译器做优化的时候:</p>
<p>test_1多次写一个变量的时候, 编译器认为，只要最后一次写就可以了。如果，test1对应<br>的是一个IO寄存器，或者test_1的值，会被另一个执行流程识别，这里就会出问题。解决<br>办法就是给test_1加上volatile，告诉编译器每次写操作都不能被编译器优化掉。</p>
<p>test_3多次读一个变量的时候，编译器后两次的读都没有去正真读内存，而是使用了第一次<br>缓存在寄存器里的值。如果，test_3对应的是一个IO寄存器，或者如代码里一样test_3在<br>另一个执行流里被修改了(volatile_test_3_write), 但是read操作还是读的寄存器里的值，<br>这样就会出问题。要修改也是加volatile。</p>
<p>在上面的第一里提到，写内存的时候不是马上生效的, 这里也和cache没有啥关系，在硬件<br>保证cache一致性的系统里，对于单个操作，可以认为是写内存就马上生效的。这里说的<br>不生效是指CPU的多个写操作写下去的写完成的先后是不保证的。要想保证一个写操作的先后<br>顺序，就需要使用内存屏障指令。这个不是本问讨论的问题, 另外再说。这里只是说明，<br>volatile只是告诉编译器不做额外优化，它是不能保证写生效顺序的。</p>
]]></content>
      <tags>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>GIC ITS学习笔记(一)</title>
    <url>/2021/07/11/GIC-ITS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/</url>
    <content><![CDATA[<h2 id="current-GIC-ITS-code-in-v4-2-v4-3"><a href="#current-GIC-ITS-code-in-v4-2-v4-3" class="headerlink" title="current GIC ITS code in v4.2/v4.3"></a>current GIC ITS code in v4.2/v4.3</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* this is the arch of interrupt sub-system */</span><br><span class="line">struct irq_desc</span><br><span class="line">    --&gt; struct irq_data</span><br><span class="line">        --&gt; struct irq_chip</span><br><span class="line">        --&gt; struct irq_domain</span><br><span class="line">            --&gt; struct irq_domain_ops</span><br><span class="line"></span><br><span class="line">/* this is the arch of ITS sub-system, for v4.1, for v4.2, kernel changed a lot */</span><br><span class="line">struct its_node</span><br><span class="line">    --&gt; struct irq_domain</span><br><span class="line">        --&gt; struct irq_domain_ops (its_domain_ops)</span><br><span class="line">	        /* alloc put struct irq_chip (its_irq_chip) to related</span><br><span class="line">		 * struct irq_data</span><br><span class="line">		 *</span><br><span class="line">		 * Fix me for other work alloc does</span><br><span class="line">		 */</span><br><span class="line">	    --&gt; .alloc (its_irq_domain_alloc)</span><br><span class="line">	    --&gt; .activate (its_irq_domain_activate)</span><br><span class="line">	    --&gt; ...</span><br><span class="line">    --&gt; struct msi_controller</span><br><span class="line">            /* father irq_domain is irq_domain above */</span><br><span class="line">        --&gt; struct irq_domain</span><br><span class="line">	        /* msi_domain_ops defined in kernel/irq/msi.c */</span><br><span class="line">	    --&gt; struct irq_domain_ops (struct irq_domain_ops msi_domain_ops)</span><br><span class="line">	    --&gt; void *host_data (struct msi_domain_info its_pci_msi_domain_info)</span><br><span class="line">                --&gt; struct msi_domain_ops (its_pci_msi_ops)</span><br><span class="line">                    --&gt; .msi_prepare (its_msi_prepare)</span><br><span class="line">                    --&gt; ...</span><br><span class="line">                --&gt; struct irq_chip (its_msi_irq_chip)</span><br></pre></td></tr></table></figure>
<p>/* this is the arch of ITS sub-system for v4.2, ITS driver changed a lot in v4.2 */<br>there is no irq_domain in struct its_node. In drivers/irqchip/irq-gic-v3-its.c,<br>just build up below irq_domain.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gic irq_domain --&gt; irq_domain_ops(gic_irq_domain_ops)</span><br><span class="line">      ^                --&gt; .alloc(gic_irq_domain_alloc)</span><br><span class="line">      |</span><br><span class="line">its irq_domain --&gt; irq_domain_ops(its_domain_ops)</span><br><span class="line">      ^                --&gt; .alloc(its_irq_domain_alloc)</span><br><span class="line">      |                --&gt; ...</span><br><span class="line">      |        --&gt; host_data(struct msi_domain_info)</span><br><span class="line">      |            --&gt; msi_domain_ops(its_msi_domain_ops)</span><br><span class="line">      |                --&gt; .msi_prepare(its_msi_prepare)</span><br><span class="line">      |            --&gt; irq_chip, chip_data, handler...</span><br><span class="line">      |            --&gt; void *data(struct its_node)</span><br></pre></td></tr></table></figure>
<p>In drvers/irqchip/irq-gic-v3-its-pci-msi.c,<br>   drvers/irqchip/irq-gic-v3-its-platform-msi.c, it seems that we create two<br>other irq_domain:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_its irq_domain                      platform_its irq_domain</span><br><span class="line">        /* kernel/irq/msi.c */                  /* kernel/irq/msi.c */</span><br><span class="line">    --&gt; irq_domain_ops(msi_domain_ops)      --&gt; irq_domain_ops(msi_domain_op)</span><br><span class="line">        /* irq-gic-v3-its-pci-msi.c             /* irq-gic-v3-its-platform-msi.c</span><br><span class="line">	 * (its_pci_msi_domain_info)             * (its_pmsi_domain_info)</span><br><span class="line">	 */                                      */</span><br><span class="line">    --&gt; void *host_data                     --&gt; void *host_data</span><br><span class="line">        --&gt; .ops(its_pci_msi_ops)               --&gt; .ops(its_pmsi_ops)</span><br><span class="line">	        /* its_pci_msi_prepare */   	    /* its_pmsi_prepare */</span><br><span class="line">	    --&gt; .msi_prepare                    --&gt; .msi_prepare</span><br><span class="line">	--&gt; .chip(its_msi_irq_chip)             --&gt; .chip(its_pmsi_irq_chip)</span><br></pre></td></tr></table></figure>
<h2 id="msi-domain-struct"><a href="#msi-domain-struct" class="headerlink" title="msi domain struct"></a>msi domain struct</h2><p>basic struct:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct msi_domain_info</span><br><span class="line">    --&gt; struct msi_domain_ops</span><br><span class="line">        --&gt; .msi_prepare</span><br><span class="line">    --&gt; struct irq_chip</span><br><span class="line">        --&gt; .irq_write_msi_msg</span><br></pre></td></tr></table></figure>
<p>msi related struct should be part of interrupt sub-system as showed in part 1.<br>struct msi_domain_info will be stored in void *host_data of a irq_domain.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* will find below funtion in drvers/irqchip/irq-gic-v3-its-pci-msi.c */</span><br><span class="line">pci_msi_create_irq_domain(device_node, msi_domain_info, parent)</span><br><span class="line">        /* core function */</span><br><span class="line">    --&gt; msi_create_irq_domain(node, info, parent);</span><br><span class="line">            /* msi_domain_ops is irq_domain_ops in kernel/irq/msi.c</span><br><span class="line">             * info below will be stored in host_data of irq_domain</span><br><span class="line">             *</span><br><span class="line">             * both pci_its irq_domain and platform_its irq_domain use</span><br><span class="line">             * same msi_domain_ops, but different msi_domain_info.</span><br><span class="line">             * above msi_domain_ops is a struct irq_domain_ops.</span><br><span class="line">             */</span><br><span class="line">        --&gt; irq_domain_add_hierarchy(parent, 0, 0, node, &amp;msi_domain_ops, info)</span><br></pre></td></tr></table></figure>
<h2 id="pci-msi-struct"><a href="#pci-msi-struct" class="headerlink" title="pci msi struct "></a>pci msi struct </h2><p>in part 2, it creats related domain, then we will see how to use callbacks<br>in above domain. This part shows how to allocate irqs in a msi domain.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* this is the work flow of PCI MSI */</span><br><span class="line">/* kernel/drivers/pci/msi.c */</span><br><span class="line">pci_msi_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)</span><br><span class="line">        /* so this irq_domain is pci_its irq_domain ? */</span><br><span class="line">    --&gt; pci_msi_domain_alloc_irqs(domain, dev, nvec, type);</span><br><span class="line">        --&gt; msi_domain_alloc_irqs(domain, &amp;dev-&gt;dev, nvec);</span><br><span class="line">	        /* should be its_pci_msi_prepare ?</span><br><span class="line">                 * if below function, first get dev_id, then call parent domain</span><br><span class="line">                 * msi_prepare which is its domain msi_prepare and will build</span><br><span class="line">                 * up device table of ITS.</span><br><span class="line">                 */</span><br><span class="line">	    --&gt; ps-&gt;msi_prepare(domain, dev, nvec, &amp;arg);</span><br><span class="line">	       /* domain, virq, desc-&gt;nvec_used, dev_to_node(dev), &amp;arg, false */</span><br><span class="line">	    --&gt; __irq_domain_alloc_irqs()</span><br><span class="line"></span><br><span class="line">/* details of how ITS work, this prepare function just allocat device table and</span><br><span class="line"> * related structure in ITS</span><br><span class="line"> */</span><br><span class="line">/* in pci_msi irq_domain */</span><br><span class="line">its_pci_msi_prepare</span><br><span class="line">    --&gt; ... (get dev_ip)</span><br><span class="line">        /* in its irq_domain */</span><br><span class="line">    --&gt; its_msi_prepare</span><br><span class="line">        --&gt; its_create_device(its, dev_id, nvec);</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* there is an important point: how to configure MSI capability registers of one</span><br><span class="line"> * PCI device, this action takes place in irq_write_msi_msg mentioned in part 2&#x27;s</span><br><span class="line"> * &quot;basic struct&quot;. how to call this function ? obviouly, this function is stored</span><br><span class="line"> * in msi_domain_info, so we must use some msi layer function to access this</span><br><span class="line"> * function to interrup sub-system, in fact, it uses msi_domain_ops to do this.</span><br><span class="line"> */</span><br><span class="line">/* it stored irq_chip in msi_domain_info to interrupt sub-system in </span><br><span class="line"> * pci_msi_create_irq_domain, details as below:</span><br><span class="line"> */</span><br><span class="line">pci_msi_create_irq_domain</span><br><span class="line">    --&gt; msi_create_irq_domain</span><br><span class="line">        --&gt;  msi_domain_update_dom_ops</span><br><span class="line">                 /* (struct msi_domain_ops msi_domain_ops_default --&gt;</span><br><span class="line">                  * msi_domain_ops_init</span><br><span class="line">                  */</span><br><span class="line">             --&gt; ops-&gt;msi_init = msi_domain_ops_default.msi_init;</span><br><span class="line">                     /* para: domain, virq, hwirq, info-&gt;chip, info-&gt;chip_data */</span><br><span class="line">                 --&gt; irq_domain_set_hwirq_and_chip</span><br></pre></td></tr></table></figure>
<p>in function request_irq, it will call .active in irq_domain struct, here is will<br>call function msi_domain_activate in struct irq_domain_ops msi_domain_ops.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">request_irq</span><br><span class="line">    ...</span><br><span class="line">    --&gt; msi_domain_activate</span><br><span class="line">        --&gt; irq_chip_write_msi_msg</span><br><span class="line">                /* here is irq_write_msi_msg in struct msi_domain_info */</span><br><span class="line">            --&gt; data-&gt;chip-&gt;irq_write_msi_msg(data, msg);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>GIC</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言温故而知新</title>
    <url>/2021/06/21/C%E8%AF%AD%E8%A8%80%E6%B8%A9%E6%95%85%E8%80%8C%E7%9F%A5%E6%96%B0/</url>
    <content><![CDATA[<ol>
<li>宏</li>
</ol>
<hr>
<h2 id="用来拼接前后的符号"><a href="#用来拼接前后的符号" class="headerlink" title="用来拼接前后的符号"></a>用来拼接前后的符号</h2><h1 id="用来把一个符号字符串化"><a href="#用来把一个符号字符串化" class="headerlink" title="用来把一个符号字符串化"></a>用来把一个符号字符串化</h1><p>  宏参数还是一个宏的展开规则是，如果宏是一个简单的宏就先展开里面的宏，再依次<br>  展开外面的宏。</p>
<p>  如果宏的实现是一个使用##或者#的宏，那么直接使用##或者#的规则。</p>
<p>  如果要使得比如下面test case 4的到“19”的输出，需要中间转下，类似case 5的写法。<br>  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">#define TEST(a, b) a##b</span><br><span class="line">#define STR(a) #a </span><br><span class="line">#define A 1000</span><br><span class="line">#define ADD(a, b) ((a) + (b))</span><br><span class="line">#define _TEST(a, b) TEST(a, b)</span><br><span class="line"></span><br><span class="line">#define _STR(a) STR(a)</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	printf(&quot;test 1: %d\n&quot;, TEST(1, 5));</span><br><span class="line">	printf(&quot;test 2: %s\n&quot;, STR(just_test));</span><br><span class="line">	printf(&quot;test 3: %d\n&quot;, ADD(A, A));</span><br><span class="line"></span><br><span class="line">	/* test case 4 */</span><br><span class="line">	printf(&quot;test 4: %s\n&quot;, STR(TEST(1, 9)));</span><br><span class="line"></span><br><span class="line">	/* test case 5 */</span><br><span class="line">	printf(&quot;test 5: %s\n&quot;, _STR(TEST(1, 9)));</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">xxx@kllp05:~/tests/c_note$ ./a.out </span><br><span class="line">test 1: 15</span><br><span class="line">test 2: just_test</span><br><span class="line">test 3: 2000</span><br><span class="line">test 4: TEST(1, 9)</span><br><span class="line">test 5: 19</span><br></pre></td></tr></table></figure><br>  <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vYWxhbnR1MjAxOC9wLzg0NjU5MTEuaHRtbA==">https://www.cnblogs.com/alantu2018/p/8465911.html<i class="fa fa-external-link-alt"></i></span></p>
<ol start="2">
<li>自动数据类型转换</li>
</ol>
<hr>
<p>  各种截断数据</p>
<p>  float类型不能比较相等，不相等</p>
<p>  sizeof() 返回一个unsigned int</p>
<p>  有符号数和无符号数运算、赋值: <span class="exturl" data-url="aHR0cHM6Ly9ha2FlZHUuZ2l0aHViLmlvL2Jvb2svY2gxNXMwMy5odG1s">https://akaedu.github.io/book/ch15s03.html<i class="fa fa-external-link-alt"></i></span></p>
<p>  有符号右移，带符号位右移</p>
<ol start="3">
<li>大小端和位域</li>
</ol>
<hr>
<p>  <span class="exturl" data-url="aHR0cDovL21qZnJhemVyLm9yZy9tamZyYXplci9iaXRmaWVsZHMv">http://mjfrazer.org/mjfrazer/bitfields/<i class="fa fa-external-link-alt"></i></span></p>
<p>  这个实例说明为什么写代码的时候，特别是涉及底层硬件寄存器操作的驱动代码，尽量<br>  不要用位域。原因是在大小端不同的系统上，位域的表示方法是不同的。</p>
<ol start="4">
<li>格式化打印</li>
</ol>
<hr>
<p>  printf %n: 已输出的字符数目(用户对齐下一行的输出比较有用 :))</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	int num;</span><br><span class="line"></span><br><span class="line">	printf(&quot;123456:%n\n&quot;, &amp;num);</span><br><span class="line">	printf(&quot;%*c %d\n&quot;, num, &#x27; &#x27;, 123);</span><br><span class="line">	printf(&quot;%*c %d\n&quot;, num, &#x27; &#x27;, 456);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">xxx@kllp05:~/tests/c_note$ ./a.out </span><br><span class="line">123456:</span><br><span class="line">        123</span><br><span class="line">        456</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>长跳转</li>
</ol>
<hr>
<p>  setjmp, longjmp: <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vaGF6aXIvcC9jX3NldGptcF9sb25nam1wLmh0bWw=">https://www.cnblogs.com/hazir/p/c_setjmp_longjmp.html<i class="fa fa-external-link-alt"></i></span></p>
<ol start="6">
<li>可变参数函数</li>
</ol>
<hr>
<p>  <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vY3BvaW50L3AvMzM2ODk5My5odG1s">https://www.cnblogs.com/cpoint/p/3368993.html<i class="fa fa-external-link-alt"></i></span></p>
<ol start="7">
<li>realloc</li>
</ol>
<hr>
<p>  <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vemhhb3lsL3AvMzk1NDIzMi5odG1s">https://www.cnblogs.com/zhaoyl/p/3954232.html<i class="fa fa-external-link-alt"></i></span></p>
<ol start="8">
<li>二维数组和指针</li>
</ol>
<hr>
<ol start="9">
<li>const指针</li>
</ol>
<hr>
<p>  const char ** p; char * const * p; char ** const p;<br>  分别是**p, *p, p是常量。</p>
<ol start="10">
<li>double类型的存储格式</li>
</ol>
<hr>
<p>其他：<br>  c中简单结构体直接复制是ok的<br>  int fun(int a[10])的入参和指针一样</p>
]]></content>
      <tags>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>Guest and host communication for QEMU</title>
    <url>/2021/06/28/Guest-and-host-communication-for-QEMU/</url>
    <content><![CDATA[<p>We can use 9p fs to do this, qemu cmdline like  below:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 -machine virt -cpu cortex-a57 \</span><br><span class="line">-m 2G \</span><br><span class="line">-kernel ~/repos/kernel-dev/arch/arm64/boot/Image \</span><br><span class="line">-initrd ~/repos/buildroot/output/images/rootfs.cpio.gz \</span><br><span class="line">-append &quot;console=ttyAMA0 root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">-fsdev local,id=p9fs,path=p9root,security_model=mapped \</span><br><span class="line">-device virtio-9p-pci,fsdev=p9fs,mount_tag=p9 \</span><br><span class="line">-nographic \</span><br></pre></td></tr></table></figure>
<p>Here path=p9root is the directory which we can see in host and guest.<br>We can use path=/home/your_account/p9root for example also, but it should be a<br>full path.</p>
<p>Then we can start qemu, and in qemu do:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mount -t 9p p9 /mnt</span><br></pre></td></tr></table></figure>

<p>Then you can see the files in host p9root directory in guest /mnt.</p>
<p>Then we can also debug the kernel running in qemu by gdb. We should add<br>“-gdb tcp::1234” to start a gdb server in qemu and wait on local port 1234 of tcp.<br>Whole qemu cmdline is like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 -machine virt -cpu cortex-a57 \</span><br><span class="line">-m 2G \</span><br><span class="line">-kernel ~/repos/kernel-dev/arch/arm64/boot/Image \</span><br><span class="line">-initrd ~/repos/buildroot/output/images/rootfs.cpio.gz \</span><br><span class="line">-append &quot;console=ttyAMA0 root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">-fsdev local,id=p9fs,path=p9root,security_model=mapped \</span><br><span class="line">-device virtio-9p-pci,fsdev=p9fs,mount_tag=p9 \</span><br><span class="line">-nographic \</span><br><span class="line">-gdb tcp::1234</span><br></pre></td></tr></table></figure>
<p>After kernel in qemu boots up, you can start a gdb cline in host and connect to<br>the gdb server in qemu.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aarch64-linux-gnu-gdb</span><br><span class="line">(gdb) file ~/repos/kernel-dev/vmlinux</span><br><span class="line">(gdb) target remote :1234</span><br></pre></td></tr></table></figure>
<p>Here we use a arm64 based gdb as an example. After “target remote :1234”, we<br>can use gdb to debug kernel in qemu. You can just set break points, print value<br>of variables…</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>How to assign more than 31 VFs to one VM</title>
    <url>/2021/07/11/How-to-assign-more-than-31-VFs-to-one-VM/</url>
    <content><![CDATA[<p>用QEMU模拟PCIe设备的时候，一般最多可以在系统中配置31个PCIe设备。比如，我们有这样<br>的QEMU启动参数配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 \</span><br><span class="line">-machine virt,gic-version=3 \</span><br><span class="line">-enable-kvm \</span><br><span class="line">-cpu host \</span><br><span class="line">-m 1024 \</span><br><span class="line">-kernel ./Image \</span><br><span class="line">-initrd ./minifs.cpio.gz \</span><br><span class="line">-nographic \</span><br><span class="line">-net none \</span><br><span class="line">-device vfio-pci,host=0002:81:10.0,id=net0 \</span><br><span class="line">-device vfio-pci,host=0002:81:20.0,id=net1 \</span><br></pre></td></tr></table></figure>
<p>上面的配置中，我们使用了host上的两个82599网卡的vf: 0002:81:10.0, 0002:81:20.0,<br>把它们直通到了guest上，可以看到这两个vf在guest上会直接连到pci bus 0上。按照同样<br>的方法，我们可以给一个虚拟机继续增加网口。但是这样的方式增加最多只能到31个vf[1]</p>
<p>这是因为PCIe中规定一条总线下最多只能接32个设备。</p>
<p>为了在一个虚拟机上接入更多的设备，我们可以接入一个PCIe switch，在switch的下游<br>端口上再接需要的PCIe设备。比如，我有可以如下启动一个QEMU虚拟机：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 \</span><br><span class="line">-machine virt,gic-version=3 \</span><br><span class="line">-enable-kvm \</span><br><span class="line">-cpu host \</span><br><span class="line">-m 1024 \</span><br><span class="line">-kernel ./Image \</span><br><span class="line">-initrd ./minifs.cpio.gz \</span><br><span class="line">-nographic \</span><br><span class="line">-net none \</span><br><span class="line">-device ioh3420,id=root_port1 \</span><br><span class="line">-device x3130-upstream,id=upstream_port1,bus=root_port1 \</span><br><span class="line">-device xio3130-downstream,id=downstream_port1,bus=upstream_port1,chassis=1,slot=1 \</span><br><span class="line">-device xio3130-downstream,id=downstream_port2,bus=upstream_port1,chassis=1,slot=2 \</span><br><span class="line">-device xio3130-downstream,id=downstream_port3,bus=upstream_port1,chassis=1,slot=3 \</span><br><span class="line">-device xio3130-downstream,id=downstream_port4,bus=upstream_port1,chassis=1,slot=4 \</span><br><span class="line">-device xio3130-downstream,id=downstream_port5,bus=upstream_port1,chassis=1,slot=5 \</span><br><span class="line">-device vfio-pci,host=0002:81:10.0,id=net0,bus=downstream_port1 \</span><br><span class="line">-append &#x27;console=ttyAMA0 root=/dev/vda2&#x27; \</span><br><span class="line">-nographic \</span><br></pre></td></tr></table></figure>
<p>上面的命令行参数可以搭建一个如下图所示的PCIe switch</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pcie.0 bus</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line">                             |</span><br><span class="line">                       -------------</span><br><span class="line">                       | Root Port1|</span><br><span class="line">                       -------------</span><br><span class="line">    -------------------------|-------------------------------------------</span><br><span class="line">    |                 -----------------------------------------         |</span><br><span class="line">    |    PCI Express  | Upstream Port1                        |         |</span><br><span class="line">    |      Switch     -----------------------------------------         |</span><br><span class="line">    |                  |            |                                   |</span><br><span class="line">    |    -------------------    -------------------                     |</span><br><span class="line">    |    | Downstream Port1|    | Downstream Port2|       ....          |</span><br><span class="line">    |    -------------------    -------------------                     |</span><br><span class="line">    -------------|-----------------------|-------------------------------</span><br><span class="line">           ------------                                                 </span><br><span class="line">           | PCIe Dev | vfio-pci device</span><br><span class="line">           ------------</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>[1] Fix me: 为什么是31个不是32个?<br>[2] qemu/docs/pcie.txt</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>How to do UT test in software</title>
    <url>/2021/07/05/How-to-do-UT-test-in-software/</url>
    <content><![CDATA[<p>The idea of UT is to test the logic of your codes match with your plan, it is<br>in function level.</p>
<p>Let’s see an example function:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int tested_function(a, b)</span><br><span class="line">&#123;</span><br><span class="line">	c = function(a, b);</span><br><span class="line">	return c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Above example is very simple. We can see this tested_function inside is combined<br>logic. Every time you set inputs, e.g. a, b here, you get a output c. So the UT<br>test of this function is to find proper input combinations to test if return<br>value is expected.</p>
<p>However, we have another kind of function which likes timed-based logic. Even<br>“input” is same, but it can return different value. The reason of this is<br>because there is a memory inside of this kind of function. See blow example:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int tested_function(a, b)</span><br><span class="line">&#123;</span><br><span class="line">	void *c = system_api();</span><br><span class="line">	function1(global_d);</span><br><span class="line">	function2(global_e);</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Everytime you call system_api, its return value is different, as there are some<br>memory in this system_api. Everytime you use global_a, global_b, their values<br>are different, as the scope of these value is outside of this function, so from<br>the view of this tested_function, global_d/global_e is memory part.</p>
<p>So for this kind of time-based logic function. Its inputs are a, b, c, global_d,<br>global_e. So the UT test is to prepare proper inputs combinations and test if<br>return value are expected.</p>
<p>So we should control system_api to create expected c to test our logic in<br>tested_function. So we create a system_api() ourself to replace real system_api(),<br>it will be like below, which is called a stub function of system_api().</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void *system_api()</span><br><span class="line">&#123;</span><br><span class="line">	switch (control_system_api) &#123;</span><br><span class="line">	case CONTROL_1:</span><br><span class="line">		return c_1;</span><br><span class="line">	case CONTROL_2:</span><br><span class="line">		return c_2;</span><br><span class="line">	case CONTROL_3:</span><br><span class="line">		return c_3;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>An ut test for a C file, all outside functions should have related stub functions.<br>Stub functions are the preparations for after ut test.</p>
<p>We should write ut test case to do ut test: first setting inputs, remember if<br>your tested function is a time-base logic function, you should set “input points”<br>for all inputs and memory inputs, e.g. a, b, c, global_d, global_e; second, run<br>tested function; third, check return value comparing with expected ones. A test<br>case will be like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void prepare_input()</span><br><span class="line">&#123;</span><br><span class="line">	control_system_api = 1;</span><br><span class="line">	global_d = 123;</span><br><span class="line">	global_e = 456;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void check_result()</span><br><span class="line">&#123;</span><br><span class="line">	assert(c == 789);</span><br><span class="line">	log();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int test_case_1()</span><br><span class="line">&#123;</span><br><span class="line">	prepare_input();</span><br><span class="line">	c = tested_function(a, b);</span><br><span class="line">	check_result(c);</span><br><span class="line">	clear_input()；</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件测试</tag>
      </tags>
  </entry>
  <entry>
    <title>Hixxxx PCIe + SMMU bad performance debug</title>
    <url>/2021/07/11/Hixxxx-PCIe-SMMU-bad-performance-debug/</url>
    <content><![CDATA[<p>When we enabled SMMU in Dxx board, we found that the performance of 82599 pluged<br>in PCIe slot is very bad. LeiZhen and I spent some to debug this problem. This<br>document just shares the related results and information.</p>
<ol>
<li>test scenarios and results</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     +----------------+        +---------------+</span><br><span class="line">     |   Dxx   82599  |&lt;------&gt;|  82599   Dxx  |</span><br><span class="line">     +----------------+        +---------------+</span><br><span class="line"></span><br><span class="line">           +-----+         +-----+</span><br><span class="line">           | cpu |         | ddr |</span><br><span class="line">           +--+--+         +--+--+</span><br><span class="line">              |               |</span><br><span class="line">      --------+-----+---------+------- bus</span><br><span class="line">                    |</span><br><span class="line">                 +--+---+</span><br><span class="line">                 | smmu |</span><br><span class="line">                 +--+---+</span><br><span class="line">                    |                   </span><br><span class="line">                 +--+---+</span><br><span class="line">                 |  rp  |</span><br><span class="line">                 +--+---+</span><br><span class="line">                    |</span><br><span class="line">                 +--+---+</span><br><span class="line">                 | 82599|</span><br><span class="line">                 +------+</span><br><span class="line">```           </span><br><span class="line">Hardware topology as showed above. In order to use SMMU to translate data from</span><br><span class="line">82599 to DDR, we need enable SMMU node in ACPI table.[1]</span><br><span class="line"></span><br><span class="line">Then boot up two Dxx boards connected by two 82599 networking cards. When using</span><br><span class="line">iperf to test the performance between two 82599 networking cards, it is very bad,</span><br><span class="line">nearly 100Mbps.[2]</span><br><span class="line"></span><br><span class="line">2. analysis</span><br><span class="line">-----------</span><br><span class="line"></span><br><span class="line">The only difference is disable SMMU and enable SMMU. So the difference is we use</span><br><span class="line">diffferent DMA callbacks.</span><br><span class="line"></span><br><span class="line">We can see arch/arm64/mm/dma-mapping.c, when configuring CONFIG_IOMMU_DMA,</span><br><span class="line">callbacks in struct dma_map_ops iommu_dma_ops will be used.</span><br><span class="line"></span><br><span class="line">And we also know when 82599 sending/receiving packages, its driver will call</span><br><span class="line">ixgbe_tx_map/ixgbe_alloc_mapped_page to allocate DMA memory function which will</span><br><span class="line">finally call map_page in iommu_dma_ops when SMMU is enable. So we guess there</span><br><span class="line">is something wrong with the map_page here.</span><br><span class="line"></span><br><span class="line">So we should analyze related function to find the hot point. Here we firstly use</span><br><span class="line">ftrace to confirm our idea, then use perf to locate the hot point explicitly.</span><br><span class="line"></span><br><span class="line"> * ftrace</span><br><span class="line"></span><br><span class="line">   We can use function profiling in ftrace to see durations of related function.</span><br><span class="line">   please refer[3] to know how to use ftrace function profiling.</span><br><span class="line"></span><br><span class="line">   Here we get:</span><br></pre></td></tr></table></figure>
<p>10)&lt;…&gt;-4313   |               |   ixgbe_xmit_frame_ring() {<br>10)&lt;…&gt;-4313   |               |     __iommu_map_page() {<br>10)&lt;…&gt;-4313   |   0.080 us    |       dma_direction_to_prot();<br>10)&lt;…&gt;-4313   |               |       iommu_dma_map_page() {<br>10)&lt;…&gt;-4313   |               |         __iommu_dma_map() {<br>10)&lt;…&gt;-4313   |   0.480 us    |           iommu_get_domain_for_dev();<br>10)&lt;…&gt;-4313   |               |           __alloc_iova() {<br>10)&lt;…&gt;-4313   |               |             alloc_iova() {<br>10)&lt;…&gt;-4313   |               |               kmem_cache_alloc() {<br>10)&lt;…&gt;-4313   |               |                 __slab_alloc.isra.21()<br>10)&lt;…&gt;-4313   |   0.040 us    |                 memcg_kmem_put_cache();<br>10)&lt;…&gt;-4313   | + 16.160 us   |               }<br>[…]<br>10)&lt;…&gt;-4313   |   0.120 us    |               _raw_spin_lock_irqsave();<br>10)&lt;…&gt;-4313   |               |               _raw_spin_unlock_irqrestore() {<br>10)&lt;…&gt;-4313   |   ==========&gt; |<br>10)&lt;…&gt;-4313   |               |                 gic_handle_irq()<br>10)&lt;…&gt;-4313   |   &lt;========== |<br>10)&lt;…&gt;-4313   | + 88.620 us   |               }<br>10)&lt;…&gt;-4313   | ! 679.760 us  |             }<br>10)&lt;…&gt;-4313   | ! 680.480 us  |           }</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Most time has been spent in alloc_iova.</span><br><span class="line"></span><br><span class="line">* perf</span><br><span class="line"></span><br><span class="line">  Sadly, there was no perf(PMU hardware event) support in ACPI in plinth</span><br><span class="line">  kernel :( So we directly set PMU related registers to get how many CPU cycles</span><br><span class="line">  each function has spent.</span><br></pre></td></tr></table></figure>
<pre><code>/* Firstly call this init function to init PMU */
static void pm_cycle_init(void)
&#123;
    u64 val;

    asm volatile(&quot;mrs %0, pmccfiltr_el0&quot; : &quot;=r&quot; (val));
    if (val &amp; ((u64)1 &lt;&lt; 31)) &#123;
        val &amp;= ~((u64)1 &lt;&lt; 31);
        asm volatile(&quot;msr pmccfiltr_el0, %0&quot; :: &quot;r&quot; (val));
        dsb(sy);
        asm volatile(&quot;mrs %0, pmccfiltr_el0&quot; : &quot;=r&quot; (val));
    &#125;

    asm volatile(&quot;mrs %0, pmcntenset_el0&quot; : &quot;=r&quot; (val));
    if (!(val &amp; ((u64)1 &lt;&lt; 31))) &#123;
        val |= ((u64)1 &lt;&lt; 31);
        asm volatile(&quot;msr pmcntenset_el0, %0&quot; :: &quot;r&quot; (val));
        dsb(sy);
        asm volatile(&quot;mrs %0, pmcntenset_el0&quot; : &quot;=r&quot; (val));
    &#125;

    asm volatile(&quot;mrs %0, pmcr_el0&quot; : &quot;=r&quot; (val));
    if (!(val &amp; ((u64)1 &lt;&lt; 6))) &#123;
        val |= ((u64)1 &lt;&lt; 6) | 0x1;
        asm volatile(&quot;msr pmcr_el0, %0&quot; :: &quot;r&quot; (val));
        dsb(sy);
        asm volatile(&quot;mrs %0, pmcr_el0&quot; : &quot;=r&quot; (val));
    &#125;
&#125;

/* Get the CPU cycles in PMU counter */
u64 pm_cycle_get(void)
&#123;
    u64 val;

    asm volatile(&quot;mrs %0, pmccntr_el0&quot; : &quot;=r&quot; (val));

    return val;
&#125;
EXPORT_SYMBOL(pm_cycle_get);
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Using above debug functions, we found almost 600000 CPU cycles will happen in</span><br><span class="line">  a while loop in function __alloc_and_insert_iova_range. If CPU frequency is</span><br><span class="line">  2G Hz, then 600000 CPU cycles is 300us! This is the hot point.</span><br><span class="line"></span><br><span class="line">  We found it will loop almost 10000 times in above while loop!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">* Code analysis </span><br><span class="line"></span><br><span class="line">  Firstly, this DMA software modules is like this:</span><br></pre></td></tr></table></figure>
<p>   VA = kmalloc();<br>   IOVA = dma_map_function(PA = fun(VA)); </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   Firstly, allocate memory for DMA memory and map a VA which can be used by CPU,</span><br><span class="line">   Then, build map between IOVA and PA in dma map function. In the case of SMMU</span><br><span class="line">   enable, .map_page(__iommu_map_page) in iommu_dma_ops will be call to build</span><br><span class="line">   above map.</span><br><span class="line"></span><br><span class="line">   Then common function iommu_dma_map_page in drivers/iommu/dma-iommu.c will be</span><br><span class="line">   called. There are two steps in above function: 1. allocate iova, this is a</span><br><span class="line">   common function; 2. build map between IOVA and PA, this is SMMU specific</span><br><span class="line">   function.   </span><br><span class="line"></span><br><span class="line">   The hot point is in the point 1 above, so we need understand the module of</span><br><span class="line">   how to allocate iova. A red black tree in iova_domain is used to store all iova</span><br><span class="line">   range in system, after allocating or freeing an iova range, an iova range</span><br><span class="line">   should be inserted or remove from above red black tree. Now we allocate the</span><br><span class="line">   iova range from the end of the iova domain, for 32 DMA mask, it is 0xffffffff,</span><br><span class="line">   for 64bit DMA mask it is 0xffffffff_ffffffff. There is a cache for 32bit DMA</span><br><span class="line">   mask to store the iova range in last time, but for 64bit DMA MASK, there is</span><br><span class="line">   no such cache. so for 64bit DMA, when we want to allocate a DMA range in</span><br><span class="line">   iova domain, we have to search from the 0xffffffff_ffffffff. If we already allocate</span><br><span class="line">   a lot iova range, then we have to search all iova range allocated before.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3. solution</span><br><span class="line">-----------</span><br><span class="line"></span><br><span class="line">So we can fix this bug like this:</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>diff –git a/drivers/iommu/iova.c b/drivers/iommu/iova.c<br>index 080beca..1e582d8 100644<br>— a/drivers/iommu/iova.c<br>+++ b/drivers/iommu/iova.c<br>@@ -46,6 +46,7 @@ init_iova_domain(struct iova_domain *iovad, unsigned long granule,<br>     spin_lock_init(&amp;iovad-&gt;iova_rbtree_lock);<br>     iovad-&gt;rbroot = RB_ROOT;<br>     iovad-&gt;cached32_node = NULL;</p>
<ul>
<li> iovad-&gt;cached64_node = NULL;<br>   iovad-&gt;granule = granule;<br>   iovad-&gt;start_pfn = start_pfn;<br>   iovad-&gt;dma_32bit_pfn = pfn_32bit;<br>@@ -56,13 +57,19 @@ EXPORT_SYMBOL_GPL(init_iova_domain);<br>static struct rb_node *<br>__get_cached_rbnode(struct iova_domain *iovad, unsigned long *limit_pfn)<br>{</li>
</ul>
<ul>
<li>   if ((*limit_pfn &gt; iovad-&gt;dma_32bit_pfn) ||</li>
<li><pre><code>   (iovad-&gt;cached32_node == NULL))
</code></pre>
</li>
</ul>
<ul>
<li>   struct rb_node *cached_node;</li>
<li></li>
<li>   if (*limit_pfn &lt; iovad-&gt;dma_32bit_pfn)</li>
<li><pre><code>   cached_node = iovad-&gt;cached32_node;
</code></pre>
</li>
<li>   else</li>
<li><pre><code>   cached_node = iovad-&gt;cached64_node;
</code></pre>
</li>
<li></li>
<li> if (cached_node == NULL)<pre><code>   return rb_last(&amp;iovad-&gt;rbroot);
</code></pre>
   else {</li>
</ul>
<ul>
<li><pre><code>   struct rb_node *prev_node = rb_prev(iovad-&gt;cached32_node);
</code></pre>
</li>
</ul>
<ul>
<li><pre><code> struct rb_node *prev_node = rb_prev(cached_node);
   struct iova *curr_iova =
</code></pre>
</li>
</ul>
<ul>
<li><pre><code>       container_of(iovad-&gt;cached32_node, struct iova, node);
</code></pre>
</li>
</ul>
<ul>
<li><pre><code>     container_of(cached_node, struct iova, node);         *limit_pfn = curr_iova-&gt;pfn_lo - 1;
   return prev_node;
</code></pre>
   }<br>@@ -72,9 +79,10 @@ static void<br>__cached_rbnode_insert_update(struct iova_domain *iovad,<br>   unsigned long limit_pfn, struct iova *new)<br>{</li>
</ul>
<ul>
<li>   if (limit_pfn != iovad-&gt;dma_32bit_pfn)</li>
<li><pre><code>   return;
</code></pre>
</li>
<li>   iovad-&gt;cached32_node = &amp;new-&gt;node;</li>
</ul>
<ul>
<li>   if (limit_pfn &lt;= iovad-&gt;dma_32bit_pfn)</li>
<li><pre><code>   iovad-&gt;cached32_node = &amp;new-&gt;node;
</code></pre>
</li>
<li>   else</li>
<li><pre><code> iovad-&gt;cached64_node = &amp;new-&gt;node;
</code></pre>
}</li>
</ul>
<p> static void<br>@@ -82,21 +90,26 @@ __cached_rbnode_delete_update(struct iova_domain *iovad, struct iova *free)<br> {<br>     struct iova *cached_iova;<br>     struct rb_node *curr;</p>
<ul>
<li>   struct rb_node **cached_node;</li>
<li></li>
<li>   if (free-&gt;pfn_hi &lt;= iovad-&gt;dma_32bit_pfn)</li>
<li><pre><code>   cached_node = &amp;iovad-&gt;cached32_node;
</code></pre>
</li>
<li>   else</li>
<li><pre><code> cached_node = &amp;iovad-&gt;cached64_node;
</code></pre>
</li>
</ul>
<ul>
<li>   if (!iovad-&gt;cached32_node)</li>
</ul>
<ul>
<li>   curr = *cached_node;</li>
<li> if(!curr)<pre><code>   return;
</code></pre>
</li>
</ul>
<ul>
<li> curr = iovad-&gt;cached32_node;<br>   cached_iova = container_of(curr, struct iova, node);   if (free-&gt;pfn_lo &gt;= cached_iova-&gt;pfn_lo) {<pre><code>   struct rb_node *node = rb_next(&amp;free-&gt;node);
   struct iova *iova = container_of(node, struct iova, node);
</code></pre>
</li>
<li><pre><code>   /* only cache if it&#39;s below 32bit pfn */
</code></pre>
</li>
<li><pre><code>   if (node &amp;&amp; iova-&gt;pfn_lo &lt; iovad-&gt;dma_32bit_pfn)
</code></pre>
</li>
<li><pre><code>       iovad-&gt;cached32_node = node;
</code></pre>
</li>
</ul>
<ul>
<li><pre><code>   if (node)
</code></pre>
</li>
<li><pre><code>     *cached_node = node;
   else
</code></pre>
</li>
</ul>
<ul>
<li><pre><code>       iovad-&gt;cached32_node = NULL;
</code></pre>
</li>
</ul>
<ul>
<li><pre><code>     *cached_node = NULL;
</code></pre>
   }<br>}</li>
</ul>
<p>diff –git a/include/linux/iova.h b/include/linux/iova.h<br>index f27bb2c..d4670c1 100644<br>— a/include/linux/iova.h<br>+++ b/include/linux/iova.h<br>@@ -41,6 +41,7 @@ struct iova_domain {<br>     spinlock_t    iova_rbtree_lock; /* Lock to protect update of rbtree <em>/<br>     struct rb_root    rbroot;        /</em> iova domain rbtree root */<br>     struct rb_node    <em>cached32_node; /</em> Save last alloced node */</p>
<ul>
<li> struct rb_node    <em>cached64_node; /</em> Save last 64bit alloced node <em>/<br>   unsigned long    granule;    /</em> pfn granularity for this domain <em>/<br>   unsigned long    start_pfn;     /</em> Lower limit for this domain */<br>   unsigned long    dma_32bit_pfn;<br>```</li>
</ul>
<p>above solution just adds another cache for 64bit DMA Mask.</p>
<p>But now Linux kernel community just merged a PATCH:<br>        iommu/dma: Implement PCI allocation optimisation<br>into mainline kernel.</p>
<p>This patch just castes 64bit DMA mask to 32bit DMA mask, so we can still use<br>32bit DMA cache to improve the performance.</p>
<p>NOTE: but if this we can not allocate a 64bit iova to a PCIe device’s DMA target<br>      address. This is a problem :(</p>
<ol start="4">
<li>problem</li>
</ol>
<hr>
<ul>
<li><p>Performance</p>
<p>After SMMU enable and applying above patch, 82599 performance is 7.5Gbps,<br>only 80% performance comparing SMMU disable. We need check if this is correct<br>considering both hardware and software limitation.</p>
</li>
<li><p>NIC panic</p>
<p>After SMMU enable and applying above patch, xxx net will panic :( should fix this.<br>(p.s. already find where the problem is, xxx net dma map once, but unmap multiple<br> times)</p>
</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p>[1] xxx<br>[2] JIRA bug<br>[3] <span class="exturl" data-url="aHR0cHM6Ly9sd24ubmV0L0FydGljbGVzLzM3MDQyMy8=">https://lwn.net/Articles/370423/<i class="fa fa-external-link-alt"></i></span></p>
<pre><code>cd /sys/kernel/debug/tracing
echo ixgbe_* &gt; set_graph_function
echo function_graph &gt; current_tracer
cat trace &gt; ~/smmu_test
</code></pre>
]]></content>
      <tags>
        <tag>SMMU</tag>
        <tag>软件性能</tag>
      </tags>
  </entry>
  <entry>
    <title>How to test PCIe SR-IOV in D0x</title>
    <url>/2021/07/11/How-to-test-PCIe-SR-IOV-in-D0x/</url>
    <content><![CDATA[<ol start="0">
<li>preparation</li>
</ol>
<hr>
<p>kernel: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hpc2lsaWNvbi9rZXJuZWwtZGV2LmdpdA==">https://github.com/hisilicon/kernel-dev.git<i class="fa fa-external-link-alt"></i></span> branch: private-topic-sriov-v3-4.10</p>
<p>UEFI: openlab1.0 101 server /home/wangzhou/repo/plinth_uefi/uefi</p>
<p>qemu: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2VhdWdlci9xZW11LmdpdA==">https://github.com/eauger/qemu.git<i class="fa fa-external-link-alt"></i></span> branch: v2.7.0-passthrough-rfc-v5</p>
<p>hardware topology:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------------+       +----------------------+</span><br><span class="line">|  D05 I        |       |         D05 II       |</span><br><span class="line">|               |       |                      |</span><br><span class="line">|   +-----------+       +------------------+   |</span><br><span class="line">|   |1P NA PCIe2|&lt;-----&gt;|any 10G networking|   |</span><br><span class="line">|   +-----------+       +------------------+   |</span><br><span class="line">+---------------+       +----------------------+</span><br></pre></td></tr></table></figure>
<ol>
<li>compile kernel and UEFI</li>
</ol>
<hr>
<p>configure kernel: Add SMMU_V3=y, 82599 PF driver = m, 82599 VF driver = m,<br>                  VFIO PCI driver = m (p.s. ACPI boot)<br>compile kernel image and ko<br>compile UEFI using: ./uefi-tools/uefi-build.sh -c ./LinaroPkg/platforms.config d05</p>
<ol start="2">
<li>basic test</li>
</ol>
<hr>
<pre><code>1. boot up host OS (firstly update UEFI above)[1]

2. copy modules to host OS:
   ixgbe.ko mdio.ko vfio_iommu_type1.ko vfio.ko vfio-pci.ko
   vfio_virqfd.ko irqbypass.ko

3. prepare host environment:

   mkdir /lib/modules/`uname -r`
   touch /lib/modules/`uname -r`/modules.order
   touch /lib/modules/`uname -r`/modules.builtin
   depmod ixgbe.ko  mdio.ko vfio_iommu_type1.ko vfio.ko vfio-pci.ko
          vfio_virqfd.ko irqbypass.ko

   /*
    * here we insert ixgbe as it will use function in ixgbe driver to triggre
    * VF. If we do no insert ixgbe here, we will not get VF&#39;s sysfs interface,
    * and lspci will not show information of VF.
    *
    * here we need not insert ixgbevf(VF driver), as we will try to bind vfio-pci
    * to VF device. However, if we want to use VF in host, we should insert
    * ixgbevf here. After inserting ixgbevf, if we want to use vfio-pci driver,
    * we should firstly unbind ixgbevf driver for VF device using:
    * echo 0002:81:10.0 &gt; /sys/bus/pci/drivers/ixgbevf/unbind, then we can
    * use  echo vfio-pci &gt; /sys/bus/pci/devices/0002:81:10.0/driver_override
    *      echo 0002:81:10.0 &gt; /sys/bus/pci/drivers_probe
    * to bind vfio-pci driver with VF device.
    */
   modprobe ixgbe

   /* trigger one VF, 0002:81:00.0 is the PF in which you want trigger a VF */
   echo 1 &gt; /sys/devices/pci0002:80/0002:80:00.0/0002:81:00.0/sriov_numvfs

   modprobe -v vfio-pci disable_idle_d3=1
   modprobe -r vfio_iommu_type1
   modprobe -v vfio_iommu_type1 allow_unsafe_interrupts=1

   /* set related PF up */
   ifconfig eth26 up

   /* 0002:81:10.0 is BDF of VF */
   echo vfio-pci &gt; /sys/bus/pci/devices/0002:81:10.0/driver_override
   echo 0002:81:10.0 &gt; /sys/bus/pci/drivers_probe

4. run qemu[2]

   qemu-system-aarch64 \
   -machine virt,gic-version=3 \
   -enable-kvm \
   -cpu host \
   -m 1024 \
   -kernel ./Image \
   -initrd ./minifs.cpio.gz \
   -nographic \
   -net none -device vfio-pci,host=0002:81:10.0,id=net0 \
   -D trace_log

5. set networking configurations in guest machine and remote machine
   
   run ping and iperf to test
</code></pre>
<ol start="3">
<li>more scenarios</li>
</ol>
<hr>
<pre><code>1. enable multiple VFs, assigned to one VM
2. enable multiple VFs, assigned to different VMs
3. use VF directly
4. VF and PF communicate
</code></pre>
<ol start="4">
<li>performance</li>
</ol>
<hr>
<pre><code>1. should at least reach the performance of PF[3]
</code></pre>
<p>reference:<br>[1] should add pcie_acs_override=downstream in kernel command line:<br>    As our PCIe RP does not support ACS capbility, we should add this command line<br>    and related patch(<span class="exturl" data-url="aHR0cHM6Ly9sa21sLm9yZy9sa21sLzIwMTMvNS8zMC81MTM=">https://lkml.org/lkml/2013/5/30/513<i class="fa fa-external-link-alt"></i></span>) which will never be<br>    merged into mainline to tell kernel our PCIe RP indeed did something to<br>    provide address isolation just like ACS does.<br>    we will upstream in ACS quirk patch to fix this.<br>[2] how to compile qemu locally in D05:<br>    <span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2NhcmVjcm93X2J5ci9hcnRpY2xlL2RldGFpbHMvNTE0OTQwMjA=">http://blog.csdn.net/scarecrow_byr/article/details/51494020<i class="fa fa-external-link-alt"></i></span><br>[3] should add 82599 patch to test, firstly make sure performance of 82599 PF is<br>    good.</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>软件测试</tag>
      </tags>
  </entry>
  <entry>
    <title>How to dump acpi tables in CentOS</title>
    <url>/2021/07/05/How-to-dump-acpi-tables-in-CentOS/</url>
    <content><![CDATA[<p>My system is ARM64 CentOS7.4</p>
<p>1.yum install acpica-tools.aarch64</p>
<p>2.acpidump &gt; acpidump.out </p>
<p>3.acpixtract -a acpidump.out</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@157 acpi_test]# acpixtract -a acpidump.out</span><br><span class="line"></span><br><span class="line">Intel ACPI Component Architecture</span><br><span class="line">ACPI Binary Table Extraction Utility version 20160527-64</span><br><span class="line">Copyright (c) 2000 - 2016 Intel Corporation</span><br><span class="line"></span><br><span class="line">Acpi table [SPCR] -      80 bytes written to spcr.dat</span><br><span class="line">Acpi table [MCFG] -     172 bytes written to mcfg.dat</span><br><span class="line">Acpi table [GTDT] -     124 bytes written to gtdt.dat</span><br><span class="line">Acpi table [APIC] -    5348 bytes written to apic.dat</span><br><span class="line">Acpi table [IORT] -    1300 bytes written to iort.dat</span><br><span class="line">Acpi table [SLIT] -      60 bytes written to slit.dat</span><br><span class="line">Acpi table [DSDT] -   26427 bytes written to dsdt.dat</span><br><span class="line">Acpi table [SRAT] -    1400 bytes written to srat.dat</span><br><span class="line">Acpi table [DBG2] -      90 bytes written to dbg2.dat</span><br><span class="line">Acpi table [FACP] -     276 bytes written to facp.dat</span><br><span class="line"></span><br><span class="line">10 binary ACPI tables extracted</span><br><span class="line">[root@157 acpi_test]# ls</span><br><span class="line">acpidump.out  apic.dat  dbg2.dat  dsdt.dat  facp.dat  gtdt.dat  iort.dat  mcfg.dat  slit.dat  spcr.dat  srat.dat</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>   After this command, you can see related ACPI tables.</p>
<p>4.iasl -d *.dat</p>
<p>e.g. un-compile mcfg.dat:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@157 acpi_test]# iasl -d mcfg.dat</span><br><span class="line"></span><br><span class="line">Intel ACPI Component Architecture</span><br><span class="line">ASL+ Optimizing Compiler version 20160527-64</span><br><span class="line">Copyright (c) 2000 - 2016 Intel Corporation</span><br><span class="line"></span><br><span class="line">Input file mcfg.dat, Length 0xAC (172) bytes</span><br><span class="line">ACPI: MCFG 0x0000000000000000 0000AC (v01 HISI   HIP07    00000000 INTL 20151124)</span><br><span class="line">Acpi Data Table [MCFG] decoded</span><br><span class="line">Formatted output:  mcfg.dsl - 3955 bytes</span><br><span class="line">[root@157 acpi_test]# ls mcfg.dsl</span><br><span class="line">mcfg.dsl</span><br></pre></td></tr></table></figure>

<p>5.Then you can view it: vim mcfg.dsl</p>
]]></content>
      <tags>
        <tag>UEFI</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use IO BAR in linux PCIe device driver</title>
    <url>/2021/07/05/How-to-use-IO-BAR-in-linux-PCIe-device-driver/</url>
    <content><![CDATA[<ol>
<li><p>IO window parse analysis</p>
<p>In an ACPI based system, we parse the IO window configured in DSDT table, as<br>showed in this link: blog.csdn.net/scarecrow_byr/article/details/53966460.</p>
<p>We can see in pci_acpi_root_prepare_resources:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_acpi_root_prepare_resources</span><br><span class="line">    --&gt; acpi_pci_probe_root_resources</span><br><span class="line">        --&gt; acpi_pci_root_remap_iospace</span><br></pre></td></tr></table></figure>
<p>in acpi_pci_root_remap_iospace, CPU address of one PCIe IO window will be<br>mapped to PCI_IOBASE based system IO space, like below picture:</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  PCI_IOBASE            PCI_IOBASE + PCIE_IO_SIZE - 1</span><br><span class="line">        |&lt;-------------&gt;|</span><br><span class="line">--------+---------------+---------------------------   &lt;- CPU VA in kernel</span><br><span class="line">    |   |      |\      \</span><br><span class="line">    |   |      | \      \                  </span><br><span class="line">    v   |      |  \      \                  maps supported by MMU</span><br><span class="line">        |      |   \      \       </span><br><span class="line">        |      |    \      \</span><br><span class="line">        |&lt;----&gt;|     |&lt;----&gt;|   &lt;- CPU PA</span><br><span class="line">    |    \      \     \      \</span><br><span class="line">    |     \      \     \      \             maps supported by ATU</span><br><span class="line">    v      \      \     \      \</span><br><span class="line">            \      \     \      \</span><br><span class="line">             \      \     \      \</span><br><span class="line">              |&lt;----&gt;|     |&lt;----&gt;|  &lt;- PCI address</span><br><span class="line"></span><br><span class="line">             IO window 1   IO window 2</span><br><span class="line">            host bridge 1  host bridge 2</span><br></pre></td></tr></table></figure>

<p>   and offset between CPU VA and PCI_IOBASE will be stored in resource_entry,<br>   which will be passed to PCI enumeration. The offset in resource_entry will be<br>   offset(above) - PCI address. In the process of the enumeration, IO BAR will<br>   be allocated in related IO window.</p>
<p>   the base of IO BAR will be stored in (pci_dev -&gt; resource[IO BAR].start), which<br>   is the offset to PCI_IOBASE in CPU VA.</p>
<ol start="2">
<li><p>How to use in PCIe device driver</p>
<p>In one hardware arch, we use inb/outb, inw/outw … function to access IO<br>space. In ARM64, these functions are defined in linux/include/asm-generic/io.h,<br>like: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#ifndef outb</span><br><span class="line">#define outb outb</span><br><span class="line">static inline void outb(u8 value, unsigned long addr)</span><br><span class="line">&#123;</span><br><span class="line">        writeb(value, PCI_IOBASE + addr);</span><br><span class="line">&#125;</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure>
<p>So when we want to read/write IO BAR in PCIe device driver, we should:</p>
<ol>
<li>get the base of one IO BAR by: addr = pci_resource_start(dev, bar)</li>
<li>use outb(value, addr) for an example to do port input by byte-width.</li>
</ol>
</li>
</ol>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use Linux kernel crypto compression</title>
    <url>/2021/06/28/How-to-use-Linux-kernel-crypto-compression/</url>
    <content><![CDATA[<p>Linux kernel crypto subsystem supports different algorithms including compression<br>algorithms. A hardware or software implementation can be added to crypto<br>subsystem to do specific work.</p>
<p>If you have a hardware module to do Zlib/Gzip compressions, you can write a<br>driver, which can be registered into Linux kernel crypto subsystem.</p>
<p>To use this hardware to do compression, you need not care about hardware detail,<br>only thing you need to know is crypto API, here, you should know crypto compress<br>related API.</p>
<p>Firstly, we should create a crypto compression context using:<br>We only consider synchronized API crypto_alloc_comp here.(struct crypto_comp)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_alloc_comp(const char *alg_name, u32 type, u32 mask)</span><br><span class="line">crypto_alloc_acomp(const char *alg_name, u32 type, u32 mask)</span><br></pre></td></tr></table></figure>
<p>alg_name here can be specific driver name or standard algorithm name. You can<br>find driver name in cra_driver_name item in struct crypto_alg, which may be<br>offered in hardware engine driver’s crypto_alg, we do not offer this; algorithm<br>name is offered in cra_name in struct crypto_alg, which also can be offered in<br>hardware engine driver, we do this, we offer “zlib-deflate” and “gzip”.</p>
<p>Crypto subsystem tries to find registered algorithms by driver name or standard<br>algorithm name. Driver name has the top priority, standard algorithm name and<br>priority item in struct crypto_alg also can be used to find proper algorithm.<br>In user space, you can use “cat /proc/crypto” to find registered crypto<br>algorithms, e.g.:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">name         : crct10dif</span><br><span class="line">driver       : crct10dif-pclmul</span><br><span class="line">module       : crct10dif_pclmul</span><br><span class="line">priority     : 200</span><br><span class="line">refcnt       : 1</span><br><span class="line">selftest     : passed</span><br><span class="line">internal     : no</span><br><span class="line">type         : shash</span><br><span class="line">blocksize    : 1</span><br><span class="line">digestsize   : 2</span><br><span class="line"></span><br><span class="line">name         : crc32</span><br><span class="line">driver       : crc32-pclmul</span><br><span class="line">module       : crc32_pclmul</span><br><span class="line">priority     : 200</span><br><span class="line">refcnt       : 1</span><br><span class="line">selftest     : passed</span><br><span class="line">internal     : no</span><br><span class="line">type         : shash</span><br><span class="line">blocksize    : 1</span><br><span class="line">digestsize   : 4</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>u32 type and u32 mask here, we use below, which are defined in[1]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define CRYPTO_ALG_TYPE_ACOMPRESS	0x0000000a</span><br><span class="line">#define CRYPTO_ALG_TYPE_COMPRESS	0x00000002</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">#define CRYPTO_ALG_TYPE_MASK		0x0000000f</span><br></pre></td></tr></table></figure>
<p>Here, we only support CRYPTO_ALG_TYPE_COMPRESS currently.</p>
<p>After creating compression context, we can use it to do compression/decompression by:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_comp_compress(struct crypto_comp *tfm, const u8 *src,</span><br><span class="line">		     unsigned int slen, u8 *dst, unsigned int *dlen)</span><br><span class="line">crypto_comp_decompress() (currrently we do not support)</span><br></pre></td></tr></table></figure>
<p>src, slen, dst, dlen are input/output buffer’s address and size.</p>
<p>After compression/decompress finished, we call below function to free related<br>context:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_free_comp()</span><br></pre></td></tr></table></figure>

<p>Currently, if you want to use HiSilicon’s Compression engine in D06, which<br>driver is under upstreaming.[2] You should open below kernel configures:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CONFIG_CRYPTO_DEV_HISILICON</span><br><span class="line">CONFIG_CRYPTO_DEV_HISI_QM</span><br><span class="line">CONFIG_CRYPTO_DEV_HISI_ZIP</span><br></pre></td></tr></table></figure>

<p>NOTE:</p>
<p> [1] linux/include/linux/crypto.h<br> [2] <span class="exturl" data-url="aHR0cHM6Ly9sa21sLm9yZy9sa21sLzIwMTgvOS8zLzY=">https://lkml.org/lkml/2018/9/3/6<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>crypto</tag>
      </tags>
  </entry>
  <entry>
    <title>Intel QAT ZIP初步分析</title>
    <url>/2021/06/27/Intel-QAT-ZIP%E5%88%9D%E6%AD%A5%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol>
<li>基本信息 </li>
</ol>
<hr>
<p> QAT的官网: <span class="exturl" data-url="aHR0cHM6Ly93d3cuaW50ZWwuY24vY29udGVudC93d3cvY24vemgvYXJjaGl0ZWN0dXJlLWFuZC10ZWNobm9sb2d5L2ludGVsLXF1aWNrLWFzc2lzdC10ZWNobm9sb2d5LW92ZXJ2aWV3Lmh0bWw=">https://www.intel.cn/content/www/cn/zh/architecture-and-technology/intel-quick-assist-technology-overview.html<i class="fa fa-external-link-alt"></i></span></p>
<p> QAT相关的代码在01.org网站: <span class="exturl" data-url="aHR0cHM6Ly8wMS5vcmcvemgvaW50ZWwtcXVpY2thc3Npc3QtdGVjaG5vbG9neT9sYW5ncmVkaXJlY3Q9MQ==">https://01.org/zh/intel-quickassist-technology?langredirect=1<i class="fa fa-external-link-alt"></i></span></p>
<p> 基本QAT相关的用户手册，代码都可以从上面的URL获得。</p>
<ol start="2">
<li>用户APP、QATzip、libqat库的关系</li>
</ol>
<hr>
<p> 我们这里看看QAT里支持的压缩解压缩是怎么最终叫用户APP使用到的。</p>
<p> QAT整个压缩解压缩的软件栈由: 内核驱动，libqat用户态基础库，QATzip用户态库组成。<br> 我们这里以ceph作为APP，一起看下，ceph里的压缩解压缩可以直接调用QATzip库提供的<br> 接口使用QAT的硬件压缩解压缩引擎。</p>
<p> 以上软件的位置在：1. QAT的内核态驱动和libqat用户态基础库合在一起放在上面的01.org<br> 的这个包里：Intel® QuickAssist Technology Driver for Linux* - HW version 1.7.<br> 2. QATzip的代码在：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ludGVsL1FBVHppcA==">https://github.com/intel/QATzip<i class="fa fa-external-link-alt"></i></span>. 3. ceph的代码：github.com/ceph.</p>
<p> 整个调用链的逻辑是：</p>
<ol>
<li><p>内核态QAT crypto驱动除了向crypto子系统上注册外，也会向内核UIO子系统注册，<br>通过UIO把QAT的硬件资源暴露给用户态。由于UIO存在安全上的问题，可以看到主线<br>内核里注册到UIO的驱动很少，这也是QAT内核驱动中注册UIO这部分无法上传到主线<br>的原因。</p>
</li>
<li><p>libqat用户态基础库封装UIO用户态接口，向上提供一组基础的API。这个在01.org<br>网站的接口说明文档中有介绍。</p>
</li>
<li><p>QATzip这个库调用libqat API对外提供QAT的压缩解压缩基本接口。提供的接口在<br>QATzip/include/qatzip.h这个头文件中。</p>
</li>
<li><p>Ceph代码里压缩解压缩的部分ceph/src/compressor/有QatAccel.cc, 这部分代码<br>调用QATzip的接口封装ceph里的压缩解压缩接口供同目录下的zlib/zlibCompressor.cc<br>使用。(目前竟然是HAVE_QATZIP这个宏隔开的 :( )</p>
</li>
<li><p>接口分析</p>
</li>
</ol>
<hr>
<ol>
<li><p>libqat有一个qat_service的服务，这个服务可能和/dev/qat_adf_ctl的这个字符设备<br>配合提供一些管理工作。</p>
</li>
<li><p>qat卡可以load不同的固件，从而改变qat卡自身的功能。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>ZIP</tag>
        <tag>QAT</tag>
      </tags>
  </entry>
  <entry>
    <title>LDD3 study note 1</title>
    <url>/2021/07/05/LDD3-study-note-1/</url>
    <content><![CDATA[<ol>
<li>这是一个什么设备</li>
</ol>
<hr>
<p>这是一个内存模拟出来的设备，简单说就是一些内存区域。在用户态，可以对相应的设备<br>文件read/write，相应的结果是读写这些内存区域中的数据。</p>
<p>这些内存的区域按照下面的数据结构组成：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-------+         +-------+          +-------+         +-------+   </span><br><span class="line">| 链表  +--------&gt;+       +---------&gt;+       +--------&gt;+       |   ....</span><br><span class="line">|       |         |       |          |       |         |       |</span><br><span class="line">+---+---+         +-------+          +-------+         +-------+</span><br><span class="line">    |               ...                                            </span><br><span class="line">    +--------+      +-------------------------+                 </span><br><span class="line">    | 指针   +-----&gt;+ quantum(e.g. 4000) Byte |</span><br><span class="line">    +--------+      +-------------------------+</span><br><span class="line">    |        +-----&gt; ...                                            </span><br><span class="line">    +--------+                                                  </span><br><span class="line">    |        |                                                  </span><br><span class="line">    +--------+                                                  </span><br><span class="line">    ...                                                 </span><br><span class="line">    +--------+                                                  </span><br><span class="line">    |        |                                                  </span><br><span class="line">    +--------+                                                  </span><br><span class="line">    |        |                                                  </span><br><span class="line">    +--------+                                                  </span><br></pre></td></tr></table></figure>
<p>可以看到数据只是存在quantum的单元里的，其他的数据结构都是为了索引到这个结构。<br>这个数据结构的特点是，链表的长度是不固定的(quantum, 指针数组的大小是固定的), 这样<br>只要一直往这个设备里写数据，链表就不断的加长，同时不断的创建新的quantum, 把数据<br>存在里面。这个过程可以吃光系统里所有的内存。还有一个特点就是，设备一开始不会分配<br>quantum, 只有有数据需要写入，又没有quantum时才开始分配。所有分配的quantum在模块<br>卸载时释放掉。同时，设备的读写函数设计成如果一次读写的数据超过一个quantum的界限，<br>那么只读写当前一个quantum中的数据。</p>
<ol start="2">
<li>open/release/read/write的实现</li>
</ol>
<hr>
<p>open: 把scull_dev挂成struce file的数据，方便以后其他的操作找到设备<br>read/write: …<br>release: 关闭文件的时候要调用到, 对scull_dev不做处理。模块退出的时候, 释放设备对应的<br>         内存, 这个操作应该是在exit函数里，不放到这里。</p>
<ol start="3">
<li>调试宏</li>
</ol>
<hr>
<p>在代码中插入了一些断言的宏，方便调试。</p>
<ol start="4">
<li>自动创建设备节点</li>
</ol>
<hr>
<p>LDD3源代码中是加了一个脚本创建设备节点，这里创建一个class, 然后在class里创建这个<br>字符设备对应的设备文件。模块加载时会自动在/dev下创建设备文件。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>LDD3</tag>
      </tags>
  </entry>
  <entry>
    <title>KAE笔记</title>
    <url>/2021/06/20/KAE%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>梳理KAE实现Openssl engine的基本逻辑。本文基于的KAE代码在这个位置:<br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2t1bnBlbmdjb21wdXRlL0tBRQ==">https://github.com/kunpengcompute/KAE<i class="fa fa-external-link-alt"></i></span></p>
<ol>
<li>注册为openssl engine的位置在: engine_kae.c</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IMPLEMENT_DYNAMIC_BIND_FN(bind_kae)                                             </span><br><span class="line">	+-&gt; kae_engine_setup() // 配置引擎的参数</span><br><span class="line">		+-&gt; cipher_module_init() // 初始化sec引擎</span><br><span class="line">			初始化队列池，但是没有申请队列</span><br><span class="line">			+-&gt; wd_ciphers_init_qnode_pool()</span><br><span class="line"></span><br><span class="line">			+-&gt; sec_create_ciphers()</span><br><span class="line"></span><br><span class="line">				+—&gt; sec_ciphers_set_cipher_method(cipher_info_t cipherinfo)      </span><br><span class="line"></span><br><span class="line">					注册一个实例的回调函数。每个实例的创建、做任务、销毁分别是：init, do_cipher, cleanup。</span><br><span class="line">					EVP_CIPHER *cipher = EVP_CIPHER_meth_new(cipherinfo.nid, cipherinfo.blocksize, cipherinfo.keylen);</span><br><span class="line">					EVP_CIPHER_meth_set_iv_length(cipher, cipherinfo.ivlen);             </span><br><span class="line">					EVP_CIPHER_meth_set_flags(cipher, cipherinfo.flags);                 </span><br><span class="line">					EVP_CIPHER_meth_set_init(cipher, sec_ciphers_init);                  </span><br><span class="line">					EVP_CIPHER_meth_set_do_cipher(cipher, sec_ciphers_do_cipher);        </span><br><span class="line">					EVP_CIPHER_meth_set_set_asn1_params(cipher, EVP_CIPHER_set_asn1_iv); </span><br><span class="line">					EVP_CIPHER_meth_set_get_asn1_params(cipher, EVP_CIPHER_get_asn1_iv); </span><br><span class="line">					EVP_CIPHER_meth_set_cleanup(cipher, sec_ciphers_cleanup);            </span><br><span class="line">					EVP_CIPHER_meth_set_impl_ctx_size(cipher, sizeof(cipher_priv_ctx_t));</span><br><span class="line"></span><br><span class="line">			注册sec的回调处理函数</span><br><span class="line">			+-&gt; async_register_poll_fn(ASYNC_TASK_CIPHER, sec_cipher_engine_ctx_poll)</span><br><span class="line">		...</span><br><span class="line"></span><br><span class="line">		+-&gt; async_module_init() //起异步线程, 整个引擎里只有一个异步的轮训线程</span><br><span class="line">			+-&gt; async_polling_thread_init()</span><br><span class="line"></span><br><span class="line">	+-&gt; ENGINE_set_ciphers(e, sec_engine_ciphers) // 向系统里设置sec引擎</span><br></pre></td></tr></table></figure>
<p>所以, 一个openssl engine使用wd队列的模型是：每次上层用户请求创建一个实例，每个实例<br>里申请一个wd队列，可以反复向这一个队列上发送任务，发送任务的时候在这个队列上创建<br>一个ctx，反复发送任务时, 每次都复用这个队列和ctx。</p>
<p>发送异步任务和以上同步任务情况是一样的。不一样的是，每一个异步任务发送后，都把队列<br>的信息加入到一个自定义的软件队列里，然后向异步轮寻线程发送通知，异步轮寻线程接受<br>到通知后轮寻对应的硬件队列:</p>
<p>sec_ciphers_do_cipher(EVP_CIPHER_CTX *ctx, unsigned char *out, const unsigned char *in, size_t inl)<br>    +-&gt; sec_ciphers_async_do_crypto(e_cipher_ctx, &amp;op_done)<br>        +-&gt; async_add_poll_task(e_cipher_ctx, op_done, type)</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h2><ol>
<li><p>KAE openssl并没有整理规划wd队列的使用，如果是一个新请求就创建一个队列<br>的话，最大并发请求个数会是2048</p>
</li>
<li><p>KAE在支持存储的时候，spark，ceph都会出现大量同步并发请求的情况。<br>目前需要把ctx-&gt;q, 多对1的情况支持起来。</p>
</li>
<li><p>异步的时候，使用少量线程就可以跑满硬件性能。所以，异步并发请求不大。<br>同步会有大的并发请求</p>
</li>
<li><p>一个进程里的请求大概是相同大小的。</p>
</li>
<li><p>KAE一个engine一个异步轮寻队列，目前可以满足他的性能需求。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>openssl engine</tag>
      </tags>
  </entry>
  <entry>
    <title>LDD3 study note 2</title>
    <url>/2021/07/05/LDD3-study-note-2/</url>
    <content><![CDATA[<ol>
<li>ioctl</li>
</ol>
<hr>
<p>驱动可以通过ioctl函数定义一组和用户态程序交互的接口.<br>ioctl的用户态接口是：int ioctl(int d, int request, …). 一般我们可以认为是：<br>int ioctl(int fd, int cmd, int arg)。其中fd是要操作的文件，cmd是下发的命令字,<br>和驱动里ioctl实现的命令字是一一对应的。arg是传入内核的参数，可以看到一般的情况<br>下arg这个参数是一个指针变量。</p>
<ol start="2">
<li>命令码</li>
</ol>
<hr>
<p>cmd不是可以随便定义的，具体可以参考linux/Documentation/ioctl/ioctl-number.txt<br>这个文档。简单来讲，一个命令字是四段组成的。每段具体是什么内容可以查看LDD3或者<br>上面的文档。生成命令字需要用内核提供的一组宏，这组宏的定义在：<br>linux/include/uapi/asm-generic/ioctl.h</p>
<p>一般定义命令字用下面这组宏:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define _IO(type,nr)		_IOC(_IOC_NONE,(type),(nr),0)</span><br><span class="line">#define _IOR(type,nr,size)	_IOC(_IOC_READ,(type),(nr),(_IOC_TYPECHECK(size)))</span><br><span class="line">#define _IOW(type,nr,size)	_IOC(_IOC_WRITE,(type),(nr),(_IOC_TYPECHECK(size)))</span><br><span class="line">#define _IOWR(type,nr,size)	_IOC(_IOC_READ|_IOC_WRITE,(type),(nr),(_IOC_TYPECHECK(size)))</span><br></pre></td></tr></table></figure>

<p>type是在ioctl-number.txt里讲过的魔术字，作为命令字最基本的区分。nr是ioctl接口<br>的第几个命令，一般是0,1,2…, size LDD3上解释的不是很清楚，暂时用int(Fix me)。<br>命令的类型会合成命令字中的一段。这样，以上四段内容会拼成一个ioctl的命令字。</p>
<p>可以看到一个ioctl支持的接口命令中type，size一样的，命令的类型和序号有可能有区别。</p>
<ol start="3">
<li>用户态头文件</li>
</ol>
<hr>
<p>ioctl是一种用户态和内核交流信息的方式。用户态调用ioctl的时候，发起的命令、传入<br>内核的数据结构, 在用户态都要有定义。目前的本人的做法是，在用户态的头文件中拷贝<br>内核中命令码生成的相关宏定义(Fix me)，对于交流信息的数据结构，也在用户态头文件中<br>再定义一次。</p>
<ol start="4">
<li>传变量和传指针</li>
</ol>
<hr>
<p>受到ioctl接口的限制, 用户态可以使用传变量和传指针的方式向内核发送信息, 传变量<br>也只能是一个unsigned long类型。内核向用户态传信息，就只能向用户态传进来的指针里<br>写数据了。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>LDD3</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux IO DMA地址映射流程分析</title>
    <url>/2021/06/27/Linux-IO-DMA%E5%9C%B0%E5%9D%80%E6%98%A0%E5%B0%84%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol>
<li>SMMU页表以及相关配置的初始化流程</li>
</ol>
<hr>
<p> iommu_ops里的attach_dev回调用来在SMMU一侧为master设备建立各种数据结构。如下是<br> arm_smmu_attach_dev的流程:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">arm_smmu_attach_dev</span><br><span class="line">  +-&gt; arm_smmu_domain_finalise</span><br><span class="line">        /*</span><br><span class="line">         * 目的是创建smmu_domain里的pgtbl_ops， 这个结构的原型是struct io_pgtable_ops</span><br><span class="line">         * struct io_pgtable_ops</span><br><span class="line">         *   +-&gt; map   </span><br><span class="line">         *   +-&gt; unmap</span><br><span class="line">         *   +-&gt; iova_to_phys</span><br><span class="line">         */</span><br><span class="line">    +-&gt; alloc_io_pgtable_ops</span><br><span class="line">        /*</span><br><span class="line">         * 以64bit s1为例子， 如下函数初始化页表的pgd， 并且初始化页表map/unmap的</span><br><span class="line">         * 操作函数</span><br><span class="line">         */</span><br><span class="line">      +-&gt; arm_64_lpae_alloc_pgtable_s1</span><br><span class="line"></span><br><span class="line">            /* 主要创建页表操作函数 */</span><br><span class="line">        +-&gt; arm_lpae_alloc_pgtable</span><br><span class="line">          +-&gt; map = arm_lpae_map</span><br><span class="line">          +-&gt; unmap = arm_lpae_unmap</span><br><span class="line">          +-&gt; iova_to_phys = arm_lpae_iova_to_phys</span><br><span class="line"></span><br><span class="line">            /* 创建pgd */</span><br><span class="line">        +-&gt; __arm_lpae_alloc_pages</span><br><span class="line"></span><br><span class="line">            /* 得到页表基地址 */</span><br><span class="line">        +-&gt; cfg-&gt;arm_lpae_s1_cfg.ttbr = virt_to_phys(data-&gt;pgd);</span><br><span class="line"></span><br><span class="line">        /* 收尾的配置再搞下，目前是用来配置CD表 */</span><br><span class="line">    +-&gt; finalise_stage_fn</span><br><span class="line">        /* 得到的io_pgtable_ops存放到smmu_domain中 */</span><br><span class="line">    +-&gt; smmu_domain-&gt;pgtbl_ops</span><br><span class="line"></span><br><span class="line">  +-&gt; arm_smmu_install_ste_for_dev</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>map流程分析</li>
</ol>
<hr>
<p> 我们从内核DMA API接口向下跟踪，观察dma内存的申请和map的流程。以dma_alloc_coherent<br> 为例分析，这个接口按照用户的请求申请内存，返回CPU虚拟地址和iova。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dma_alloc_coherent</span><br><span class="line">  +-&gt; dma_alloc_attrs /* kernel/dma/mapping.c */</span><br><span class="line">    +-&gt; iommu_dma_alloc /* drivers/iommu/dma-iommu.c */</span><br><span class="line">          /*</span><br><span class="line">           * 如下是内存分配和map的主体逻辑，大概可以分成两块。第一块是iomm_dma_alloc_remap，</span><br><span class="line">           * 这个内存分配和map在这个函数里一起完成，第二块是其余的逻辑，这部分逻辑把分配</span><br><span class="line">           * 内存和map分开了。第二部分里又有从dma pool里分内存和直接分内存，我们不分析</span><br><span class="line">           * dma pool里的case。</span><br><span class="line">           *</span><br><span class="line">           * 如上的case1和case3的核心区别是有没有开DMA_REMAP的内核配置，对应到具体的实现</span><br><span class="line">           * 是，REMAP的情况可以申请不连续的物理页面，调用remap函数得到连续的CPU虚拟地址。</span><br><span class="line">           * 可以看到REMAP的情况才真正支持大范围的dma地址。如果REMAP没有开，也就是case3，</span><br><span class="line">           * iommu_dma_alloc_pages中实际是调用伙伴系统的接口(不考虑CMA的情况)，受MAX_ORDER</span><br><span class="line">           * 的影响，一次可分配的连续物理内存是有限制的。</span><br><span class="line">           */</span><br><span class="line">      +-&gt; iommu_dma_alloc_remap</span><br><span class="line">            /*</span><br><span class="line">             * 根据size分配物理页面，多次调用伙伴系统接口分配不连续的物理页面块。同时</span><br><span class="line">             * 这个函数还做了iommu的map。我们仔细看下这个函数的细节。</span><br><span class="line">             */</span><br><span class="line">        +-&gt; __iommu_dma_alloc_noncontiguous</span><br><span class="line">              /* 这个风骚的bit运算取到的是最小一级的数值， 一般最小一级就是系统页大小 */</span><br><span class="line">          +-&gt; min_size = alloc_sizes &amp; -alloc_sizes;</span><br><span class="line">              /*</span><br><span class="line">               * 分配的算法在如下的函数里，count是需要分配页面的个数，这里的页是指系统</span><br><span class="line">               * 页大小。order_mask是页表里各级block的大小的mask，显然拿到这个信息是为了</span><br><span class="line">               * 分配的时候尽量从block分配，这个信息从iommu_domain的pgsize_bitmap中得到，</span><br><span class="line">               * pgsize_bitmap和具体的页表实现有关，在具体的iommu驱动里赋值，比如ARM的</span><br><span class="line">               * SMMUv3在4K页大小下，他的各级block大小是4K、2M和1G，所以，pgsize_bitmap</span><br><span class="line">               * 是 SZ_4K | SZ_2M | SZ_1G。</span><br><span class="line">               */</span><br><span class="line">          +-&gt; __iommu_dma_alloc_pages(...， count， order_mask， ...)</span><br><span class="line">                /*</span><br><span class="line">                 * 这段while循环是分配的主逻辑，通过位运算计算每次分配内存的大小。</span><br><span class="line">                 * (2U &lt;&lt; __fls(count) - 1)得到count的mask，比如count是0b1000，</span><br><span class="line">                 * mask就是0b1111， mask和order_mask相与，取出最高bit，就是针对</span><br><span class="line">                 * 当前count可以分配内存的最大block size，然后调用伙伴系统的接口</span><br><span class="line">                 * 去分配连续的物理内存。然后，跳出循环，更新下次需要分配的count，</span><br><span class="line">                 * 把本次分配的物理内存一页一页的放到输出pages数组里。虽然分配</span><br><span class="line">                 * 出的可以是物理地址连续的一个block，但是输出还是已page保存的。</span><br><span class="line">                 */</span><br><span class="line">            +-&gt; while (count) &#123;...&#125;</span><br><span class="line">              /* 分配iova */</span><br><span class="line">          +-&gt; iommu_dam_alloc_iova</span><br><span class="line">              /*</span><br><span class="line">               * 把如上分配的物理页用一个sgl的数据组合起来，注意连续的物理也会</span><br><span class="line">               * 又合并到一个sgl节点里。后面的iommu_map就可以把一个block映射到</span><br><span class="line">               * 一个页表的block里。不过具体的map逻辑还要在具体iommu驱动的map</span><br><span class="line">               * 回调函数中实现。从这里的分析可以看出，iommu驱动map回调函数输入</span><br><span class="line">               * 的size值并不是一定是page size大小。</span><br><span class="line">               */</span><br><span class="line">          +-&gt; sg_alloc_table_from_pages</span><br><span class="line">              /* 把iova到物理页面的map建立好 */</span><br><span class="line">          +-&gt; iommu_map_sg_atomic</span><br><span class="line">       </span><br><span class="line">            /* 把离散的物理页面remap到连续的CPU虚拟地址上 */</span><br><span class="line">        +-&gt; dma_common_pages_remap </span><br><span class="line"></span><br><span class="line">      +-&gt; iommu_dma_alloc_pages</span><br><span class="line"></span><br><span class="line">      +-&gt; __iommu_dma_map</span><br><span class="line">        [...]</span><br><span class="line">            /* 可以看到这个函数的while循环里也是如上从最大block分配的类似算法 */</span><br><span class="line">        +-&gt; __iommu_map</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/iommu/io-pgtbl-arm.c */</span><br><span class="line">/*</span><br><span class="line"> * 这个函数就是具体做页表映射的函数，函数的输入iova，paddr， size，iomm_prot已经</span><br><span class="line"> * 表示了要映射地址的va， pa， size和属性。这里iova和paddr的具体分配在上层的dma</span><br><span class="line"> * 框架里已经搞定。lvl是和ARM SMMUv3页表level相关的一个参数，不同页大小、VA位数</span><br><span class="line"> * stage对应的页表level以及起始level有不一样的情况。比如，如下的48bit、4K page</span><br><span class="line"> * size的情况，就是有level0/1/2/3四级页表。__arm_lpae_map具体要做的把一个给定</span><br><span class="line"> * map参数的翻译添加到页表里。</span><br><span class="line"> */</span><br><span class="line">arm_smmu_map</span><br><span class="line">  +-&gt; arm_lpae_map</span><br><span class="line">    +-&gt; __arm_lpae_map(...， iova， paddr， size， prot， lvl， ptep， ...)</span><br></pre></td></tr></table></figure>
<p> __arm_lpae_map的实现比较直白，就是递归的创建页表。完全按照上层给的page或者block<br> 的map来做页表映射。</p>
<ol start="3">
<li>页表相关的细节</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    level              0       1        3        3</span><br><span class="line">    block size                 1G       2M       4K</span><br><span class="line">+--------+--------+--------+--------+--------+--------+--------+</span><br><span class="line">|63    56|55    48|47    39|38    30|29    21|20    12|11     0|</span><br><span class="line">+--------+--------+--------+--------+--------+--------+--------+</span><br><span class="line"> |                 |         |         |         |         |</span><br><span class="line"> |                 |         |         |         |         v</span><br><span class="line"> |                 |         |         |         |   [11:0]  in-page offset</span><br><span class="line"> |                 |         |         |         +-&gt; [20:12] L3 index</span><br><span class="line"> |                 |         |         +-----------&gt; [29:21] L2 index</span><br><span class="line"> |                 |         +---------------------&gt; [38:30] L1 index</span><br><span class="line"> |                 +-------------------------------&gt; [47:39] L0 index</span><br><span class="line"> +-------------------------------------------------&gt; [63] TTBR0/1</span><br></pre></td></tr></table></figure>
<p> 如上是一个ARM64(SMMUv3)48bit、4K page size的VA用来做每级页表索引的划分, 这个划分<br> 比较常见riscv sv39也是这样的划分，只不过是少了最高的一级。在这样的划分下，每一级<br> 页表都有512个entry，如果一个页表项是64bit，每一级页表的每个table就正好占用4KB内存。</p>
]]></content>
      <tags>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
        <tag>DMA</tag>
        <tag>iommu</tag>
        <tag>页表</tag>
      </tags>
  </entry>
  <entry>
    <title>LDD3 study note 3</title>
    <url>/2021/07/05/LDD3-study-note-3/</url>
    <content><![CDATA[<ol>
<li>mmap</li>
</ol>
<hr>
<p> 在linux用户态调用mmap函数可以把文件内容直接映射到内存，这样用户态程序可以像访问<br> 内存一样访问文件。同样，使用mmap也可以把设备的一段IO空间映射到用户态，用户态程序<br> 可以直接访问这个设备的寄存器。当然，要在程序驱动里添加mmap的对应支持。</p>
<ol start="2">
<li>驱动实现</li>
</ol>
<hr>
<p>为了支持把设备的IO空间映射到用户态，驱动里要实现.mmap的回调，struct vm_area_struct<br>会把用户态想要映射到的用户态虚拟地址传到内核。在.mmap回调里需要把这些参数，连同<br>想要映射的实际物理地址传递给remap_pfn_range函数，这个函数帮助建立虚拟地址到物理<br>地址的映射。</p>
<p>下面的例子是在scull驱动里，申请了一段内核态的内存，我们可以通过下面的操作把它映射<br>到用户态。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int scull_mmap(struct file *file, struct vm_area_struct *vma)</span><br><span class="line">&#123;</span><br><span class="line">        unsigned long page = virt_to_phys(scull_device-&gt;mmap_memory);</span><br><span class="line">        unsigned long start = (unsigned long)vma-&gt;vm_start;</span><br><span class="line">        unsigned long size = (unsigned long)(vma-&gt;vm_end - vma-&gt;vm_start);</span><br><span class="line">        vma-&gt;vm_flags |= (VM_IO | VM_LOCKED | VM_DONTEXPAND | VM_DONTDUMP);</span><br><span class="line"></span><br><span class="line">        if (remap_pfn_range(vma, start, page &gt;&gt; PAGE_SHIFT, size, PAGE_SHARED)) &#123;</span><br><span class="line">                printk(KERN_ALERT &quot;remap_pfn_range failed!\n&quot;);</span><br><span class="line">                return -1;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>LDD3上讲，remap_pfn_range只能映射IO空间和系统保留内存，但是上面例子的mmap_memory<br>实际在就是用get_free_page分配的(把get_free_page换成kmalloc也是可以做映射的), 测试<br>的结果是用get_free_page分配的内存也是可以被映射到用户态的。所以，实际上<br>remap_pfn_range也可以把系统内存映射到用户态。</p>
<p>内核用struct vm_area_struct管理进程空间的各个虚拟地址区域。cat /proc/pid/maps<br>可以看到一个进程所有的虚拟地址区域。在下面的例子中，我们可以看到测试程序(read.c)<br>的进程的各个地址区域。可以看到调用mmap创建起来的一个地址区域/dev/scull0.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">estuary:/$ cat /proc/1251/maps</span><br><span class="line">00400000-00401000 r-xp 00000000 00:01 882            /a.out</span><br><span class="line">00410000-00411000 rw-p 00000000 00:01 882            /a.out</span><br><span class="line">ffffae624000-ffffae634000 rw-p 00000000 00:00 0 </span><br><span class="line">ffffae634000-ffffae764000 r-xp 00000000 00:01 629    /lib/aarch64-linux-gnu/libc-2.21.so</span><br><span class="line">ffffae764000-ffffae773000 ---p 00130000 00:01 629    /lib/aarch64-linux-gnu/libc-2.21.so</span><br><span class="line">ffffae773000-ffffae777000 r--p 0012f000 00:01 629    /lib/aarch64-linux-gnu/libc-2.21.so</span><br><span class="line">ffffae777000-ffffae779000 rw-p 00133000 00:01 629    /lib/aarch64-linux-gnu/libc-2.21.so</span><br><span class="line">ffffae779000-ffffae77d000 rw-p 00000000 00:00 0 </span><br><span class="line">ffffae77d000-ffffae799000 r-xp 00000000 00:01 575    /lib/aarch64-linux-gnu/ld-2.21.so</span><br><span class="line">ffffae7a1000-ffffae7a2000 rw-s 00000000 00:01 1385   /dev/scull0</span><br><span class="line">ffffae7a2000-ffffae7a7000 rw-p 00000000 00:00 0 </span><br><span class="line">ffffae7a7000-ffffae7a8000 r--p 00000000 00:00 0      [vvar]</span><br><span class="line">ffffae7a8000-ffffae7a9000 r-xp 00000000 00:00 0      [vdso]</span><br><span class="line">ffffae7a9000-ffffae7aa000 r--p 0001c000 00:01 575    /lib/aarch64-linux-gnu/ld-2.21.so</span><br><span class="line">ffffae7aa000-ffffae7ac000 rw-p 0001d000 00:01 575    /lib/aarch64-linux-gnu/ld-2.21.so</span><br><span class="line">fffff5f89000-fffff5faa000 rw-p 00000000 00:00 0      [stack]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>LDD3</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux dma_map_sg API</title>
    <url>/2021/06/27/Linux-dma-map-sg-API/</url>
    <content><![CDATA[<p>如linux/Documentation/DMA-API-HOW.txt里提到的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">With scatterlists, you map a region gathered from several regions by::</span><br><span class="line"></span><br><span class="line">	int i, count = dma_map_sg(dev, sglist, nents, direction);</span><br><span class="line">	struct scatterlist *sg;</span><br><span class="line"></span><br><span class="line">	for_each_sg(sglist, sg, count, i) &#123;</span><br><span class="line">		hw_address[i] = sg_dma_address(sg);</span><br><span class="line">		hw_len[i] = sg_dma_len(sg);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">where nents is the number of entries in the sglist.</span><br><span class="line"></span><br><span class="line">The implementation is free to merge several consecutive sglist entries</span><br><span class="line">into one (e.g. if DMA mapping is done with PAGE_SIZE granularity, any</span><br><span class="line">consecutive sglist entries can be merged into one provided the first one</span><br><span class="line">ends and the second one starts on a page boundary - in fact this is a huge</span><br><span class="line">advantage for cards which either cannot do scatter-gather or have very</span><br><span class="line">limited number of scatter-gather entries) and returns the actual number</span><br><span class="line">of sg entries it mapped them to. On failure 0 is returned.</span><br><span class="line"></span><br><span class="line">Then you should loop count times (note: this can be less than nents times)</span><br><span class="line">and use sg_dma_address() and sg_dma_len() macros where you previously</span><br><span class="line">accessed sg-&gt;address and sg-&gt;length as shown above.</span><br><span class="line"></span><br><span class="line">To unmap a scatterlist, just call::</span><br><span class="line"></span><br><span class="line">	dma_unmap_sg(dev, sglist, nents, direction);</span><br><span class="line"></span><br><span class="line">Again, make sure DMA activity has already finished.</span><br><span class="line"></span><br><span class="line">.. note::</span><br><span class="line"></span><br><span class="line">	The &#x27;nents&#x27; argument to the dma_unmap_sg call must be</span><br><span class="line">	the _same_ one you passed into the dma_map_sg call,</span><br><span class="line">	it should _NOT_ be the &#x27;count&#x27; value _returned_ from the</span><br><span class="line">	dma_map_sg call.</span><br></pre></td></tr></table></figure>
<p>dma_map_sg对一个sgl做map的时候，返回的map的sge的个数可能是小于输入sgl的sge的个数<br>的，这是因为连续页做了合并操作，在有IOMMU的情况下，也可能是因为IOMMU的存在把一段<br>连续的IOVA映射成了一些离散的物理地址块，而这些离散的物理地址块正是前面的sgl输入,<br>这个连续的IOVA正式dma_map_sg的输出。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>DMA</tag>
        <tag>iommu</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux PCIe DPC analysis</title>
    <url>/2021/07/11/Linux-PCIe-DPC-analysis/</url>
    <content><![CDATA[<p>linux implements PCIe dpc feature as a service, which is registed in PCIe port<br>driver subsystem in drivers/pci/pcie/pcie-dpc.c</p>
<p>to check: if dpc_probe will be called for any port type, as in<br>          struct pcie_port_service_driver dpcdriver, port_type = PCIE_ANY_PORT.</p>
<p>let’s assume dpc_probe will be called for any port type, then the flow of probe:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; INIT_WORK(&amp;dpc-&gt;work, interrupt_event_handler);</span><br><span class="line">--&gt; devm_request_irq(&amp;dev-&gt;device, dev-&gt;irq, dpc_irq, IRQF_SHARED, &quot;pcie-dpc&quot;, dpc);</span><br><span class="line"></span><br><span class="line">    /* set PCI_EXP_DPC_CTL_EN_NONFATAL, 0x6 offset in dpc cap: 0x02</span><br><span class="line">     * PCI_EXP_DPC_CTL_INT_EN</span><br><span class="line">     */</span><br><span class="line">--&gt; pci_write_config_word(pdev, dpc-&gt;cap_pos + PCI_EXP_DPC_CTL, ctl);</span><br></pre></td></tr></table></figure>
<p>so it is quiet easy above, just enable the non-fatal and top interrupt.</p>
<p>In the irq handler:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    /* just read some data from dpc cap, then print info to user, we can</span><br><span class="line">     * see which kind of error happened: unconrrectable, non-fatal, fatal.</span><br><span class="line">     */</span><br><span class="line">--&gt; pci_read_config_word(pdev, dpc-&gt;cap_pos + PCI_EXP_DPC_STATUS, &amp;status);</span><br><span class="line">--&gt; pci_read_config_word(pdev, dpc-&gt;cap_pos + PCI_EXP_DPC_SOURCE_ID, &amp;source);</span><br><span class="line"></span><br><span class="line">    /* run work queue register above, to check: how long to wait */</span><br><span class="line">--&gt; schedule_work(&amp;dpc-&gt;work);</span><br><span class="line"></span><br><span class="line">        /* This handler will try to stop and remove all devices under this port.</span><br><span class="line">         * Then wait link inactive, and set dpc cap&#x27;s status register.</span><br><span class="line">         *</span><br><span class="line">         * If your rp implement dpc, and there is just one ep connnected to</span><br><span class="line">         * rp directly, then this ep will be stopped and removed. when stopping</span><br><span class="line">         * it, pme/aspm functions will be called.</span><br><span class="line">         *</span><br><span class="line">         * If your rp connects a switch, and there are some devices connected</span><br><span class="line">         * to dp of this switch, when this rp </span><br><span class="line">         * under this rp will be stopped and removed.</span><br><span class="line">         */</span><br><span class="line">    --&gt; interrupt_event_handler</span><br><span class="line">            /* If your rp implements dpc, then the parent below should be</span><br><span class="line">             * the subordinate bus. Then for devices under this bus, it will</span><br><span class="line">             * call pci_stop_and_remove_bus_device(dev) to them one by one</span><br><span class="line">             */</span><br><span class="line">        --&gt; struct pci_bus *parent = pdev-&gt;subordinate;</span><br><span class="line">        --&gt; pci_stop_and_remove_bus_device(dev);</span><br><span class="line">                /* below function will be called in a recursive way, so the</span><br><span class="line">                 * first device to be stopped and removed will be the deepest one.</span><br><span class="line">                 * Then one by one, all devices will be stopped and removed.</span><br><span class="line">                 */</span><br><span class="line">            --&gt; pci_stop_bus_device(dev);</span><br><span class="line">                    /* to check: it will call pme/aspm functions in below function */ </span><br><span class="line">                --&gt; pci_stop_dev(dev);</span><br><span class="line">                /* in a same recursive way in below function */</span><br><span class="line">            --&gt; pci_remove_bus_device(dev);</span><br><span class="line">                    /* to check: will call pm functions */</span><br><span class="line">                --&gt; pci_bridge_d3_device_removed(dev);</span><br><span class="line">                    /* all software structures destroy */</span><br><span class="line">                --&gt; pci_destroy_dev(dev);</span><br><span class="line"></span><br><span class="line">            /* here stop maximum 1s to wait Data Link Layer Link Active bit</span><br><span class="line">             * in Link status register to become 0.</span><br><span class="line">             * If this bit is 1, it indicates the DL_Active state ?</span><br><span class="line">             */</span><br><span class="line">        --&gt; dpc_wait_link_inactive(pdev);</span><br><span class="line">            /* set dpc trigger status bit and dpc interrupt status bit in</span><br><span class="line">             * dpc status register.</span><br><span class="line">             */</span><br><span class="line">        --&gt; pci_write_config_word(pdev, dpc-&gt;cap_pos + PCI_EXP_DPC_STATUS,</span><br><span class="line">                    PCI_EXP_DPC_STATUS_TRIGGER | PCI_EXP_DPC_STATUS_INTERRUPT);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux PCIe hotplug arch analysis</title>
    <url>/2021/07/11/Linux-PCIe-hotplug-arch-analysis/</url>
    <content><![CDATA[<p>This analysis is based on v4.8-rc5.</p>
<p>There are two kinds of PCIe hotplug: one is PCIe native hotplug which is implemented<br>just used the codes in linux kernel, second is PCIe hotplug based on ACPI. Here<br>we just analyze the native hotplug.</p>
<p>PCIe hotplug is implemented as a pcie port service, it is registered in pcie port<br>driver in drivers/pci/hotplug/pciehp_core.c:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; pcied_init(void)</span><br><span class="line">    --&gt; pcie_port_service_register(&amp;hpdriver_portdrv)</span><br></pre></td></tr></table></figure>
<p>so system will finally call pciehp_probe in struct pcie_port_service_driver hpdriver_portdrv,<br>main flow of this probe:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pciehp_probe(struct pcie_device *dev)</span><br><span class="line">        /* create struct controller, struct slot, get slot capability and slot status info */</span><br><span class="line">    --&gt; pcie_init(dev);</span><br><span class="line">            /* will register a delay_work in slot:</span><br><span class="line">             * INIT_DELAYED_WORK(&amp;slot-&gt;work, pciehp_queue_pushbutton_work)</span><br><span class="line">             *</span><br><span class="line">             * This will be called, e.g. in hot-insertion, after inserting card</span><br><span class="line">             * and press button and wait 5s.</span><br><span class="line">             */</span><br><span class="line">        --&gt; pcie_init_slot</span><br><span class="line">        /* create struct hotplug_slot, struct hotplug_slot_info, struct hotplug_slot_ops,</span><br><span class="line">         * fill functions in hotplug_slot_ops</span><br><span class="line">         *</span><br><span class="line">         *   controller</span><br><span class="line">         *        +------&gt; slot</span><br><span class="line">         *                   +----&gt; hotplug_slot</span><br><span class="line">         *                               +-------&gt; pci_slot</span><br><span class="line">         *                                         hotplug_slot_ops</span><br><span class="line">         *                                         hotplug_slot_info</span><br><span class="line">         *                   +----&gt; delayed_work</span><br><span class="line">         *                   +----&gt; workqueue_struct</span><br><span class="line">         *</span><br><span class="line">         * then the whole structures will like above.</span><br><span class="line">         *</span><br><span class="line">         * After above structure been built, it will call pci_hp_register</span><br><span class="line">         */</span><br><span class="line">    --&gt; init_slot(ctrl);</span><br><span class="line">            /* add hotplug_slot into pci_hotplug_slot_list */</span><br><span class="line">        --&gt; pci_hp_register(hotplug, ctrl-&gt;pcie-&gt;port-&gt;subordinate, 0, name);</span><br><span class="line">                /* to check: where to init pci_slot */</span><br><span class="line">            --&gt; pci_create_slot</span><br><span class="line">            --&gt; list_add(&amp;slot-&gt;slot_list, &amp;pci_hotplug_slot_list);</span><br><span class="line">                /* expose pci_slot related file in sys-fs */</span><br><span class="line">            --&gt; fs_add_slot(pci_slot);</span><br><span class="line">        /* register irq */</span><br><span class="line">    --&gt; pcie_init_notification(ctrl);</span><br><span class="line">            /* when request_irq, will let struct controller to be its private data */</span><br><span class="line">        --&gt; pciehp_request_irq(ctrl)</span><br><span class="line">            /* hardware set here ?? enable some bits in slot control reg,</span><br><span class="line">             *</span><br><span class="line">             * here, in pcie_write_cmd function, we need wait former cmd finished,</span><br><span class="line">             * if needed, we should check the details of this wait function.</span><br><span class="line">             */</span><br><span class="line">        --&gt; pcie_enable_notification(ctrl);</span><br><span class="line">        /* if there is a device on the slot */</span><br><span class="line">    --&gt; pciehp_enable_slot(slot);</span><br><span class="line">            /* after check the status of device, if needed, will enable the device</span><br><span class="line">             *</span><br><span class="line">             * As we already implements linkup in UEFI, if we enable PCIe controller</span><br><span class="line">             * driver, it will not call board_added. If we do not enable PCIe controller,</span><br><span class="line">             * it will call board_added here.</span><br><span class="line">             */</span><br><span class="line">        --&gt; board_added(struct slot *p_slot)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pcie_isr</span><br><span class="line">        /* to check: where to define the handle of below queue */</span><br><span class="line">    --&gt; wake_up(&amp;ctrl-&gt;queue);</span><br><span class="line">        /* create related work_struct, then put work_struct to the work_queue in struct slot,</span><br><span class="line">         * Here, we may have: Attention Button Pressed, Presence Detect Changed</span><br><span class="line">         * Power Fault Detected, Link up/down check.</span><br><span class="line">         *</span><br><span class="line">         * same handle will be added to work_struct: interrupt_event_handler, </span><br><span class="line">         * in above function, it handle all cases.</span><br><span class="line">         *</span><br><span class="line">         * We can not analyze all cases, the key point to understand the standard</span><br><span class="line">         * hot-insertion and hot-removal is to consider the whole flow together</span><br><span class="line">         * with INIT_DELAYED_WORK(&amp;slot-&gt;work, pciehp_queue_pushbutton_work) in</span><br><span class="line">         * pcie_init mentioned above.</span><br><span class="line">         *</span><br><span class="line">         * when you analyze the different cases, please pay great attention on</span><br><span class="line">         * struct slot -&gt; state, which indicates the current status of hot-plug</span><br><span class="line">         * state machine. And we should get clear the map between these state</span><br><span class="line">         * and real physical state.</span><br><span class="line">         *</span><br><span class="line">         * STATIC_STATE: at this state, pcie ep already runs fine or ep already</span><br><span class="line">         * had been remove the system.</span><br><span class="line">         *</span><br><span class="line">         * BLINKINGON_STATE: in hot-insertion, after step1, card insertion; step2,</span><br><span class="line">         * press button, power indicator led will blinking 5s. This state just </span><br><span class="line">         * indicates this.</span><br><span class="line">         *</span><br><span class="line">         * BLINKINGOFF_STATE: same as above, but in hot-removal.</span><br><span class="line">         *</span><br><span class="line">         * POWERON_STATE: in hot-insertion, in the 5s of power indicator led blinking,</span><br><span class="line">         * if we do not press button again, it will try to bring the ep power on,</span><br><span class="line">         * linkup, then add ep to system. This state just shows this.</span><br><span class="line">         *</span><br><span class="line">         * POWEROFF_STATE: same as above, but in hot-removal.</span><br><span class="line">         */</span><br><span class="line">    --&gt; pciehp_queue_interrupt_event(slot, INT_BUTTON_PRESS);</span><br></pre></td></tr></table></figure>

<p>If we only see the flow of “interrupt_event_handler: INT_PRESENCE_ON”, it can not<br>match the stardard hot-inertion flow, which firstly inserting a PCIe card and then<br>pressing the attention button.</p>
<p>If in a system, when a pcie card been plugged in, a hotplug interrupt will be<br>triggered and presence detect change bit set 1 in slot status register.<br>This will trigger the operation in flow 1 below. you can see that now the whole<br>hot-insertion process has no relationship with attention button press.</p>
<p>pcie device hot insertion, trigger Presence Detect interrupt:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; interrupt_event_handler: INT_PRESENCE_ON</span><br><span class="line">    --&gt; handle_surprise_event</span><br><span class="line">            /* if presence detect bit in slot status set 1 */</span><br><span class="line">        --&gt; pciehp_queue_power_work(..., ENABLE_REQ)</span><br><span class="line">                /* will create work_queue, and add it to struct slot&#x27;s wq,</span><br><span class="line">                 * the handler function is pciehp_power_thread</span><br><span class="line">                 *</span><br><span class="line">                 * above handler will be called by work queue in slot.</span><br><span class="line">                 */</span><br><span class="line">            --&gt; pciehp_power_thread: case ENABLE_REQ</span><br><span class="line">                --&gt; pciehp_enable_slot(p_slot)</span><br><span class="line">                        /* before doing this, we will check slot status */</span><br><span class="line">                    --&gt; board_added</span><br><span class="line">                        --&gt; pciehp_green_led_blink</span><br><span class="line">                                /* send green led blink */</span><br><span class="line">                            --&gt; pcie_write_cmd_nowait</span><br><span class="line">                            /* check linkup, here maximum wait 1s */</span><br><span class="line">                        --&gt; pciehp_check_link_status</span><br><span class="line">                            /* to check resource assign if has problem */</span><br><span class="line">                        --&gt; pciehp_configure_device</span><br><span class="line">                        --&gt; pciehp_green_led_on</span><br><span class="line">                --&gt; p_slot-&gt;state = STATIC_STATE;</span><br></pre></td></tr></table></figure>

<p>Let’s see standard pcie hot-removal:<br>button pressed, trigger Attention Button Pressed interrupt</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; interrupt_event_handler: INT_BUTTON_PRESS</span><br><span class="line">        /* In this function, it depends on hot-plug state machine to decide</span><br><span class="line">         * what to do next. Here assuming, power on, we operate hot-removal.</span><br><span class="line">         */</span><br><span class="line">    --&gt; handle_button_press_event(p_slot)</span><br><span class="line">            /* if pcie card is running fine, we are in STATIC_STATE,</span><br><span class="line">             * and at this time, we check power, it is power on, so we know</span><br><span class="line">             * this operation will be a hot-removal, print:</span><br><span class="line">             * &quot;powering off due to button press&quot;</span><br><span class="line">             *</span><br><span class="line">             * Then we change state to BLINKINGOFF_STATE, which means</span><br><span class="line">             * PCIe controller/card will wait 5s to turn off power.</span><br><span class="line">             */</span><br><span class="line">            /* blink power indicator to show power unstable */</span><br><span class="line">        --&gt; pciehp_green_led_blink(p_slot)</span><br><span class="line">            /* turn off attention indicator to show everything fine */</span><br><span class="line">        --&gt; pciehp_set_attention_status(p_slot, 0)</span><br><span class="line">            /* delay work queue 5s to wait if we will cancel this operation */</span><br><span class="line">        --&gt; queue_delayed_work(p_slot-&gt;wq, &amp;p_slot-&gt;work, 5*HZ)</span><br></pre></td></tr></table></figure>
<p>As [1] page 394, step 4, it said, hotplug driver should let the driver of PCIe card<br>stop, and card driver should handle stop the data/interrupt. We should check<br>pci_stop_and_remove_bus_device below to get the details.</p>
<p>If in 5s, we do not press button, it will eventually call pciehp_queue_pushbutton_work<br>which initialized in pcie_init.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pciehp_queue_pushbutton_work</span><br><span class="line">        /* when hot-removal, state is BLINKINGOFF_STATE */</span><br><span class="line">    --&gt; pciehp_queue_power_work(p_slot, DISABLE_REQ)</span><br><span class="line">            /* POWEROFF_STATE */</span><br><span class="line">        --&gt; pciehp_power_thread</span><br><span class="line">            --&gt; pciehp_disable_slot</span><br><span class="line">                --&gt; remove_board(p_slot)</span><br><span class="line">                    --&gt; pciehp_unconfigure_device(p_slot)</span><br><span class="line">                            /* the point here is how to removal the ep specific</span><br><span class="line">                             * work. If needed, we should check the details here.</span><br><span class="line">                             */</span><br><span class="line">                        --&gt; pci_stop_and_remove_bus_device</span><br></pre></td></tr></table></figure>
<p>above is the code map of standard hot-removal.</p>
<p>If at any time, the pcie card has been surprise hot-removed, linux kernel can<br>handle it by:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; interrupt_event_handler: INT_PRESENCE_OFF</span><br><span class="line">    --&gt; handle_surprise_event</span><br><span class="line">        --&gt; pciehp_queue_power_work(p_slot, DISABLE_REQ)</span><br><span class="line">                /* add this to slot&#x27;s work queue */</span><br><span class="line">            --&gt; pciehp_power_threa</span><br><span class="line">                --&gt; pciehp_disable_slot(p_slot)</span><br><span class="line">                    --&gt; remove_board(p_slot)</span><br><span class="line">                --&gt; p_slot-&gt;state = STATIC_STATE;</span><br></pre></td></tr></table></figure>
<p>but the point here is how to handle ep specific work, need more investigation here.</p>
<p>Let’s see standard pcie hot-insertion:<br>If we consider pciehp_queue_pushbutton_work in the flow of standard hot-insertion,<br>it is easy to get the work flow analysis like standard hot-removal above.</p>
<p>The point here is that only if the power of slot is off before inserting ep, can<br>we use the button to trigger standard hot-insertion flow. If power of slot is on,<br>once a ep has been inserted in slot, it will trigger presence ditect change interrupt,<br>then code in kernel can handle following part of hot-insertion, we indeed do not<br>need to press a button. We just show this in above section.</p>
<p>Resource assignment problem in linux hotplug:<br>kernel use below code to add a ep to system.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--&gt; pciehp_enable_slot</span><br><span class="line">    --&gt; board_added(p_slot)</span><br><span class="line">        --&gt; pciehp_configure_device</span><br><span class="line">            ...</span><br><span class="line">            --&gt; pci_hp_add_bridge(dev)</span><br></pre></td></tr></table></figure>
<p>But how does linux assign bus number ahead for a hotplug capable slot ?<br>we should pay attention to a value: max in pci enumeration code.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     --&gt; pci_scan_child_bus</span><br><span class="line">  --&gt; if (bus-&gt;self &amp;&amp; bus-&gt;self-&gt;is_hotplug_bridge &amp;&amp; pci_hotplug_bus_size)</span><br><span class="line">if (max - bus-&gt;busn_res.start &lt; pci_hotplug_bus_size - 1)</span><br><span class="line">	max = bus-&gt;busn_res.start + pci_hotplug_bus_size - 1;</span><br></pre></td></tr></table></figure>
<p>It will check if this is a hotplug bridge and assign more pci buses for this bridge<br>according to pci_hotplug_bus_size which can be set in command line of kernel.</p>
<h2 id="note"><a href="#note" class="headerlink" title="note"></a>note</h2><p>struct slot -&gt; state:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(to finish)</span><br><span class="line">                         +-------------------+</span><br><span class="line">                         | BLINKINGOFF_STATE |</span><br><span class="line">                         +-------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">+--------------+ slot enable +------------+  slot disable +--------------+</span><br><span class="line">|POWERON_STATE | ----------&gt; |STATIC_STATE|  &lt;----------- |POWEROFF_STATE|</span><br><span class="line">+--------------+             +------------+               +--------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                          +----------------+</span><br><span class="line">                          |BLINKINGON_STATE|</span><br><span class="line">                          +----------------+</span><br></pre></td></tr></table></figure>

<p>from PCIe spec, power indicator should be a green led:<br>off means power off, so we can insertion/removal<br>on means power on, so we can not insertion/removal<br>blinking means, it is powering up/down or a feedback from attention button pressed<br>                or hot-plug operation is initiated through software.</p>
<p>attention indicator should be a yellow or amber led:<br>on means, should be pay attention to<br>off means, everything fine</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>PCI.EXPRESS系统体系结构标准教材].(美)Pavi.Budruk,Don.Anderson,Tom.Shanley.扫描版.pdf</li>
</ol>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux/UEFI memory block note</title>
    <url>/2021/07/05/Linux-UEFI-memory-block-note/</url>
    <content><![CDATA[<p>D05 CPU and memory hardware topology</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|||| +--------+  |||| ||||  +---------+  ||||</span><br><span class="line">|||| |        |  |||| ||||  |         |  ||||</span><br><span class="line">|||| |  CPU   |  |||| ||||  |   CPU   |  ||||</span><br><span class="line">|||| |        |  |||| ||||  |         |  ||||</span><br><span class="line">|||| +--------+  |||| ||||  +---------+  ||||</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> Dim A        Dim B</span><br><span class="line">   |     | |    |   | |</span><br><span class="line">   +---&gt; | | &lt;--+   | |</span><br><span class="line">         | |        | |</span><br><span class="line">         | |        | |</span><br><span class="line"> +-------+-+--------+-+---------+</span><br><span class="line"> |      ++-+-+     ++-+-+       | &lt;-- CPU</span><br><span class="line"> |      |ddrc|     |ddrc|       |</span><br><span class="line"> |     ++----+-+  ++----+-+     |</span><br><span class="line"> |     |       |  |       |     |</span><br><span class="line"> |     |       +--+       |     |</span><br><span class="line"> |     | C Die |  | C Die |     |</span><br><span class="line"> |     |       +--+       |     |</span><br><span class="line"> |     |       |  |       |     |</span><br><span class="line"> |     ++----+-+  ++----+-+ ... |</span><br><span class="line"> |      |ddrc|     |ddrc|       |</span><br><span class="line"> |      ++-+-+     ++-+-+       |</span><br><span class="line"> +-------+-+--------+-+---------+</span><br><span class="line">         | |        | |</span><br><span class="line">         | |        | |</span><br><span class="line">         | |        | |</span><br><span class="line">         | |        | |</span><br></pre></td></tr></table></figure>
<p>ACPI SRAT table will discribe this topology. In SRAT table CPU will be discribed<br>statically, but memory block will be discribed drynamically. UEFI will detect<br>the DDR in the Dims and report memory blocks in SRAT table to OS.</p>
<p>The logic of how UEFI defines a memmroy block is based on specific UEFI.<br>Now D05 UEFI will assigned memory range for each ddrc, e.g. 0 ~ 32G. ddr plugging<br>in specific dim will get the cpu address from its ddrc.</p>
<p>D05 UEFI can connect two memory rangs of different ddrc to one memory block and<br>report this to OS, if these two memory ranges’ ddr is contiguous:</p>
<p>  contiguous:      ddrc 0 ~ 32G, plugging 2 16G ddr.</p>
<p>  not contiguous:  ddrc 0 ~ 32G, plugging 1 16G ddr.</p>
<p>In D05, run dmesg | grep SRAT:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[    0.000000] ACPI: SRAT 0x00000000397F0000 000578 (v03 HISI   HIP07    00000000 INTL 20151124)</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10000 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10001 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10002 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10003 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10100 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10101 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10102 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10103 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10200 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10201 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10202 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10203 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10300 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10301 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10302 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 0 -&gt; MPIDR 0x10303 -&gt; Node 0</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30000 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30001 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30002 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30003 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30100 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30101 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30102 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30103 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30200 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30201 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30202 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30203 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30300 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30301 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30302 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 1 -&gt; MPIDR 0x30303 -&gt; Node 1</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50000 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50001 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50002 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50003 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50100 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50101 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50102 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50103 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50200 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50201 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50202 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50203 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50300 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50301 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50302 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 2 -&gt; MPIDR 0x50303 -&gt; Node 2</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70000 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70001 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70002 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70003 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70100 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70101 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70102 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70103 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70200 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70201 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70202 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70203 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70300 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70301 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70302 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: NUMA: SRAT: PXM 3 -&gt; MPIDR 0x70303 -&gt; Node 3</span><br><span class="line">[    0.000000] ACPI: SRAT: Node 0 PXM 0 [mem 0x00000000-0x3fffffff]</span><br><span class="line">[    0.000000] ACPI: SRAT: Node 1 PXM 1 [mem 0x1400000000-0x17ffffffff]</span><br><span class="line">[    0.000000] ACPI: SRAT: Node 0 PXM 0 [mem 0x1000000000-0x13ffffffff]</span><br><span class="line">[    0.000000] ACPI: SRAT: Node 3 PXM 3 [mem 0x8800000000-0x8bffffffff]</span><br><span class="line">[    0.000000] ACPI: SRAT: Node 2 PXM 2 [mem 0x8400000000-0x87ffffffff]</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux irq domain</title>
    <url>/2021/07/17/Linux-irq-domain/</url>
    <content><![CDATA[<p> linux中断管理中有irq domain的概念。顾名思义irq domain是针对有中断控制功能的IP模<br> 块中断管理域，是一个软件概念。以linux kernel中的GPIO举个例子。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pins  +-----+     +-----+     +-----+</span><br><span class="line">    ——|     |     |     |     |     |</span><br><span class="line">    ——|     |----&gt;|     |----&gt;|     |</span><br><span class="line">    ——|     |     |     |     |     |</span><br><span class="line">    ——|     |     |     |     |     |</span><br><span class="line">      +-----+     +-----+     +-----+</span><br><span class="line">       GPIO         GIC       CPU core</span><br><span class="line">     controller    </span><br></pre></td></tr></table></figure>
<p> GPIO，GIC，CPU core的硬件示意图如上。 一个GPIO控制器有多个输入输出管脚, 可以输入<br> 输出高低电平信号，也可以接收外部信号作为中断输入。GPIO控制器把多个外部输入的中断<br> 信号转成一个中断信号，通过一条中断线报给GIC。CPU core接收到GIC上报的中断后, 通过<br> 查询GIC的相关寄存器得到中断来自GIC的哪个输入管脚，再通过查询GPIO控制器的相关寄存<br> 器得到中断真正来自GPIO的哪个输入管脚。然后调用相应的中断处理程序。</p>
<p> linux内核中断管理的最核心数据结构是struct irq_desc, 为了使用irq号申请中断，中断<br> 管理系统要为每个中断建立相应的struct irq_desc结构(之后驱动可以使用request_irq()在<br> 相应的struct irq_desc中注册中断处理程序)。</p>
<p> 上面的硬件框图中，GPIO controller到GIC的中断信号线是在制作Soc的时候就连死的，Soc<br> 手册中会有该中断线的中断号。在驱动程序中可以直接使用request_irq()直接注册中断处理<br> 函数。当然在该中断处理函数中也可以直接去读GPIO controller中的中断相关的寄存器，查<br> 看实际的中断来自哪个GPIO输入管脚，然后调用相应的函数去处理。这样也就没有irq domain<br> 什么事情了。</p>
<p> 但是如果像上面这样做，会在GPIO controller的驱动代码中加入GPIO所接下级设备的中断<br> 处理函数。破坏了程序的结构。合理的处理方式是GPIO controller只需要暴露给下级设备输入<br> 管脚号，下级设备在其自身的驱动程序中，通过管脚号调用GPIO驱动提供的接口，获得相应的<br> irq号，然后通过request_irq()注册自己的中断处理函数。基于这样的逻辑，需要在GPIO驱动<br> 中加入一个struct irq_domain的结构，该结构的作用是：1. 为GPIO的各个输入管脚，在中断<br> 管理系统中建立相应的struct irq_desc。2. 建立一个输入管脚号到irq号的映射表。</p>
<p> 下面分析一段具体的代码，以内核代码linux/drivers/gpio/gpio-mvebu.c为例：<br> 在struct mvebu_gpio_chip中包含struct irq_domain *domain用来管理GPIO controller的<br> irq domain。在probe函数中<br>    mvchip-&gt;irqbase = irq_alloc_descs(-1, 0, ngpios, -1);<br> 分配了ngpios个struct irq_desc结构体，返回第一个结构对应的irq号。</p>
<pre><code>mvchip-&gt;domain = irq_domain_add_simple();
</code></pre>
<p> 建立并初始化irq_domain结构, 并建立从管脚号到中断号的映射。</p>
<pre><code> mvebu_gpio_to_irq();
</code></pre>
<p> 通过管脚号找到中断号。</p>
<p> 本文以一个GPIO控制器的例子介绍了linux内核中irq domain的基本逻辑。只要是相似的地方都<br> 可以引入irq domain的处理方式。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux kernel crypto scomp/acomp arch analysis</title>
    <url>/2021/06/28/Linux-kernel-crypto-scomp-acomp-arch-analysis/</url>
    <content><![CDATA[<ol>
<li>General introduce of linux kernel crypto compression API</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_register_alg()</span><br><span class="line">crypto_alloc_comp()</span><br><span class="line"></span><br><span class="line">crypto_register_scomp() </span><br><span class="line">crypto_alloc_acomp()</span><br><span class="line"></span><br><span class="line">crypto_register_acomp() </span><br><span class="line">crypto_alloc_acomp()</span><br></pre></td></tr></table></figure>
<p>As showed in above, we can only use crypto_register_alg() to register a general<br>crypto compress alg, and use crypto_alloc_comp() to get the crypto_comp to do<br>compression/decompression. We can not register a new compress alg by this old<br>general API in mainline kernel now and in future.</p>
<p>Crypto system also supports to register a sync compress alg using<br>crypto_register_scomp, and use crypto_register_acomp to register an async<br>compress alg. Now we use only crypto_alloc_acomp() to get crypto_scomp/crypto_acomp.<br>There is no interface to get crypto_scomp, but crypto_alloc_acomp().</p>
<p>For the detail usage of scomp/acomp, we can have an example in kenrel/crypto/testmgr.c<br>alg_test_comp().</p>
<p>Currently all compression/decompression algorithms are registered to kernel<br>crypto system by crypto_register_alg() or crypto_register_scomp(). And all users<br>in kernel use crypto_alloc_comp() to get a compression/decompression context.</p>
<p>In future, as mentioned in above, all compression/decompression alg should<br>register to crypto by scomp/acomp.</p>
<ol start="2">
<li>comp</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_alloc_comp()</span><br><span class="line">crypto_comp_compress()/crypto_comp_decompress()</span><br><span class="line">crypto_free_comp()</span><br></pre></td></tr></table></figure>
<p>We can use above APIs to do compression/decompress. Crypto subsystem has done<br>little about them. So When we add a hardware/software implementation for these<br>APIs, there is little limitation about them.</p>
<ol start="3">
<li>Scomp</li>
</ol>
<hr>
<p>We should use acomp interface to use scomp, they are as below:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_alloc_acomp()</span><br><span class="line">acomp_request_alloc()</span><br><span class="line">acomp_request_set_params()</span><br><span class="line">acomp_request_set_callback()</span><br><span class="line">crypto_init_wait()</span><br><span class="line">crypto_wait_req()</span><br><span class="line">crypto_acomp_compress()/crypto_acomp_decompress()</span><br><span class="line">acomp_request_free();</span><br><span class="line">crypto_free_acomp()</span><br></pre></td></tr></table></figure>
<p>For the view of user, acomp APIs offer scatterlists to store input/output data.</p>
<p>crypto scomp has internal buffers: when creating a scomp context, it allocates<br>a input buffer and a output buffer in every cpu core, each buffer is 128KB.</p>
<p>scomp arch first copies data in scatterlist into above internal input buffer,<br>then call compression/decompression inplementations to do real work. As one<br>cpu core use the its own input/output buffer, so before doing real compression/<br>decompression work, scomp arch will call get_cpu to disable scheduling, otherwise<br>another scomp job may run in the same cpu core, which will break the data in<br>input/output buffer. This means struct scomp_alg’s compress/decompress can not<br>do scheduling in themselves.</p>
<ol start="4">
<li>Acomp</li>
</ol>
<hr>
<p>We also use acomp interface to use acomp, they are as above.<br>As it is an async interface, the difference from scomp is user can offer a<br>callback to software/hardware implementation, which can call the callback after<br>compression/decompression is done.</p>
<p>The callback is set by acomp_request_set_callback(), and it will be passed to<br>compression/decompression process by struct acomp_req.</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>crypto</tag>
        <tag>ZIP</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux kernel PCI is_physfn的逻辑</title>
    <url>/2021/06/27/Linux-kernel-PCI-is-physfn%E7%9A%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>Linux内核里struct pci_dev里有一个叫is_physfn的域段, 从名字上来看，这个域段可以<br>用来表示一个pci设备是不是PF。对应的有一个is_virtfn的域段。</p>
<p>显然在驱动里如果有的操作需要区分PF和VF, 我们可以用这个域段作为判断依据。但是，<br>实际上如果你去看pci代码的实现，他的语义要以SRIOV cap存在为前提的。</p>
<p>他的调用关系是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_iov_init</span><br><span class="line">  +-&gt; pci_find_ext_capability(dev, PCI_EXT_CAP_ID_SRIOV)</span><br><span class="line">    +-&gt; sriov_init</span><br><span class="line">      +-&gt; dev-&gt;is_physfn = 1</span><br></pre></td></tr></table></figure>

<p>他的语义是pci设备只有在SRIOV cap存在的时候，才有区分PF和VF的必要, 才会去设定<br>is_physfn。</p>
<p>所以，如果我们用一套驱动同时支持PF和VF, 需要区分PF和VF的操作的时候，如果这时<br>系统里有SRIOV cap时，依然可以用is_phyfn这个标记。当系统里没有SRIOV cap时，is_physfn<br>和is_virtfn都不会设置，这时判断就会错。这样的场景发生在PF和VF公用一套驱动，而且<br>VF直通到guest里工作之时。</p>
<p>这种情况下，我们一般要在设备驱动里增加新的标记位，明确的表示这是一个PF还是一个VF。<br>这个标记位只和设备相关，和SRIOV cap无关。</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux rsync备份数据</title>
    <url>/2021/07/17/Linux-rsync%E5%A4%87%E4%BB%BD%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<p> 需要备份的数据在服务器上，这里叫做server。把数据备份到client上，这里也叫<br> backup server。下面是具体的步骤：</p>
<p>步骤：<br>0. run “sudo apt-get install rsync”, if do not have one</p>
<ol>
<li>configure /etc/rsyncd.conf in server, need to touch a file yourself.</li>
<li>configure /home/***/security/rsync.pass in server indicating client’s<br>user_name and key.</li>
<li>configure /etc/default/rsync: RSYNC_ENABLE=true in server</li>
<li>run “sudo /etc/init.d/rsync start” in server</li>
<li>run “rsync -vzrtopg –progress user_name@server_ip::test /home/test” in client<br>(backup server) to backup directory indicating in [test] in /etc/rsyncd.conf<br>in server to /home/test in client, using user user_name</li>
</ol>
<p>说明：</p>
<ol>
<li>遇到这样的错误”auth failed on module xxx”, 可以查看：<br><span class="exturl" data-url="aHR0cDovL2Jsb2cuc2luYS5jb20uY24vcy9ibG9nXzRkYTA1MWE2MDEwMWg4YW0uaHRtbA==">http://blog.sina.com.cn/s/blog_4da051a60101h8am.html<i class="fa fa-external-link-alt"></i></span></li>
<li>配置中遇到错误，可以在/var/log/rsyncd.log查看log记录</li>
<li>/etc/rsyncd.conf中的配置([test]), 可以参考：<br><span class="exturl" data-url="aHR0cDovL3d3dy5pdGV5ZS5jb20vdG9waWMvNjA0NDM2">http://www.iteye.com/topic/604436<i class="fa fa-external-link-alt"></i></span></li>
<li>在ubuntu 14.04上，/etc/rsyncd.conf中的pid file为：/var/run/rsyncd.pid</li>
<li>步骤1～4在服务器上完成，其中user_name是client执行备份命令时的用户名，key是密码</li>
<li>步骤5说明在client上应该用什么样的命令备份服务器上的目录，user_name是步骤2中<br>的user_name, server_ip是服务器的ip, test是在/etc/rsyncd.conf中的[test],<br>/home/test表示把[test]中指示的目录备份到client的/home/test下</li>
<li>具体可以根据需求把步骤5中的命令在每天的某个时候定时执行，这样每天的数据都得到了备份</li>
</ol>
]]></content>
      <tags>
        <tag>运维</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux kernel poll</title>
    <url>/2021/06/20/Linux-kernel-poll/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* fs/select.c */</span><br><span class="line">SYSCALL_DEFINE3(poll, struct pollfd __user *, ufds, unsigned int, nfds,</span><br><span class="line">		int, timeout_msecs)</span><br><span class="line">  +-&gt; do_sys_poll</span><br><span class="line">    +-&gt; do_poll(head, &amp;table, end_time)</span><br><span class="line"></span><br><span class="line">    /* poll的主流程在do_poll这个函数里。可以看到这个函数是一个大for循环，基本逻辑是：*/</span><br><span class="line"></span><br><span class="line">    for () &#123;</span><br><span class="line">	/*</span><br><span class="line">	 * poll系统调用可能是poll一组fd，所以这里循环调用这组fd对应的底层poll</span><br><span class="line">	 * 回调函。以uacce驱动为例, 驱动的poll回调函数里，一般先调用poll_wait</span><br><span class="line">	 * 把自己挂到一个等待队列上，注意这个操作就是在等待队列的链表上加上一个</span><br><span class="line">	 * entry, 函数会马上返回，这之后会检测一下是否已经poll到数据。注意，真正</span><br><span class="line">	 * 把当前进程睡眠的点在下面的poll_schedule_timeout函数处，所以如果第一次</span><br><span class="line">	 * poll的时候已经有任务完成，程序走到下面if count的地方会直接返回，不会</span><br><span class="line">	 * 睡眠。</span><br><span class="line">	 */</span><br><span class="line">	for () &#123;</span><br><span class="line">		do_pollfd();</span><br><span class="line">		  +-&gt; vfs_poll();</span><br><span class="line">		    +-&gt; file-&gt;f_op-&gt;poll(file, pt);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	/* poll到有event发生或者超时就退出大循环 */</span><br><span class="line">	if (count || timed_out)</span><br><span class="line">		break;</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * 睡眠等待event发生，以uacce驱动为例，当硬件有任务完成的时候，会在中断</span><br><span class="line">	 * 处理中唤醒等待队列上的进程。进程被唤醒后，从这里继续执行，在上面的</span><br><span class="line">	 * 内部for循环里依次调用各个fd的poll函数查找有event发生的fd。</span><br><span class="line">	 */</span><br><span class="line">	poll_schedule_timeout();</span><br><span class="line"></span><br><span class="line">    &#125;;</span><br></pre></td></tr></table></figure>

<p>uacce驱动里的poll回调函数:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static __poll_t uacce_fops_poll(struct file *file, poll_table *wait)</span><br><span class="line">&#123;</span><br><span class="line">	struct uacce_queue *q = file-&gt;private_data;</span><br><span class="line">	struct uacce_device *uacce = q-&gt;uacce;</span><br><span class="line"></span><br><span class="line">	poll_wait(file, &amp;q-&gt;wait, wait);</span><br><span class="line">	if (uacce-&gt;ops-&gt;is_q_updated &amp;&amp; uacce-&gt;ops-&gt;is_q_updated(q))</span><br><span class="line">		return EPOLLIN | EPOLLRDNORM;</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux pin page测试</title>
    <url>/2021/06/20/Linux-pin-page%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<p>我们写一个pin page的代码观察pin page系统的具体行为。这个测试代码分为两部分，一部分<br>是一个内核驱动，一部分是一个用户态的测试代码。</p>
<p>内核驱动的代码在：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL3RyZWUvbWFzdGVyL3BhZ2VfcGlu">https://github.com/wangzhou/tests/tree/master/page_pin<i class="fa fa-external-link-alt"></i></span><br>page_pin_test.c, 用户态代码在相同目录的u_page_pin.c里。</p>
<p>内核驱动暴露一个字符设备到用户态，用户态测试代码可以通过这个字符设备的ioctl接口<br>做pin page的操作。这个操作把用户态申请的VA对应的物理地址pin住，使得对应的物理<br>地址常驻内存，而且不会在不同numa节点之间迁移。</p>
<p>对应的用户态测试代码先用mmap申请一段匿名页，然后调用上述字符设备的ioctl接口把这<br>段匿名页pin住。</p>
<p>pin page的过程会触发缺页流程然后拿到具体的物理页。可以在pin ioctl的前后分别加上<br>getrusage()统计系统缺页的次数, 我们用struct rusage里的ru_minflt统计建立物理页时<br>的缺页流程。使用如上目录中的u_uacce_pin_page.c可以看到pin ioctl前后的的ru_minflt<br>差值正好就是mmap申请内存页的数目。</p>
<p>我们使用虚拟机观察内存迁移的情况。具体的虚拟机启动参数如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./qemu/aarch64-softmmu/qemu-system-aarch64 -machine virt,gic-version=3,iommu=smmuv3 \</span><br><span class="line">-bios ./QEMU_EFI.fd \</span><br><span class="line">-enable-kvm -cpu host -m 4G \</span><br><span class="line">-smp 8 \</span><br><span class="line">-numa node,nodeid=0,mem=2G,cpus=0-3 \</span><br><span class="line">-numa node,nodeid=1,mem=2G,cpus=4-7 \</span><br><span class="line">-kernel ./linux-kernel-warpdrive/arch/arm64/boot/Image \</span><br><span class="line">-initrd ~/repos/buildroot/output/images/rootfs.cpio.gz -nographic -append \</span><br><span class="line">&quot;rdinit=init console=ttyAMA0 earlycon=pl011,0x9000000 acpi=force&quot; \</span><br><span class="line">-device virtio-9p-pci,fsdev=p9fs,mount_tag=p9 \</span><br><span class="line">-fsdev local,id=p9fs,path=p9root,security_model=mapped</span><br></pre></td></tr></table></figure>
<p>如上，我们给虚拟机配置了两个numa节点，一个numa节点配置了2G内存。在u_uacce_pin_page.c<br>里再启动一个线程，并把第二个线程绑定到cpu4，就是numa node1上跑。主线程先分配内存<br>然后写内存，过10s后，第二个线程写同样的地址。分别测试pin page和没有pin page的情况，<br>并用numastat的跟踪查看：(下面a.out是u_uacce_pin_page.c编译出来的可执行程序)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">./a.out &amp;</span><br><span class="line">PID=`pidof a.out`</span><br><span class="line"></span><br><span class="line">for i in `seq 10`</span><br><span class="line">do</span><br><span class="line">	numastat -p $PID</span><br><span class="line">	sleep 2</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">kill -9 $PID</span><br></pre></td></tr></table></figure>
<p>可以看出，在有pin page的时候，内存始终在一个numa节点上。在没有pin page的时候，<br>内存可能是一开始在numa node0上，但是10s后被迁移到numa node1上。这个是因为在没有<br>pin page时，内核会根据实际使用内存的cpu，动态的调整实际物理内存的分布。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Per-node process memory usage (in MBs) for PID 417 (a.out)</span><br><span class="line">                           Node 0          Node 1           Total</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         0.01            0.00            0.01</span><br><span class="line">Stack                        0.00            0.00            0.01</span><br><span class="line">Private                      0.57            0.00            0.57</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                        0.58            0.00            0.59</span><br><span class="line"></span><br><span class="line">Per-node process memory usage (in MBs) for PID 417 (a.out)</span><br><span class="line">                           Node 0          Node 1           Total</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         0.01            0.00            0.01</span><br><span class="line">Stack                        0.00            0.00            0.01</span><br><span class="line">Private                      0.57            0.00            0.57</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                        0.58            0.01            0.59</span><br><span class="line"></span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">Per-node process memory usage (in MBs) for PID 417 (a.out)</span><br><span class="line">                           Node 0          Node 1           Total</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         0.01            0.00            0.01</span><br><span class="line">Stack                        0.00            0.00            0.01</span><br><span class="line">Private                      0.17            0.40            0.57</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                        0.18            0.41            0.59</span><br><span class="line"></span><br><span class="line">Per-node process memory usage (in MBs) for PID 417 (a.out)</span><br><span class="line">                           Node 0          Node 1           Total</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         0.01            0.00            0.01</span><br><span class="line">Stack                        0.00            0.00            0.01</span><br><span class="line">Private                      0.17            0.40            0.57</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                        0.18            0.41            0.59</span><br><span class="line"></span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux kernel kthread</title>
    <url>/2021/06/28/Linux-kernel-kthread/</url>
    <content><![CDATA[<p>Linux kernel has kthread_* APIs to create/stop thread in kernel.<br>You can use kthread_run to create a thread and put it into running.<br>Normally, after thread function running over, created thread will stop. However,<br>what is the logic of the stop of thread. We can write a small driver to test it.</p>
<p>In test case 1 beblow, we create a kthread which is alway running. We will find<br>that kthread_stop can not stop it.</p>
<p>In test case 2 below, we use kthread_stop to stop a thread which is created<br>righ now. You can find that the “created thread” will never put into running,<br>and kthread_stop will return -4.</p>
<p>In test case 3 below, created thread will use thread_should_stop to check<br>if it should stop. If it finds it should stop, it will go to return by itself.<br>This test case can work well.</p>
<p>So the logic of thread stop should be:</p>
<ol>
<li><p>If thread function is finished, related thread will stop.</p>
</li>
<li><p>If thread function is not finished, there should be another code who tells<br>the thread that others want it to be stopped. Other code uses kthread_stop()<br>to set related flag for the thread. This is not an async message, which<br>means thread should check this flag to find if others want it to stop. If<br>thread does not to check this flag, it will be running alway. So it seems<br>even kernel schedule will not stop a thread whose “should stop flag” is<br>already set by “other code”.</p>
<p>So normally you can see the usage in test case 3. Or we can do thread<br>function, and at the end of function wait “other code” to stop it like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kthread_function ()</span><br><span class="line">&#123;</span><br><span class="line">	ret = do_thread_job();		</span><br><span class="line"></span><br><span class="line">	while (!kthread_should_stop())</span><br><span class="line">		schedule();</span><br><span class="line"></span><br><span class="line">	return ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>In this way, “other code” can get the return value of thread.</p>
</li>
<li><p>We can not use kthread_stop right after the creation of a thread, it seems<br>kernel thinks that maybe we do not want the thread to be put into running.</p>
<p>(to do: when can we use kthread_stop after kthread_run?)</p>
</li>
<li><p>Last but not least, as always mentioned in other blogs, kthread_stop should<br>be called when thread is running.</p>
</li>
</ol>
<p>Test driver:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;linux/kernel.h&gt;</span><br><span class="line">#include &lt;linux/kthread.h&gt;</span><br><span class="line">#include &lt;linux/module.h&gt;</span><br><span class="line">#include &lt;linux/delay.h&gt;</span><br><span class="line"></span><br><span class="line">MODULE_LICENSE(&quot;Dual BSD/GPL&quot;);</span><br><span class="line">static int case_num = 1;</span><br><span class="line">module_param(case_num, int, S_IRUGO);</span><br><span class="line"></span><br><span class="line">static int thread_fun(void *date)</span><br><span class="line">&#123;</span><br><span class="line">	unsigned long i;</span><br><span class="line"></span><br><span class="line">	while (1) &#123;</span><br><span class="line">		i++;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* test if kthread_stop can stop a thread right now */</span><br><span class="line">static int test_kthread_stop_a_thread(void)</span><br><span class="line">&#123;</span><br><span class="line">	struct task_struct *thread;</span><br><span class="line"></span><br><span class="line">	thread = kthread_run(thread_fun, NULL, &quot;kthread_test&quot;);</span><br><span class="line">	if (IS_ERR(thread)) &#123;</span><br><span class="line">		pr_info(&quot;test: fail to create a kthread\n&quot;);</span><br><span class="line">		return -EPERM;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	msleep(400);</span><br><span class="line"></span><br><span class="line">	kthread_stop(thread);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static int thread_hello(void *date)</span><br><span class="line">&#123;</span><br><span class="line">	pr_info(&quot;hello world!\n&quot;);</span><br><span class="line">	</span><br><span class="line">	while (!kthread_should_stop())</span><br><span class="line">		schedule();</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* test to find when to call thread_stop */</span><br><span class="line">static int test_kthread_stop_righ_after_run(void)</span><br><span class="line">&#123;</span><br><span class="line">	struct task_struct *thread;</span><br><span class="line"></span><br><span class="line">	thread = kthread_run(thread_hello, NULL, &quot;kthread_hello&quot;);</span><br><span class="line">	if (IS_ERR(thread)) &#123;</span><br><span class="line">		pr_info(&quot;test: fail to create a kthread\n&quot;);</span><br><span class="line">		return -EPERM;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	kthread_stop(thread);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static int thread_loop_hello(void *date)</span><br><span class="line">&#123;</span><br><span class="line">	int i = 0;</span><br><span class="line"></span><br><span class="line">	while (!kthread_should_stop()) &#123;</span><br><span class="line">		pr_info(&quot;hello world: %d!\n&quot;, i++);</span><br><span class="line">		msleep(1000);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	pr_info(&quot;bye :)\n&quot;);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* normal */</span><br><span class="line">static int test_kthread_stop_normal(void)</span><br><span class="line">&#123;</span><br><span class="line">	struct task_struct *thread;</span><br><span class="line"></span><br><span class="line">	thread = kthread_run(thread_loop_hello, NULL, &quot;kthread_loop_hello&quot;);</span><br><span class="line">	if (IS_ERR(thread)) &#123;</span><br><span class="line">		pr_info(&quot;test: fail to create a kthread\n&quot;);</span><br><span class="line">		return -EPERM;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	msleep(4000);</span><br><span class="line"></span><br><span class="line">	kthread_stop(thread);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static int __init kthread_init(void)</span><br><span class="line">&#123;</span><br><span class="line">	switch (case_num) &#123;</span><br><span class="line">	case 1:</span><br><span class="line">		test_kthread_stop_a_thread();</span><br><span class="line">		break;</span><br><span class="line">	case 2:</span><br><span class="line">		test_kthread_stop_righ_after_run();</span><br><span class="line">		break;</span><br><span class="line">	case 3:</span><br><span class="line">		test_kthread_stop_normal();</span><br><span class="line">		break;</span><br><span class="line">	default:		</span><br><span class="line">		pr_err(&quot;no test case found!\n&quot;);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static void __exit kthread_exit(void)</span><br><span class="line">&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module_init(kthread_init);</span><br><span class="line">module_exit(kthread_exit);</span><br><span class="line"></span><br><span class="line">MODULE_AUTHOR(&quot;Sherlock&quot;);</span><br><span class="line">MODULE_DESCRIPTION(&quot;The driver for kthread study&quot;);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux syscon and regmap</title>
    <url>/2021/07/17/Linux-syscon-and-regmap/</url>
    <content><![CDATA[<h2 id="What-is-regmap-and-syscon"><a href="#What-is-regmap-and-syscon" class="headerlink" title="What is regmap and syscon"></a>What is regmap and syscon</h2><p>regmap was introduced by <span class="exturl" data-url="aHR0cHM6Ly9sd24ubmV0L0FydGljbGVzLzQ1MTc4OS8=">https://lwn.net/Articles/451789/<i class="fa fa-external-link-alt"></i></span><br>From my understanding, it provided us a set API to read/write non memory-map I/O<br>(e.g. I2C and SPI read/write) at first. Then after introduced regmap-mmio.c,<br>we can use regmap to access memory-map I/O.</p>
<p>code path: drivers/base/regmap/*</p>
<p>syscon was introduced by <span class="exturl" data-url="aHR0cHM6Ly9sa21sLm9yZy9sa21sLzIwMTIvOS80LzU2OA==">https://lkml.org/lkml/2012/9/4/568<i class="fa fa-external-link-alt"></i></span><br>It provides a set API to access a misc device(e.g. pci-sas subsystem registers<br>in P660) based on regmap, explicitly based on regmap-mmio.c I guess.</p>
<p>code path: drivers/mfd/*</p>
<h2 id="arch-of-regmap-and-syscon"><a href="#arch-of-regmap-and-syscon" class="headerlink" title="arch of regmap and syscon"></a>arch of regmap and syscon</h2><p>basic structure of regmap:</p>
<p>struct regmap:                   per base address per regmap<br>struct regmap_bus:               include read/write callback, different “bus”<br>                                 (e.g. I2C, SPI, mmio) have different regmap_bus<br>struct regmap_mmio_context:      don’t know…<br>struct regmap_config:            confiuration info.</p>
<p>regmap-mmio call flow:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/base/regmap/regmap-mmio.c */</span><br><span class="line">__devm_regmap_init_mmio_clk</span><br><span class="line">    --&gt; __devm_regmap_init</span><br><span class="line">        /* regmap_bus(regmap_mmio), config as input, create regmap */</span><br><span class="line">        --&gt; __regmap_init</span><br><span class="line">	    /* if don&#x27;t have bus-&gt;read or bus-&gt;write */</span><br><span class="line">	    --&gt; map-&gt;reg_read = _regmap_bus_reg_read;</span><br><span class="line">	    --&gt; map-&gt;reg_write = _regmap_bus_reg_write;</span><br><span class="line">	    ...</span><br><span class="line">	    /* if have bus-&gt;read */</span><br><span class="line">            --&gt; map-&gt;reg_read  = _regmap_bus_read;</span><br><span class="line">	        --&gt; map-&gt;bus-&gt;read</span><br><span class="line"></span><br><span class="line">/* drivers/base/regmap/regmap.c */</span><br><span class="line">regmap_read(struct regmap *map, unsigned int reg, unsigned int *val)</span><br><span class="line">    --&gt; _regmap_read</span><br><span class="line">        /* _regmap_bus_reg_read */</span><br><span class="line">        --&gt; map-&gt;reg_read(context, reg, val);</span><br><span class="line">	    --&gt; map-&gt;bus-&gt;reg_read(map-&gt;bus_context, reg, val)</span><br><span class="line"></span><br><span class="line">/* drivers/base/regmap/regmap.c */</span><br><span class="line">regmap_update_bits</span><br><span class="line">    --&gt; _regmap_update_bits</span><br><span class="line">        --&gt; _regmap_read</span><br><span class="line">	--&gt; _regmap_write</span><br><span class="line">	    --&gt; map-&gt;reg_write</span><br></pre></td></tr></table></figure>
<p>basic structure of syscon:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct syscon:                 include a strutct regmap; an element in list below</span><br><span class="line">static LIST_HEAD(syscon_list)</span><br><span class="line"></span><br><span class="line">syscon driver init a regmap:</span><br><span class="line">syscon-&gt;regmap = devm_regmap_init_mmio(dev, base, &amp;syscon_regmap_config);</span><br></pre></td></tr></table></figure>
<h2 id="why-we-need-a-syscon-to-describe-a-misc-device"><a href="#why-we-need-a-syscon-to-describe-a-misc-device" class="headerlink" title="why we need a syscon to describe a misc device"></a>why we need a syscon to describe a misc device</h2><p>To understand this, we shoudl search related discussion in community:<br><span class="exturl" data-url="aHR0cHM6Ly9saXN0cy5vemxhYnMub3JnL3BpcGVybWFpbC9kZXZpY2V0cmVlLWRpc2N1c3MvMjAxMi1BdWd1c3QvMDE4NzA0Lmh0bWw=">https://lists.ozlabs.org/pipermail/devicetree-discuss/2012-August/018704.html<i class="fa fa-external-link-alt"></i></span></p>
<p>From my understanding, syscon firstly registers a syscon dts node to syscon_list,<br>we could find this node when we try to access related registers.</p>
<h2 id="how-to-use-syscon-to-access-a-misc-device"><a href="#how-to-use-syscon-to-access-a-misc-device" class="headerlink" title="how to use syscon to access a misc device"></a>how to use syscon to access a misc device</h2><p>e.g.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">need dts node:</span><br><span class="line">	pcie_sas: pcie_sas@0xb0000000 &#123;</span><br><span class="line">		compatible = &quot;hisilicon,pcie-sas-subctrl&quot;, &quot;syscon&quot;;</span><br><span class="line">		reg = &lt;0xb0000000 0x10000&gt;;</span><br><span class="line">	&#125;;</span><br></pre></td></tr></table></figure>
<p>use below function read/write:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">regmap_read(hisi_pcie-&gt;subctrl, PCIE_SUBCTRL_SYS_STATE4_REG +</span><br><span class="line">	    0x100 * hisi_pcie-&gt;port_id, &amp;val);</span><br><span class="line"></span><br><span class="line">regmap_update_bits(pcie-&gt;subctrl, reg, bit_mask, mode &lt;&lt; bit_shift);</span><br></pre></td></tr></table></figure>
<p>use below function create struct regmap:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hisi_pcie-&gt;subctrl =</span><br><span class="line">	syscon_regmap_lookup_by_compatible(&quot;hisilicon,pcie-sas-subctrl&quot;);</span><br></pre></td></tr></table></figure>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference:"></a>reference:</h2><ol>
<li>Documentation/devicetree/bindings/regmap/regmap.txt</li>
<li>../mfd/mfd.txt</li>
<li>./syscon.txt</li>
</ol>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux thermal子系统和lm_sensors用户态工具</title>
    <url>/2021/06/27/Linux-thermal%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%92%8Clm-sensors%E7%94%A8%E6%88%B7%E6%80%81%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<ol>
<li>Linux thermal驱动</li>
</ol>
<hr>
<p>   Linux thermal是一个内核和温度检测控制有关的驱动子系统，他的位置在drivers/thermal/*.<br>   相关的内核头文件在include/linux/thermal.h。具体的设备驱动需要向thermal框架注册<br>   thermal_zone_device, thermal框架会在thermal_zone_device里封装一个device向系统<br>   注册，通过这个device向用户态暴露一组sysfs属性文件。用户态可以通过这组文件设置相关<br>   参数、获取相关信息。</p>
<p>   在我的笔记本上，这组属性文件大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wangzhou@kllp05:/sys/class$ tree thermal/thermal_zone0</span><br><span class="line">thermal/thermal_zone0</span><br><span class="line">├── available_policies</span><br><span class="line">├── device -&gt; ../../../LNXSYSTM:00/LNXSYBUS:01/LNXTHERM:00</span><br><span class="line">├── emul_temp</span><br><span class="line">├── integral_cutoff</span><br><span class="line">├── k_d</span><br><span class="line">├── k_i</span><br><span class="line">├── k_po</span><br><span class="line">├── k_pu</span><br><span class="line">├── mode</span><br><span class="line">├── offset</span><br><span class="line">├── passive</span><br><span class="line">├── policy</span><br><span class="line">├── power</span><br><span class="line">│   ├── async</span><br><span class="line">│   ├── autosuspend_delay_ms</span><br><span class="line">│   ├── control</span><br><span class="line">│   ├── runtime_active_kids</span><br><span class="line">│   ├── runtime_active_time</span><br><span class="line">│   ├── runtime_enabled</span><br><span class="line">│   ├── runtime_status</span><br><span class="line">│   ├── runtime_suspended_time</span><br><span class="line">│   └── runtime_usage</span><br><span class="line">├── slope</span><br><span class="line">├── subsystem -&gt; ../../../../class/thermal</span><br><span class="line">├── sustainable_power</span><br><span class="line">├── temp</span><br><span class="line">├── trip_point_0_temp</span><br><span class="line">├── trip_point_0_type</span><br><span class="line">├── type</span><br><span class="line">└── uevent</span><br></pre></td></tr></table></figure>
<p>   这里只是展示了thermal_zone0, 当然一个系统里可以多个这样的设备。</p>
<p>   实际上一个具体的驱动和thermal子系统的关系可以通过三个对象去描述，一个就是<br>   这里的thermal_zone_thermal，它表示测量温度的sensor；还可以注册这个sensor所在<br>   温度管理域的降温设备；有了降温设备，还可以注册相应的温度调节策略。（to do: …）</p>
<p>   实际上，你要是只想读温度出来，只注册一个thermal_zone_thermal并提供其中一个<br>   获取温度的回调函数的实现就可以了：thermal_zone_device_ops-&gt;.get_temp。这个函数<br>   会被/sys/class/thermal/<dev>/temp的show函数调用，从而显示测量的温度。</p>
<p>   thermal子系统还会根据注册thermal_zone_device时的参数，把设备的信息通过<br>   /sys/class/hwmon子系统暴露出来。如果你不带注册参数，thermal子系统默认会通过<br>   /sys/class/hwmon暴露信息, 比如这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wangzhou@kllp05:/sys/class$ tree hwmon/hwmon0</span><br><span class="line">hwmon/hwmon0</span><br><span class="line">├── name</span><br><span class="line">├── power</span><br><span class="line">│   ├── async</span><br><span class="line">│   ├── autosuspend_delay_ms</span><br><span class="line">│   ├── control</span><br><span class="line">│   ├── runtime_active_kids</span><br><span class="line">│   ├── runtime_active_time</span><br><span class="line">│   ├── runtime_enabled</span><br><span class="line">│   ├── runtime_status</span><br><span class="line">│   ├── runtime_suspended_time</span><br><span class="line">│   └── runtime_usage</span><br><span class="line">├── subsystem -&gt; ../../../../class/hwmon</span><br><span class="line">├── temp1_crit</span><br><span class="line">├── temp1_input</span><br><span class="line">└── uevent</span><br></pre></td></tr></table></figure>
<p>   temp1_input的show函数会最终调用到驱动里的.get_temp。</p>
<ol start="2">
<li>lm_sensors用户态工具的使用</li>
</ol>
<hr>
<p>  lm_sensors可以读取系统上sensor的信息。它的源代码可以在这里下载到：<br>  <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xtLXNlbnNvcnMvbG0tc2Vuc29ycy5naXQ=">https://github.com/lm-sensors/lm-sensors.git<i class="fa fa-external-link-alt"></i></span></p>
<p>  粗略从代码上看，它使用的是hwmon接口提供的信息。</p>
<p>  在ubuntu系统上你可以使用如下命令简单尝试下lm_sensors:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install lm-sensors</span><br><span class="line"></span><br><span class="line">/* this is a Perl script in /usr/sbin/ */</span><br><span class="line">sudo sensors-detect</span><br><span class="line">输入这个命令后，一路YES。</span><br><span class="line"></span><br><span class="line">wangzhou@kllp05:~/notes$ sensors</span><br><span class="line">iwlwifi-virtual-0</span><br><span class="line">Adapter: Virtual device</span><br><span class="line">temp1:        +37.0°C  </span><br><span class="line"></span><br><span class="line">thinkpad-isa-0000</span><br><span class="line">Adapter: ISA adapter</span><br><span class="line">fan1:           0 RPM</span><br><span class="line"></span><br><span class="line">acpitz-virtual-0</span><br><span class="line">Adapter: Virtual device</span><br><span class="line">temp1:        +30.0°C  (crit = +128.0°C)</span><br><span class="line"></span><br><span class="line">coretemp-isa-0000</span><br><span class="line">Adapter: ISA adapter</span><br><span class="line">Package id 0:  +33.0°C  (high = +100.0°C, crit = +100.0°C)</span><br><span class="line">Core 0:        +31.0°C  (high = +100.0°C, crit = +100.0°C)</span><br><span class="line">Core 1:        +33.0°C  (high = +100.0°C, crit = +100.0°C)</span><br><span class="line"></span><br><span class="line">pch_skylake-virtual-0</span><br><span class="line">Adapter: Virtual device</span><br><span class="line">temp1:        +30.0°C  </span><br></pre></td></tr></table></figure>

<ol start="3">
<li>lm_sensors分析</li>
</ol>
<hr>
<p>  (to do: sensors-detect, sensors, sensord, config…)</p>
<ol start="4">
<li>Linux thermal和lm_sensors的关系</li>
</ol>
<hr>
<p>  如上，现在的lm_sensors使用的是hwmon接口获取信息。<br>  (to do: …)</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux vfio driver arch analysis</title>
    <url>/2021/07/11/Linux-vfio-driver-arch-analysis/</url>
    <content><![CDATA[<p>The whole vfio subsystem should support 3 sub-features:</p>
<ol>
<li>cfg/mem/io support: user space can access cfg/mem/io of vf.</li>
<li>dma support: data in vf can be translated to user space dma memory range.</li>
<li>interrupt from vf can be routed to VM OS.</li>
</ol>
<p>From view of code, we can see whole vfio driver as:</p>
<ol>
<li>init base vfio arch in drivers/vfio/vfio.c</li>
<li>init pci/platform vfio device driver in drivers/vfio/pci/vfio_pci.c, drivers/vfio/platform/vfio_platform.c</li>
<li>init vfio iommu driver and register to vfio system in drivers/vfio/vfio_iommu*</li>
</ol>
<p>This vfio system will create /dev/vfio/vfio as a vfio container, which indicates<br>an address space share by multiple devices. It will also create /dev/vfio/<group_number><br>as a vfio group, which indicates a group shared by multiple devices using a iommu<br>or smmu unit. when we open a /dev/vfio/<group_number>, we will get a fd, which<br>indicates a device handled by vfio system. Device can be controlled by this fd.</p>
<p>vfio system does not create new bus, however, we should unbind original device<br>driver, and bind device with vfio device driver. So for a PCI device, we need<br>vfio pci driver to handle this device. This vfio pci driver becomes the agent of<br>this device and export all its resource to user space.</p>
<p>The interfaces for userspace:</p>
<h2 id="vfio-init-in-vfio-c"><a href="#vfio-init-in-vfio-c" class="headerlink" title="vfio init in vfio.c"></a>vfio init in vfio.c</h2><p>vfio registers a misc device in /dev/vfio/vfio.</p>
<p>initialize items in vfio: </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">register vfio_dev(miscdevice) in misc sub-system, file: /dev/vfio/vfio</span><br><span class="line">              |</span><br><span class="line">              |--&gt; fops(vfio_fops(file_operations))</span><br><span class="line">                             |</span><br><span class="line">                             |--&gt; open: create vfio_container and</span><br><span class="line">                                        set this as file&#x27;s private data</span><br><span class="line">                                  unlocked_ioctl:</span><br><span class="line">                                          /* bind vfio_container and</span><br><span class="line">                                           * vfio_iommu_driver which had been</span><br><span class="line">                                           * registered in vfio.iommu_driver_list</span><br><span class="line">                                           * in specific iommu file, like:</span><br><span class="line">                                           * vfio_iommu_type1.c</span><br><span class="line">                                           */</span><br><span class="line">                                      --&gt; vfio_ioctl_set_iommu</span><br><span class="line">                                   read/write/mmap: will call functions in vfio_iommu_driver</span><br></pre></td></tr></table></figure>

<p>vfio.c creat a vfio class, this will work together with device_create in<br>vfio_create_group. vfio creates a vfio group indeed is creating a device in this<br>vfio class, vfio group file will be /dev/vfio/<group_number>.</p>
<p>vfio_create_group is called in vfio_pci_probe and vfio_platform_probe. In the probe,<br>we get the devices which we want to handle by vfio system, then find which iommu group<br>these devices belong to, then create the related vfio_group to help to store related<br>iommu group. Here just use device_creat to create a file under /dev/vfio/ to refer to<br>the vfio_group. At last, we creat vfio_pci/vfio_platform_device for the devices<br>which we want vfio system to take care of. For details, please refer to part2.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio.class = class_create(THIS_MODULE, &quot;vfio&quot;)</span><br></pre></td></tr></table></figure>
<p>when we operate /dev/vfio/<group_number>, indeed we will call<br>functions in vfio_group_fops.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">register chr device: vfio.group_cdev(struct cdev)</span><br><span class="line">                              |</span><br><span class="line">                              |--&gt; file_operations(vfio_group_fops)</span><br><span class="line">                                               |</span><br><span class="line">                                               |--&gt; open</span><br><span class="line">                                                    unlocked_ioctl</span><br><span class="line">                                                            /* register ops of</span><br><span class="line">                                                             * vfio_device</span><br><span class="line">                                                             */</span><br><span class="line">                                                        --&gt; vfio_group_get_device_fd</span><br></pre></td></tr></table></figure>
<p>so what happen if we call above callback:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">open: find vfio_group --&gt; share vfio_group to private_data of related struct file.</span><br><span class="line">      above vfio_group was created and added in vfio_pci_probe.</span><br><span class="line">unlocked_ioctl: </span><br><span class="line">    VFIO_GROUP_GET_DEVICE_FD:</span><br><span class="line">	--&gt;vfio_group_get_device_fd</span><br><span class="line">    VFIO_GROUP_SET_CONTAINER: </span><br><span class="line">        --&gt; vfio_group_set_container</span><br></pre></td></tr></table></figure>
<p>An ioctl of vfio_group can get a fd for the device.</p>
<p>We already get the iommu_group of a device, why do we use vfio_group_set_container<br>to add this vfio_group to a vfio container?</p>
<p>The concept of vfio container is to build an address space shared by multiple<br>devices.</p>
<pre><code>  vfio container


          ------+--------------+--------------+-------
                |              |              |
              +-+--+         +-+--+         +-+--+
              |smmu|         |smmu|         |smmu|
              +-+--+         +-+--+         +-+--+
                |              |              |
              +-+--+         +-+--+         +-+--+
              |dev |         |dev |         |dev |
              +----+         +----+         +----+
</code></pre>
<p>When vfio_group is added to vfio container, mappings in this vfio_group will be<br>added to other smmus physically. So all smmus above have same mapping if vfio_groups<br>have been added into same vfio container. All mappings are maintained in vfio<br>container. </p>
<p>how to add vfio_group to vfio_container:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio_ioctl_set_iommu</span><br><span class="line">    --&gt; __vfio_container_attach_groups(container, driver, data)</span><br><span class="line">            --&gt; driver-&gt;ops-attach_group(vfio_iommu, group-&gt;iommu_group)</span><br></pre></td></tr></table></figure>

<h2 id="probe-of-vfio-pci-c-vfio-platform-c"><a href="#probe-of-vfio-pci-c-vfio-platform-c" class="headerlink" title="probe of vfio_pci.c/vfio_platform.c"></a>probe of vfio_pci.c/vfio_platform.c</h2><p>All working in vfio system will help build below vfio struct:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">global: vfio</span><br><span class="line">            --&gt; group_list(vfio_group)</span><br><span class="line">                               |</span><br><span class="line">                               |--&gt; iommu_group</span><br><span class="line">                                    device_list(vfio_device)</span><br><span class="line">                                                     |--&gt;group(vfio_group)</span><br><span class="line">                                                         ops(vfio_device_ops)</span><br><span class="line">                                                                   |--&gt; open</span><br><span class="line">                                                                        ...</span><br><span class="line">            --&gt; device_list(vfio_device)</span><br><span class="line">                               |</span><br><span class="line">                               |--&gt; ops(vfio_device_ops)</span><br><span class="line">                                    group(vfio_group)</span><br><span class="line">         </span><br><span class="line">            --&gt; iommu_drivers_list(vfio_iommu_driver)</span><br><span class="line">                                          |</span><br><span class="line">                                          |--&gt; ops(vfio_iommu_driver_ops)</span><br></pre></td></tr></table></figure>
<p>Here we analyze the flows in vfio_pci.</p>
<p>in vfio_pci_init, use pci_register_driver(&amp;vfio_pci_driver) to probe the PCIe<br>devices in the whole PCIe domain, which devices we had already build up in<br>standard PCIe enumeration process.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio_pci_probe</span><br><span class="line">        /* get iommu_group from device, iommu_group had been added to device&#x27;s</span><br><span class="line">         * iommu_group during the init of iommu(smmu)</span><br><span class="line">         */</span><br><span class="line">    --&gt; vfio_iommu_group_get</span><br><span class="line">    --&gt; allocate memory for vfio_pci_device, which in vfio system refers to above device</span><br><span class="line">        /* this will be called in pci or platform device file to create vfio_group,</span><br><span class="line">         * vfio_device. ops for specific kind of vfio_device is different.</span><br><span class="line">         */</span><br><span class="line">    --&gt; vfio_add_group_dev</span><br><span class="line">            /* create vfio_group, and add it to vfio.group_idr and vfio.group_list.</span><br><span class="line">             * and add iommu_group to this vfio_group.</span><br><span class="line">             */</span><br><span class="line">        --&gt; vfio_create_group</span><br><span class="line">                /* create vfio_group file as /dev/vfio/&lt;group number&gt; */</span><br><span class="line">            --&gt; device_create</span><br><span class="line">            /* create vfio_device and add it to vfio_group.device_list, ops will be</span><br><span class="line">             * called at:</span><br><span class="line">             *</span><br><span class="line">             * vfio_group_fops_unl_ioctl can get a fd which refers to related</span><br><span class="line">             * devcie&#x27;s fd. the operations to this fd will route to the operations</span><br><span class="line">             * in vfio_device_fops(struct file_operations) which will call the</span><br><span class="line">             * callbacks in vfio_device.</span><br><span class="line">             *</span><br><span class="line">             * vfio_pci_device is vfio_device&#x27;s private data.</span><br><span class="line">             */</span><br><span class="line">        --&gt; vfio_group_create_device</span><br></pre></td></tr></table></figure>

<h2 id="vfio-register-iommu-driver-in-specific-iommu-file"><a href="#vfio-register-iommu-driver-in-specific-iommu-file" class="headerlink" title="vfio_register_iommu_driver in specific iommu file"></a>vfio_register_iommu_driver in specific iommu file</h2><p>Physically we can use different iommu implementation, e.g. SMMU in ARM, IOMMU for<br>Intel. This vfio iommu driver is used to control this.</p>
<p>register vfio_iommu_driver to vfio:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio</span><br><span class="line">    --&gt; iommu_drivers_list(vfio_iommu_driver)</span><br><span class="line">                                   |</span><br><span class="line">                                   |--&gt; ops(vfio_iommu_driver_ops)</span><br><span class="line">                                                   |--&gt; open: create vfio_iommu.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>This ops is called by vfio_container-&gt;vfio_smmu_driver-&gt;ops, we bind<br>vfio_container and vfio_smmu_driver together in unlocked_ioctl(using VFIO_SET_IOMMU)<br>of /dev/vfio/vfio. Here we can register specific iommu driver to vfio, now there are<br>vfio iommu driver from X86(vfio_iommu_type1.c) and POWER(vfio_iommu_spapr_tce.c).</p>
<p>how to bind vfio_container and vfio_smmu_driver:<br>for a /dev/vfio/vfio container fd, its ioctl VFIO_SET_IOMMU will set specific<br>IOMMU for the container:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio_fops_unl_ioctl</span><br><span class="line">    --&gt; VFIO_SET_IOMMU</span><br><span class="line">        --&gt; vfio_ioctl_set_iommu(container, arg)</span><br></pre></td></tr></table></figure>

<p>how to call the ops in vfio_smmu_driver:<br>vfio_container-&gt;ops will call the ops in vfio_smmu_driver.</p>
<h2 id="how-to-access-cfg-mem-io-of-VFs"><a href="#how-to-access-cfg-mem-io-of-VFs" class="headerlink" title="how to access cfg/mem/io of VFs"></a>how to access cfg/mem/io of VFs</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* ioctl of vfio_group to get a fd of device */</span><br><span class="line">vfio_group_get_device_fd</span><br><span class="line"></span><br><span class="line">/* open in vfio_pci_device */</span><br><span class="line">--&gt; device-&gt;ops-&gt;open</span><br><span class="line">    --&gt; anon_inode_getfile(&quot;[vfio-device]&quot;, &amp;vfio_device_fops, device, O_RDWR);</span><br><span class="line"></span><br><span class="line">/* to do: So it seems we can use both ways to access vf cfg/mem/io range */</span><br><span class="line">read/write: use fd&#x27;s read/write to access vf cfg/mem/io.</span><br><span class="line">mmap: map cfg/mem/io range in pci_dev to user space.</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><p>[1] <span class="exturl" data-url="aHR0cHM6Ly93d3cuaWJtLmNvbS9kZXZlbG9wZXJ3b3Jrcy9jb21tdW5pdHkvYmxvZ3MvNTE0NDkwNGQtNWQ3NS00NWVkLTlkMmItY2YxNzU0ZWU5MzZhL2VudHJ5LzIwMTYwNjA1P2xhbmc9ZW4=">https://www.ibm.com/developerworks/community/blogs/5144904d-5d75-45ed-9d2b-cf1754ee936a/entry/20160605?lang=en<i class="fa fa-external-link-alt"></i></span><br>[2] <span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXExMjMzODY5MjYvYXJ0aWNsZS9kZXRhaWxzLzQ3NzU3MDg5">http://blog.csdn.net/qq123386926/article/details/47757089<i class="fa fa-external-link-alt"></i></span><br>[3] <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNTQ4OTAzNQ==">https://zhuanlan.zhihu.com/p/35489035<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>vfio</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux workqueue分析</title>
    <url>/2021/06/27/Linux-workqueue%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol>
<li>workqueue基本使用方法</li>
</ol>
<hr>
<p> workqueue机制可以异步执行内核其他模块的任务。内核其他模块有任务需要异步执行的<br> 时候，可以调用workqueue提供的接口，把任务丢给一个对应的内核线程去执行。这里<br> 提到的任务定义是一个函数。</p>
<p> 这里我们提到的workqueue是内核最新的版本，它的正规名字叫做Concurrency Managed<br> Workqueue。代码路劲在：linux/kernel/workqueue.c, linux/include/linux/workqueue.h</p>
<p> workqueue相关的基本概念有: work, workqueue, pool workqueue, worker pool, worker,<br> per-cpu worker pool, unbound worker pool, per-cpu workqueue, unbound workqueue.</p>
<p> work, workqueue是workqueue对其他模块的接口。使用者要先创建需要执行的任务, 即work。<br> 然后调用workqueue API把这个任务放入相关的workqueue里执行。</p>
<p> 整个workqueue又分per-cpu workqueue和unbound workqueue。per-cpu workqueue是系统<br> 启动的时候就分配好的(to do: 需要分析)。unbound workqueue需要用户自己创建。</p>
<p> 如果用户使用per-cpu workqueue，只需要调用schedule_work(struct work_struct *work)<br> 即可。这个API会把work放到当前cpu的normal kworker pool中的一个worker上跑。<br> 使用queue_work_on(cpu, system_highpri_wq, work)可以把一个work放到指定cpu 高优先<br> 级pool上跑，使用queue_work_on(cpu, system_wq, work)可以把work放到指定cpu normal<br> kworker pool上跑。</p>
<p> 如果用户使用unbound workqueue, 需要先使用<br> alloc_workqueue(const char *fmt, unsigned int flags, int max_active, …)申请<br> workqueue, 其中flags中要使用WQ_UNBOUND。随后使用<br> queue_work(struct workqueue_struct *wq, struct work_strct *work)把work放到wq<br> 里执行，也可以用queue_work_on(int cpu, struct workqueue_struct *wq, struct work_strct *work)，<br> 但是，上面的这个函数执行的时候并不能精确的把work放在cpu上执行。这两个函数的<br> 效果都是，把work放到当前cpu对应的numa node上的一个cpu跑。</p>
<p> 可以看到，虽然workqueue的用户接口是work定义和work queue，但是最新workqueue的设计<br> 把前端的用户接口和后端的执行线程解耦开来，而且进一步加入了work pool即线程池的<br> 概念，把具体的执行线程即worker的创建和销毁变成了一个动态的过程。所以，我们的讨论<br> 要先基于线程池来。对于per-cpu workqueue, 在workqueue初始化的过程中，为每一个cpu<br> 都创建一个normal work pool和高优先级的work pool。对于unbound workqueue使用的<br> unbound work pool，系统建立一个全局的hash表保存所有不同种类的unbound work pool,<br> 所谓的种类由nice、cpumask、no numa三个参数决定，如果发现需要的unbound work pool<br> 在系统里已经有了，那么直接使用已有的unbound work pool, 一个work可以显示的放在<br> 一个unbound workqueue上跑，但是真正调度到哪个unbound work pool、还是新建立一个<br> unbound work pool, 这个需要运行时才能决定。</p>
<p> 如上，现在workqueue的设计分了前端和后端, 前端是workqueue，后端是kworker pool，<br> worker。中间靠一个pool workqueue的东西连接。这里kworker pool就是一个线程池，<br> worker就是一个个线程，pool workqueue的pool是一个动作，这个动作把前端的workqueue<br> 和后端的一个线程池建立起联系。</p>
<p> 至此，我们还需要知道线程池里线程创建和销毁的机制。首先创建线程池的时候会默认<br> 创建一个线程，注意，这里不是在创建队列的时候。也就是说，对于一个unbound work<br> queue, 在复用系统已经的unbound线程池的时候(绝大多数是复用已有的, 从调试结果看,<br> 系统一开始就会给每个numa node上创建一个normal unbound work pool和一个高优先级<br> unbound work pool), 是完全可能不新建线程的。当线程池里的线程没有被使用的时候，<br> 会自动进入idle状态，idle装态的线程在一定时间后会被销毁。线程池里至少要保持有一个<br> idle线程在，即使超时也不会被销毁。所以，当一个线程池里只有一个idle线程，这时我们<br> 又queue work到这个线程池时，workqueue的代码会首先wake up这个idle线程，这个idle<br> 线程起来后首先要做的就是看看有没有idle线程，如果没有，就要创建一个线程出来，可以<br> 看到idle线程被wake up后，这个线程池里已经没有idle线程，所以这里就会在线程池里<br> 创建新线程。当线程池里的线程被阻塞，这时又有新的work被调度的到这个线程池里时，<br> 也会创建新的线程(to do: 分析code)。处于运行状态的线程会把work queue里缓存的任务<br> 都执行一遍。当一个任务需要执行的时候，如果work pool里有运行状态的线程时，workqueue<br> 代码不会wake up idle线程。</p>
<p> 当频繁把work放到一个unbound work pool上时，会有新worker创建被创建出来。但是<br> 当频繁的把work放到per-cpu work pool上的时候，任务只会在一个相同的worker上执行。<br> (to do: 分析code)</p>
<p> 以上的数据结构大致是这样的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+----------+</span><br><span class="line">|unbound wq+-+- kworker pool(node0)-+- worker0 (u&lt;work pool id&gt;:&lt;worker num&gt;)</span><br><span class="line">+----------+ |                      +- worker1</span><br><span class="line">             |                      `- ...</span><br><span class="line">             |</span><br><span class="line">             `- kworker pool(node1)-+- worker0</span><br><span class="line">                                    +- worker1</span><br><span class="line">                                    `- ...</span><br><span class="line">+------------+</span><br><span class="line">|numa node 0 |</span><br><span class="line">|            |  .-- kworker pool    -+- worker0 (&lt;cpu num&gt;:&lt;worker num&gt;)</span><br><span class="line">|   cpu 0 ---+--+-- kworker pool[H]  +- worker1 </span><br><span class="line">|            |  .-- kworker pool     `- ...</span><br><span class="line">|   cpu 1 ---+--+-- kworker pool[H] -+- worker0 (&lt;cpu num&gt;:&lt;worker num&gt;H)</span><br><span class="line">+------------+                       +- worker1</span><br><span class="line">                                     `- ...</span><br><span class="line">+------------+</span><br><span class="line">|numa node 1 |</span><br><span class="line">|            |</span><br><span class="line">|   cpu 2    |      ...</span><br><span class="line">|            |</span><br><span class="line">|   cpu 3    |</span><br><span class="line">+------------+</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>workqueue代码的基本结构</li>
</ol>
<hr>
<p> 详见简介中的连接, 这四篇文章已经讲的很好。</p>
<ol start="3">
<li>workqeueu API再挖掘</li>
</ol>
<hr>
<p> 如上，workqueue的work不会直接和worker绑定，而是在queue_work的时候选择一个<br> kworker pool。</p>
<p> 对于unbound workqueue, alloc_workqueue中会判断系统里是否有相同的kworker pool,<br> 如果有，就用已有的kworker pool。判断相同的依据是nice，cpumask，no numa。对于<br> per-cpu workqueue，直接选择当前cpu上的kworker pool。</p>
<p> 创建一个新的kworker pool肯定至少要创建一个内核线程。所以，如果alloc_workqueue<br> 复用旧的kworker pool可能会用之前kworker pool里的线程，如果alloc_workqueue里创建<br> 新的kworker pool，那么这个步骤就会新建一个内核线程。</p>
<p> kworker pool的worker是动态调整的，pool里的线程都处于阻塞状态的时候，pool就会<br> 新起一个线程执行work。下面的测试中，我们给一个unbound wq里发work，每个work里<br> sleep一段时间，可以看到，这个unbound wq是复用的系统已经有的kworker pool，在用完<br> 该kworker pool里本来已有的线程后，该worker pool会起新的线程执行work, 过一会<br> 可以发现，该worker pool里的线程有一部分被回收。(case 3)</p>
<p> 注意, 在申请workqueue的时候加上WQ_SYSFS参数，可以把该workqueue的信息通过sysfs<br> 暴露到用户态：e.g. /sys/bus/workqueue/devices/writeback</p>
<ol start="4">
<li>测试</li>
</ol>
<hr>
<p> 如下附录中的测试代码, 以下是测试得到的结论:</p>
<p> case 1: queue_work_on, 对unbound work queue操作时只能把work放在对应numa node上<br>         的cpu，无法具体到cpu。</p>
<p> case 2: schedule_work可以把work放到当前的cpu上。</p>
<p> case 3: 当kwork pool里的线程都处于block状态的时候，如果有work需要执行，对应<br>         的pool就会新分配内核线程。(这个对于per-cpu和unbound work queue都是一样的)</p>
<p> case 4: schedule_work会把work放到当前cpu上跑，block的时候，pool会新分配线程。</p>
<p> case 5: schedule_work_on会只把work放到指定cpu上，block的时候，pool会新分配线程。</p>
<p> case 6: 使用unboud pool, 频繁queue_work_on, 即使没有block，pool里也会创建新<br>         线程。</p>
<p> case 7: 使用queue_work_on(cpu, system_highpri_wq, work)可以把一个work放到指定<br>         cpu上的高优先级pool上跑。如果有block, pool里会分配新线程，如果没有block，<br>     即使频繁调用queue_work_on pool也不会创建新线程。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p> wq_test.c测试代码：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL2t3cS93cV90ZXN0LmM=">https://github.com/wangzhou/tests/blob/master/kwq/wq_test.c<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux zswap构架分析</title>
    <url>/2021/06/27/Linux-zswap%E6%9E%84%E6%9E%B6%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol>
<li>基本模型</li>
</ol>
<hr>
<ul>
<li><p>zswap初始化流程</p>
<ol>
<li><p>为系统里的每一个cpu core分配per cpu dst内存。</p>
</li>
<li><p>为每个cpu core分配内核压缩解压缩的上下文。</p>
</li>
<li><p>创建zpool, 并根据用户态模块参数选择zpool后端使用的内存分配器。创建<br>zpool的时候需要给zpool注册一个evict的回调函数，这个函数用于在zpool的<br>后端内存分配器没有内存的时候, 把zpool里的压缩内存向swap设备写入。可以<br>看到现在的evict在向swap设备写数据的时候还要先把压缩的数据解压缩，如果写<br>入swap设备的数据将来被使用，重新加载回内存的代码路径是标准的缺页流程，<br>和zswap没有关系。这个步骤整体上把创建出来的各种基础数据结构封装在一个<br>struct zswap_pool的结构中, 上面步骤里的per cpu压缩解压缩上下文也放在了<br>zswap_pool。</p>
</li>
<li><p>注册frontswap的回调函数。根据内核Documentation/vm/frontswap.rst，store<br>用于把swap页存入zpool, load用于从zpool重新加载swap的页，invalidate_*<br>用于把zpool里存放的压缩页面释放。</p>
</li>
</ol>
</li>
<li><p>核心数据结构</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   struct zswap_pool</span><br><span class="line"></span><br><span class="line">   swap page ---&gt; frontswap</span><br><span class="line">                     .store</span><br><span class="line">.load</span><br><span class="line">.invalidate_page</span><br><span class="line">.invalidate_area</span><br><span class="line">.init</span><br></pre></td></tr></table></figure>
<p>zswap初始化的时候注册frontswap的一个实例，frontswap的各个回调中使用zpool接口<br>存储压缩的内存，zpool接口的后端可以是不同的专用于存储压缩内存的内存分配器。</p>
</li>
<li><p>对用户态的接口</p>
<ol>
<li><p>zswap用多个模块参数用来配置zswap的参数: zpool的后端内存分配器是可以选的<br>(默认是zbud); 压缩解压算法(默认是LZO)，测试硬件offload的时候，这里要选<br>则相应的算法; zpool占内存的大小; 对相同页的优化处理。</p>
<p>这些参数在/sys/module/zswap/parameters/下也可以配置。</p>
</li>
<li><p>在/sys/kernel/debug/zswap/下有zswap的相关统计项。</p>
</li>
</ol>
</li>
<li><p>对内核crypto comp的接口</p>
<ol>
<li>当前代码使用的是内核crypto comp压缩解压缩接口，没有使用crypto acomp接口。</li>
</ol>
</li>
</ul>
<ol start="2">
<li>实现细节</li>
</ol>
<hr>
<p>  不清楚这里的逻辑，为什么要一个zswap pool的链表。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">__zswap_pool_create_fallback(void)</span><br><span class="line">  +-&gt; zswap_pool_create(char *type, char *compressor)</span><br><span class="line">    +-&gt; cpuhp_state_add_instance(CPUHP_MM_ZSWP_POOL_PREPARE, &amp;pool-&gt;node)</span><br><span class="line"></span><br><span class="line">list_add(&amp;pool-&gt;list, &amp;zswap_pools);</span><br></pre></td></tr></table></figure>
<p>  这里选一个zswap_frontswap_store的实现分析:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> zswap_frontswap_store</span><br><span class="line">   +-&gt; entry = zswap_entry_cache_alloc(GFP_KERNEL)</span><br><span class="line">   	分配一个zswap_entry, 用来描述要压缩存储的一个页。</span><br><span class="line">   +-&gt; crypto_comp_compress</span><br><span class="line">       调用crypto comp API做压缩。</span><br><span class="line">   +-&gt; zpool_malloc(entry-&gt;pool-&gt;zpool, hlen + dlen, gfp, &amp;handle)</span><br><span class="line">       从zpool里分配一段内存, 用来存压缩的页。</span><br><span class="line">   +-&gt; zpool_map_handle(entry-&gt;pool-&gt;zpool, handle, ZPOOL_MM_RW)</span><br><span class="line">   +-&gt; memcpy(buf, &amp;zhdr, hlen)</span><br><span class="line">   +-&gt; memcpy(buf + hlen, dst, dlen)</span><br><span class="line">       把压缩后的swap页存入zpool。</span><br><span class="line">   +-&gt; zswap_rb_insert(&amp;tree-&gt;rbroot, entry, &amp;dupentry)</span><br><span class="line">       把对应的swap页的entry插入一个红黑树，以后load，invalidate等可以从这个</span><br><span class="line">红黑树查找对应的swap页。</span><br></pre></td></tr></table></figure>
<pre><code>从上面store函数的分析中可以看到，因为整个设计都是基于per cpu的，所以做
crypto_comp_compress的时候都是关闭抢占的(是否要关闭调度), 这和acomp的基本
使用方式是不兼容的，在crypto testmgr.c里acomp的test case是wait等待任务完成
的。还有一点，压缩完的数据需要copy到zpool的内存里。
</code></pre>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>zswap</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux中断学习笔记1</title>
    <url>/2021/07/17/Linux%E4%B8%AD%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<h2 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h2><p>  cpu在正常执行一条条指令的时候，可以由于某些原因跳到一些异常处理程序，执行一些<br>  代码，然后再返回原来的程序运行。按照触发异常的源头，异常包括：</p>
<ol>
<li><p>中断，异步的由外部器件发起，cpu响应后，保存现场，然后跳到特定位置执行，然后返回<br>中断处继续执行下面的程序</p>
</li>
<li><p>同步中断，由系统中一些错误的指令引起（比如除零，非法指令等），是和cpu同步的。<br>软中断（实现系统调用的地方）就是人为的通过指令来产生一个异常，然后cpu切换工作模式，<br>对应的进入内核空间开始执行代码</p>
</li>
</ol>
<p>  中断系统的整个硬件包括：cpu,中断控制器，外设。在arm体系中这里的cpu指的是cpu核，<br>  中断控制器一般是标准的gic,通常gic已经被Soc厂商集成在Soc中了, gic的输出接cpu核<br>  上的irq和frq引脚, gic的输入接Soc内各ip模块的中断产生引脚, ip模块对应的中断线<br>  （gic上的输入引脚）在Soc内已经定死。不清楚gic的输入引脚可否直接连Soc的io管脚</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Soc pin |  in Soc:</span><br><span class="line">   \--&gt;      -|--ip模块中断控制----&gt;gic中断控制----&gt;cpu中断控制  </span><br><span class="line">              | </span><br></pre></td></tr></table></figure>
<p>  一个中断从发起到cpu接收到的流程可以用上面的图来表示,需要配置各个中断控制中的<br>  相关寄存器，使得中断在物理上被cpu接收到</p>
<h2 id="中断实现"><a href="#中断实现" class="headerlink" title="中断实现 "></a>中断实现 </h2><p>  linux内核和中断相关的核心数据结构有：struct irq_desc (linux/irqdesc.h), 每个<br>  中断号对应着一个这样的结构，所有的irq_desc以数组或是树的形式存在</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  struct irq_desc</span><br><span class="line">      |--&gt; struct irq_data irq_data -----&gt; |-&gt; mask,irq,hwirq,state_use_accessors</span><br><span class="line">      |                                                           |-&gt; struct irq_chip *chip</span><br><span class="line">      |                                                           |-&gt; struct irq_domain *domain </span><br><span class="line">      |                                                           |-&gt; void *handler_data, *chip_data...</span><br><span class="line">      |--&gt; irq_flow_handler_t handle_irq   </span><br><span class="line">      |--&gt; struct irqaction *action ----&gt;|-&gt; irq_handler_t handler, thread_fn</span><br><span class="line">      |--&gt; raw_spinlock_t lock           |-&gt; void *dev_id, int irq, flags</span><br><span class="line">      |--&gt; struct proc_dir_entry *dir    |-&gt; unsigned long thread_flags, thread_mask</span><br><span class="line">      |--&gt; const char *name...           |-&gt; const char *name</span><br><span class="line">      |-&gt; struct proc_dir_entry *dir...</span><br></pre></td></tr></table></figure>
<p>  对着上面的数据结构分析中断流程:(arm体系)</p>
<p>  中断被cpu接收到之后，汇编程序处理后调用的第一个程序是：<br>  asm_do_IRQ(unsigned int irq, struct pt_regs *regs)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        --&gt;handle_IRQ(irq, regs);</span><br><span class="line">             --&gt;struct pt_regs *old_regs = set_irq_regs(regs);</span><br><span class="line">             irq_enter();</span><br><span class="line">             generic_handle_irq(irq);</span><br><span class="line">                  --&gt;generic_handle_irq_desc(irq, desc);</span><br><span class="line">                             --&gt;desc-&gt;handle_irq(irq, desc);</span><br><span class="line">     irq_exit();</span><br><span class="line">     set_irq_regs(old_regs);</span><br></pre></td></tr></table></figure>

<p>  最后调用的 handle_irq() 是注册在对应irq_desc中的的中断流函数, 处理中断嵌套等问题<br>  一般电平中断用标准的: handle_level_irq() (kernel/irq/chip.c)<br>      边沿中断用标准的: handle_edge_irq(), 上面的chip.c还有另外几种中断流函数,接着<br>  中断流函数向下分析:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  handle_edge_irq(unsigned int irq, struct irq_desc *desc)</span><br><span class="line">      --&gt;raw_spin_lock(&amp;desc-&gt;lock);</span><br><span class="line">      ...</span><br><span class="line">     desc-&gt;irq_data.chip-&gt;irq_ack(&amp;desc-&gt;irq_data); </span><br><span class="line">     ...</span><br><span class="line">    handle_irq_event(desc);</span><br><span class="line">     --&gt;struct irqaction *action = desc-&gt;action;</span><br><span class="line">     raw_spin_unlock(&amp;desc-&gt;lock);</span><br><span class="line">     handle_irq_event_percpu(desc, action);</span><br><span class="line">     --&gt;...</span><br><span class="line">            action-&gt;handler(irq, action-&gt;dev_id);</span><br><span class="line">          ...</span><br><span class="line">     raw_spin_lock(&amp;desc-&gt;lock);</span><br><span class="line"> raw_spin_unlock(&amp;desc-&gt;lock);</span><br></pre></td></tr></table></figure>
<p>  <br>  最后调用的 handle()就是request_irq中注册的中断处理函数, 对应irq_desc中的action <br>  中的handler。上面的中断流函数中调用了irq_ack(), irq_ack()是注册在irq_chip中的<br>  函数, 最上面的数据结构显示irq_chip在irq_desc中的irq_data中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  struct irq_chip &#123;</span><br><span class="line">    ...</span><br><span class="line">void (*irq_enable)(struct irq_data *data);</span><br><span class="line">void (*irq_ack)(struct irq_data *data);</span><br><span class="line">void (*irq_mask)(struct irq_data *data);</span><br><span class="line">void (*irq_unmask)(struct irq_data *data);</span><br><span class="line">void (*irq_eoi)(struct irq_data *data);</span><br><span class="line">int (*irq_set_type)(struct irq_data *data, unsigned int flow_type);</span><br><span class="line">    ...</span><br><span class="line">unsigned long flags;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>  irq_chip包括和硬件相关的一组回调函数，他们直接操作中断相关的寄存器, 比如irq_enble<br>  使能中断线，irq_mask屏蔽中断线</p>
<h2 id="中断使用"><a href="#中断使用" class="headerlink" title="中断使用"></a>中断使用</h2><p>  在驱动程序中只需要调用request_irq()注册中断处理程序即可使用中断，上面的中断实现<br>  为我们做了很多工作。注册中断和释放中断：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,</span><br><span class="line">const char *name, void *dev)</span><br><span class="line">    request_irq有可能会睡眠，里面会调用kmalloc()分配内存</span><br><span class="line">    free_irq(unsigned int irq, void *dev_id)</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">  中断处理函数: irqreturn_t handler(int irq, void *dev_id) 无需要可重入，一个中</span><br><span class="line">  断在执行的时候，相同的中断在其他的所有的处理器上的中断线都会被屏蔽?</span><br><span class="line"></span><br><span class="line">  request_irq()中使用中断号irq注册中断处理函数, 一个中断号也可以注册多个处理函数。</span><br><span class="line">  中断发生了，cpu就跳去执行中断处理函数，一个外设也可以发出几个不同的中断, 这时</span><br><span class="line">  可以在外设的驱动程序中注册多个中断处理函数</span><br><span class="line"></span><br><span class="line">  当外设的中断引脚和gic的输入直接相连时,直接注册中断处理函数就可以使用中断。这是</span><br><span class="line">  因为gic的驱动程序(drivers/irqchip/irq-gic.c)已经实现了开头的数据结构和回调函数,</span><br><span class="line">  其中包括：gic对应的irq_chip中的回调函数(这些回调函数操作gic的输入，可以mask,</span><br><span class="line">  unmask, enable等等?), gic对应的中断流函数(gic_handle_irq()?)</span><br><span class="line"></span><br><span class="line">  若是Soc内部的ip存在中断控制，比如gpio, 它一端接N个输入管脚，每个输入管脚可以接</span><br><span class="line">  收中断，它的中断输出接一个gic的输入，也就是说gpio的N个管脚上的中断输入，都反应</span><br><span class="line">  在gic的一个输入上。gpio中也有一组寄存器可以控制中断(enable, mask, unmask gpio</span><br><span class="line">  输入管脚上的中断)。这时我们要自己为这些中断建立开头的那写数据结构和回调函数</span><br><span class="line">  </span><br><span class="line">中断流函数</span><br><span class="line">----------</span><br><span class="line">   </span><br><span class="line">   以一个gpio控制器的驱动程序为例说明，linux内核中各个厂商的gpio驱动在/drivers/gpio</span><br><span class="line">   内, 以gpio-mvebu.c来分析</span><br><span class="line">   </span><br><span class="line">   irq = platform_get_irq(pdev, i) 取出gpio对应的中断号，这个中断号是系统一开是就</span><br><span class="line">   定好的，是gic给gpio分配的中断号，这一个中断号可以对应gpio的多个输入。当gpio</span><br><span class="line">   引脚配置成中断引脚时，任何一个引脚上产生的中断都通过这个中断号上报给gic</span><br></pre></td></tr></table></figure>
<p>irq_set_handler_data(irq, mvchip)<br>   –&gt;desc-&gt;irq_data.handler_data = data;</p>
<p>irq_set_chained_handler(irq, mvebu_gpio_irq_handler)<br>   –&gt;__irq_set_handler(irq, handle, 1, NULL); // 1 表示：is_chained<br>   desc-&gt;handled_irq = handle;<br>         <br>mvchip-&gt;irqbase = irq_alloc_descs(-1, 0, ngpios, -1)<br>__irq_alloc_descs(irq, from, cnt, node, THIS_MODULE)<br>   –&gt;start=bitmap_find_next_zero_area(allocated_irqs,IRQ_BITMAP_BITS,from,cnt,0);<br>            bitmap_set(allocated_irqs, start, cnt);<br>   alloc_descs(start, cnt, node, owner);<br>    以上分配了ngpios个连续的中断号，最后返回的是第一个中断号</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">```</span><br><span class="line">gc = irq_alloc_generic_chip(&quot;mvebu_gpio_irq&quot;, 2, mvchip-&gt;irqbase,</span><br><span class="line">   mvchip-&gt;membase, handle_level_irq); </span><br><span class="line">   --&gt;struct irq_chip_generic *gc;</span><br><span class="line">   unsigned long sz = sizeof(*gc) + num_ct * sizeof(struct irq_chip_type);</span><br><span class="line">   gc = kzalloc(sz, GFP_KERNEL);</span><br><span class="line">            irq_init_generic_chip(gc, name, num_ct, irq_base, reg_base, handler);</span><br><span class="line">        --&gt; raw_spin_lock_init(&amp;gc-&gt;lock);</span><br><span class="line">                gc-&gt;num_ct = num_ct;</span><br><span class="line">                gc-&gt;irq_base = irq_base;</span><br><span class="line"></span><br><span class="line">                gc-&gt;reg_base = reg_base;</span><br><span class="line"></span><br><span class="line">                gc-&gt;chip_types-&gt;chip.name = name;</span><br><span class="line"></span><br><span class="line">                gc-&gt;chip_types-&gt;handler = handler;</span><br></pre></td></tr></table></figure>
<p>以上动态分配了一个irq_chip_generic结构, 然后用之前分配的连续中断号中的第一个<br>中断号填充irq_base, 并且填充中断流函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    stuct irq_chip_generic</span><br><span class="line">    |--&gt; irq_base</span><br><span class="line">    |--&gt; u32 mask_cache, tpye_cache</span><br><span class="line">    |--&gt; void *private</span><br><span class="line">    |...</span><br><span class="line">    |--&gt; struct irq_chip_type chip_types[0]---&gt;|--&gt; struct irq_chip</span><br><span class="line">    |                                                                         |--&gt; irq_flow_handler_t handler</span><br><span class="line">                                                                              |--&gt; type</span><br><span class="line">                                                                              |...</span><br><span class="line">gc-&gt;private = mvchip; // gc: *irq_chip_generic </span><br><span class="line">ct = &amp;gc-&gt;chip_types[0]; // ct: *irq_chip_type</span><br><span class="line">ct-&gt;type = IRQ_TYPE_LEVEL_HIGH | IRQ_TYPE_LEVEL_LOW;</span><br><span class="line">ct-&gt;chip.irq_mask = mvebu_gpio_level_irq_mask;</span><br><span class="line">ct-&gt;chip.irq_unmask = mvebu_gpio_level_irq_unmask;</span><br><span class="line">ct-&gt;chip.irq_set_type = mvebu_gpio_irq_set_type;</span><br><span class="line">ct-&gt;chip.name = mvchip-&gt;chip.label;</span><br></pre></td></tr></table></figure>

<p>以上向irq_chip中注册了相应的回调函数，这些回调函数处理gpio中断相关，如mask,<br>ack, unmask gpio中断线。现在irq_desc结构也分配好了, irq_chip中的回调函数也注册<br>好了, 需要做的是向irq_desc中注册相应的irq_chip</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">irq_setup_generic_chip(gc, IRQ_MSK(ngpios), 0,</span><br><span class="line">      IRQ_NOREQUEST, IRQ_LEVEL | IRQ_NOPROBE);</span><br><span class="line">        --&gt;list_add_tail(&amp;gc-&gt;list, &amp;gc_list);</span><br><span class="line">irq_set_chip_and_handler(i, chip, ct-&gt;handler); // for loop to do this</span><br><span class="line">irq_set_chip_data(i, gc);</span><br></pre></td></tr></table></figure>
<p>以上先把上面动态生成的irq_chip_generic结构加入gc_list链表，然后向之前申请的<br>irq_desc注册各自的irq_chip和中断流函数</p>
<p>mvchip-&gt;domain = irq_domain_add_simple(); <br>添加一个irq_domain, 作用暂时不清楚</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内存相关的测试工具</title>
    <url>/2021/06/19/Linux%E5%86%85%E5%AD%98%E7%9B%B8%E5%85%B3%E7%9A%84%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<ol>
<li>smem</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  PID User     Command                         Swap      USS      PSS      RSS </span><br><span class="line">40200 test     bash                               0   524.0K   687.0K     3.1M </span><br><span class="line">39757 test     bash                               0   524.0K   692.0K     3.2M </span><br><span class="line">40203 test     /usr/bin/python /usr/bin/sm        0     6.1M     6.2M     7.7M </span><br></pre></td></tr></table></figure>
<p>运行smem -k命令可以看到系统里进程占用内存的情况。如上各个SS的含义是:</p>
<p> USS: unique set size. 表示的是进程独占的物理内存，不包含动态库占的内存。<br> PSS: proportional set size. 进程自己的物理内存 + 动态库折算到进程里的内存。<br> RSS: resident set size. 进程实际使用的物理内存，包括共享库内存。<br> VSS: virtual set size. 进程使用的所有虚拟内存，包括共享库虚拟内存。</p>
<ol start="2">
<li>/proc/buddyinfo</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Node 1, zone      DMA     10     12     11     11     13     11     10     10      6      6    226 </span><br><span class="line">Node 1, zone    DMA32      1      0      1      1      4      2      4      3      4      4    176 </span><br><span class="line">Node 1, zone   Normal  15807  52091  12723    877    258    703    579    194     35     34   5546 </span><br><span class="line">Node 3, zone   Normal  54266  41801  23819  10038   3476    641     65     29     13     19   6714 </span><br></pre></td></tr></table></figure>
<p>表示系统里伙伴系统中，2^1, 2^2…连续页有多少个，这个可以看到伙伴系统内存碎片的<br>情况。</p>
<ol start="3">
<li>/sys/kernel/mm/*</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── hugepages</span><br><span class="line">├── ksm</span><br><span class="line">├── swap</span><br><span class="line">└── transparent_hugepage</span><br></pre></td></tr></table></figure>
<p>可以看到如上各个特性的一些统计值。</p>
<ol start="4">
<li>pidof + pmap + /proc/pid/maps /proc/pid/smaps /proc/pid/stack</li>
</ol>
<hr>
<p>./a.out &amp;<br>pmap -x <code>pidof a.out</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">41677:   ./a.out</span><br><span class="line">Address           Kbytes     RSS   Dirty Mode  Mapping</span><br><span class="line">0000000000400000       4       4       0 r-x-- a.out</span><br><span class="line">0000000000400000       0       0       0 r-x-- a.out</span><br><span class="line">0000000000410000       4       4       4 r---- a.out</span><br><span class="line">0000000000410000       0       0       0 r---- a.out</span><br><span class="line">0000000000411000       4       4       4 rw--- a.out</span><br><span class="line">0000000000411000       0       0       0 rw--- a.out</span><br><span class="line">0000ffff8dac8000    1204     480       0 r-x-- libc-2.23.so</span><br><span class="line">0000ffff8dac8000       0       0       0 r-x-- libc-2.23.so</span><br><span class="line">0000ffff8dbf5000      60       0       0 ----- libc-2.23.so</span><br><span class="line">0000ffff8dbf5000       0       0       0 ----- libc-2.23.so</span><br><span class="line">0000ffff8dc04000      16      16      16 r---- libc-2.23.so</span><br><span class="line">0000ffff8dc04000       0       0       0 r---- libc-2.23.so</span><br><span class="line">0000ffff8dc08000       8       8       8 rw--- libc-2.23.so</span><br><span class="line">0000ffff8dc08000       0       0       0 rw--- libc-2.23.so</span><br><span class="line">0000ffff8dc0a000      16       8       8 rw---   [ anon ]</span><br><span class="line">0000ffff8dc0a000       0       0       0 rw---   [ anon ]</span><br><span class="line">0000ffff8dc0e000     112     112       0 r-x-- ld-2.23.so</span><br><span class="line">0000ffff8dc0e000       0       0       0 r-x-- ld-2.23.so</span><br><span class="line">0000ffff8dc2b000       8       8       8 rw---   [ anon ]</span><br><span class="line">0000ffff8dc2b000       0       0       0 rw---   [ anon ]</span><br><span class="line">0000ffff8dc37000       8       0       0 r----   [ anon ]</span><br><span class="line">0000ffff8dc37000       0       0       0 r----   [ anon ]</span><br><span class="line">0000ffff8dc39000       4       4       0 r-x--   [ anon ]</span><br><span class="line">0000ffff8dc39000       0       0       0 r-x--   [ anon ]</span><br><span class="line">0000ffff8dc3a000       4       4       4 r---- ld-2.23.so</span><br><span class="line">0000ffff8dc3a000       0       0       0 r---- ld-2.23.so</span><br><span class="line">0000ffff8dc3b000       8       8       8 rw--- ld-2.23.so</span><br><span class="line">0000ffff8dc3b000       0       0       0 rw--- ld-2.23.so</span><br><span class="line">0000ffffc48f9000     132      12      12 rw---   [ stack ]</span><br><span class="line">0000ffffc48f9000       0       0       0 rw---   [ stack ]</span><br><span class="line">---------------- ------- ------- ------- </span><br><span class="line">total kB            1592     672      72</span><br></pre></td></tr></table></figure>

<p>/proc/pid/smaps反应一个进程里的各个vma里的信息。提供的信息比/proc/pid/maps多</p>
<ol start="5">
<li>numastat -p <pid></li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Per-node process memory usage (in MBs) for PID 417 (a.out)</span><br><span class="line">                           Node 0          Node 1           Total</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         0.01            0.00            0.01</span><br><span class="line">Stack                        0.00            0.00            0.01</span><br><span class="line">Private                      0.17            0.40            0.57</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                        0.18            0.41            0.59</span><br></pre></td></tr></table></figure>
<p>可以看到一个进程在各个numa节点上的内存使用情况，可以用这个命令观察一段时间进程<br>的内存在各个numa之间使用的情况。</p>
<ol start="6">
<li>mallopt</li>
</ol>
<hr>
<p>C库中设定内存分配配置的接口, 通过mallopt这个函数可以改变malloc/free的行为。</p>
<ol start="7">
<li>mbind/madvise</li>
</ol>
<hr>
<p>mbind是对应系统调用的封装，可以把虚拟内存对应的物理内存和对应的numa节点绑定。<br>madvise也是对应系统调用的封装，可以从用户态传递一些内存相关的属性给内核。</p>
<ol start="8">
<li>getrusage</li>
</ol>
<hr>
<p>getrusage是一个库函数，可以得到一个进程的相关资源的使用情况。其中包括内存使用<br>的统计。比如其中，ru_minflt就是系统里初次分配内存这种缺页的次数，而ru_majflt<br>是从swap分区里换入内存这样缺页的数目。</p>
<ol start="9">
<li>proc下的关于内存的信息</li>
</ol>
<hr>
<p>/proc/slabinfo<br>/proc/iomem<br>/proc/meminfo<br>/proc/vmstat<br>/proc/zoneinfo<br>/proc/vmallocinfo<br>/proc/sys/vm/*</p>
]]></content>
      <tags>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux信号笔记</title>
    <url>/2021/06/27/Linux%E4%BF%A1%E5%8F%B7%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>Linux里有进程，线程，基于此有进程组，线程组的概念。Linux内核把进程，线程做一样<br>的实现，都用一个task_struct表示, 这里我们统一用linux内核的概念来看待以上的概念。<br>内核include/linux/pid.h中对PID的种类有如下的定义:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">enum pid_type</span><br><span class="line">&#123;</span><br><span class="line">	PIDTYPE_PID,</span><br><span class="line">	PIDTYPE_TGID,</span><br><span class="line">	PIDTYPE_PGID,</span><br><span class="line">	PIDTYPE_SID,</span><br><span class="line">	PIDTYPE_MAX,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这里PID指的是一个内核线程ID，对应到用户态可以是一个进程或是一个线程。TGID指线程组<br>ID一个用户态进程和他创建的所有线程组成一个线程组。PGID是指进程组ID，这个要调用linux<br>系统函数创建，把多个用户态进程组成一个进程组。SID是指会话ID。</p>
<p>信号可以从一个用户态进程发到一个用户态进程，也可以从一个用户态线程发到一个用户态<br>线程。当然信号还可以从内核发到用户态，这里就涉及到上面PID的类型，通过指定不同的<br>PID类型, 内核可以把信号发到单个线程(进程)、线程组、进程组等。</p>
<p>信号是一种进程的系统资源, 而且传递时携带的信息很少。这样的性质决定，使用信号最好<br>由整个系统的顶层设计规划，不然如果底层设计中使用了信号, 很容易和其他的软件部件<br>相互冲突。因为信号传递的信息很少，必然要再加入其他的逻辑才可以完成整个业务逻辑，<br>这就会带来系统的复杂度。信号执行是打断原有进程(线程)执行流的，编写信号处理函数<br>要使用可重入函数，而且为了防止死锁，信号处理函数里不能使用锁，这些限制都使得信号<br>处理函数的编写很容易出错。</p>
<p>可以把信号处理从使用信号处理函数转变到使用线程，在线程中等待信号到来，然后处理。<br>这样可以把之前异步的处理放在线程里处理，避免上面说的信号处理函数里不能加锁的限制，<br>(fix me: 是否可以使用可重入函数)。像这里介绍的那样:<br><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWJtLmNvbS9kZXZlbG9wZXJ3b3Jrcy9jbi9saW51eC9sLWNuLXNpZ25hbHNlYy9pbmRleC5odG1s">https://www.ibm.com/developerworks/cn/linux/l-cn-signalsec/index.html<i class="fa fa-external-link-alt"></i></span>.<br>但是这样处理在架构上也是有代价的，他要求你的业务线程和信号处理的线程在一个线程<br>组里，这样才可以在信号处理线程里即使的做处理。还有一个要求是，需要在这个线程组<br>主线程里就设置屏蔽信号处理线程要处理的信号，这样在随后的新线程里才可以继承这个<br>信号屏蔽，然后单独在信号处理线程里不去屏蔽这个信号。可以看到这种方案适用于自己<br>构建的方案，每个部分是自己可以控制的。但是，当你的方案要嵌入到更高一层的方案里<br>时, 用一个线程单独处理信号会带来非常多的麻烦。</p>
<p>内核驱动可以使用fasync发信号给一个fd绑定的进程。具体参考:<br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzg5NzY2Mzkx">https://blog.csdn.net/scarecrow_byr/article/details/89766391<i class="fa fa-external-link-alt"></i></span><br>以上内核向用户态进程发信号需要用户态先执行fd和进程绑定的fcntl操作。实际上内核<br>可以直接类似send_sig_info的函数给一个PID发信号。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>信号</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核DMA子系统分析</title>
    <url>/2021/08/21/Linux%E5%86%85%E6%A0%B8DMA%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="DMA-engine使用"><a href="#DMA-engine使用" class="headerlink" title="DMA engine使用"></a>DMA engine使用</h2><p> DMA子系统下有一个帮助测试的测试驱动(drivers/dma/dmatest.c), 从这个测试驱动入手<br> 我们了解到内核里的其他部分怎么使用DMA engine。配置内核，选则CONFIG_DMATEST可以<br> 把这个模块选中，编译会生成dmatest.ko。可以参考这个文档来快速了解怎么使用dmatest.ko:<br> <span class="exturl" data-url="aHR0cHM6Ly93d3cua2VybmVsLm9yZy9kb2MvaHRtbC92NC4xNS9kcml2ZXItYXBpL2RtYWVuZ2luZS9kbWF0ZXN0Lmh0bWw=">https://www.kernel.org/doc/html/v4.15/driver-api/dmaengine/dmatest.html<i class="fa fa-external-link-alt"></i></span>.</p>
<p> 具体上来讲，内核的其他模块使用dma engine的步骤是:</p>
<ul>
<li><p>使用dma_request_channel先申请一个dma channel，之后的dma请求都基于这个申请<br>的dma channel。</p>
</li>
<li><p>调用dma_dev-&gt;device_prep_dma_memcpy(chan, dst, src, len, flag)把dma操作的参<br>数传给dma子系统。同时返回一个从chan申请的异步传输描述符: struct dma_async_tx_descriptor.</p>
<p>可以把用户的回调函数设置在上面的描述符里。通常这里的回调函数里是一个complete<br>函数，用来在传输完成后通知用户业务流程里的wait等待。</p>
</li>
<li><p>tx-&gt;tx_submit(tx) 把请求提交。</p>
</li>
<li><p>dma_submit_error 判断提交的请求是否合法。</p>
</li>
<li><p>dma_async_issue_pending 触发请求真正执行。</p>
<p>如上，在发送请求之后，一般可以在这里wait等待，通过上面注册的回调函数在dma<br>执行完成后通知这里的wait。</p>
</li>
<li><p>dma_async_is_tx_complete 查看请求的状态。</p>
</li>
<li><p>做完dma操作之后使用dma_release_channel释放申请的dma channel。</p>
</li>
</ul>
<h2 id="DMA子系统分析"><a href="#DMA子系统分析" class="headerlink" title="DMA子系统分析"></a>DMA子系统分析</h2><p> 分析一个现有的dmaengine驱动可以看到，dmaengine驱动需要使用dmaenginem_async_device_register<br> 向dma子系统注册驱动自己的struct dma_device结构。在注册之前，设备驱动要先填充<br> 这个结构里的一些域段。cap_mask是设备驱动支持的特性，还有dma子系统需要的各种<br> 回调函数。</p>
<p> DMA子系统用一个全局链表记录系统里的dma engine设备。对于dma engine设备上的各个<br> channel，DMA子系统为每个channel创建一个struct device设备，这个设备的class是dma_dev<br> class, DMA子系统把创建的device用device_register向系统注册，这样在用户态sysfs<br> 的/sys/class/dma下面就会出现dma<xx>chan<xx>的dma channel描述文件。每个dma<xx>chan<xx><br> 下有对应的属性描述文件。</p>
<p> DMA子系统还对外提供一套第一节中所描述的API。</p>
<h2 id="DMA-engine驱动分析"><a href="#DMA-engine驱动分析" class="headerlink" title="DMA engine驱动分析"></a>DMA engine驱动分析</h2><p> 可以看到，DMA系统在dma engine注册的时候需要设备驱动提供的一套回调函数来支持<br> 第一小节里的各个API，这些回调函数操作具体硬件，完成相关硬件的配置。我们这里可以<br> 以MEMCPY要提供的回调函数示例说明回调函数的意义。</p>
<ul>
<li><p>device_alloc_chan_resources</p>
<p>分配chan的硬件资源</p>
</li>
<li><p>device_free_chan_resources</p>
<p>释放chan的硬件资源</p>
</li>
<li><p>device_prep_dma_memcpy</p>
<p>接收用户传入的请求，分配驱动层面的用户请求</p>
</li>
<li><p>device_issue_pending</p>
<p>操作硬件发起具体的dma请求</p>
</li>
</ul>
<p> 分析现有的dma驱动，可以看到里面用了virt-dma.[ch]里提供的接口。这里也简单看下<br> virt-dma的使用方法。virt-dma的核心数据结构是一组链表，这组链表记录处于不同阶段<br> 的dma请求。当用 e.g. device_prep_dma_memcpy创建一个请求后，这个请求应该挂入<br> desc_allocated链表，当用tx-&gt;tx_submit提交这个请求后，应该把请求挂入desc_submitted<br> 链表，当用dma_async_issue_pending执行请求后，应该把请求挂入desc_issued链表，<br> 当最后请求执行完成后，应该挂入desc_completed链表。virt-dma在原来的dma_chan上<br> 封装了virt_dma_chan，在virt_dma_chan创建的时候, vchan_init为每一个vchan创建<br> 一个tasklet，设备驱动可以在中断处理里调用 e.g. vchan_cookie_complete-&gt;tasklet_schedule<br> 执行tasklet函数vchan_complete, 这个函数里会执行dma请求中用户设置的回调函数。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>DMA</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核crypto子系统学习笔记</title>
    <url>/2021/07/05/Linux%E5%86%85%E6%A0%B8crypto%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>Analysis will start from crypto test cases in crypto/testmgr.c, e.g. deflate.<br>上面的路径上是内核里这对crypto子系统的一个测试程序。通过分析这个程序可以大概<br>看出crypto子系统向外提供的API. 整个系统的情况大概是这样的：</p>
<p>crypto API &lt;—&gt; crypto core &lt;—&gt; crypto_register_alg</p>
<p>设备驱动通过crypto_register_alg把一个设备支持的算法注册到crypto系统里。<br>注册的时候会通过struct crypto_alg把相关的信息传递给crypto core.</p>
<p>struct crypto_alg的结构是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct crypto_alg &#123;</span><br><span class="line">	struct list_head cra_list;</span><br><span class="line">	struct list_head cra_users;</span><br><span class="line"></span><br><span class="line">	u32 cra_flags;</span><br><span class="line">	unsigned int cra_blocksize;</span><br><span class="line">	unsigned int cra_ctxsize;</span><br><span class="line">	unsigned int cra_alignmask;</span><br><span class="line"></span><br><span class="line">	int cra_priority;</span><br><span class="line">	atomic_t cra_refcnt;</span><br><span class="line"></span><br><span class="line">	char cra_name[CRYPTO_MAX_ALG_NAME];</span><br><span class="line">	char cra_driver_name[CRYPTO_MAX_ALG_NAME];</span><br><span class="line"></span><br><span class="line">	const struct crypto_type *cra_type;</span><br><span class="line"></span><br><span class="line">	union &#123;</span><br><span class="line">		struct ablkcipher_alg ablkcipher;</span><br><span class="line">		struct blkcipher_alg blkcipher;</span><br><span class="line">		struct cipher_alg cipher;</span><br><span class="line">		struct compress_alg compress;</span><br><span class="line">	&#125; cra_u;</span><br><span class="line"></span><br><span class="line">	int (*cra_init)(struct crypto_tfm *tfm);</span><br><span class="line">	void (*cra_exit)(struct crypto_tfm *tfm);</span><br><span class="line">	void (*cra_destroy)(struct crypto_alg *alg);</span><br><span class="line">	</span><br><span class="line">	struct module *cra_module;</span><br><span class="line">&#125; CRYPTO_MINALIGN_ATTR;</span><br></pre></td></tr></table></figure>
<p>这个结构的几个关键的信息是: cra_ctxsize, cra_u(下面以compress_alg说明), cra_init,<br>                cra_exit.<br>这个结构表述的是算法相关的系统，但是在执行一个请求的时候，还有维护一组上下文的信息，<br>这些信息记录在结构体: struct crypto_tfm.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct crypto_tfm &#123;</span><br><span class="line"></span><br><span class="line">	u32 crt_flags;</span><br><span class="line">	</span><br><span class="line">	union &#123;</span><br><span class="line">		struct ablkcipher_tfm ablkcipher;</span><br><span class="line">		struct blkcipher_tfm blkcipher;</span><br><span class="line">		struct cipher_tfm cipher;</span><br><span class="line">		struct compress_tfm compress;</span><br><span class="line">                    --&gt; cot_compress</span><br><span class="line">                    --&gt; cot_decompress</span><br><span class="line">	&#125; crt_u;</span><br><span class="line"></span><br><span class="line">	void (*exit)(struct crypto_tfm *tfm);</span><br><span class="line">	</span><br><span class="line">	struct crypto_alg *__crt_alg;</span><br><span class="line"></span><br><span class="line">	void *__crt_ctx[] CRYPTO_MINALIGN_ATTR;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>其中最后一个__crt_ctx是这个上下文的私有数据。上面的cra_ctxsize就是这个私有数据的<br>size. cra_init是准备上下文的函数，比如，你用一个硬件设备压缩数据，实际的物理操作<br>发生在这个硬件的一个队列上，那么就需要准备这个队列，准备必要的缓存等等。cra_exit<br>是退出上下文。cra_u里是具体执行算法的函数，比如可以压缩和解压缩的函数。</p>
<p>从设备驱动的角度讲, 设备驱动只是看到了crypto_alg这个结构。这个结构里的crypt_tfm<br>即一个操作执行的上下问是从哪里知道的呢？毕竟crypto_alg这个结构里的.cra_init,<br>.cra_exit, .cra_u里的.coa_compress和.coa_decompress都需要这个执行上下文。<br>我们在下面具体看一下。</p>
<p>知道这些内部的数据结构对我们理解外部的API有帮助。现在假设crypto的设备驱动已经有了，<br>那么，其他的内核模块怎么用呢？ 其实一开头我们已经讲到crypto/testmgr.c测试程序。</p>
<p>测试的代码里有异步的测试和同步的测试流程，我们这里先看同步的测试:</p>
<p>主要的逻辑就三个函数, 第一先需要分配一个压缩的上下文(本文用压缩的例子), 其实它<br>就是crypto_tfm的包装，和cryto_tfm是一样的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct crypto_comp &#123;</span><br><span class="line">	struct crypto_tfm base;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>struct crypto_comp = crypto_alloc_comp(driver, type, mask), 这个过程中会调用到<br>cra_init函数，这个函数是设备驱动实现的，完成硬件相关的配置，上面已经提到过。<br>调用关系如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* 分配一个压缩解压缩的上下文, 可以看到这里的压缩解压缩的上下文完全就是crypto_tfm */</span><br><span class="line">struct crypto_comp = crypto_alloc_comp(driver, type, mask);</span><br><span class="line">    --&gt; crypto_alloc_base(alg_name, type, mask)</span><br><span class="line">            /* find algrithm: use alg_name, driver name */</span><br><span class="line">        --&gt; alg = crypto_alg_mod_lookup(alg_name, type, mask);</span><br><span class="line">            /* 上下文是依据具体的算法去分配的 */</span><br><span class="line">        --&gt; tfm = __crypto_alloc_tfm(alg, type, mask);</span><br><span class="line">	        /* 上下文中指定相关的算法 */</span><br><span class="line">            --&gt; tfm-&gt;__crt_alg = alg;</span><br><span class="line">            --&gt; crypto_init_ops</span><br><span class="line">	            /* 把相应的算法中的压缩解压缩函数传递给上下文 */</span><br><span class="line">                --&gt; crypto_init_compress_ops(tfm)</span><br><span class="line">                        /* ops is struct compress_tfm */</span><br><span class="line">	            --&gt; ops-&gt;cot_compress = crypto_compress;</span><br><span class="line">                            /* tfm-&gt;__crt_alg-&gt;cra_u.compress.coa_compress */ </span><br><span class="line">                            /*</span><br><span class="line">                             * e.g. drivers/crypto/cavium/zip/zip_main.c</span><br><span class="line">                             *      struct crypto_alg zip_comp_deflate.</span><br><span class="line">                             * will finally call zip_comp_compress!</span><br><span class="line">                             */</span><br><span class="line">                        --&gt; tfm-&gt;__crt_alg-&gt;cra_compress.coa_compress</span><br><span class="line"></span><br><span class="line">	            --&gt; ops-&gt;cot_decompress = crypto_decompress;</span><br><span class="line">		/*</span><br><span class="line">                 * 在创建上下文的最后调用下，算法里的初始化函数，如果是和一个硬件</span><br><span class="line">		 * 的驱动适配，那么这里就可以执行相应硬件初始化的内容。</span><br><span class="line">		 */</span><br><span class="line">            --&gt; if (!tfm-&gt;exit &amp;&amp; alg-&gt;cra_init &amp;&amp; (err = alg-&gt;cra_init(tfm)))</span><br></pre></td></tr></table></figure>

<p>第二，就是执行压缩的操作:<br>crypto_comp_compress(tfm, input, ilen, result, &amp;dlen)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto_comp_compress(crypto_comp, src, slen, dst, dlen)</span><br><span class="line">        /* so hardware can do compress here! */</span><br><span class="line">    --&gt; compress_tfm-&gt;cot_compress;</span><br></pre></td></tr></table></figure>

<p>第三，就是释放这个压缩的上下文<br>crypto_free_comp(comp)</p>
<p>内核虽然现在提供了压缩的异步接口，但是貌似还没有驱动会用到。异步接口的使用要比同步<br>接口复杂一点。下面具体看看。</p>
<p>In alg_test_comp, async branch:<br>/* 和同步一样，这里也创建一个异步的上下文 <em>/<br>acomp = crypto_alloc_acomp(driver, type, mask);<br>/</em></p>
<ul>
<li>不过和同步接口不一样的是，这里又创建一个acomp_req的上下文, 后续的操作都围绕</li>
<li>着这个req结构展开。可以看到req里面包含了异步接口需要的回调函数。</li>
<li>/<br>req = acomp_request_alloc(tfm);<br>acomp_request_set_params(req, &amp;src, &amp;dst, ilen, dlen);<br>acomp_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG,<pre><code>         crypto_req_done, &amp;wait);
</code></pre>
crypto_acomp_compress(req)<br>```<br>这里需要说明的是，testmsg.c里的这个acomp的测试程序里加了wait/complete的相关<br>内容。这里应该是为了测试方便而加的，一般的异步接口里, 当硬件完成操作的时候，在<br>中断函数里直接调用异步接口的回调函数就可以了。</li>
</ul>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>crypto</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核scatterlist用法</title>
    <url>/2021/07/05/Linux%E5%86%85%E6%A0%B8scatterlist%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<ol start="0">
<li><p>为什么有scatterlist</p>
<p>这里说的scatterlist，说的是用聚散表管理的内存，这是一个一般的概念。这个概念主要<br>是配合硬件产生的。从物理上讲，我们可以分配一段连续的内存使用，但是，我们也<br>可以分配很多段不连续的内存，然后用一个软件数据结构把这些数据结构串联起来使用。</p>
<p>那在什么时候会用呢？CPU看到的内存可以通过MMU做映射，一段连续的虚拟内存可以被<br>映射到不连续的物理内存上, 显然这里是不需要scatterlist这个概念支持的。当有了<br>IOMMU这个概念的时候，设备看到的地址可以是一个连续的虚拟地址，这块地址通过IOMMU<br>可以映射到不连续的物理地址上，这里也不需要scatterlist这个概念。可以注意到上面<br>说的设备的DMA必须是支持连续地址的。</p>
<p>但是有的时候，有的设备的DMA还支持scatterlist这种数据结构提供的内存模型。简单<br>来讲，就是你可以配置这个设备，使得它的DMA可以一次DMA接入一串不连续的物理地址。</p>
<p>这个时候内核的scatterlist数据结构就出现了。</p>
</li>
<li><p>硬件结构</p>
<p>如上所述，就是硬件寄存器可以配置一组离散的内存，设备启动DMA可以接入这组内存。</p>
</li>
<li><p>API使用</p>
<p>在使用硬件的这个特性的时候，需要首先分配好各个内存区域，当然，这个时候还只知道<br>各个内存区域的cpu地址(cpu虚拟地址)</p>
<p>然后把各个内存区域的信息填到scatterlist里，组成相应的内核数据结构。</p>
<p>把scatterlist这个结构用dma_map_sg处理下，得到每段内存区域对应的dma地址(总线地址),<br>注意设备发起DMA操作的时候用的就是这个地址。</p>
<p>使用各个内存区域的dma地址, 配置具体的硬件。</p>
</li>
<li><p>内核支持</p>
<p>…</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>scatterlist</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核链表笔记</title>
    <url>/2021/07/17/Linux%E5%86%85%E6%A0%B8%E9%93%BE%E8%A1%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>linux kernel中的链表实现在/linux/include/linux/list.h中。我们可以在一个c程序中<br>很快的实现一个链表，那么kernel中链表有什么独特之处呢？一般的我们这样定义一个链<br>表的节点：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct list_node_a &#123;</span><br><span class="line">	data_a; </span><br><span class="line">	struct list_node *next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样写的不足是，链表上的数据和链表本身紧密耦合在了一起。现在链表中每个节点存放的<br>是data_a, 如果又有一个data_b需要串成一个链表，那么就需要再建立一个struct list_note_b<br>这样的链表节点。简而言之，就是这样的通用性不强。</p>
<p>linux kernel中的实现是，首先实现一个</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct list_head &#123;</span><br><span class="line">	struct list_head *prev;</span><br><span class="line">	struct list_head *next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>的基础结构,这个是链表最核心的结构。然后再加上一些链表操作的接口函数。OK, 通用的<br>链表实现就完成了。</p>
<p>但是我要怎么使用呢？现在有一个结构 struct data 需要用一个链表管理起来(用一个链表<br>把一堆struct data串起来)。就在struct data的定义中嵌入一个struct list_head结构，<br>看起来是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct data &#123;</span><br><span class="line">	...;	</span><br><span class="line">	struct list_head node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>除此之外，一般还会定义一个struct list_head作为这个链表的表头节点。表头节点是一个<br>struct list_head结构, 代表整个链表，他不嵌入struct data中。最后组成的链表如下图<br>所示(其中struct A就是struct data)。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                   +--------------+        +--------------+        +--------------+    </span><br><span class="line">                   |___struct A___|        |___struct A___|        |___struct A___|</span><br><span class="line">                   | ...          |        | ...          |        | ...          |</span><br><span class="line">+----------+       | +----------+ |        | +----------+ |        | +----------+ |</span><br><span class="line">|list node_|       | |list node_| |        | |list node_| |        | |list node_| |</span><br><span class="line">|  prev    |&lt;--------|  prev    |&lt;-----------|  prev    |&lt;-----------|  prev    |&lt;------\</span><br><span class="line">|  next    |--------&gt;|  next    |-----------&gt;|  next    |-----------&gt;|  next    |-----\ |</span><br><span class="line">+----------+       | +----------+ |        | +----------+ |        | +----------+ |   | |</span><br><span class="line">   |  /|\          +--------------+        +--------------+        +--------------+   | |</span><br><span class="line">   |   \                                                                              | |</span><br><span class="line">   \   -------------------------------------------------------------------------------/ /</span><br><span class="line">    ------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<p>但是一个个的struct data节点是怎么被加入链表中的呢? 我们可去/linux/include/linux/list.h<br>找到答案。假设我们有空的链表 sample_list:<br>    LIST_HEAD(sample_list);<br>这里我们初始化了一个链表的表头节点sample_list, 他代表整个链表。在整个链表中的位置<br>就相当于上图中的最左面哪个list node, 只不过现在他的prev和next指针都是指向自己。</p>
<p>现在我们有一个struct data sample_data结构要加入sample_list链表, 可以调用list.h中的函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	list_add(struct list_head *new, struct list_head *head);</span><br><span class="line">实现：</span><br><span class="line">	list_add(&amp;sample_data.node, &amp;sample_list);</span><br></pre></td></tr></table></figure>
<p>可以想像知道struct data sample_data就知道了他里面的struct list_head node的指针，<br>又知道链表头节点的指针，把一个节点加入链表中是非常容易的。具体可以分析list.h中<br>的代码。</p>
<p>现在假设我们知道了链表表头节点的指针，我要遍历链表，读取其中的数据。其中关键的就是<br>知道struct data sample_data中的struct list_head node的指针p，怎么得到对应的<br>struct data sample_data的指针。没有关系，我们可以使用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">container_of(p, struct data, node);</span><br></pre></td></tr></table></figure>
<p>返回对应struct data sample_data的指针。</p>
<p>基本内容到这里就完了，当你发现一个结构中嵌入了一个struct list_head结构时，他一定<br>将要被连到某个链表中。当然也可能在一个结构中嵌入了多个不同的struct list_head结构，<br>那一定是这个结构同时被连入了多个链表中。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux异步通知</title>
    <url>/2021/06/28/Linux%E5%BC%82%E6%AD%A5%E9%80%9A%E7%9F%A5/</url>
    <content><![CDATA[<p>Linux系统中有很多内核和用户态程序通知的机制，比如event fd, netlink和异步通知。<br>通过这些机制内核可以主动给用户态程序发送消息。本文讨论异步通知的基本用法。</p>
<p>利用异步通知机制可以实现从内核中向设备文件绑定的进程发送特定信号。把异步通知用<br>起来需要内核中设备驱动和用户态程序的配合。在这里有一个实例程序可以直接下载，<br>然后在虚拟机里运行: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL3RyZWUvbWFzdGVyL2Zhc3luY190ZXN0">https://github.com/wangzhou/tests/tree/master/fasync_test<i class="fa fa-external-link-alt"></i></span></p>
<p>如这个示例中的驱动代码，为了支持异步通知，我们需要实现设备文件操作中的.fasync<br>回调，实现的方式也很简单，就是调用下标准的fasync_helper函数向内核注册一个<br>fasync_struct, 其中最后一个参数是一个fasync_struct结构的二维指针，一般设备驱动里<br>应该定义一个特定file相关的fasync_struct的指针，用于保存内核分配的fasync_struct<br>的地址, 其实前面所谓注册一个fasync_struct, 就是请求内核分配一个fasync_struct的<br>结构，然后返回该结构的地址给设备驱动。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int test_fasync(int fd, struct file *file, int mode)</span><br><span class="line">&#123;</span><br><span class="line">	return fasync_helper(fd, file, mode, &amp;async_queue);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>当内核要发送信号给用户态进程的时候需要调用下kill_fasync函数, 该函数会向<br>fasync_struct对应的fd所绑定的进程发送一个SIGIO信号。这里的SIGIO也可以换成其他的<br>信号，但是一般用SIGIO信号发异步通知。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void test_trigger_sigio(struct timer_list *tm)</span><br><span class="line">&#123;</span><br><span class="line">	kill_fasync(&amp;async_queue, SIGIO, POLL_IN);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看到在测试程序里，我们为了方便测试，其实是起了一个内核定时器，在测试内核模块<br>加载10s后，向fd绑定的用户态进程发送一个SIGIO信号。(注意，这个测试程序是在主线<br>内核5.1上调试的，主线内核在4.15更新了内核定时器的API，这里用的是新内核定时器API)</p>
<p>在用户态程序中，需要设置设备fd，使其和当前进程绑定，还要使其可以接受fasync信号。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ret = fcntl(fd, F_SETOWN, getpid());</span><br><span class="line">if (ret == -1) &#123;</span><br><span class="line">	printf(&quot;u fasync: fail to bind process\n&quot;);</span><br><span class="line">	return -2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">fcntl(fd, F_SETFL, fcntl(fd, F_GETFL) | FASYNC);</span><br><span class="line">if (ret == -1) &#123;</span><br><span class="line">	printf(&quot;u fasync: fail to set fasync\n&quot;);</span><br><span class="line">	return -3;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样设置后在内核调用kill_fasync，内核就知道把信号发给哪个进程。不同shell可能对<br>于信号设定有所不同，这里在测试之前先把SIGIO信号unmask，避免SIGIO被shell默认mask<br>掉，然后子进程继承父进程的信号设定，也mask住SIGIO的情况。</p>
<p>Linux信号的实现基本逻辑是在进程的signal pending表里标记其他进程或者内核给自己发<br>的信号，然后在进程从内核态切回用户态的时候再去扫描signal pending表以响应信号。<br>如果用户态进程一直没有系统调用，那么内核态发的SIGIO会不会得不到即使的响应? 另外<br>是否其他的内核活动也会引起测试进程发生内核态向用户态切换的过程，其中一个最可能<br>的情况就是内核周期性的调度。测试程序中做了一个简单的测试，用户态程序在设置好<br>fd后就进入死循环，而内核设备驱动在加载10s后会给用户态进程发一个SIGIO信号。测试<br>的结果是用户态进程可以在10s左右收到内核发的SIGIO信号。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux内核模块编译</title>
    <url>/2021/07/17/Linux%E5%86%85%E6%A0%B8%E6%A8%A1%E5%9D%97%E7%BC%96%E8%AF%91/</url>
    <content><![CDATA[<p> <br>linux模块编译, 采用源码树外编译的方式</p>
<hr>
<ol>
<li><p>编写模块代码：hello.c[1]</p>
</li>
<li><p>在同一目录下编写Makefile[2]</p>
</li>
<li><p>在同一目录下编译：<br>    make ARCH=arm CROSS_COMPILE=arm-linux-gnueabi- -C /vm/wangzhou/linux-iks  M=<code>pwd</code> modules<br>    ARCH：平台, CROSS_COMPILE：交叉编译器, -C：kernel源码路径, M=：module<br>    代码的路径, modules：指明编译模块<br>    (注：这里编译arm平台下的module)</p>
</li>
<li><p>在hello.c同一目录下将会生成hello.ko</p>
</li>
</ol>
<h2 id="在kernel的源码树中编译"><a href="#在kernel的源码树中编译" class="headerlink" title="在kernel的源码树中编译"></a>在kernel的源码树中编译</h2><ol>
<li>把写好的模块代码放入源码树的一个目录：linux-src/drivers/char/hello.c</li>
</ol>
<ol start="2">
<li>在…/char/下的Makefile中加入：obj-m += hello.o<br>    可以看到Makefile中的项目多是：<br>    obj-$(CONFIG_VIRTIO_CONSOLE)+= virtio_console.o</li>
</ol>
<p>    obj-$(CONFIG_RAW_DRIVER)        += raw.o</p>
<p>     …</p>
<ol start="3">
<li>在linux-src/下：<br>    make ARCH=arm CROSS_COMPILE=arm-linux-gnueabi- zImage -j 8 modules<br>    内核中所有配置成模块的程序将得到编译, 本例子上在linux-src/drivers/ch<br>    ar/下将会看到编译好的hello.ko</li>
</ol>
<h2 id="模块由多个文件组成，放在一个目录里编译"><a href="#模块由多个文件组成，放在一个目录里编译" class="headerlink" title="模块由多个文件组成，放在一个目录里编译"></a>模块由多个文件组成，放在一个目录里编译</h2><ol>
<li><p>把文件夹放在源码树的一个目录下：<br>   linux-src/drivers/char/hello/hello.c  …hello/add.c[3]<br>   为了测试的目的, 需要在hello.c中加入调用add()的语句…</p>
</li>
<li><p>在…/char/的Makefile文件中加：obj-m += hello/ 表示该module在hello目<br>   录下</p>
</li>
<li><p>在…/hello/下创建Makefile文件[4]</p>
</li>
<li><p>同上面第三步, 在hello/下会生成hello_add.ko模块</p>
</li>
</ol>
<h2 id="利用Kconfig图形化配置kernel、module"><a href="#利用Kconfig图形化配置kernel、module" class="headerlink" title="利用Kconfig图形化配置kernel、module: "></a>利用Kconfig图形化配置kernel、module: </h2><p> 用make menuconfig可以图形化配置kernel。<br> 可以使用下面的方式, 把自己写的module加入配置菜单：</p>
<ol>
<li><p>把…/char/下的Makefile中之前加的行修改成：<br>   obj-${CONFIG_HELLO_ADD} += hello/ 表示在最后在.config文件中显示的配置项是CONFIG_HELLO_ADD</p>
</li>
<li><p>把…/char/hello/下的Makefile修改成：<br>   obj-${CONFIG_HELLO_ADD} += hello_add.o<br>   hello_add-objs := hello.o add.o</p>
</li>
</ol>
<p>   可以看到, 编译时会根据CONFIG_HELLO_ADD的值去决定编译成什么, 下面要说的Kconfig<br>   即用来设置CONFIG_HELLO_ADDde值<br>   <br>3. 在…/char/下的Kconfig文件中加入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">config HELLO_ADD</span><br><span class="line">	tristate &quot;HELLO_ADD MODUEL(try the Kconfig)&quot;</span><br><span class="line">	default n</span><br><span class="line">	help</span><br><span class="line">	  something about help information of hello_add module</span><br></pre></td></tr></table></figure>
<p> tristate表示可以配置成n, y, m三种状态(不编译，直接编译入kernel，编译成module),<br> 后面的字符串最后会显示到菜单上, default 指明默认的情况, config 指明配置的变量,<br> 即CONFIG_HELLO_ADD</p>
<ol start="4">
<li><p>make menuconfig 依照下面路径, 将会看到HELLO_ADD模块的配置菜单</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  Device Drivers ---&gt;</span><br><span class="line">          Character devices ---&gt;</span><br><span class="line">          ...</span><br><span class="line">          &lt;&gt; HELLO_ADD MODULE(try the Kconfig)</span><br><span class="line">          ...</span><br></pre></td></tr></table></figure></li>
<li><p>同(二)中第三步，在对应目录下即可得到hello_add.ko</p>
</li>
</ol>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>[1] 编写模块代码：hello.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* hello.c */</span><br><span class="line">#include &lt;linux/init.h&gt;</span><br><span class="line">#include &lt;linux/module.h&gt;</span><br><span class="line">MODULE_LICENSE(&quot;Dual BSD/GPL&quot;);</span><br><span class="line"></span><br><span class="line">static int hello_init(void)</span><br><span class="line">&#123;</span><br><span class="line">	printk(KERN_ALERT &quot;Hello, begin to test ...\n&quot;);</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static void hello_exit(void)</span><br><span class="line">&#123;</span><br><span class="line">	printk(KERN_ALERT&quot;Goodbye, cruel world\n&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module_init(hello_init);</span><br><span class="line">module_exit(hello_exit);</span><br><span class="line"></span><br><span class="line">MODULE_AUTHOR(&quot;***&quot;);</span><br><span class="line">MODULE_DESCRIPTION(&quot;A simple test Module&quot;);</span><br><span class="line">MODULE_ALIAS(&quot;a simplest module&quot;);</span><br></pre></td></tr></table></figure>

<p>[2] Makefile文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    obj-m := hello.o</span><br><span class="line">    clean:</span><br><span class="line">        rm  hello.ko hello.mod.c hello.mod.o hello.o modules.order Module.symvers</span><br></pre></td></tr></table></figure>

<p>[3] 编写文件: add.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    /* add.c */</span><br><span class="line">    int add(int a, int b)</span><br><span class="line">    &#123;</span><br><span class="line">return (a+b);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>[4] Makefile文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    obj-m := hello_add.o  /* 指明module的名字 */</span><br><span class="line">    hello_add-objs := add.o hello.o /* 指明hello_add是由几个文件链接到一起的*/</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux巨页的使用</title>
    <url>/2021/06/20/Linux%E5%B7%A8%E9%A1%B5%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>普通巨页的内核说明文档在linux/Documentation/admin-guide/mm/hugetlbpage.rst</p>
<p>使用巨页需要在内核编译的时候打开相关的配置选项：CONFIG_HUGETLBFS, CONFIG_HUGETLB_PAGE</p>
<p>使能系统的巨页可以通过内核的启动参数或者是/sys/kernel/mm/hugepages/下的文件。<br>我们具体看下sysfs下的相关目录。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hugepages/</span><br><span class="line">├── hugepages-1048576kB</span><br><span class="line">│   ├── free_hugepages</span><br><span class="line">│   ├── nr_hugepages</span><br><span class="line">│   ├── nr_hugepages_mempolicy</span><br><span class="line">│   ├── nr_overcommit_hugepages</span><br><span class="line">│   ├── resv_hugepages</span><br><span class="line">│   └── surplus_hugepages</span><br><span class="line">├── hugepages-2048kB</span><br><span class="line">│   ├── free_hugepages</span><br><span class="line">│   ├── nr_hugepages</span><br><span class="line">│   ├── nr_hugepages_mempolicy</span><br><span class="line">│   ├── nr_overcommit_hugepages</span><br><span class="line">│   ├── resv_hugepages</span><br><span class="line">│   └── surplus_hugepages</span><br><span class="line">├── hugepages-32768kB</span><br><span class="line">│   ├── free_hugepages</span><br><span class="line">│   ├── nr_hugepages</span><br><span class="line">│   ├── nr_hugepages_mempolicy</span><br><span class="line">│   ├── nr_overcommit_hugepages</span><br><span class="line">│   ├── resv_hugepages</span><br><span class="line">│   └── surplus_hugepages</span><br><span class="line">└── hugepages-64kB</span><br><span class="line">    ├── free_hugepages</span><br><span class="line">    ├── nr_hugepages</span><br><span class="line">    ├── nr_hugepages_mempolicy</span><br><span class="line">    ── nr_overcommit_hugepages</span><br><span class="line">    ├── resv_hugepages</span><br><span class="line">    └── surplus_hugepages</span><br></pre></td></tr></table></figure>
<p>可以看到在hugepages目录下为每个巨页大小都开了专门的目录。可以看到在我的ARM64<br>系统上，巨页有64KB，32KB，2MB，1GB四种类型。向nr_hugepages写数值可以创建指定数目<br>的巨页，读free_hugepages可以得到还没有使用的巨页数目。</p>
<p>一般巨页属于系统配置，我们只去使用，不去更改配置。使用的方法是在mmap的时候在<br>flags参数中加上MAP_HUGETLB。如果只用MAP_HUGETLB，巨页的分配算法是内核里定的，<br>比如，你要mmap 128KB的内存，在64KB和2MB都可以满足的时候，我们希望从64KB里搞两<br>页出来就好了，内核可能是从2MB里分配的，实际上，用5.10-rc2的内核，真的是从2MB的<br>页里分一页出来。</p>
<p>所以，针对巨页，mmap flags里还有宏可以指定从哪种大小的页里分巨页。man mmap下有：<br>MAP_HUGE_2MB, MAP_HUGE_1GB，不过直接用这个宏会报没有定义，这个是因为gcc版本比较<br>低，我们可以直接找见内核里定义的地方: linux/include/uapi/linux/mman.h</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define MAP_HUGE_64KB	HUGETLB_FLAG_ENCODE_64KB</span><br><span class="line">#define MAP_HUGE_2MB	HUGETLB_FLAG_ENCODE_2MB</span><br><span class="line"></span><br><span class="line">#define HUGETLB_FLAG_ENCODE_64KB	(16 &lt;&lt; HUGETLB_FLAG_ENCODE_SHIFT)</span><br><span class="line">#define HUGETLB_FLAG_ENCODE_2MB		(21 &lt;&lt; HUGETLB_FLAG_ENCODE_SHIFT)</span><br><span class="line"></span><br><span class="line">#define HUGETLB_FLAG_ENCODE_SHIFT	26</span><br></pre></td></tr></table></figure>
<p>可以看到16, 21正好是页大小以2为底的对数。所以，举个例子，我们可以如下指定从<br>64KB的巨页中申请内存：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int len = 128 * 1024;</span><br><span class="line">unsigned long *p;</span><br><span class="line">int i;</span><br><span class="line"></span><br><span class="line">p = mmap(NULL, len, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS |</span><br><span class="line">	 MAP_HUGETLB | (16 &lt;&lt; 26), -1, 0);</span><br><span class="line"></span><br><span class="line">for (i = 0; i &lt; len / sizeof(*p); i++) &#123;</span><br><span class="line">	*(p + i) = 20;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>之后再去看/sys/kernel/mm/hugepages/hugepages-64kB/free_hugepages, 会发现减少了2。<br>需要注意的是，mmap到的内存要去写下，内存才会真实分配，如果在mmap之后就去看<br>free_hugepages的值，其中还是原来的值。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux文件引用计数的逻辑</title>
    <url>/2021/06/27/Linux%E6%96%87%E4%BB%B6%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0%E7%9A%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>考虑这样一个场景，打开一个字符设备文件/dev/A，得到一个fd，用户态可以对这个<br>fd做相关的文件操作，包括ioctl, mmap, close等，内核如果保证close操作和其他<br>操作的同步，即不会出现close和其他文件并发执行，其他文件访问已经close掉的文件<br>这种情况。</p>
<p>内核是靠打开文件的引用计数来保证这一点的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kernel/fs/open.c</span><br><span class="line">filp_open</span><br><span class="line">  +-&gt; file_open_name</span><br><span class="line">    +-&gt; do_filp_open</span><br><span class="line">      +-&gt; path_openat</span><br><span class="line">        +-&gt; alloc_empty_file</span><br><span class="line">	这里在创建struct file结构的时候会把里面的f_count引用计数设置为1。</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kernel/fs/ioctl.c</span><br><span class="line">ksys_ioctl系统调用</span><br><span class="line">  +-&gt; fdget</span><br><span class="line">    +-&gt; __fdget</span><br><span class="line">      在rcu锁里得到file结构的指针</span><br><span class="line">      +-&gt; __fget_light</span><br><span class="line">        +-&gt; __fget</span><br><span class="line">	  +-&gt; get_file_rcu_many (atomic_long_add_unless(&amp;(x)-&gt;f_count, xx, 0))</span><br><span class="line">          这里只有在f_count非0的时候才会把引用计数加1。如果是0，表明已经file</span><br><span class="line">	  的引用计数已经是0。__fget会去files里查fd对应的file。</span><br><span class="line">  +-&gt; fdput</span><br><span class="line">    +-&gt; fput</span><br><span class="line">      +-&gt; fput_many</span><br><span class="line">        +-&gt; atomic_long_dec_and_test(&amp;file-&gt;f_count)</span><br><span class="line">	如果减到0，在另一个内核线程中，延迟执行delay_work：</span><br><span class="line">	  +-&gt; delayed_fput_work</span><br><span class="line">	    +-&gt; delayed_fput</span><br><span class="line">	      +-&gt; __fput</span><br><span class="line">	        +-&gt; file_free(file)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kernel/fs/open.c</span><br><span class="line">close系统调用</span><br><span class="line">  +-&gt; __close_fd</span><br><span class="line">    +-&gt; spin_lock(&amp;files-&gt;file_lock)</span><br><span class="line">    在锁里拿到fd对应的file结构的指针</span><br><span class="line">      +-&gt; filp_close</span><br><span class="line">        +-&gt; fput     </span><br><span class="line">        如上</span><br><span class="line">    +-&gt; spin_unlock(&amp;files-&gt;file_lock)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux线程学习- APUE11/12章</title>
    <url>/2021/06/19/Linux%E7%BA%BF%E7%A8%8B%E5%AD%A6%E4%B9%A0-APUE11-12%E7%AB%A0/</url>
    <content><![CDATA[<ol>
<li>线程的创建和退出</li>
</ol>
<hr>
<p> Linux系统下线程和进程的概念是比较模糊的。一般来说，线程是调度的单位，进程是资源<br> 的单位。本质上来说，内核看到都是一个个线程，但是线程之间可以通过共享资源，相互<br> 之间又划分到不同的进程里。</p>
<p> 从内核视角上看各种不同的ID会比较清楚。之前的这篇文章对进程ID, 线程ID，进程组，<br> 线程组，会话已经有了简单描述：<br> <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzUwNDM3NjI2P3NwbT0xMDAxLjIwMTQuMzAwMS41NTAx">https://blog.csdn.net/scarecrow_byr/article/details/50437626?spm=1001.2014.3001.5501<i class="fa fa-external-link-alt"></i></span></p>
<p> 在一个进程的多个线程里调用getpid，这个是一个c库对getpid系统调用的封装，所以这<br> 个函数返回调用线程的进程ID, 这个ID其实就是这个线程组的线程组ID，也是这个线程组<br> 的主线程的内核pid，这个从kernel代码kernel/sys.c getpid的系统调用可以看的很清楚。<br> 使用gettid系统调用可以得到一个线程的对应内核pid。</p>
<p> 而所谓pthread库的pthread_self()得到的只是pthread库自定义的线程ID，这个东西和上<br> 面内核里真正的各种ID是完全不同的。</p>
<p> 线程的创建用pthread_create, 用的系统调用是clone。我们考虑线程结束的方法，线程执<br> 行完线程函数后自己会退出，这其中也包括线程函数自己调用pthread_exit把自己结束。<br> 线程也可以被进程中的其他线程取消，取消一个线程使用的函数是：pthread_cancel(pthread_t tid)。<br> 用strace跟踪下，可以发现pthread_cancel中的系统调用是tgkill(tgid, tid, SIGRTMIN),<br> 这个系统调用向线程组tgid里的tid线程发送SIGRTMIN信号。</p>
<p> 线程可以用pthread_jorn在阻塞等待相关线程结束，并且得到线程结束所带的返回值。</p>
<p> 线程可以注册退出的时候要调用的函数。使用pthread_cleanup_push, pthread_cleanup_pop。<br> 注册的函数在线程调用pthread_exit或者是线程被pthread_cancel的时候执行，线程正常<br> 执行结束时不执行。用strace -f跟踪进程中所有线程的系统调用可以发现，对应的系统调用<br> 是set_robust_list。</p>
<ol start="2">
<li>线程同步的锁</li>
</ol>
<hr>
<p> 线程之间共享变量的时候需要加锁，以pthread库为例，我们有pthread_mutex, pthead_spinlock,<br> pthread读写锁。此外我们还可以自己实现锁，这个的好处是不受具体线程库的限制，<br> 不好的地方是，我们自己实现的锁一定没有pthread库实现的性能高。如下的测试代码中<br> 比较了pthread mutex/spinlock，以及自己实现的spinlock的性能:<br> <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL2xvY2tfdGVzdC90ZXN0LmM=">https://github.com/wangzhou/tests/blob/master/lock_test/test.c<i class="fa fa-external-link-alt"></i></span></p>
<p> apue这里的例子11-5很好的展示了一个引用计数的实现方式。<br> 一定要避免A-B/B-A这种交叉加锁的情况，但是注意这里只是加锁，减锁的顺序可以随意。</p>
<p> 如果一定要出现交叉加锁的情况，要做成如果第二把锁没有抢到(所以，第二把锁要用try<br> 锁)，那么已经锁上的第一把锁也要放开，要留出一条通道把可能的死锁情况放过去。过<br> 一段时间再锁上第一把锁，再try第二把锁。</p>
<p> 11-6的代码对于交叉加锁的处理方式是，需要上第二把锁的时候，直接放开第一把锁，<br> 然后先上第二把锁，再上第一把锁，由于这个时候有个时间的空隙，可能不满足之前的<br> 条件了，所以要再check下之前需要第二把锁的条件。当然apue举这个例子的目的这里是<br> 为了说明可以简化加锁，需要在复杂度和性能之间权衡。</p>
<ol start="3">
<li>条件变量和信号量</li>
</ol>
<hr>
<p> 条件变量和信号量都是为了线程/进程之间同步用的，提供了线程/进程之间相互等待、<br> 通知的机制。简单写一个测试:<br> <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL3B0aHJlYWQvdGhyZWFkLmM=">https://github.com/wangzhou/tests/blob/master/pthread/thread.c<i class="fa fa-external-link-alt"></i></span><br> <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL3B0aHJlYWQvc2VtLmM=">https://github.com/wangzhou/tests/blob/master/pthread/sem.c<i class="fa fa-external-link-alt"></i></span><br> 在thread.c和sem.c里都会出现同一个问题，如果不在enqueue_msg函数里unlock和发信号<br> 之间增加时延，会出现发送线程一直持有锁，接收线程得不到锁从而无法及时接收的情况。<br> 测试发现，在unlock和发送信号(比如thread.c的pthread_mutex_unlock和pthread_cond_signal)<br> 之间增加usleep(1)的时延就可以使如上的这种情况不出现。</p>
<ol start="4">
<li>线程控制</li>
</ol>
<hr>
<p> pthread库还有很多控制接口，这些接口可以改变如上接口的语义，或者增减新的功能。<br> 比如，每种锁的初始化接口都可以控制这些锁在嵌套加锁、不对称加锁等的语义，遇到<br> 这样的情况，可以配置成容许或者是返回错误值; 可以配置锁在线程之间还是进程之间<br> 共享; 可以配置线程栈的大小等; 可以申请线程的私有数据; 可以使得一个函数只执行<br> 一次。</p>
<p> 简单写一个线程私有数据的测试:<br> <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL3B0aHJlYWQvcHJpdi5j">https://github.com/wangzhou/tests/blob/master/pthread/priv.c<i class="fa fa-external-link-alt"></i></span><br> 可以看到，pthread_key_t key1的create没有和线程绑定。基本的逻辑是，pthread_key_create<br> 可以在任意一个线程里，pthread_setspecific和pthread_getspecific是在线程里对应的，<br> 在线程A里set的数据，可以在线程A里通过get拿到，但是在另外的一个线程B里对全局的<br> key1使用pthread_getspecific只能拿到NULL。apue里的例子把pthread_key_create放到<br> 了每个线程里，为了只调用一次pthread_key_create, 还介绍只跑一个函数一次的接口：<br> int pthread_once(pthread_once_t *initflag, void (*initfn)(void))，可以想象这个<br> 函数的实现是先上锁，然后检查initflag，initflag没有置上已经跑过的flag就调用initfn，<br> 如果已经跑过就不用跑了。这个只跑一次的接口在某些库的设计里是很有用的，比如一个<br> 库需要在进程使用的时候初始化一次，这里就可以用相似的设计或者直接用这个API。</p>
<p> 关于信号可以参考如下link，线程和信号的细节内容需要另外描述。<br> <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzk3NjIxNDMyP3NwbT0xMDAxLjIwMTQuMzAwMS41NTAx">https://blog.csdn.net/scarecrow_byr/article/details/97621432?spm=1001.2014.3001.5501<i class="fa fa-external-link-alt"></i></span></p>
<p> 线程在fork系统调用上遇到的问题，我们自己写一个独立的应用的时候比较难遇到，因为<br> 全局受自己的控制，我们只要一开始fork进程，做好规划就好。但是，当我们要写一个库，<br> 这个库可以被其他上层代码调用，我们的库里又要起独立的线程时，这个问题就会出来。<br> 这个问题的本质是自己写的库里向上层export出了全局的资源，比如，如果库里只出函数<br> 接口，就没有问题，所有库里的资源都在函数的栈上，但是，如果库里有了全局资源，相当<br> 于，我们的库向调用进程里增加了全局的资源，调用进程将需要考虑这些全局资源(当然库<br> 可以向上层的调用者提出诉求或者接口)。具体看，子进程会从父进程那里继承:<br> 全局变量(子进程在没有写之前，如果去读这个变量，依然得到的是父进程里的值，如果拿<br> 这个值去做判断就有可能出错)、各种锁、信号量和条件变量。如果fork出来的进程不是<br> 调用exec系列函数去执行一个新的程序，那么子进程里拥有和父进程一样的锁、信号量和<br> 条件变量。可以通过pthread_atfork提前挂上fork时候的回调函数进行处理，即一定是在<br> prepare回调里先获取所有的锁，在parent、child里再释放所有的锁，注意这里的获取释放<br> 的操作和父进程里的可能获取锁的行为做了互斥。</p>
<p> preaed/pwrite可以保证多线程对一个fd的操作是原子的。</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>APUE</tag>
        <tag>Linux用户态编程</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux抢占概念</title>
    <url>/2021/06/20/Linux%E6%8A%A2%E5%8D%A0%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<ol>
<li>抢占的概念</li>
</ol>
<hr>
<p> linux内核支持抢占, 可以通过抢占相关的编译宏打开或者禁止抢占。</p>
<p> 一个线程在一个cpu上运行, 不是这个线程主动让出cpu导致的其他线程跑到这个cpu上<br> 的情况都是抢占。关抢占就是禁止这样的行为发生。</p>
<p> 线程主动让出cpu的行为有，比如一个线程发送了一个I/O任务，然后调用<br> wait_for_completion睡眠等待completion。其他的各种行为我们都认为是内核抢占行为，<br> 比如，周期性的tick中断中调度器把另外一个线程调度到cpu上跑；各种中断程序里，<br> 触发了调度把另一个线程调度到cpu上跑。</p>
<p> 所以，内核里一段程序之前关了内核抢占之后又把内核抢占打开，如果这块代码中没有<br> 主动让出cpu的行为，我们可以认为这段代码在一个cpu上会持续的执行完，中间不会被<br> 打断。但是，这里说的是单个cpu上的情况, 当这段代码里有访问全局的数据结构的时候，<br> 对于那个全局的数据结构，还是可能和其他cpu上的程序并发访问的。</p>
<ol start="2">
<li>开关抢占的实现</li>
</ol>
<hr>
<p> 可以看到禁止抢占这个宏就是给task_struct结构里的preempt count加引用计数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define preempt_disable() \</span><br><span class="line">do &#123; \</span><br><span class="line">	preempt_count_inc(); \</span><br><span class="line">	barrier(); \</span><br><span class="line">&#125; while (0)</span><br></pre></td></tr></table></figure>

<p> 开启抢占是先减引用计数, 然后执行调度。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define preempt_enable() \</span><br><span class="line">do &#123; \</span><br><span class="line">	barrier(); \</span><br><span class="line">	if (unlikely(preempt_count_dec_and_test())) \</span><br><span class="line">		__preempt_schedule(); \</span><br><span class="line">&#125; while (0)</span><br></pre></td></tr></table></figure>

<p> 是否可以抢占的判断标准除了如上preempt的计数外，还有中断的情况。如果中断是关闭的<br> 同样认为是不能抢占的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define preemptible()	(preempt_count() == 0 &amp;&amp; !irqs_disabled())</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>关抢占的用途</li>
</ol>
<hr>
<p> 关抢占的目的就是叫一段代码不被内核调度打断下执行完，因为有的时候换一个程序进来<br> 跑可能把原来程序的数据破坏掉。比如内核zswap里，在每一个cpu上分配了压缩解压缩<br> 的tfm，然后用per-cpu变量保存相应的内容，他在使用这些per-cpu变量的时候就需要<br> 关闭内核抢占, 不然中途换另一个线程在cpu上跑就会有数据不一致的问题。</p>
<p> 可以看到在使用per-cpu变量的宏里，一进来就把抢占关了:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define get_cpu_ptr(var)						\</span><br><span class="line">(&#123;									\</span><br><span class="line">	preempt_disable();						\</span><br><span class="line">	this_cpu_ptr(var);						\</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux设备驱动中DMA接口的使用</title>
    <url>/2021/05/22/Linux%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E4%B8%ADDMA%E6%8E%A5%E5%8F%A3%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="Linux设备驱动中DMA接口的使用"><a href="#Linux设备驱动中DMA接口的使用" class="headerlink" title="Linux设备驱动中DMA接口的使用"></a>Linux设备驱动中DMA接口的使用</h2><p>-v0.1 2018.3.13 Sherlock init Westford</p>
<ol start="0">
<li><p>DMA的概念</p>
<p>DMA就是说设备可以直接进行内存的读写，不需要CPU的参与。当然，在设备启动DMA<br>进行读写之前，你需要通过CPU把读写的地址，大小等一些信息配置给设备。设备完成<br>数据读写后可以发一个中断告诉CPU，之后CPU就可以做相关的操作。但是，CPU要把<br>什么地址告诉设备呢？</p>
</li>
</ol>
<ol>
<li><p>几个地址的概念</p>
<p>在kernel/Documentation/DMA-API-HOWTO.txt里讲的比较清楚，它里面有一副图是<br>这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">             CPU                  CPU                  Bus</span><br><span class="line">           Virtual              Physical             Address</span><br><span class="line">           Address              Address               Space</span><br><span class="line">            Space                Space</span><br><span class="line"></span><br><span class="line">          +-------+             +------+             +------+</span><br><span class="line">          |       |             |MMIO  |   Offset    |      |</span><br><span class="line">          |       |  Virtual    |Space |   applied   |      |</span><br><span class="line">        C +-------+ --------&gt; B +------+ ----------&gt; +------+ A</span><br><span class="line">          |       |  mapping    |      |   by host   |      |</span><br><span class="line">+-----+   |       |             |      |   bridge    |      |   +--------+</span><br><span class="line">|     |   |       |             +------+             |      |   |        |</span><br><span class="line">| CPU |   |       |             | RAM  |             |      |   | Device |</span><br><span class="line">|     |   |       |             |      |             |      |   |        |</span><br><span class="line">+-----+   +-------+             +------+             +------+   +--------+</span><br><span class="line">          |       |  Virtual    |Buffer|   Mapping   |      |</span><br><span class="line">        X +-------+ --------&gt; Y +------+ &lt;---------- +------+ Z</span><br><span class="line">          |       |  mapping    | RAM  |   by IOMMU</span><br><span class="line">          |       |             |      |</span><br><span class="line">          |       |             |      |</span><br><span class="line">          +-------+             +------+</span><br></pre></td></tr></table></figure>
<p>这里有一堆地址概念，不同地址有不用的作用。硬件可以把物理的内存和设备的寄存器<br>空间映射(MMIO)到CPU物理地址空间, 这里的映射和kernel没有关系，我们可以认为固件<br>已经为我们做好了，代码里里直接访问对应的物理地址就可以了。CPU通过CPU虚拟地址<br>访问物理内存和设备的MMIO, CPU虚拟地址到实际地址的映射是MMU做的，当然如果在内核<br>的线性映射区，这个映射只是加上一个偏移。</p>
<p>从概念上说，设备看到的地址叫总线地址。一般，总线地址比较难以理解，这需要一点<br>体系结构的知识。一般，一个计算机系统类似这样的结构。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        +-----+   +------+</span><br><span class="line">        | CPU |   | CPU  |  ...</span><br><span class="line">        +--+--+   +--+---+</span><br><span class="line">           |         |          +-----+</span><br><span class="line">    -------+---+-----+----------+ DDR |</span><br><span class="line">               |                +-----+</span><br><span class="line">       +-------+--------+</span><br><span class="line">       | Bus controller |</span><br><span class="line">       +----+-----------+</span><br><span class="line">            |</span><br><span class="line">       +----+----+</span><br><span class="line">| Devices |</span><br><span class="line">       +---------+</span><br></pre></td></tr></table></figure>
<p>CPU, DDR，总线控制器连接的是系统总线，外设是连接到外部总线里的。两个总线域里<br>的物理信号，总线报文等都不一样。两个总线域是靠总线控制器联通的。所以，比较<br>容易理解，实际上CPU和外设是处在两个不同的地址空间里的。设备看到的地址，是外部<br>总线域里的地址，我们叫总线地址。其实CPU要访问外设，最终也是通过总线控制器，把<br>CPU地址翻译成总线地址，才能访问到，只不过是硬件把设备地址映射到了系统总线，<br>软件访问直接访问系统总线地址，如果落在映射的区域，总线控制器帮你翻译下，发给<br>设备。</p>
<p>同样的道理，设备做DMA访问，设备一开始发出的地址是总线地址，当这个访问到了总线<br>控制器，总线控制器帮忙翻译成为系统总线地址。(这里，我们可以认为IOMMU(ARM上<br>叫SMMU)也是总线控制器的一部分)</p>
<p>有了这样的认识，下面就好理解了。</p>
</li>
</ol>
<ol start="2">
<li><p>流式DMA和一致性DMA</p>
<p>所以，一个DMA操作，至少要有两个地址，一个CPU可以访问CPU虚拟地址，一个是设备<br>可以访问的设备总线地址(dma_addr_t)，他们其实对应的是一个物理地址。<br>(有回弹缓冲区的不是一个)</p>
<p>dma_alloc_coherent可以分配一段物理地址, 函数的返回是指向这段物理地址的CPU<br>虚拟地址和这段物理地址对应的总线地址。然后你就可以把这个总线地址配置给硬件。</p>
<p>dma_map_single和上面的一致性DMA分配不一样，假设我们已经分配好了一段物理地址，<br>要算出来这段地址对应的总线地址，我们就可以用dma_map_single这个函数。这种DMA<br>的使用方式，叫流式DMA。</p>
</li>
</ol>
<ol start="3">
<li><p>将DMA内存映射到用户态</p>
<p>可以注意到，你用一致性DMA分配”一段”物理内存。是根本不保证分配的物理内存在<br>内核的线性地址空间，而且不保证分配的物理内存是连续的。</p>
<p>那你想把这些物理内存映射到用户态，叫用户直接访问怎么才能做到？</p>
<p>dma_mmap_coherent这个API就做的是这个事情, 在驱动的mmap接口里调用这个函数就<br>可以了。这个函数把DMA物理区域映射到用户态的连续的虚拟地址上。</p>
</li>
</ol>
<ol start="4">
<li><p>聚散表DMA</p>
<p>有的时候，做DMA的数据在内存里是不连续存放的，而且设备也支持这种不连续内存的<br>DMA。这里的不连续，是指设备的DMA地址的描述就是一个聚散表类似的结构。</p>
<p>这时我们可以用内核数据结构struct scatterlist来描述数据的初始内存结构，随后用<br>dma_map_sg的到每一块的总线地址。然后再把这些总线地址配置到设备对应的数据结构<br>里。</p>
</li>
</ol>
<p>知道这些概念对我们编程有什么作用? 首先编程应该是基于正确语义的。你用get_free_page<br>或者是kmalloc分配一段地址给DMA，在特性的条件下或许没有问题。但是，语义完全是错的,<br>这些得到的地址都是CPU虚拟地址，CPU可以用这些地址访问数据。但是设备用这些地址发起<br>DMA操作，是很可能有问题的。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI ACPI笔记1</title>
    <url>/2021/07/17/PCI-ACPI%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<ol>
<li>ACPI</li>
</ol>
<hr>
<p>For the basic knowledge, you can refer to “PCI Express 体系结构导读” by<br>WangQi, in charpter 14, there are related stuffs about ACPI.</p>
<p>And you can also refer to kernel source code linux/Documentation/acpi/ to find<br>ACPI related documents. ACPI Definition Blocks is as below, this figure is<br>copied from linux/Documentation/acpi/namespace.txt.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------+    +-------+    +--------+    +------------------------+</span><br><span class="line">|  RSDP   | +-&gt;| XSDT  | +-&gt;|  FADT  |    |  +-------------------+ |</span><br><span class="line">+---------+ |  +-------+ |  +--------+  +-|-&gt;|       DSDT        | |</span><br><span class="line">| Pointer | |  | Entry |-+  | ...... |  | |  +-------------------+ |</span><br><span class="line">+---------+ |  +-------+    | X_DSDT |--+ |  | Definition Blocks | |</span><br><span class="line">| Pointer |-+  | ..... |    | ...... |    |  +-------------------+ |</span><br><span class="line">+---------+    +-------+    +--------+    |  +-------------------+ |</span><br><span class="line">               | Entry |------------------|-&gt;|       SSDT        | |</span><br><span class="line">               +- - - -+                  |  +-------------------| |</span><br><span class="line">               | Entry | - - - - - - - -+ |  | Definition Blocks | |</span><br><span class="line">               +- - - -+                | |  +-------------------+ |</span><br><span class="line">                                        | |  +- - - - - - - - - -+ |</span><br><span class="line">                                        +-|-&gt;|       SSDT        | |</span><br><span class="line">                                          |  +-------------------+ |</span><br><span class="line">                                          |  | Definition Blocks | |</span><br><span class="line">                                          |  +- - - - - - - - - -+ |</span><br><span class="line">                                          +------------------------+</span><br><span class="line">                                                      |</span><br><span class="line">                                         OSPM Loading |</span><br><span class="line">                                                     \|/</span><br><span class="line">                                               +----------------+</span><br><span class="line">                                               | ACPI Namespace |</span><br><span class="line">                                               +----------------+</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>PCI using ACPI</li>
</ol>
<hr>
<p>a. work flows:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* just register struct acpi_bus_type acpi_pci_bus to list: bus_type_list */</span><br><span class="line">acpi_pci_init() /* arch_initcall(acpi_pci_init) */</span><br><span class="line"></span><br><span class="line">acpi_init() /* subsys_initcall(acpi_init) */</span><br><span class="line">    --&gt; acpi_scan_init()</span><br><span class="line">        /*</span><br><span class="line">         * register pci_root_handler to list: acpi_scan_handlers_list.</span><br><span class="line">	 * the attach will be called in acpi_scan_attach_handler().</span><br><span class="line">	 * there attach is assigned as acpi_pci_root_add()</span><br><span class="line">	 */</span><br><span class="line">        --&gt; acpi_pci_root_init()</span><br><span class="line">	/*</span><br><span class="line">         * register pci_link_handler to list: acpi_scan_handlers_list.</span><br><span class="line">	 * this handler has relationship with PCI IRQ.</span><br><span class="line">	 */</span><br><span class="line">	--&gt; acpi_pci_link_init()</span><br><span class="line">	/* we facus on PCI-ACPI, ignore other handlers&#x27; init */</span><br><span class="line">	...</span><br><span class="line">        --&gt; acpi_bus_scan()</span><br><span class="line">	    /* create struct acpi_devices for all device in this system */</span><br><span class="line">	    --&gt; acpi_walk_namespace()</span><br><span class="line">	    --&gt; acpi_bus_attach()</span><br><span class="line">	        --&gt; acpi_scan_attach_handler()</span><br><span class="line">		    --&gt; acpi_scan_match_handler()</span><br><span class="line">		    --&gt; handler-&gt;attach /* attach is acpi_pci_root_add */</span><br><span class="line"></span><br><span class="line">acpi_pci_root_add()</span><br><span class="line">    /*</span><br><span class="line">     * in kernel, there are two pci_acpi_scan_root, they are in</span><br><span class="line">     * arch/ia64/pci/pci.c and arch/x86/pci/acpi.c.</span><br><span class="line">     * if we will implement PCI using ACPI in ARM64, we should implement</span><br><span class="line">     * another this kind of function in arch/arm64/kernel/pci.c.</span><br><span class="line">     * in pci_acpi_scan_root, will allocate struct pci_controller and</span><br><span class="line">     * struct pci_root_info.</span><br><span class="line">     */</span><br><span class="line">    --&gt; pci_acpi_scan_root()</span><br><span class="line">        --&gt; probe_pci_root_info()</span><br><span class="line">	    /*</span><br><span class="line">	     * will called twice, first for count_window, second for add window.</span><br><span class="line">	     * this function will get infomation from ACPI table.</span><br><span class="line">	     */</span><br><span class="line">	    --&gt; acpi_walk_resources() /* drivers/acpi/acpica/rsxface.c */</span><br></pre></td></tr></table></figure>
<p>b. basic structs:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">global list: acpi_scan_handlers_list /* drivers/acpi/scan.c */</span><br><span class="line">element: struct acpi_scan_handler</span><br><span class="line"></span><br><span class="line">static list: bus_type_list /* drivers/acpi */</span><br><span class="line">element: struct acpi_bus_type</span><br><span class="line"></span><br><span class="line">struct acpi_scan_handler:</span><br><span class="line">     struct acpi_device_id *ids;</span><br><span class="line">     attach;</span><br><span class="line">     ...</span><br><span class="line"></span><br><span class="line">struct acpi_bus_type</span><br><span class="line">struct acpi_device</span><br><span class="line"></span><br><span class="line">struct pci_controller</span><br><span class="line">struct pci_root_info:</span><br><span class="line">    struct pci_controller *controller;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux透明大页(THP)分析</title>
    <url>/2021/06/19/Linux%E9%80%8F%E6%98%8E%E5%A4%A7%E9%A1%B5-THP-%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol start="0">
<li>THP是什么</li>
</ol>
<hr>
<p> Linux有两种使用大页的方式，一种是普通大页，另一种是透明大页。本文说的就是透明<br> 大页，透明大页的本意是系统在可以搞成大页的时候，自动的给你做大页的映射，这样<br> 有两个好处，一是减少缺页的次数，另一个是减少TLB miss的数量。</p>
<p> 可以在这里找见THP的说明文档：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">linux/Documentation/vm/transhuge.rst</span><br><span class="line">linux/Documentation/admin-guide/mm/transhuge.rst</span><br></pre></td></tr></table></figure>

<ol>
<li>THP的使用方式</li>
</ol>
<hr>
<p> 上面的内核文档已经详细介绍了THP的使用方式。简单总结，就是sysfs中提供了三大类<br> 接口去控制THP的使用方式。</p>
<p> 第一类接口配置THP的使用范围: /sys/kernel/mm/transparent_hugepage/enabled<br> alway是指在整个系统里使用THP, madvise是指可以用madvise指定使用THP的地址范围，<br> never是关掉THP。内核提供了CONFIG_TRANSPARENT_HUGEPAGE_ALWAYS以及<br> CONFIG_TRANSPARENT_HUGEPAGE_MADVISE去配置如上enable里的默认值。</p>
<p> 第二类接口配置生成大页的方式：/sys/kernel/mm/transparent_hugepage/defrag<br> always是在内存申请或者madvise的时候直接stall住内存申请，直到内核通过各种手段<br> 把大页搞出来。内存申请返回或者是madvise返回的时候，已经是大页了; defer是随后<br> 内核会异步的把大页给你搞好; defer+madvise是只对madvise是立即搞定大页，其他的<br> 情况还是按照defer的来; madvise是只对madvise立即搞定大页。</p>
<p> 第三类接口，配置内核khugepaged线程的一些参数，比如多少长时间做一次扫描, 具体<br> 可以参考上面的内核文档：/sys/kernel/mm/transparent_hugepage/khugepaged/*</p>
<p> 内核文档里也介绍了THP相关的一些统计参数，比如/proc/vmstat里的thp_*的各个参数，<br> /proc/meminfo、/proc/PID/smap里的AnonHugePages。从内核代码里还可以看到有debugfs<br> 下的接口/sys/kernel/debug/split_huge_pages。</p>
<p> 整个系统使用THP，打开enabled always就好。单独一段内存使用THP，需要用madvise打<br> 上一个MADV_HUGEPAGE, 类似:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">p = mmap(NULL, MEM_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE |</span><br><span class="line">	 MAP_ANONYMOUS, -1, 0);</span><br><span class="line">if (p == MAP_FAILED)</span><br><span class="line">	exit(1);</span><br><span class="line"></span><br><span class="line">ret = madvise(p, MEM_SIZE, MADV_HUGEPAGE);</span><br><span class="line">if (ret)</span><br><span class="line">	exit(1);</span><br><span class="line"></span><br><span class="line">munmap(p, MEM_SIZE);</span><br></pre></td></tr></table></figure>

<p> 实际上, 现在的内核代码和defrag中的定义已经没有严格对应。比如在enable: always,<br> defrag: madvise的时候，vma merge还是会把大于2MB的vma扔给khugepaged线程去扫描、<br> 然后触发huge page collapse。</p>
<ol start="2">
<li>THP内核代码分析</li>
</ol>
<hr>
<p> 透明巨页的内核配置是CONFIG_TRANSPARENT_HUGEPAGE，代码主要在mm/khugepages.c，<br> mm/huge_memory.c，相关的头文件在include/linux/huge_mm.h，include/linux/khugepaged.h。</p>
<p> THP的初始化在huge_memory.c的hugepage_init(), 这个函数初始化THP, 并且start<br> khugepaged内核线程。/sys/kernel/mm/transparent_hugepage/enabled里写always或者<br> madvise也可以start khugepaged内核线程。</p>
<p> 我们跟踪代码的从三个地方入手，一个是__transparent_hugepage_enabled，这个是内核<br> 缺页流程__handle_mm_fault里, 这里会判断系统有没有使能透明大页，如果使能了，会直<br> 接分配大物理页。另外一个是，系统会启动一个内核线程持续扫描系统里的页，进行大页<br> 的合并。代码分析依赖5.11-rc4。第三个地方是madvise系统调用。</p>
<p> mm/memory.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">__handle_mm_fault</span><br><span class="line">  +-&gt; __transparent_hugepage_enabled</span><br><span class="line">    /* 忽略细节，可以看到在下面的函数里申请page、安装页表 */</span><br><span class="line">    +-&gt; create_huge_pmd/pud</span><br><span class="line">      +-&gt; do_huge_pmd_anonymous_page</span><br><span class="line">        /* 分配内存 */</span><br><span class="line">        +-&gt; alloc_hugepage_vma</span><br><span class="line">	/* 安装页表并且处理和其他mm部件的关系 */</span><br><span class="line">	+-&gt; __do_huge_pmd_anonymous_page</span><br><span class="line"></span><br><span class="line">  /* ? */</span><br><span class="line">  +-&gt; handle_pte_fault</span><br></pre></td></tr></table></figure>
<p> khugepaged的线程函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">khugepaged</span><br><span class="line">  +-&gt; khugepaged_do_scan</span><br><span class="line">    /* 一般情况不会进去这里 */</span><br><span class="line">    +-&gt; collapse_pte_mapped_thp</span><br><span class="line">    /*</span><br><span class="line">     * 有这两个入口做小页换成大页的操作, 第一个和file-back的内存有关系，</span><br><span class="line">     * 下面调用的是: collapse_file。第二个处理匿名页或者是堆上的内存，</span><br><span class="line">     * 下面调用的是：collapse_huge_page。</span><br><span class="line">     */</span><br><span class="line">    +-&gt; khugepaged_scan_file</span><br><span class="line">    /* 先扫描整个pmd下的页，可以做collapase的时候，最后走到collapse_huge_page */</span><br><span class="line">    +-&gt; khugepaged_scan_pmd</span><br><span class="line">      /*</span><br><span class="line">       * 这个是小页换成大页的核心函数，先分配2MB连续页面，然后断开pmd，</span><br><span class="line">       * 同时做tlb invalidate，然后把小页的内存copy到大页，然后把pmd页表</span><br><span class="line">       * 安装上。其中涉及的同步逻辑在最后collapse_huge_page页表同步里</span><br><span class="line">       * 展开分析。</span><br><span class="line">       */</span><br><span class="line">      +-&gt; collapse_huge_page</span><br></pre></td></tr></table></figure>
<p> khugepaged的扫描需要内核的其他部分提供需要扫描的对象，这个入口函数是<br> khugepaged_enter。可以发现整个系统里有mm/mmap.c、mm/huge_memory.c、mm/shmem.c<br> 里调用了。</p>
<p> madvise系统调用，mm/madvise.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">madvise_behavior</span><br><span class="line">  +-&gt; hugepage_madvise</span><br><span class="line">   ...</span><br><span class="line">     /*</span><br><span class="line">      * 这个函数为对应的mm生成一个mm_slot，把mm_slot添加到khugepaged的</span><br><span class="line">      * 扫描链表里，然后wakeup khugepaged线程。</span><br><span class="line">      */</span><br><span class="line">   +-&gt; __khugepaged_enter</span><br></pre></td></tr></table></figure>
<p> mmap.c里的调用是在mmap或者brk系统调用，在不断的做vma_merge的时候，如果发现有<br> vma的range跨越了2MB的连续va，就会把对应的vma交给khugepaged扫描，做大页的替换。</p>
<p> collapse_huge_page页表同步分析:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    /* 分配大页 */</span><br><span class="line">+-&gt; khugepaged_alloc_page</span><br><span class="line"></span><br><span class="line">+-&gt; mmap_write_lock</span><br><span class="line"></span><br><span class="line">    /* 做secondory mmu tlbi */</span><br><span class="line">+-&gt; mmu_notifier_invalidate_range_start</span><br><span class="line"></span><br><span class="line">    /* 清空pmd页表项，做cpu tlbi */</span><br><span class="line">+-&gt; pmdp_collapse_flush</span><br><span class="line"></span><br><span class="line">+-&gt; mmu_notifier_invalidate_range_end</span><br><span class="line"></span><br><span class="line">    /* 清空pte页表, free相关页面 */</span><br><span class="line">+-&gt; __collapse_huge_page_isolate</span><br><span class="line"></span><br><span class="line">    /* 把小页里的内容copy到大页 */</span><br><span class="line">+-&gt; __collapse_huge_page_copy</span><br><span class="line"></span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">    /* 装大页页表 */</span><br><span class="line">+-&gt; set_pmd_at</span><br><span class="line"></span><br><span class="line">+-&gt; mmap_write_unlock</span><br></pre></td></tr></table></figure>
<p>  注意其他的cpu或者设备可以这个过程中还在写原来的小页内存，这些操作和如上页表<br>  变动的同步点在上面两个tlbi处，tlbi和随后的barrier可以保证之前正在总线上的相关<br>  地址操作都完成。这样在barrier后的新访存操作都会触发fault，这些fault都会在<br>  mmap_write_lock上排队。等到新访问的fault可以执行的时候，发现已经有大页，就可以<br>  做后续的处理。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI ACPI笔记2</title>
    <url>/2021/07/11/PCI-ACPI%E7%AC%94%E8%AE%B02/</url>
    <content><![CDATA[<h2 id="Basic-points-about-PCI-ACPI"><a href="#Basic-points-about-PCI-ACPI" class="headerlink" title="Basic points about PCI ACPI"></a>Basic points about PCI ACPI</h2><p>There are three APCI tables involved in: DSDT, MCFG and IORT</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MCFG: should configure ECAM compliant configure space address(CPU address).</span><br><span class="line">|</span><br><span class="line">|    _CBA: MMCONFIG base address</span><br><span class="line"></span><br><span class="line">DSDT: we can configure IO/MEM range and some SoC specific info in DSDT.</span><br><span class="line">|</span><br><span class="line">|    _CRS indicates: MEM/IO space, bus range</span><br><span class="line">|</span><br><span class="line">|    ...</span><br><span class="line">|</span><br><span class="line">|    PNP0A03 ?</span><br><span class="line"></span><br><span class="line">IORT: build up the map among RID, stream ID and device ID.</span><br><span class="line">|</span><br><span class="line">|    ...</span><br><span class="line"></span><br><span class="line">_PRT: PCI routing table for INTx</span><br></pre></td></tr></table></figure>

<p>let’s see a sample of above tables in [2]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// for details, refer to p.42 in [4]</span><br><span class="line">EFI_ACPI_5_0_PCI_EXPRESS_MEMORY_MAPPED_CONFIGURATION_SPACE_TABLE Mcfg=</span><br><span class="line">&#123;</span><br><span class="line">  &#123;</span><br><span class="line">      &#123;</span><br><span class="line">        EFI_ACPI_5_0_PCI_EXPRESS_MEMORY_MAPPED_CONFIGURATION_SPACE_BASE_ADDRESS_DESCRIPTION_TABLE_SIGNATURE,</span><br><span class="line">        sizeof (EFI_ACPI_5_0_PCI_EXPRESS_MEMORY_MAPPED_CONFIGURATION_SPACE_TABLE),</span><br><span class="line">        ACPI_5_0_MCFG_VERSION,</span><br><span class="line">        0x00,                                             // Checksum will be updated at runtime</span><br><span class="line">        &#123;EFI_ACPI_ARM_OEM_ID&#125;,</span><br><span class="line">        EFI_ACPI_ARM_OEM_TABLE_ID,</span><br><span class="line">        EFI_ACPI_ARM_OEM_REVISION,</span><br><span class="line">        EFI_ACPI_ARM_CREATOR_ID,</span><br><span class="line">        EFI_ACPI_ARM_CREATOR_REVISION</span><br><span class="line">      &#125;,</span><br><span class="line">      0x0000000000000000,                                 //Reserved</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line"></span><br><span class="line">    // just a sample, so only list one Segment</span><br><span class="line">    &#123;</span><br><span class="line">      0xb0000000,                                         //Base Address</span><br><span class="line">      0x0,                                                //Segment Group Number</span><br><span class="line">      0x0,                                                //Start Bus Number</span><br><span class="line">      0x1f,                                               //End Bus Number</span><br><span class="line">      0x00000000,                                         //Reserved</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">so structure of MCFG table is:</span><br><span class="line">&#123;</span><br><span class="line">    EFI_ACPI_5_0_MCFG_TABLE_CONFIG</span><br><span class="line">    &#123;</span><br><span class="line">        EFI_ACPI_DESCRIPTION_HEADER</span><br><span class="line">        UINT64 Reserved1</span><br><span class="line">    &#125;</span><br><span class="line">    EFI_ACPI_5_0_MCFG_CONFIG_STRUCTURE[]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Scope(_SB)</span><br><span class="line">&#123;</span><br><span class="line">  // PCIe Root bus, for the ASL grammar, please refer to 19 charpter in [5]</span><br><span class="line">  Device (PCI0)                              // acpi_get_devices(&quot;HISI0080&quot;, ...) ?</span><br><span class="line">  &#123;</span><br><span class="line">    // for the details of items below, please refer to charpter 6 in [5]</span><br><span class="line">    Name (_HID, &quot;HISI0080&quot;)                  // PCI Express Root Bridge</span><br><span class="line">    Name (_CID, &quot;PNP0A03&quot;)                   // Compatible PCI Root Bridge, Compatible ID</span><br><span class="line">    Name(_SEG, 0)                            // Segment of this Root complex</span><br><span class="line">    Name(_BBN, 0)                            // Base Bus Number</span><br><span class="line">    Name(_CCA, 1)                            // cache coherence attribute ??</span><br><span class="line">    Method (_CRS, 0, Serialized) &#123;           // Root complex resources, _CRS: current resource setting</span><br><span class="line">                                             // Method is defined in 19.6.82 in [5]</span><br><span class="line">      Name (RBUF, ResourceTemplate () &#123;      // Name: 19.6.87, ResourceTemplate: 19.6.111,</span><br><span class="line">                                             // 19.3.3 in [5]</span><br><span class="line">        WordBusNumber (                      // Bus numbers assigned to this root,</span><br><span class="line">                                             // wordBusNumber: 19.6.144</span><br><span class="line">          ResourceProducer,</span><br><span class="line">          MinFixed,</span><br><span class="line">          MaxFixed,</span><br><span class="line">          PosDecode,</span><br><span class="line">          0,                                 // AddressGranularity</span><br><span class="line">          0x0,                               // AddressMinimum - Minimum Bus Number</span><br><span class="line">          0x1f,                              // AddressMaximum - Maximum Bus Number</span><br><span class="line">          0,                                 // AddressTranslation - Set to 0</span><br><span class="line">          0x20                               // RangeLength - Number of Busses</span><br><span class="line">        )</span><br><span class="line">        QWordMemory (                        // 64-bit BAR Windows</span><br><span class="line">          ResourceProducer,</span><br><span class="line">          PosDecode,</span><br><span class="line">          MinFixed,</span><br><span class="line">          MaxFixed,</span><br><span class="line">          Cacheable,</span><br><span class="line">          ReadWrite,</span><br><span class="line">          0x0,                               // Granularity</span><br><span class="line">          0xb2000000,                        // Min Base Address pci address</span><br><span class="line">          0xb7feffff,                        // Max Base Address</span><br><span class="line">          0x0,                               // Translate</span><br><span class="line">          0x5ff0000                          // Length</span><br><span class="line">        )</span><br><span class="line">        QWordIO (</span><br><span class="line">          ResourceProducer,</span><br><span class="line">          MinFixed,</span><br><span class="line">          MaxFixed,</span><br><span class="line">          PosDecode,</span><br><span class="line">          EntireRange,</span><br><span class="line">          0x0,</span><br><span class="line">          0x0,</span><br><span class="line">          0xffff,</span><br><span class="line">          0xb7ff0000,</span><br><span class="line">          0x10000</span><br><span class="line">        )</span><br><span class="line">      &#125;)                                      // Name(RBUF)</span><br><span class="line">      Return (RBUF)</span><br><span class="line">    &#125;                                         // Method(_CRS), this method return RBUF!</span><br><span class="line">  &#125; // Device(PCI0)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// did not include smmu related description, for the detail, please refer to [6]</span><br><span class="line">// IORT Head:</span><br><span class="line">[0004]                          Signature : &quot;IORT&quot;    [IO Remapping Table]</span><br><span class="line">[0004]                       Table Length : 0000029e</span><br><span class="line">[0001]                           Revision : 00</span><br><span class="line">[0001]                           Checksum : BC</span><br><span class="line">[0006]                             Oem ID : &quot;HISI &quot;</span><br><span class="line">[0008]                       Oem Table ID : &quot;D03&quot;</span><br><span class="line">[0004]                       Oem Revision : 00000000</span><br><span class="line">[0004]                    Asl Compiler ID : &quot;INTL&quot;</span><br><span class="line">[0004]              Asl Compiler Revision : 20150410</span><br><span class="line"></span><br><span class="line">[0004]                         Node Count : 00000003</span><br><span class="line">[0004]                        Node Offset : 00000034</span><br><span class="line">[0004]                           Reserved : 00000000</span><br><span class="line">[0004]                   Optional Padding : 00 00 00 00</span><br><span class="line"></span><br><span class="line">/* ITS 0, for dsa */</span><br><span class="line">[0001]                               Type : 00</span><br><span class="line">[0002]                             Length : 0018</span><br><span class="line">[0001]                           Revision : 00</span><br><span class="line">[0004]                           Reserved : 00000000</span><br><span class="line">[0004]                      Mapping Count : 00000000</span><br><span class="line">[0004]                     Mapping Offset : 00000000</span><br><span class="line"></span><br><span class="line">[0004]                           ItsCount : 00000001</span><br><span class="line">[0004]                        Identifiers : 00000000</span><br><span class="line"></span><br><span class="line">/* mbi-gen dsa  mbi0 - usb, named component */</span><br><span class="line">[0001]                               Type : 01</span><br><span class="line">[0002]                             Length : 0046</span><br><span class="line">[0001]                           Revision : 00</span><br><span class="line">[0004]                           Reserved : 00000000</span><br><span class="line">[0004]                      Mapping Count : 00000001</span><br><span class="line">[0004]                     Mapping Offset : 00000032</span><br><span class="line"></span><br><span class="line">[0004]                         Node Flags : 00000000</span><br><span class="line">[0008]                  Memory Properties : [IORT Memory Access Properties]</span><br><span class="line">[0004]                    Cache Coherency : 00000000</span><br><span class="line">[0001]              Hints (decoded below) : 00</span><br><span class="line">                                Transient : 0</span><br><span class="line">                           Write Allocate : 0</span><br><span class="line">                            Read Allocate : 0</span><br><span class="line">                                 Override : 0</span><br><span class="line">[0002]                           Reserved : 0000</span><br><span class="line">[0001]       Memory Flags (decoded below) : 00</span><br><span class="line">                                Coherency : 0</span><br><span class="line">                         Device Attribute : 0</span><br><span class="line">[0001]                  Memory Size Limit : 00</span><br><span class="line">[0017]                        Device Name : &quot;\_SB_.MBI0&quot;</span><br><span class="line">[0004]                            Padding : 00 00 00 00</span><br><span class="line"></span><br><span class="line">[0004]                         Input base : 00000000</span><br><span class="line">[0004]                           ID Count : 00000001</span><br><span class="line">[0004]                        Output Base : 00040080  // device id</span><br><span class="line">[0004]                   Output Reference : 00000034  // point to its dsa</span><br><span class="line">[0004]              Flags (decoded below) : 00000000</span><br><span class="line">                           Single Mapping : 1</span><br><span class="line"></span><br><span class="line">/* RC 0 */</span><br><span class="line">[0001]                               Type : 02</span><br><span class="line">[0002]                             Length : 0034</span><br><span class="line">[0001]                           Revision : 00</span><br><span class="line">[0004]                           Reserved : 00000000</span><br><span class="line">[0004]                      Mapping Count : 00000001</span><br><span class="line">[0004]                     Mapping Offset : 00000020</span><br><span class="line"></span><br><span class="line">[0008]                  Memory Properties : [IORT Memory Access Properties]</span><br><span class="line">[0004]                    Cache Coherency : 00000001</span><br><span class="line">[0001]              Hints (decoded below) : 00</span><br><span class="line">                                Transient : 0</span><br><span class="line">                           Write Allocate : 0</span><br><span class="line">                            Read Allocate : 0</span><br><span class="line">                                 Override : 0</span><br><span class="line">[0002]                           Reserved : 0000</span><br><span class="line">[0001]       Memory Flags (decoded below) : 00</span><br><span class="line">                                Coherency : 0</span><br><span class="line">                         Device Attribute : 0</span><br><span class="line">[0004]                      ATS Attribute : 00000000</span><br><span class="line">[0004]                 PCI Segment Number : 00000000</span><br><span class="line"></span><br><span class="line">// Input, output means BDF of pcie host as the device ID in ITS</span><br><span class="line">[0004]                         Input base : 00000000</span><br><span class="line">[0004]                           ID Count : 00002000       // the number of IDs in range</span><br><span class="line">[0004]                        Output Base : 00000000</span><br><span class="line">// refer to the a node, here refer to ITS node in 0x34 offset</span><br><span class="line">[0004]                   Output Reference : 00000034</span><br><span class="line">[0004]              Flags (decoded below) : 00000000</span><br><span class="line">                           Single Mapping : 0</span><br></pre></td></tr></table></figure>

<h2 id="Work-flow-of-ACPI-parse"><a href="#Work-flow-of-ACPI-parse" class="headerlink" title="Work flow of ACPI parse"></a>Work flow of ACPI parse</h2><p>based on kernel v4.8</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* from the view of ACPI, this function is the start */</span><br><span class="line">acpi_init</span><br><span class="line">    --&gt; acpi_bus_init</span><br><span class="line"></span><br><span class="line">        /* this is a week function */</span><br><span class="line">    --&gt; pci_mmcfg_late_init</span><br><span class="line">            /* parse the MCFG table: drivers/acpi/pci_mcfg.c */</span><br><span class="line">        --&gt; acpi_table_parse(ACPI_SIG_MCFG, pci_mcfg_parse) </span><br><span class="line">                /* add mcfg_entry which contain info in mcfg to pci_mcfg_list,</span><br><span class="line">                 * this will add all mcfg region to above list.</span><br><span class="line">                 */</span><br><span class="line">            --&gt; pci_mcfg_parse</span><br><span class="line"></span><br><span class="line">        /* this function is not in v4.8 now, it is used by PCIe controller to map</span><br><span class="line">         * itself to correct ITS and SMMU. This will be modified in future maybe.</span><br><span class="line">         *</span><br><span class="line">         * Now, it parses iort table in iort_table_detect. other moduldes like ITS</span><br><span class="line">         * will directly call the functions in iort_table_detect.</span><br><span class="line">         */</span><br><span class="line">    --&gt; iort_table_detect </span><br><span class="line"></span><br><span class="line">        /* this function will scan all ACPI resource, including PCI */</span><br><span class="line">    --&gt; acpi_scan_init</span><br><span class="line">	--&gt; acpi_pci_root_init</span><br><span class="line">	--&gt; acpi_pci_link_init</span><br><span class="line"></span><br><span class="line">	--&gt; acpi_bus_scan</span><br><span class="line">                /* each Device in DSDT will be a device below, which has a root</span><br><span class="line">                 * bus. normally, we configure related address translate hardware</span><br><span class="line">                 * unit in UEFI for this device(rp).</span><br><span class="line">                 */</span><br><span class="line">            --&gt; acpi_bus_attach(device)</span><br><span class="line"></span><br><span class="line">            ...     /* why we call acpi_pci_root_add, please refer to</span><br><span class="line">                     * http://blog.csdn.net/scarecrow_byr/article/details/42619749</span><br><span class="line">                     * scan PCI info in DSDT</span><br><span class="line">                     */</span><br><span class="line">	        --&gt; acpi_pci_root_add</span><br><span class="line">		    --&gt; pci_acpi_scan_root</span><br><span class="line">		        --&gt; acpi_pci_root_create</span><br><span class="line">			    --&gt; acpi_pci_probe_root_resources</span><br><span class="line">			            /* here parse _CRS info in DSDT */</span><br><span class="line">			        --&gt; acpi_dev_get_resources</span><br></pre></td></tr></table></figure>

<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p>[1]. [RFC PATCH v3 0/3] Add ACPI support for HiSilicon PCIe Host Controllers<br>[2]. <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29wZW4tZXN0dWFyeS91ZWZpLmdpdC9PcGVuUGxhdGZvcm1Qa2cvQ2hpcHMvSGlzaWxpY29uL1B2NjYwL1B2NjYwQWNwaVRhYmxlcy8=">https://github.com/open-estuary/uefi.git/OpenPlatformPkg/Chips/Hisilicon/Pv660/Pv660AcpiTables/<i class="fa fa-external-link-alt"></i></span><br>[3]. “ACPI &amp; PCI 学习笔记” in this blog<br>[4]. PCI Firmware Specification 3.0<br>[5]. ACPI 6.0 spec<br>[6]. IORT spec</p>
]]></content>
      <tags>
        <tag>UEFI</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI FLR analysis</title>
    <url>/2021/06/29/PCI-FLR-analysis/</url>
    <content><![CDATA[<p>关于FLR的硬件操作比较简单, 相关的硬件有:</p>
<ul>
<li>配置空间里device cap里的FLR capability bit, 这个表示设备是否支持FLR。</li>
<li>配置空间里device control里的BCR_FLR bit, 写这个bit可以触发FLR。</li>
</ul>
<p>检测是否支持FLR</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/pci/pci.c */</span><br><span class="line">pcie_has_flr(struct pci_dev *dev)</span><br></pre></td></tr></table></figure>

<p>Linux kernel里pcie_flr会被下面的三个函数调用到, 触发FLR</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/pci/pci.c: below tree functions will call __pci_reset_function_locked */</span><br><span class="line">pci_reset_function(struct pci_dev *dev)</span><br><span class="line">pci_reset_function_locked(struct pci_dev *dev)</span><br><span class="line">pci_try_reset_function(struct pci_dev *dev)</span><br><span class="line">    =&gt; __pci_reset_function_locked(struct pci_dev *dev)</span><br><span class="line">        -&gt; pcie_flr(struct pci_dev *dev)</span><br></pre></td></tr></table></figure>

<p>下面来看上面的三个函数在哪里会用到。</p>
<p>首先通过这个设备的sysfs接口可以触发FLR:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/pci/pci-sysfs.c */</span><br><span class="line">reset_store</span><br><span class="line">    -&gt; pci_reset_function(struct pci_dev *dev)</span><br></pre></td></tr></table></figure>

<p>另外，vfio里也提供的接口，可以供用户触发FLR。这些接口包括，vfio设备的enable,<br>disable, 以及一个vfio设备相关的ioctl。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/vfio/pci/vfio_pci_config.c */</span><br><span class="line">vfio_exp_config_write</span><br><span class="line">    -&gt; pci_try_reset_function</span><br><span class="line"></span><br><span class="line">/* drivers/vfio/pci/vfio_pci.c */</span><br><span class="line">vfio_pci_enable</span><br><span class="line">vfio_pci_disable</span><br><span class="line">vfio_pci_ioctl (cmd == VFIO_DEVICE_RESET)</span><br><span class="line">    =&gt; pci_try_reset_function(pdev);</span><br></pre></td></tr></table></figure>

<p>单独的FLR操作需要配合整个reset流程工作, 在上面的调用pcie_flr的函数里，他们基本<br>的处理流程都是, 先做reset_prepare, 再触发FLR，最后做reset后的恢复:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">the logic of pci_reset_function and its brother functions are:</span><br><span class="line">	- reset_prepare</span><br><span class="line">	- flr operation if supporting flr</span><br><span class="line">	- reset_done</span><br></pre></td></tr></table></figure>

<p>reset_prepare and reset_done callbacks are stored in pci_driver’s pci_error_handlers,<br>these callbacks should be offered by your device driver:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct pci_driver &#123;</span><br><span class="line">	...</span><br><span class="line">	const struct pci_error_handlers &#123;</span><br><span class="line">		...</span><br><span class="line">		void (*reset_prepare)(struct pci_dev *dev);</span><br><span class="line">		void (*reset_done)(struct pci_dev *dev);</span><br><span class="line">		...</span><br><span class="line">	&#125;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从上面的代码分析中可以看出，linux内核中的flr流程并不涉及到软件中设备结构的销毁。<br>所以，只要在发生flr这段时间不去下发硬件请求，用lspci看，设备一直都在。一般硬件<br>在实现flr的时候，在硬件层面都有pf到vf的通知方式, 这样可以保证在pf flr时候通知到<br>flr做必要的处理，当pf flr完成后，可以通知vf驱动做必要的硬件配置上的恢复。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux驱动软硬件兼容问题的考虑</title>
    <url>/2021/06/19/Linux%E9%A9%B1%E5%8A%A8%E8%BD%AF%E7%A1%AC%E4%BB%B6%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98%E7%9A%84%E8%80%83%E8%99%91/</url>
    <content><![CDATA[<ol>
<li>面对的问题</li>
</ol>
<hr>
<p> 首先看我们的工作模型是怎么样。</p>
<p> 硬件设计一代接着一代进行，前后之间的设计可能有：1. 把一个功能砍了(硬件为了保<br> 证兼容性一般不会这样); 2. 新加了一个功能; 3. 改了一个功能(为了兼容性一般也不会<br> 这样); 4. 改了一个功能，但是为了兼容性还支持老的使用方式。</p>
<p> Linux驱动开发，我们一般先直接上传到Linux主线。然后，根据需要回合到各个实际使用<br> 的发行版本中。各个发型版本中因为要做质量控制，所以在发布后，有可能只回合bugfix，<br> 不做大特性的回合。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mainline: a -&gt; b -&gt; c -&gt; d -&gt; e -&gt; f -&gt; g</span><br><span class="line"></span><br><span class="line">distribution a: a -&gt; b -&gt; c</span><br><span class="line">distribution b: a -&gt; b -&gt; c -&gt; d -&gt; e</span><br></pre></td></tr></table></figure>
<p> 软件兼容性要求我们：1. 加一个新特性的时候，不能破坏老特性的使用; 2. 老的软件<br> 在新的硬件上使用时，老的特性还是可以使用的。</p>
<ol start="2">
<li>解决办法</li>
</ol>
<hr>
<p> 针对上面的，软件兼容性要求第一点，我们不能改动已有的对外接口，比如各种内核ABI<br> 接口(sysfs，debugfs，ioctl，mmap等等)。</p>
<p> 针对以上第二点，软件中不能用硬件版本号区分特性，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (harware version == X)</span><br><span class="line"> feature_one();</span><br></pre></td></tr></table></figure>
<p> 上面这样，在hardware version == X + 1硬件时，老的软件feature_one就无法使用了。</p>
<p> 一般的做法是, 通过预留的功能使能标记来判断是否有该功能。而hardware version只用<br> 来修复正常流程里的bug。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (feature_on_enable())</span><br><span class="line"> feature_one();</span><br></pre></td></tr></table></figure>

<p> 对于纯粹新加的硬件，我们只要原子的加上一个相关的支持补丁就可以，没有回合这个<br> 补丁就不支持新的功能。</p>
<p> 对于硬件设计中的第4点。我们可以把改动后的功能认为是正常流程，而把兼容之前的<br> 硬件设计作为一个bug用hardware version隔开。具体可以：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (harware version == X)</span><br><span class="line"> feature_one();</span><br><span class="line">if (feature_on_enable())</span><br><span class="line"> feature_one_new();</span><br></pre></td></tr></table></figure>
<p> 这里我们假设硬件在X + 1这个版本上对feature_one做了如上第4点里的改动。<br> 我们加一个原子的补丁，把硬件为了向前兼容做的努力看成是一个特殊硬件版本的bug，<br> 而新的改动，我们把它看成是主流程。这样，如果不合这个补丁，之前老的软件可以运行<br> 在新的硬件上。合入这个补丁，新的软件可以跑在新的硬件上，并且使用改动后的特性，<br> 新的软件也可以跑在老的硬件上，同时新的软件也可以兼容更新的硬件(只要硬件保证兼容)。</p>
]]></content>
      <tags>
        <tag>软件架构</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI/SMMU ATS analysis note</title>
    <url>/2021/07/05/PCI-SMMU-ATS-analysis-note/</url>
    <content><![CDATA[<ol>
<li>ATS</li>
</ol>
<hr>
<p>ATS is the short of Access Translation Service. In the PCIe spec, the ATS section<br>includes ATS, PRI and PASID. Here we only consider ATS.</p>
<p>Generally speaking, ATS just prefetch the PA(physical address) to store in PCIe<br>device’s side. Once this PCIe device sending a memory read/write operation<br>(e.g memory read/write TLP), it will bring a flag which means that the address<br>of this operation had already been translated. When this operation passes SMMU,<br>SMMU will parse this flag and know that this address is a PA and SMMU will do<br>nothing about this operation, SMMU then will just forward this operation to<br>read/write memory.</p>
<p>The hardware will do the prefetch automatically. But software must be involved<br>when we want to disable/invalidate a prefetched PA in PCIe device.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    +-------+</span><br><span class="line">    |  CPU  |</span><br><span class="line">    +---+---+</span><br><span class="line">        |</span><br><span class="line">--------+-----------------------------+----</span><br><span class="line">        |        PA   ^               |</span><br><span class="line">    +---+---+    ^    |            +--+--+</span><br><span class="line">    | SMMU  |    |    |            | DDR |</span><br><span class="line">    +---+---+    |    |            +-----+</span><br><span class="line">        |        VA   |</span><br><span class="line">    +---+---+         |</span><br><span class="line">    |  RP   |         |</span><br><span class="line">    +---+---+         |</span><br><span class="line">        |             |</span><br><span class="line">    +---+---+         PA</span><br><span class="line">    |  EP   |</span><br><span class="line">    +-------+</span><br></pre></td></tr></table></figure>

<p>From the above figure, with SMMU enable and without ATS enable, read/write<br>operation from EP will be with a VA, SMMU help to do the translation from VA to<br>PA. However, when enabling ATS, PA can be stored in EP ahead, EP can send<br>read/write operation with a PA directly, which will improve the performance.</p>
<ol start="2">
<li>Hareware support</li>
</ol>
<hr>
<p> For EP that supports ATS must have ATS Extended Capability in its CFG. In ATS<br> cap, we have ATS capability registers and ATS controller registers. Now all<br> bits in ATS capability registers are RO, STU and Enable in ATS control registers<br> are RW. STU means smallest translation unit, which should be matched with SMMU<br> supported page size.</p>
<p> For SMMU, we should enable ATS by IDR0_ATS in SMMU_IDR0.ATS.</p>
<ul>
<li><p>ATS enable</p>
<p>We should enable SMMU_IDR0.ATS and enable bit in PCIe device’s ATS cap.</p>
</li>
<li><p>ATS request</p>
<p>I think the ATS request which will be followed by a ATS completion is triggered<br>automatically. PCIe ATS spec says “Host system software can not modify the ATC”,<br>which is used to store the PA in PCIe device.</p>
<p>I think the ATS request will be device specific and be implemented together<br>with the DMA of PCIe device. There is only one PCIe EP card(Mellanox ConnectX-5)<br>supported ATS, ATS in PCIe device(EP) maybe will implemented together with<br>EP’s DMA, which means DMA will still get a VA from SMMU as target address,<br>however, before DMA operation, EP will firstly send a ATS request to get the<br>related PA of above VA, then EP will send DMA read/write operation with this<br>PA. But how many does EP send ATS request? For example, if 8K size DMA range<br>has been requested, and STU is 4K for system, will EP send 2 ATS request 2 PA?<br>This also has relationship with the process of ATS invalidation. From the code<br>below, it will invalidate a range of iova as the inputs of arm_smmu_atc_inv_to_cmd<br>include iova and size. If the range of iova to iova + size covers multiple<br>stu/page, what will happen in the hardware?</p>
</li>
<li><p>ATS invalidate</p>
<p>When VA -&gt; PA changed in SMMU, PA in PCIe device’s ATC is not the correct<br>address, so host software should invalidate the cached PA by leting SMMU<br>send an ATS invalidation request. ATS invalidation completion will be sent<br>from the PCIe device later.</p>
<p>SMMU will use command CMD_ATC_INV to invalidate the cached PA in EP.</p>
</li>
</ul>
<ol start="3">
<li>software support</li>
</ol>
<hr>
<p> For analysis will base on [PATCH 0/7] Add PCI ATS support to SMMUv3, we can<br> find this in git://linux-arm.org/linux-jpb.git branch: svm/ats-v1.</p>
<ul>
<li><p>ATS enable:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">arm_smmu_add_device</span><br><span class="line">    --&gt; arm_smmu_enable_ats</span><br><span class="line">            /* will configure stu in this function, stu here comes from smmu */</span><br><span class="line">        --&gt; pci_enable_ats(pdev, stu)</span><br></pre></td></tr></table></figure></li>
<li><p>ATS invalidate:</p>
<p>There are two places to call ATC_INV:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*    arm_smmu_detach_dev</span><br><span class="line">         --&gt; arm_smmu_atc_inv_master_all</span><br><span class="line"></span><br><span class="line">*    arm_smmu_unmap(struct iommu_domain *domain, unsigned long iova, size_t size)</span><br><span class="line">         --&gt; arm_smmu_atc_inv_domain(smmu_domain, 0, iova, size)</span><br><span class="line">                 /* it has an algrithm to caculate the invalidate page */</span><br><span class="line">             --&gt; arm_smmu_atc_inv_to_cmd(ssid, iova, size, &amp;cmd)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">arm_smmu_unmap input: iommu domain, iova, size</span><br><span class="line">       /*</span><br><span class="line">        * we pick each device in this iommu domain to do the invalidation operation,</span><br><span class="line">        * normally, we have one device in one iommu domain.</span><br><span class="line">        * </span><br><span class="line">        * here struct arm_smmu_master_data is for every device under a SMMU, and</span><br><span class="line">        * it has been created in arm_smmu_add_device.</span><br><span class="line">        */</span><br><span class="line">    -&gt; arm_smmu_atc_inv_domain  input: smmu_domain, ssid, iova, size</span><br><span class="line">        --&gt; arm_smmu_atc_inv_master: </span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI SMMU parse in ACPI</title>
    <url>/2021/07/11/PCI-SMMU-parse-in-ACPI/</url>
    <content><![CDATA[<ol>
<li>acpi smmu parse</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">acpi_init</span><br><span class="line">    --&gt; acpi_iort_init</span><br><span class="line">            /* parse the SMMU node in IORT table */</span><br><span class="line">        --&gt; iort_init_platform_devices</span><br><span class="line">                /* create a platform_device for SMMU, and add it to platform_bus */</span><br><span class="line">            --&gt; iort_add_smmu_platform_device</span><br><span class="line">                    /* why call this function here? */</span><br><span class="line">                --&gt; acpi_dma_configure</span><br><span class="line">                    --&gt; iort_iommu_configure</span><br><span class="line"></span><br><span class="line">            /* call functions in section: __iort_acpi_probe_table ~ __iort_acpi_probe_table_end */</span><br><span class="line">        --&gt; acpi_probe_device_table(iort)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>put arm_smmu_init in above section in compile phase</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IORT_ACPI_DECLARE(arm_smmu_v3, ACPI_SIG_IORT, acpi_smmu_v3_init);</span><br><span class="line">arm_smmu_v3_init </span><br><span class="line">        /*</span><br><span class="line">         * add SMMU driver to platform_bus, this will trigger probe function to</span><br><span class="line">         * bind this driver and above SMMU platform_device.</span><br><span class="line">         */</span><br><span class="line">    --&gt; platform_driver_register(&amp;arm_smmu_driver)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>PCIe device get its SMMU device</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_device_add</span><br><span class="line">    --&gt; pci_dma_configure(dev)</span><br><span class="line">        --&gt; acpi_dma_configure(&amp;dev-&gt;dev, attr)</span><br><span class="line"></span><br><span class="line">acpi_dma_configure</span><br><span class="line">       /* if dev is a pci device, first find related RC node in IORT table */</span><br><span class="line">    --&gt;iommu = iort_iommu_configure </span><br><span class="line">            /* RC node */</span><br><span class="line">        --&gt; node = iort_scan_node</span><br><span class="line">                /* parent here is SMMU node */</span><br><span class="line">            --&gt; parent = iort_node_map_rid</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>ACPI</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI MSI parse in ACPI</title>
    <url>/2021/07/11/PCI-MSI-parse-in-ACPI/</url>
    <content><![CDATA[<h2 id="parse-IORT-table-and-build-ITS-interrupt-domain"><a href="#parse-IORT-table-and-build-ITS-interrupt-domain" class="headerlink" title="parse IORT table and build ITS interrupt domain"></a>parse IORT table and build ITS interrupt domain</h2><p>During PCI enumeration, pci_device_add will be called for each PCIe devices.<br>In pci_device_add, we will get the ITS irq domain for this PCIe device and store<br>this domain in this device’s struct(pci_dev).</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_device_add</span><br><span class="line">    --&gt; pci_set_msi_domain(dev)</span><br><span class="line">        --&gt; pci_dev_msi_domain(dev)</span><br><span class="line">            --&gt; pci_msi_get_device_domain</span><br><span class="line">                --&gt; iort_get_device_domain(&amp;pdev-&gt;dev, rid)</span><br><span class="line">                    --&gt; iort_dev_find_its_id(dev, req_id, 0, &amp;its_id)</span><br><span class="line">                            /* key point to find the RC node in IORT table */</span><br><span class="line">                        --&gt; iort_find_dev_node(dev)</span><br><span class="line">                            /*</span><br><span class="line">			     * if the device is a pci device, we get its related</span><br><span class="line">                             * root bus, and we scan the IORT table to get the</span><br><span class="line">                             * PCI_ROOT_COMPLEX node of this root bus.</span><br><span class="line">                             * </span><br><span class="line">                             * point is we use iort_match_node_callback to confirm</span><br><span class="line">                             * that we find the right PCI_ROOT_COMPLEX. however,</span><br><span class="line">                             * above function uses &quot;segment value&quot; to do the check</span><br><span class="line">                             * </span><br><span class="line">                             * if we configure IORT as:</span><br><span class="line">                             * RC0: segment0: ITS map1</span><br><span class="line">                             * RC1: segment0: ITS map2</span><br><span class="line">                             * </span><br><span class="line">                             * when we try to find a pcie device under RC1, finally</span><br><span class="line">                             * we will get ITS map1 under RC0.[2]</span><br><span class="line">                             * </span><br><span class="line">                             */</span><br><span class="line">                    ...</span><br><span class="line">                        /* return irq domain */</span><br><span class="line">                    --&gt; irq_find_matching_fwnode(handle, DOMAIN_BUS_PCI_MSI)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iort_dev_find_its_id</span><br><span class="line">       /* find related RC node in IORT table */</span><br><span class="line">    --&gt;iort_find_dev_node</span><br><span class="line">       /*</span><br><span class="line">        * find related ITS node in MADT table here. Is there a bug?</span><br><span class="line">        *</span><br><span class="line">        * This function supports multiple mappings in one RC[1]:</span><br><span class="line">        * 1. multiple ITS mappings.</span><br><span class="line">        * 2. multiple SMMU mappings?</span><br><span class="line">        * 3. multiple ITS/SMMU mappings</span><br><span class="line">        *</span><br><span class="line">        * One RC node in IORT table maps to one PCIe segment(one PCIe domain),</span><br><span class="line">        * we can have multiple PCIe host bridges in one PCIe segment.</span><br><span class="line">        *</span><br><span class="line">        */</span><br><span class="line">    --&gt;iort_node_map_rid(node, req_id, NULL, IORT_MSI_TYPE)</span><br></pre></td></tr></table></figure>
<h2 id="get-an-interrtup-in-PCIe-device-driver"><a href="#get-an-interrtup-in-PCIe-device-driver" class="headerlink" title="get an interrtup in PCIe device driver"></a>get an interrtup in PCIe device driver</h2><p>When PCIe device wants to apply interrupts, it will call some functions like: pci_enable_msi.<br>these kind of function will first get irq domain stored in pci_dev, then will<br>call ITS driver to get an interrupt resource.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><p>[1]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* PCIe0 */</span><br><span class="line">[0001]                               Type : 02</span><br><span class="line">[0002]                             Length : 0034</span><br><span class="line">[0001]                           Revision : 00</span><br><span class="line">[0004]                           Reserved : 00000000</span><br><span class="line">[0004]                      Mapping Count : 00000001</span><br><span class="line">[0004]                     Mapping Offset : 00000020</span><br><span class="line"></span><br><span class="line">[0008]                  Memory Properties : [IORT Memory Access Properties]</span><br><span class="line">[0004]                    Cache Coherency : 00000001</span><br><span class="line">[0001]              Hints (decoded below) : 00</span><br><span class="line">                                Transient : 0</span><br><span class="line">                           Write Allocate : 0</span><br><span class="line">                            Read Allocate : 0</span><br><span class="line">                                 Override : 0</span><br><span class="line">[0002]                           Reserved : 0000</span><br><span class="line">[0001]       Memory Flags (decoded below) : 00</span><br><span class="line">                                Coherency : 0</span><br><span class="line">                         Device Attribute : 0</span><br><span class="line">[0004]                      ATS Attribute : 00000000</span><br><span class="line">[0004]                 PCI Segment Number : 00000000</span><br><span class="line"></span><br><span class="line">[0004]                         Input base : 00000000</span><br><span class="line">[0004]                           ID Count : 00004000</span><br><span class="line">[0004]                        Output Base : 00000000</span><br><span class="line">[0004]                   Output Reference : 0000007c</span><br><span class="line">[0004]              Flags (decoded below) : 00000000</span><br><span class="line">                           Single Mapping : 0</span><br><span class="line"></span><br><span class="line">[0004]                         Input base : 00006000</span><br><span class="line">[0004]                           ID Count : 00000100</span><br><span class="line">[0004]                        Output Base : 00006000</span><br><span class="line">[0004]                   Output Reference : 0000007c</span><br><span class="line">[0004]              Flags (decoded below) : 00000000</span><br><span class="line">                           Single Mapping : 0</span><br></pre></td></tr></table></figure>
<p>[2]</p>
<p>This is our orignal design. But now it seems it is wrong. Now I think the concept<br>of RC should be mapped to PCIe domain or a segment. So multiple ITS maps for<br>one RC can be configured as in [1].</p>
<p>Same rule can be applied for SMMU configuration.</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe INTx parse in ACPI</title>
    <url>/2021/07/11/PCIe-INTx-parse-in-ACPI/</url>
    <content><![CDATA[<p>When probing a PCIe device, it will assign INTx to it.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_device_probe</span><br><span class="line">        /* arch/arm64/kernel/pci.c */</span><br><span class="line">    --&gt; pcibios_alloc_irq</span><br><span class="line">        --&gt; acpi_pci_irq_enable(dev)</span><br><span class="line">            --&gt; pin = dev-&gt;pin</span><br><span class="line">                /* From a EP, we find the related INTx in the RP.</span><br><span class="line">                 * It will search from the EP which wants to be assigned a INTx</span><br><span class="line">                 * to find a host bridge which has the INTx configure(will be in</span><br><span class="line">                 * the host bridge&#x27;s DSDT device)</span><br><span class="line">                 * The device(RP or IEP) which directly connects the logic host</span><br><span class="line">                 * bridge and the pin number after swizzling will be passed to</span><br><span class="line">                 * check function to find correct irq number</span><br><span class="line">                 * (check function: acpi_pci_irq_check_entry in</span><br><span class="line">                 * acpi_pci_irq_find_prt_entry)</span><br><span class="line">                 */</span><br><span class="line">            --&gt; acpi_pci_irq_lookup(dev, pin)</span><br><span class="line">                --&gt; acpi_pci_irq_find_prt_entry(dev, pin, &amp;entry)</span><br><span class="line">                --&gt; while (bridge) &#123;</span><br><span class="line">                        --&gt; pin = pci_swizzle_interrupt_pin(dev, pin)</span><br><span class="line">                        --&gt; acpi_pci_irq_find_prt_entry(bridge, pin, &amp;entry)</span><br><span class="line">                    &#125;</span><br><span class="line">                /* get irq number in _PRT */</span><br><span class="line">            --&gt; gsi = entry-&gt;index</span><br><span class="line">                /* driver/acpi/irq.c */</span><br><span class="line">            --&gt; rc = acpi_register_gsi(&amp;dev-&gt;dev, gsi, triggering, polarity)</span><br><span class="line">                --&gt; fwspec.fwnode = acpi_gsi_domain_id</span><br><span class="line">                    /* This is the API to get virq */</span><br><span class="line">                --&gt; irq_create_fwspec_mapping(&amp;fwspec)</span><br><span class="line">                /* here rc is the related virq */</span><br><span class="line">            --&gt; dev-&gt;irq = rc</span><br></pre></td></tr></table></figure>
<p>If we use GICv3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gic_acpi_init</span><br><span class="line">    --&gt; acpi_set_irq_model(ACPI_IRQ_MODEL_GIC, domain_handle)</span><br><span class="line">        --&gt; acpi_irq_model = model</span><br><span class="line">        --&gt; acpi_gsi_domain_id = fwnode</span><br></pre></td></tr></table></figure>
<p>We can get GICD domain like above.</p>
<p>dev-&gt;pin had been set in below flow:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_setup_device</span><br><span class="line">    --&gt; pci_read_irq</span><br><span class="line">        --&gt; pci_read_config_byte(dev, PCI_INTERRUPT_PIN, &amp;irq)</span><br><span class="line">        --&gt; dev-&gt;pin = irq</span><br></pre></td></tr></table></figure>

<p>We can add INTx configure in ACPI DSDT as described in 6.2.13.1 in ACPI spec 6.1.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Name(_PRT, Package&#123;</span><br><span class="line">        Package&#123;0xFFFF,0,0,640&#125;,   // INT_A</span><br><span class="line">        Package&#123;0xFFFF,1,0,641&#125;,   // INT_B</span><br><span class="line">        Package&#123;0xFFFF,2,0,642&#125;,   // INT_C</span><br><span class="line">        Package&#123;0xFFFF,3,0,643&#125;    // INT_D</span><br><span class="line">&#125;)                |    | |  |</span><br><span class="line">    +-------------+    | |  +---------------+</span><br><span class="line">    |       -----------+ |                  |</span><br><span class="line"> All_BDF   pin   interrup_controller   hw_irq_number</span><br></pre></td></tr></table></figure>
<p>If interrup_controller field is 0, we will use “global interrupt pool” mentioned<br>in ACPI spec. In ARM world, this “global interrupt pool” will be GICD which is<br>defined in above gic_acpi_init</p>
<p>If our hardware topology is like this:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">system bus  </span><br><span class="line">     |         </span><br><span class="line">     +----- RP0(device number is 0)</span><br><span class="line">     |</span><br><span class="line">     +----- RP1(device number is 1)</span><br><span class="line">     |</span><br><span class="line">     +----- RP2(device number is 2)</span><br><span class="line">     |</span><br><span class="line">     +----- IEP(device number is y)</span><br><span class="line">     |</span><br></pre></td></tr></table></figure>
<p>We can configure the INTx _RPT as:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Name(_PRT, Package&#123;</span><br><span class="line">        Package&#123;0xFFFF,0,0,RP0_INTA&#125;,</span><br><span class="line">        Package&#123;0xFFFF,1,0,RP0_INTB&#125;,</span><br><span class="line">        Package&#123;0xFFFF,2,0,RP0_INTC&#125;,</span><br><span class="line">        Package&#123;0xFFFF,3,0,RP0_INTD&#125;,</span><br><span class="line">        Package&#123;0x1FFFF,0,0,RP1_INTA&#125;,</span><br><span class="line">        Package&#123;0x1FFFF,1,0,RP1_INTB&#125;,</span><br><span class="line">        Package&#123;0x1FFFF,2,0,RP1_INTC&#125;,</span><br><span class="line">        Package&#123;0x1FFFF,3,0,RP1_INTD&#125;,</span><br><span class="line">        Package&#123;0x2FFFF,0,0,RP2_INTA&#125;,</span><br><span class="line">        Package&#123;0x2FFFF,1,0,RP2_INTB&#125;,</span><br><span class="line">        Package&#123;0x2FFFF,2,0,RP2_INTC&#125;,</span><br><span class="line">        Package&#123;0x2FFFF,3,0,RP2_INTD&#125; </span><br><span class="line">        Package&#123;0xyFFFF,1,0,IEP_INTA&#125; </span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>NOTE:<br> 把dts下INTx中断的解析也放到这个文档里吧，不想另外起文档了。调用链大致是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_device_add()</span><br><span class="line">    --&gt;pcibios_add_device(dev)</span><br><span class="line">        --&gt;dev-&gt;irq = of_irq_parse_and_map_pci(dev, 0, 0)</span><br><span class="line">	    --&gt;of_irq_parse_and_map_pci()</span><br><span class="line">	           /* check if pci_dev has a device_node */</span><br><span class="line">	        --&gt;pci_device_to_OF_node(pdev); ?</span><br><span class="line">		   /* what is pin for? */</span><br><span class="line">		--&gt;pci_read_config_byte(pdev, PCI_INTERRUPT_PIN, &amp;pin)</span><br><span class="line">		--&gt;of_irq_parse_raw</span><br><span class="line">		      /* just parse interrupt-cell, interrupt-map,</span><br><span class="line">		       * interrupt-map-mask</span><br><span class="line">		       */</span><br><span class="line">		   --&gt; of_get_property(...,&quot;interrupt-cell&quot;,...&quot;)</span><br><span class="line"></span><br><span class="line">	    --&gt;irq_create_of_mapping()</span><br></pre></td></tr></table></figure>
<p>可见，PCI的核心代码pci_device_add()会扫面dts中的信息，然后给对应的中断分配<br>中断号资源。分配好中断号(virq)会写到pci_dev-&gt;irq中，供pci设备驱动注册中断handler<br>的时候使用。各个pci设备中注册的中断handler有时会共享一个INTx中短线(e.g. INTa)。<br>这时一旦一个INTx中断被触发，不同设备上的中断handler都会被调用到。可见注册的时候，<br>这些中断handler都应该是shareable的。</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>ACPI</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe enumeration for SR-IOV PCIe device</title>
    <url>/2021/07/11/PCIe-enumeration-for-SR-IOV-PCIe-device/</url>
    <content><![CDATA[<h2 id="Parse-VF-PF-BARs"><a href="#Parse-VF-PF-BARs" class="headerlink" title="Parse VF/PF BARs"></a>Parse VF/PF BARs</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* In PCI enumeration, once we find SR-IOV capability, sriov_init will be called */</span><br><span class="line">pci_device_add</span><br><span class="line">    --&gt; pci_init_capabilities</span><br><span class="line">        --&gt; pci_iov_init</span><br><span class="line">                /* parse sr-iov capability in pf, get bar sizef of vf */</span><br><span class="line">            --&gt; sriov_init(struct pci_dev *dev, int pos)</span><br><span class="line">                ...</span><br><span class="line">                    /* get bar_i size of vf */</span><br><span class="line">                --&gt; __pci_read_base(dev, pci_bar_unknown, res, pos + PCI_SRIOV_BAR + i * 4);</span><br><span class="line">                ...</span><br><span class="line">                    /* get bar_i size for all vf */</span><br><span class="line">                --&gt; res-&gt;end = res-&gt;start + resource_size(res) * total - 1;</span><br></pre></td></tr></table></figure>
<p>so at this step, we will see BAR0<del>BAR5, ROM_BAR, VF_BAR0</del>VF_BAR5…, the sizes<br>of them will be known. VF_BAR0 includes BAR0 in VF1,2,…. the resouces assignment<br>below will allocate resources for VF BARs ahead. Here all BAR0 in VF1,2… has<br>been seen as one block. you can see pci_enable_sriov below will directly assign<br>related BAR of certain VF to related pci_dev, as we already caculate and allocate<br>the resouces here.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * so if we get the size of bar of vf here, assignment will happen in standard</span><br><span class="line"> * assignment in PCI subsystem.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/*</span><br><span class="line"> * this function will be called in specific PCIe device driver. e.g.</span><br><span class="line"> * drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c, when user in userspace triggers</span><br><span class="line"> * vf by sysfs or loads modules with vf parameters.</span><br><span class="line"> */</span><br><span class="line">pci_enable_sriov</span><br><span class="line">    --&gt; sriov_enable(dev, nr_virtfn)</span><br><span class="line">        --&gt; pci_iov_add_virtfn</span><br><span class="line">            ...</span><br><span class="line">                /*</span><br><span class="line">                 * As we already got all VFs&#x27; BARs been assigned above, so here</span><br><span class="line">                 * just fill the BAR resource to each VF&#x27;s pci_dev.</span><br><span class="line">                 */</span><br><span class="line">            --&gt; virtfn-&gt;resource[i].start = res-&gt;start + size * id</span><br></pre></td></tr></table></figure>
<h2 id="Note"><a href="#Note" class="headerlink" title="Note:                            "></a>Note:                            </h2><ol>
<li>BARs in PF/VF<pre><code>  +--------------+    +--------&gt;+----------+
  | PF CFG       |    |         | VF1 BAR0 |
  +--------------+    |         | VF2 BAR0 |
  | PCI cfg head:|    |         | ...      |
  | ...          |    |         | VF5 BAR0 |
  | BAR0 ~ BAR5  |    |         +----------+
  | ...          |    |  +-----&gt;+----------+
  +--------------+    |  |      | VF1 BAR1 |
  | SR-IOV cap:  |    |  |      | VF2 BAR1 |
  | ...          |    |  |      | ...      |
  | VF BAR0      +----+  |      | VF5 BAR1 |
  | VF BAR1      +-------+      +----------+
  | ...          |              ...              
  | VF BAR5      +-------------&gt;+----------+
  +--------------+              | VF1 BAR5 |
                                | VF2 BAR5 |
                                | ...      |
                                | VF5 BAR5 |
                                +----------+
</code></pre>
</li>
</ol>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe userspace tools: lspci, setpci and sysfs</title>
    <url>/2021/07/05/PCIe-userspace-tools-lspci-setpci-and-sysfs/</url>
    <content><![CDATA[<h2 id="lspci"><a href="#lspci" class="headerlink" title="lspci"></a>lspci</h2><p> lspci</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">00:00.0 Host bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 DMI2 (rev 02)</span><br><span class="line">00:01.0 PCI bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 PCI Express Root Port 1 (rev 02)</span><br><span class="line">00:02.0 PCI bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 PCI Express Root Port 2 (rev 02)</span><br><span class="line">00:02.2 PCI bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 PCI Express Root Port 2 (rev 02)</span><br><span class="line">00:03.0 PCI bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 PCI Express Root Port 3 (rev 02)</span><br><span class="line">00:03.2 PCI bridge: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 PCI Express Root Port 3 (rev 02)</span><br></pre></td></tr></table></figure>
<p>A general show of PCIe devices in system.</p>
<p> lspci -t</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[...]</span><br><span class="line"> \-[0000:00]-+-00.0</span><br><span class="line">             +-01.0-[01]----00.0</span><br><span class="line">             +-02.0-[02]--+-00.0</span><br><span class="line">             |            +-00.1</span><br><span class="line">             |            +-00.2</span><br><span class="line">             |            \-00.3</span><br><span class="line">             +-02.2-[03]--</span><br><span class="line">             +-03.0-[04]--</span><br><span class="line">             +-03.2-[05]--</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p>Show PCIe topology in tree picture. As showed in above picture, 0000:00 means<br>domain 0 bus 0; 01.0 means a device under bus 0, so its BDF should be 0000:00:01.0;<br>01.0-[01] here [01] means the bus number under device 00:01.0, some times we may<br>get [xx-yy] which means the bus range under this device, apparently this device<br>is a PCIe bridge; 01.0-[01]—-00.0 here 00.0 means a device which device:function<br>is 00.0, so together with its father bus, its BDF should be 0000:01:00.0.</p>
<p> lspci -s ff:0f.1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ff:0f.1 System peripheral: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 Buffered Ring Agent (rev 02)</span><br></pre></td></tr></table></figure>
<p>To see the specific device information, use lspci -s BDF</p>
<p> lspci -vvv</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">80:05.4 PIC: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 I/O APIC (rev 02) (prog-if 20 [IO(X)-APIC])</span><br><span class="line">	Subsystem: Device 19e5:2060</span><br><span class="line">	Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-</span><br><span class="line">	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-</span><br><span class="line">	Latency: 0</span><br><span class="line">	Region 0: Memory at c8000000 (32-bit, non-prefetchable) [size=4K]</span><br><span class="line">	Capabilities: &lt;access denied&gt;</span><br></pre></td></tr></table></figure>
<p>To see more information, use -vvv/-vv/-v. You can see BAR in “Region”, different<br>bridge window range and different capabilities.</p>
<p> lspci -xxx</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">7f:13.1 System peripheral: Intel Corporation Xeon E7 v3/Xeon E5 v3/Core i7 Integrated Memory Controller 0 Target Address, Thermal &amp; RAS Registers (rev 02)</span><br><span class="line">00: 86 80 71 2f 00 00 10 00 02 00 80 08 00 00 80 00</span><br><span class="line">10: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">20: 00 00 00 00 00 00 00 00 00 00 00 00 e5 19 60 20</span><br><span class="line">30: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br></pre></td></tr></table></figure>
<p>To see configure space as bit value, use -xxx/-xx/-x</p>
<h2 id="setpci"><a href="#setpci" class="headerlink" title="setpci"></a>setpci</h2><p> setpci -s BDF 20.L=0x12345678</p>
<p> Above command set 0x20 offset of configure space of a device which BDF is BDF<br> to 0x12345678. L here means 32bit, W will mean 16bit, B will mean 8bit.</p>
<h2 id="sysfs"><a href="#sysfs" class="headerlink" title="sysfs"></a>sysfs</h2><ul>
<li><p>remove</p>
<p>We can remove a device by writing 1 to its remove file.</p>
</li>
<li><p>rescan</p>
<p>We can rescan a device/bus by writing 1 to its rescan file, we can also rescan<br>whole PCIe system by echo 1 &gt; /sys/bus/pci/rescan (need to check…).</p>
<p>When we do rescan a device, we find its father bus, and pass this bus to PCIe<br>enumeration process.</p>
<p>However, if we rescan a RP or PCIe bridge, as the structure of related RP or<br>PCIe bridge is still there, Linux kernel will do nothing about MEM/IO window<br>of related RP or PCIe bridge.(however, writing/reading MEM/IO operation will<br>be done to checkout if this bridge’s MEM/IO window register is working)</p>
</li>
<li><p>reset</p>
<p>(to do…)</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCI parse MEM/IO range in _CRS in ACPI table</title>
    <url>/2021/07/11/PCI-parse-MEM-IO-range-in-CRS-in-ACPI-table/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_acpi_scan_root(struct acpi_pci_root *root)</span><br><span class="line">    --&gt; root_ops-&gt;prepare_resources = pci_acpi_root_prepare_resources</span><br><span class="line">        --&gt; acpi_pci_root_create</span><br><span class="line">            ...</span><br><span class="line">            --&gt; ops-&gt;prepare_resources(info)</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">pci_acpi_root_prepare_resources</span><br><span class="line">    --&gt; acpi_pci_probe_root_resources(ci)</span><br><span class="line">        ...</span><br><span class="line">            /* this is the key function */</span><br><span class="line">        --&gt; acpi_dev_get_resources</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">/* will analyze acpi_dev_get_resources below */</span><br><span class="line">acpi_dev_get_resources(device, list, acpi_dev_filter_resource_type_cb, (void *)flags);</span><br><span class="line">    --&gt; c.list = list;</span><br><span class="line">	c.preproc = preproc; /* acpi_dev_filter_resource_type_cb */</span><br><span class="line">	c.preproc_data = preproc_data; /* (void *)flags */</span><br><span class="line">	c.count = 0;</span><br><span class="line">	c.error = 0;</span><br><span class="line">        /*</span><br><span class="line">         * will parse _CRS method for above device, acpi_walk_resources is defined</span><br><span class="line">         * in drivers/acpi/acpica/rsxface.c</span><br><span class="line">         *</span><br><span class="line">         * acpi_dev_process_resource will be called for every resources in _CRS,</span><br><span class="line">         * it parses a resource and adds it into the list in c.list.</span><br><span class="line">         *</span><br><span class="line">         * this means e.g. if there are 3 MEM/IO range, acpi_dev_process_resource</span><br><span class="line">         * will be called three times.</span><br><span class="line">         */</span><br><span class="line">    --&gt; acpi_walk_resources(adev-&gt;handle, METHOD_NAME__CRS, acpi_dev_process_resource, &amp;c);</span><br><span class="line"></span><br><span class="line">/* details of acpi_dev_process_resource */</span><br><span class="line">static acpi_status acpi_dev_process_resource(struct acpi_resource *ares, void *context)</span><br><span class="line">        /* filter the resource type to some kinds */</span><br><span class="line">    --&gt; c-&gt;preproc(ares, c-&gt;preproc_data)</span><br><span class="line">    ...</span><br><span class="line">        /*</span><br><span class="line">         * still do not know the difference of below two functions, prefetchable</span><br><span class="line">         * attribute will be parsed in acpi_decode_space function.</span><br><span class="line">         */</span><br><span class="line">    --&gt; acpi_dev_resource_memory(ares, res)</span><br><span class="line">    --&gt; acpi_dev_resource_address_space(ares, &amp;win)</span><br><span class="line">        --&gt; acpi_decode_space</span><br><span class="line">            --&gt; if (addr-&gt;info.mem.caching == ACPI_PREFETCHABLE_MEMORY)</span><br><span class="line">    ...</span><br><span class="line">        /* add resource to list */</span><br><span class="line">    --&gt; acpi_dev_new_resource_entry(&amp;win, c)</span><br><span class="line">        --&gt; resource_list_add_tail(rentry, c-&gt;list);</span><br></pre></td></tr></table></figure>

<h2 id="Some-ideas-about-PCI-prefetch-window-and-memory-type-attribute-of-AMRv8-1"><a href="#Some-ideas-about-PCI-prefetch-window-and-memory-type-attribute-of-AMRv8-1" class="headerlink" title="Some ideas about PCI prefetch window and memory type/attribute of AMRv8[1]"></a>Some ideas about PCI prefetch window and memory type/attribute of AMRv8[1]</h2><p>PCI prefetch and memory type/attribute has no relationship with each other,<br>as PCI prefetch is an attribute in PCI domain, and memory type/attribute is for<br>ARM bus domain. Further more, ATU(address translate unit: translate a CPU address<br>to PCI address) only translate address, do nothing about prefetch support.</p>
<p>Generally, when we get cpu physical address(PA) for a BAR, PCI device driver will<br>use ioremap(or other ioremap_*) to get a VA. memory type/attribute will be set<br>to MMU in ioremap kind of functions to support memory operations in ARM bus domain.</p>
<p>For ECAM, we use below work flow to get VA of ECAM, we can see nGnRE will be<br>set for ECAM memory type/attribute.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* will ioremap ECAM in below ioremap using nGnRE */</span><br><span class="line">pci_acpi_scan_root</span><br><span class="line">    --&gt; pci_acpi_scan_root</span><br><span class="line">        --&gt; pci_ecam_create</span><br><span class="line">            --&gt; cfg-&gt;win = ioremap</span><br></pre></td></tr></table></figure>
<p>Reference:<br>[1] ARM Cortex-A Series Programmer’s Guide for ARMv8-A(13: Memory Ordering) </p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe memory base and memory limit</title>
    <url>/2021/07/11/PCIe-memory-base-and-memory-limit/</url>
    <content><![CDATA[<p>在PCIe typeI 配置空间中有mem base, mem limit, prefetch mem base, prefetch mem limit,<br>upper 32 bit prefetch base, upper 32 bit prefetch limit，io base, io limit等一些<br>寄存器。</p>
<p>这些寄存器都是在PCI域中路由TLP包的时候用的一些窗口寄存器。如果一个TLP mem 读/写<br>的地址正好落入mem base ~ mem base + mem limit的范围内，则该TLP包从上游总线转到下游<br>总线上；如果一个TLP包到达桥的下游总线时，其访问的地址不在mem base ~ mem base + mem limit<br>的范围内时，该TLP包被传送到上游总线。一个访问从CPU域被转换到PCI域，应该使用和具体<br>实现相关的硬件转换模块，比如ATU。</p>
<p>prefetch的情况和上面的是类似的，只是prefetch可以是32bit地址也可以是64bit地址。<br>而非prefetch只有是32bit地址的。</p>
<p>所有的BAR分配的地址都是在上面的窗口中的，其实更准确的讲是先确定BAR的类型和大小然后<br>再根据一条总线下各类BAR占的总资源，填写上面的寄存器。</p>
<p>也就说BAR的类型可以有：non-prefetch 32 bit mem bar.</p>
<pre><code>                   prefetch 64 bit mem bar
                   prefetch 32 bit mem bar ?

                   16 bit io bar  io都是non-prefetch
                   32 bit io bar
</code></pre>
<p>在PCIe中，prefetch的bar(prefetch bit 置位)必须是64bit mem bar<a href="22.1.16">1</a>, 和上面<br>prefetch 32 bit mem bar是冲突的(Intel 82599上有该类型的BAR)</p>
<p>non-prefetch bar只可以被分到non-prefetch的window里，prefetch bar可以被分到non-prefetch<br>的window里。64bit bar可以被分到32bit window里。这里的窗口只的都是PCI域的窗口。</p>
<p>[1] [PCI.EXPRESS系统体系结构标准教材].(美)Pavi.Budruk,Don.Anderson,Tom.Shanley</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe学习笔记(一)</title>
    <url>/2021/07/11/PCIe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%80/</url>
    <content><![CDATA[<p> 本文是学习linux kernel中PCI子系统代码的一个笔记。PCI子系统的代码最主要的就是<br> 实现整个PCI树的枚举和资源的分配。本文先总体介绍，然后主要分析pci_create_root_bus<br> 函数，该函数实现pci_bus结构和pci_host_bridge结构的分配。本文分析的代码版本为内核<br> 3.18-rc1</p>
<p> pci_scan_root_bus()完成整个PCI树的枚举和资源分配。主要实现在下面的三个函数中：<br>    –&gt;pci_create_root_bus()<br>    –&gt;pci_scan_child_bus(b)<br>    –&gt;pci_assign_unassigned_bus_resources(b)</p>
<p> pci_create_root_bus 建立相应的数据结构pci_bus, pci_host_bridge等<br> pci_scan_child_bus 枚举整个pci总线上的设备<br> pci_assign_unassigned_bus_resources 分配总线资源</p>
<p> 下面直观的给出PCI(PCIe)硬件和软件的相互对应，也可以看出软件对硬件是怎么做抽象的。<br> 硬件结构:<br>        |   root bus: 0   —-&gt;  struct pci_bus<br>        |<br>    +—————-+        —-&gt;  struct pci_host_bridge<br>    | pcie root port |        —-&gt;  struct pci_dev<br>    +—————-+<br>        |   bus: 1        —-&gt;  struct pci_bus<br>        |<br>    +—————-+<br>    | pcie net cards |        —-&gt;  struct pci_dev<br>    +—————-+</p>
<p>先从硬件的角度说明PCIe总线系统的工作大致流程。PCIe总线系统是一个局部总线系统，<br>目的在于沟通内存和外设的存储空间。总结起来完成: 1. CPU访问外设的存储空间；2.<br>外设访问系统内存空间。一般介绍PCIe的时候说的RC，switch, EP，这里以arm Soc为例说明.<br>一般的, RC可以理解成PCIe host bridge, 有时也叫PCIe控制器，完成CPU域地址到PCI域<br>地址的转换，RC在Soc的内部。switch是一个独立的器件，和RC的接口相连，提供扩展。EP<br>是具有PCIe接口的网卡，SATA控制器等。PCI中还有一个概念是PCI桥, 实际的PCI桥存在<br>PCI总线中(不是PCIe总线), 完成系统的扩展，和host桥不同的是，PCI桥没有地址翻译的功能.<br>PCI总线中的switch每个端口相当于一个PCI桥(虚拟PCI桥), 完成PCI桥类似的功能。</p>
<p>每个PCI设备，包括host桥、PCI桥和PCI设备都有一个配置空间。PCIe中，这个配置空间的<br>大小是4K。CPU和外设根据配置空间的一些寄存器, 访问相应的地址。关键的寄存器有:<br>BAR, mem base/limit, I/O base/limit. mem base/limit, I/O base/limit指定当前PCI桥<br>mem空间和I/O空间的起始和大小, 在PCI桥中使用. BAR指示的是PCI设备的mem空间和I/O空间.<br>比如, 下图中PCIe net card的BAR空间(BAR指示的一段地址), 就可以存放网卡本身的寄存器.<br>一般情况, PCI桥的BAR是用不到的.</p>
<pre><code>        +----------------+ ----&gt; PCIe host bridge
        | pcie root port |
        +----------------+ ----&gt; in Soc
            |
--------------------------------------------------- ----&gt; switch
|            +----------------+                |
|            |   pci bridge   |                |
|            +----------------+                |
|                |                         |
|         -------------------------------         |
|         |                             |         |
| +----------------+       +----------------+ |
| |   pci bridge   |       |   pci bridge   | |
| +----------------+       +----------------+ |
---------------------------------------------------
          |
  +----------------+
  |  PCIe net card |
  +----------------+
</code></pre>
<p>整个pci枚举的过程最主要的就是配置pci桥和pci设备的BAR和mem、I/O base/limit<br>下面以此为主线分析整个pci枚举的过程。</p>
<p>在分析代码之前，先介绍基本的数据结构。代码围绕这些数据结构构成。<br>这几个核心的数据结构是: struct pci_bus, struct pci_dev, struct pci_host_bridge<br>每一个pci总线对应一个struct pci_bus结构，每一个pci设备(可以是host bridge,<br>普通pci device)对应一个pci_dev, 一个pci host bridge对应一个pci_host_bridge,<br>一般一个pci总线体系中有一个pci host bridge, 这一个pci总线系统也叫一个pci domain,<br>各个pci domain不可以直接相互访问。有些时候一个系统会有多个pcie root port, 这时<br>每个root port和下面的pci device组成各自的pci domain, 下面介绍各个结构中关键条目。</p>
<p>struct pci_bus:<br>    /* 指向该总线上游的pci桥的pci_dev结构 */<br>    struct pci_dev    <em>self;<br>    /</em> 存储该总线的mem、I/O，prefetch mem等资源。由总线上游pci桥的<br>     * pci_dev结构中的resource中的第PCI_BRIDGE_RESOURCES到<br>     * PCI_BRIDGE_RESOURCES + PCI_BRIDGE_RESOURCE_NUM -1个元素复制得到<br>     * 发生于pci_scan_bridge()<br>     *           –&gt;pci_add_new_bus()<br>     *              –&gt;pci_alloc_child_bus()<br>     */<br>    struct resource *resource[PCI_BRIDGE_RESOURCE_NUM];<br>    struct list_head resources;</p>
<p>struct pci_host_bridge:<br>    /* 整个pci系统的mem, I/O资源作为一个个链表元素 */<br>    struct list_head windows;</p>
<p>struct pci_dev:<br>    /* 若该设备是pci桥，指向该桥的下游总线 */<br>    struct pci_bus    <em>subordinate;<br>    /</em> 存该pci设备的BAR等资源, 在__pci_read_base()中初始化<br>     * pci_scan_device()<br>     *    –&gt; pci_setup_device() …<br>     *        –&gt; __pci_read_base<br>     * still do know where to init resource[PCI_BRIDGE_RESOURCES] ?<br>     */<br>    struct resource resource[DEVICE_COUNT_RESOURCE];</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe学习笔记(三)</title>
    <url>/2021/07/11/PCIe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%89/</url>
    <content><![CDATA[<p>pci_scan_child_bus(b)</p>
<p>这个函数完成pci总线的枚举，完成整个pci树各个总线号的分配。但是并没有分配各个pci桥，<br>pci device的BAR和mem, I/O, prefetch mem的base/limit寄存器</p>
<p>unsigned int pci_scan_child_bus(struct pci_bus *bus)<br>{<br>    unsigned int devfn, pass, max = bus-&gt;busn_res.start;<br>    struct pci_dev *dev;</p>
<pre><code>/* 开始pci枚举, 一个pci bus上最多有256个设备，每个设备最多有8个function
 * 所以这里最多可以扫描32个device。实际上每个function也是有配置空间的
 * 所以function可以看作是个逻辑的设备
 */
for (devfn = 0; devfn &lt; 0x100; devfn += 8)
    pci_scan_slot(bus, devfn);

/* pci虚拟化相关的东西 */
max += pci_iov_bus_range(bus);

...
if (!bus-&gt;is_added) &#123;
    dev_dbg(&amp;bus-&gt;dev, &quot;fixups for bus\n&quot;);
    /* 对于arm64来说，是个空函数 */
    pcibios_fixup_bus(bus);
    bus-&gt;is_added = 1;
&#125;

for (pass = 0; pass &lt; 2; pass++)
    list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list) &#123;
        if (pci_is_bridge(dev))
                /* 开始递归的做pci枚举，在下面的函数中会再次
             * 调用pci_scan_child_bus。总线号的确定也在这个
             * 函数中
             */
            max = pci_scan_bridge(bus, dev, max, pass);
    &#125;
...
return max;
</code></pre>
<p>}</p>
<p>pci_scan_slot(struct pci_bus <em>bus, int devfn)<br>    …<br>    –&gt; pci_scan_single_device(bus, devfn)<br>        …<br>        /</em> 扫描devfn设备, 探测设备：读vendor id, 读BAR的大小 。<br>         * 软件行为：为device分配struct pci_dev; 根据探测设备得到的数据，填<br>         * 写pci_dev里的一些值。若是按照笔记1中的硬件拓扑图现在扫描的device<br>         * 应该是pcie root port<br>         <em>/<br>    –&gt; pci_scan_device(bus, devfn)<br>            /</em> 读devfn的vendor id, 会调用到pci_scan_root_bus的参数ops传入的read操作 <em>/<br>        –&gt; pci_bus_read_dev_vendor_id()<br>            /</em> 分配pci_dev结构 <em>/<br>        –&gt; pci_alloc_dev(bus)<br>        …<br>            /</em> 处理pci配置头中的class code, Header Type, Interrupt Pin<br>         * Interrupt Line等内容，并设定新分配的pci_dev中的对应项。这里<br>         * 也会探测device的BAR大小，并设定pci_dev-&gt;resource[]。我们关注<br>         * 的重点也是pci_bus,pci_dev中相应的元素怎么变化。<br>         */<br>        –&gt; pci_setup_device(dev)<br>            …<br>    –&gt; pci_device_add(dev, bus)    </p>
<p>int pci_setup_device(struct pci_dev <em>dev)<br>    …<br>        /</em> 的到device配置空间中head type的值，可以是普通pci device, pci桥等<br>     * 这个值在下面的switch-case中决定程序是走哪个分支。如果当前的设备是<br>     * pcie root port，那么走PCI_HEADER_TYPE_BRIDGE分支。设备配置空间<br>     * head type的值一般是pci host controller驱动中配置过的, 可以参考<br>     * /drivers/pci/host/pcie-designware.c<br>     <em>/<br>    –&gt; pci_read_config_byte(dev, PCI_HEADER_TYPE, &amp;hdr_type)<br>        /</em> device的parent是上游总线之上的那个pci桥 <em>/<br>    –&gt; dev-&gt;dev.parent = dev-&gt;bus-&gt;bridge;<br>        /</em> 读出class revision的值 <em>/<br>    –&gt; pci_read_config_dword(dev, PCI_CLASS_REVISION, &amp;class)<br>    dev-&gt;revision = class &amp; 0xff;<br>    dev-&gt;class = class &gt;&gt; 8;            /</em> upper 3 bytes */</p>
<pre><code>dev-&gt;current_state = PCI_UNKNOWN;

class = dev-&gt;class &gt;&gt; 8;

switch (dev-&gt;hdr_type) &#123;            /* header type */
case PCI_HEADER_TYPE_NORMAL:            /* standard header */
    if (class == PCI_CLASS_BRIDGE_PCI)
        goto bad;
    pci_read_irq(dev);
    /*
     * 得到BAR的大小，并存在pci_dev-&gt;resource[]中，主要是对pci
     * device起作用。当输入参数dev是pci桥时，得到的值似乎对后面影响
     * 不大，这点需要确认下？
     */
    pci_read_bases(dev, 6, PCI_ROM_ADDRESS);
    pci_read_config_word(dev, PCI_SUBSYSTEM_VENDOR_ID, &amp;dev-&gt;subsystem_vendor);
    pci_read_config_word(dev, PCI_SUBSYSTEM_ID, &amp;dev-&gt;subsystem_device);

        break;

case PCI_HEADER_TYPE_BRIDGE:            /* bridge header */
    if (class != PCI_CLASS_BRIDGE_PCI)
        goto bad;
    pci_read_irq(dev);
    dev-&gt;transparent = ((dev-&gt;class &amp; 0xff) == 1);
    pci_read_bases(dev, 2, PCI_ROM_ADDRESS1);
    set_pcie_hotplug_bridge(dev);
    pos = pci_find_capability(dev, PCI_CAP_ID_SSVID);
    if (pos) &#123;
        pci_read_config_word(dev, pos + PCI_SSVID_VENDOR_ID, &amp;dev-&gt;subsystem_vendor);
        pci_read_config_word(dev, pos + PCI_SSVID_DEVICE_ID, &amp;dev-&gt;subsystem_device);
    &#125;
    break;
    ...
&#125;
</code></pre>
<p>/* 直接读硬件配置空间中的interrupt line and pin，然后写到pci_dev-&gt;line</p>
<ul>
<li>pci_dev-&gt;irq中</li>
<li>/<br>pci_read_irq(dev)</li>
</ul>
<p>/* 依次读取6个bar的内容，相关内容写入pci_dev下的struct resource resource[] <em>/<br>pci_read_bases(dev, 6, PCI_ROM_ADDRESS);<br>    –&gt;__pci_read_bases<br>        –&gt;mask = type ? PCI_ROM_ADDRESS_MASK : ~0;<br>        /</em> 最初探测bar的大小，向bar空间写入全1 <em>/<br>        –&gt;pci_write_config_dword(dev, pos, l | mask);<br>        /</em> 读出提示信息，写入sz缓存 <em>/<br>    –&gt;pci_read_config_dword(dev, pos, &amp;sz);<br>        /</em> 如果全1, 设备工作不正常，或是没有这个bar */<br>        –&gt;if (sz == 0xffffffff)<br>        sz = 0;</p>
<pre><code>--&gt;if (type == pci_bar_unknown) &#123;
            /* 根据上面sz的值，得到是io/mem bar, 32/64bits, prefectch/
             * non-prefectch。这些信息都会写入res-&gt;flags中的对应bit
             */ 
    /* 如果l = 0, 解析的结果，kernel会认为这是一个32bit,
     * non-prefetchable, mem bar
     */
    --&gt;res-&gt;flags = decode_bar(dev, l);
    res-&gt;flags |= IORESOURCE_SIZEALIGN;
    if (res-&gt;flags &amp; IORESOURCE_IO) &#123;
                ...
    &#125; else &#123;
                    /* 下面先把低32bits写到64bits变量的低32bits */
        /* 如果l = 0, l64 = 0 */
        l64 = l &amp; PCI_BASE_ADDRESS_MEM_MASK;
        /* sz是写入全1后从bar中得到的值，如果是0xf000000f
         * sz64 = 0xf0000000
         */
        sz64 = sz &amp; PCI_BASE_ADDRESS_MEM_MASK;
                    /* ul的变量是多少bits? */
        /* 应该是0xfffffff0 */
        mask64 = (u32)PCI_BASE_ADDRESS_MEM_MASK;
    &#125;
&#125; else &#123;
        ...
&#125;

if (res-&gt;flags &amp; IORESOURCE_MEM_64) &#123;
    pci_read_config_dword(dev, pos + 4, &amp;l);
    pci_write_config_dword(dev, pos + 4, ~0);
    pci_read_config_dword(dev, pos + 4, &amp;sz);
    pci_write_config_dword(dev, pos + 4, l);

    l64 |= ((u64)l &lt;&lt; 32);
    sz64 |= ((u64)sz &lt;&lt; 32);
    mask64 |= ((u64)~0 &lt;&lt; 32);
&#125;
    ...
sz64 = pci_size(l64, sz64, mask64);
    /* size = 0xf000_0000 */
    --&gt; size = sz64 &amp; mask64;
    /* size = 0x0fff_ffff, and return this */
    --&gt; size = (size &amp; ~(size-1)) - 1;

/* struct pci_bus_region, local struct. indicate a bar&#39;s resource? */
region.start = l64; /* l64 = 0 */
region.end = l64 + sz64; /* 0x0fff_ffff */

pcibios_bus_to_resource(dev-&gt;bus, res, &amp;region);
    /* 首先找到host中的对应资源 */
    --&gt; resource_list_for_each_entry(window, &amp;bridge-&gt;windows) &#123;
        --&gt; if (resource_type(res) != resource_type(window-&gt;res))
        /* 找到对应到pci域中的起始地址 */
        --&gt; bus_region.start = window-&gt;res-&gt;start - window-&gt;offset;
        --&gt; bus_region.end = window-&gt;res-&gt;end - window-&gt;offset;

pcibios_resource_to_bus(dev-&gt;bus, &amp;inverted_region, res);
</code></pre>
<p>现在回到pci_scan_child_bus中的pci_scan_bridge()函数。<br>/*</p>
<ul>
<li><p>如果一个pci_dev是pci桥的话，以它的上游总线bus和这个设备dev本身为参数，扫描这个</p>
</li>
<li><p>桥。这个函数中实现pci树的递归扫描，在这个函数中为整个pci树上的各个子总线分配</p>
</li>
<li><p>总线号。这里pci_scan_bridge会调用两次，第一次处理BIOS中已经配置的东西，不清楚</p>
</li>
<li><p>intel在BIOS中做了怎样的处理。</p>
</li>
<li><p>/<br>pci_scan_bridge(bus, dev, max, pass);<br>  …<br>  /* 这里上来就读桥设备的主bus号，是因为有些体系架构下可能在BIOS中已经对这些</p>
<ul>
<li>做过配置。在ARM64中暂时没有看到有这样做的情况。</li>
<li>/</li>
</ul>
<p>  –&gt; pci_read_config_dword(dev, PCI_PRIMARY_BUS, &amp;buses);<br>  …<br>  /* 配置了在kernel中分配bus号，应该不会进入到if中 */<br>  –&gt; if ((secondary || subordinate) &amp;&amp; !pcibios_assign_all_busses() &amp;&amp;</p>
<pre><code>  !is_cardbus &amp;&amp; !broken) &#123;
</code></pre>
<p>  …<br>  } else {</p>
<pre><code>      /* 第一次掉用pci_scan_bridge就此返回了 */
  if (!pass) &#123;
          ...
      goto out;
  &#125;

  /* Clear errors */
  pci_write_config_word(dev, PCI_STATUS, 0xffff);

  /* 确定是否已用max+1这个bus号 */
  child = pci_find_bus(pci_domain_nr(bus), max+1);
  if (!child) &#123;
          /* 分配新的struct pci_bus */
      child = pci_add_new_bus(bus, dev, max+1);
      if (!child)
          goto out;
      pci_bus_insert_busn_res(child, max+1, 0xff);
  &#125;
  max++;
  /* 合成新的bus号，准备写入pci桥的配置空间 */
  buses = (buses &amp; 0xff000000)
        | ((unsigned int)(child-&gt;primary)     &lt;&lt;  0)
        | ((unsigned int)(child-&gt;busn_res.start)   &lt;&lt;  8)
        | ((unsigned int)(child-&gt;busn_res.end) &lt;&lt; 16);
  ...
  /* 写入该pci桥primary bus, secondary bus, subordinate bus */
  pci_write_config_dword(dev, PCI_PRIMARY_BUS, buses);

  if (!is_cardbus) &#123;
      child-&gt;bridge_ctl = bctl;
      /* 递归调用pci_scan_child_bus, 扫描这个子总线下的设备*/
      max = pci_scan_child_bus(child);
  &#125; else &#123;
          /* 不关心cardbus */
      ...
  &#125;
  /*
   * Set the subordinate bus number to its real value.
   * 每次递归结束把实际的subordinate bus写入pci桥的配置空间
   * subordinate bus表示该pci桥下最大的总线号
   */
  pci_bus_update_busn_res_end(child, max);
  pci_write_config_byte(dev, PCI_SUBORDINATE_BUS, max);
</code></pre>
<p>  }<br>  …<br>out:<br>  pci_write_config_word(dev, PCI_BRIDGE_CONTROL, bctl);</p>
<p>  return max;</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe PRI分析</title>
    <url>/2021/08/21/PCIe-PRI%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="协议逻辑"><a href="#协议逻辑" class="headerlink" title="协议逻辑"></a>协议逻辑</h2><p> PRI依赖ATS，带有PRI功能的设备，可以给IOMMU发page request，IOMMU为设备申请物理页，<br> 并建立页表后，给设备发PRG response的消息。</p>
<p> PCIe协议里定义了PCIe设备和IOMMU之间的request/repsponse消息格式，并且定义了<br> PRI capability的格式，后者是软件可以读写的软硬件接口。</p>
<p> 先看PRI capability的详细定义，这个cap只在PF里有：</p>
<ul>
<li><p>cap head</p>
<p>常规cap头。</p>
</li>
<li><p>status register</p>
<p>response failure bit: 读到1表示收到一个failure的response。RW1C寄存器，写1清0。</p>
<p>unexpected page request group index bit: 读到1表示收到一个非法PRI group index。<br>RW1C寄存器，写1清。</p>
<p>stopped bit: 只在enable clear的时候有效。这个是一个只读的指示位，为0时或者设备<br>根本没有停，或者设备停了，但是链路上还有PRI request，当为1时，表示之前发出去的<br>request都完成了(对应的response都回来了?)</p>
<p>PRG response PASID required bit: 只读的指示位，PRG response带不带PASID，使能<br>PASID的时候这个都应该是1？</p>
</li>
<li><p>control register</p>
<p>enable bit: 控制是否可以发PRI request。从non-enable到enable的操作，清空status<br>flags。enable和stopped同时处于clear，可能设备和IOMMU的连接通路上还有PRI request。</p>
<p>reset bit:  只有在enable clear的时候有用，这个时候对reset写1，会clear page<br>request credit counter和pending request state。</p>
<p>估计上面这两个是硬件内部为了维护PRI request和response的具体实现, reset把这些<br>东西清掉，使得设备回到复位状态，结合上面enable clear时只是disable PRI request<br>的主动行为，并不保证清理链路上和设备里已经发出去的请求，这里的逻辑应该是这样。</p>
</li>
<li><p>outstanding page request cap</p>
<p>定义page request的最大outstanding。page request的个数是资源，PRG index的个数<br>也是资源，这里限定的是前者。</p>
</li>
<li><p>outstanding page request allocation</p>
<p>用来配置实际使用的最大page request outstanding数。提供这个接口给软件的目的是<br>要和IOMMU的处理能力做匹配，如果IOMMU的缺页处理能力比设备小，会造成PRI request<br>失败。所以，这里对IOMMU就有了限制，可以直接想到的有: IOMMU处理PRI的队列最好独立，<br>这样方便和设备的PRI outstanding能力做匹配; IOMMU需要考虑处理PRI请求队列溢出时，<br>如何恢复; IOMMU的驱动需要考虑合理的匹配逻辑。</p>
</li>
</ul>
<p> 协议中定义的各种PRI相关的消息，这一部分软件无法直接感知。相关的硬件消息包括：</p>
<ul>
<li><p>Page Request Message(PRM)</p>
<p>设备发出的消息，一组消息可以组成一个group，group中的最后一个message有last标记<br>去标记。</p>
</li>
<li><p>Stop Marker Message</p>
<p>由设备发给IOMMU，告诉IOMMU，设备不再使用相关PASID了。</p>
</li>
<li><p>Page Request Group Response Message(PRGRM)</p>
<p>IOMMU处理完一组PRM，对设备返回一个PRGRM，使用ID路由，携带pasid，group index，<br>请求完成状态信息。注意这里是不带请求到的PA的。以group为单位返回状态信息，group<br>里一个request failure，整个group就failure了。</p>
<p>所以，PRI对IOMMU的诉求就是IOMMU要给软件提供下发PRGRM的接口，这个接口至少要有bdf，<br>pasid，group index，PRM group处理返回值。</p>
</li>
</ul>
<p> 基于cap各个域段的分析，整体流程已经比较清楚了。唯一要注意的是PRGRM只返回处理<br> 结果，需要接着再发ATS请求拿到PA，然后再发地址翻译过的请求。因为PRI翻译建立的页表<br> 是可能变动的，如果收到PRGRM后直接用va访问可能会异常，所以接着发ATS拿到PA，依赖<br> ATS的同步机制保证发出的访问请求成功。</p>
<h2 id="Linux驱动分析"><a href="#Linux驱动分析" class="headerlink" title="Linux驱动分析"></a>Linux驱动分析</h2><p> 目前Linux内核主线(Linux-5.14-rc4)已经支持了PCIe PRI的基本使能函数。相关函数在<br> drivers/pci/ats.c中，这些只是一些PCIe cap的操作函数。在业务中使用PRI功能的情况<br> 还没有。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe学习笔记(四)</title>
    <url>/2021/07/11/PCIe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9B%9B/</url>
    <content><![CDATA[<p>pci_assign_unassigned_bus_resources(b)<br>void pci_assign_unassigned_bus_resources(struct pci_bus *bus)<br>{<br>    struct pci_dev <em>dev;<br>    LIST_HEAD(add_list); /</em> list of resources that<br>                    want additional resources */</p>
<pre><code>down_read(&amp;pci_bus_sem);
list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list)
    if (pci_is_bridge(dev) &amp;&amp; pci_has_subordinate(dev))
                /* 这里传入的参数是bus 1对应的pci_bus */
            __pci_bus_size_bridges(dev-&gt;subordinate,
                         &amp;add_list);
up_read(&amp;pci_bus_sem);
/* 配置各个桥和设备的BAR，配置桥的MEM，I/O，prefetch MEM base/limit */
__pci_bus_assign_resources(bus, &amp;add_list, NULL);
BUG_ON(!list_empty(&amp;add_list));
</code></pre>
<p>}</p>
<p>__pci_bus_size_bridges(struct pci_bus *bus, struct list_head <em>realloc_head)<br>    …<br>       /</em> 在当前的连接状态下，list中的代码不会执行。下面的代码层层递归，直到<br>        * 最底层设备的上的pci_bus，这时最底层设备是没有下一级的pci_bus的。<br>        * 所以，继续执行后面的代码。<br>        <em>/<br>    –&gt;list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list) {<br>        …<br>    case PCI_CLASS_BRIDGE_PCI:<br>    default:<br>        __pci_bus_size_bridges(b, realloc_head);<br>        break;<br>       }<br>    …<br>       /</em> 当前pci_bus的桥对应的pci_dev, 这里应该是host <em>/<br>    –&gt;switch (bus-&gt;self-&gt;class &gt;&gt; 8) {<br>        …<br>        case PCI_CLASS_BRIDGE_PCI:<br>            /</em> 会执行这里 <em>/<br>            pci_bridge_check_ranges(bus);<br>        …<br>    default:<br>        /</em> 这个函数改变了pci_bus-&gt;resource[]中的值。start对齐4K，size是该bus下<br>         * 所有I/O空间的总和。但是似乎realloc_head<br>         * list似乎没有node添加上去<br>         */<br>        pbus_size_io(bus, realloc_head ? 0 : additional_io_size,<br>                 additional_io_size, realloc_head);</p>
<pre><code>    /*
     * If there&#39;s a 64-bit prefetchable MMIO window, compute
     * the size required to put all 64-bit prefetchable
     * resources in it.
     */
    b_res = &amp;bus-&gt;self-&gt;resource[PCI_BRIDGE_RESOURCES];
    mask = IORESOURCE_MEM;
    prefmask = IORESOURCE_MEM | IORESOURCE_PREFETCH;
    if (b_res[2].flags &amp; IORESOURCE_MEM_64) &#123;
        prefmask |= IORESOURCE_MEM_64;
        ret = pbus_size_mem(bus, prefmask, prefmask,
              prefmask, prefmask,
              realloc_head ? 0 : additional_mem_size,
              additional_mem_size, realloc_head);

        /*
         * If successful, all non-prefetchable resources
         * and any 32-bit prefetchable resources will go in
         * the non-prefetchable window.
         */
        if (ret == 0) &#123;
            mask = prefmask;
            type2 = prefmask &amp; ~IORESOURCE_MEM_64;
            type3 = prefmask &amp; ~IORESOURCE_PREFETCH;
        &#125;
    &#125;

    /*
     * If there is no 64-bit prefetchable window, compute the
     * size required to put all prefetchable resources in the
     * 32-bit prefetchable window (if there is one).
     */
    if (!type2) &#123;
        prefmask &amp;= ~IORESOURCE_MEM_64;
        ret = pbus_size_mem(bus, prefmask, prefmask,
                 prefmask, prefmask,
                 realloc_head ? 0 : additional_mem_size,
                 additional_mem_size, realloc_head);

        /*
         * If successful, only non-prefetchable resources
         * will go in the non-prefetchable window.
         */
        if (ret == 0)
            mask = prefmask;
        else
            additional_mem_size += additional_mem_size;

        type2 = type3 = IORESOURCE_MEM;
    &#125;

    /*
     * Compute the size required to put everything else in the
     * non-prefetchable window.  This includes:
     *
     *   - all non-prefetchable resources
     *   - 32-bit prefetchable resources if there&#39;s a 64-bit
     *     prefetchable window or no prefetchable window at all
     *   - 64-bit prefetchable resources if there&#39;s no
     *     prefetchable window at all
     *
     * Note that the strategy in __pci_assign_resource() must
     * match that used here.  Specifically, we cannot put a
     * 32-bit prefetchable resource in a 64-bit prefetchable
     * window.
     */
    pbus_size_mem(bus, mask, IORESOURCE_MEM, type2, type3,
            realloc_head ? 0 : additional_mem_size,
            additional_mem_size, realloc_head);
    break;
&#125;
</code></pre>
<p>}</p>
<p>__pci_bus_assign_resources(bus, &amp;add_list, NULL)<br>        /* bus:0, 会对该bus上的所有device分别调用__dev_sort_resource<br>     * 然后统一调用一个__assign_resources_sorted()。之后程序进入<br>     * 下面的list中，又会嵌套进入__pci_bus_assign_resources, 这时<br>     * bus:1。重复上面的过程。在bus:1是__pci_bus_assign_resources<br>     * 在处理处理完pbus_assign_resources_sorted后不回往下执行,返回<br>     * 上层。这时bus:0, 进入pci_setup_bridge执行。<br>     * 其中，会在pbus_assign_resources_sorted中配置BAR，在<br>     * __pci_setup_bridge中配mem, I/O, prefetch mem的base/limit<br>     */<br>    –&gt; pbus_assign_resources_sorted(bus, realloc_head, fail_head);<br>    –&gt; list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list)<br>        –&gt; __pci_bus_assign_resources(b, realloc_head, fail_head);<br>    –&gt; switch (dev-&gt;class &gt;&gt; 8)<br>        case PCI_CLASS_BRIDGE_PCI:<br>            –&gt; pci_setup_bridge(b);</p>
<p>static void pbus_assign_resources_sorted()<br>    –&gt; LIST_HEAD(head);<br>    –&gt; list_for_each_entry(dev, &amp;bus-&gt;devices, bus_list)<br>        __dev_sort_resources(dev, &amp;head);<br>                    /* 把pci_dev中的resource[]从大到小排序, 存入链表head中 <em>/<br>            –&gt; pdev_sort_resources(dev, head);<br>    –&gt; __assign_resources_sorted(&amp;head, realloc_head, fail_head);<br>            /</em> 因为realloc_head为空链表，所以直接到requested_and_reassign <em>/<br>    –&gt; if (!realloc_head || list_empty(realloc_head))<br>        goto requested_and_reassign;<br>    …<br>        requested_and_reassign:<br>    –&gt; assign_requested_resources_sorted(head, fail_head);<br>            /</em> dev_res(pci_dev_resource)存储一个device中一个配置空间<br>         * 的资源(一个设备可有多个mem或I/O配置空间)<br>         */<br>        –&gt; list_for_each_entry(dev_res, head, list)<br>        –&gt; resource_size(res) &amp;&amp;<br>        pci_assign_resource(dev_res-&gt;dev, idx)<br>               –&gt; _pci_assign_resource(dev, resno, size, align);<br>           –&gt; pci_update_resource(dev, resno);</p>
<pre><code>    --&gt; reassign_resources_sorted(realloc_head, head);
</code></pre>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>Parsing ranges item in pcie-designware.c</title>
    <url>/2021/07/11/Parsing-ranges-item-in-pcie-designware-c/</url>
    <content><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int __init dw_pcie_host_init(struct pcie_port *pp)</span><br><span class="line">&#123;</span><br><span class="line">	...</span><br><span class="line">	/*</span><br><span class="line">	 * read out value of &quot;#address-cells&quot; as na and #size-cells as ns.</span><br><span class="line">	 * as you can see in dw pcie binding document, address-cells should</span><br><span class="line">	 * be set as 3 and size-cells should be set as 2 which are we already</span><br><span class="line">	 * done in pcie dts node. so here na = 3, np = 2.</span><br><span class="line">	 *</span><br><span class="line">	 * in fact, address-cells indicates how many bytes do we use to describe</span><br><span class="line">	 * a pci address: of cause thit is 3, one byte is bits flage which</span><br><span class="line">	 * indicates if it is a mem/io/cfg/pref, other two bytes are for a 64</span><br><span class="line">	 * bits address(pci address can be a 64 bits address). </span><br><span class="line">	 * size-cells indicate how many bytes do we use to describe a size of</span><br><span class="line">	 * above pci address, of cause it should be at least 2 to describe a</span><br><span class="line">	 * 64 bits address.</span><br><span class="line">	 */</span><br><span class="line">	of_property_read_u32(np, &quot;#address-cells&quot;, &amp;na);</span><br><span class="line">	ns = of_n_size_cells(np);</span><br><span class="line"></span><br><span class="line">	cfg_res = platform_get_resource_byname(pdev, IORESOURCE_MEM, &quot;config&quot;);</span><br><span class="line">	if (cfg_res) &#123;</span><br><span class="line">		pp-&gt;cfg0_size = resource_size(cfg_res)/2;</span><br><span class="line">		pp-&gt;cfg1_size = resource_size(cfg_res)/2;</span><br><span class="line">		pp-&gt;cfg0_base = cfg_res-&gt;start;</span><br><span class="line">		pp-&gt;cfg1_base = cfg_res-&gt;start + pp-&gt;cfg0_size;</span><br><span class="line"></span><br><span class="line">		/* Find the untranslated configuration space address */</span><br><span class="line">		index = of_property_match_string(np, &quot;reg-names&quot;, &quot;config&quot;);</span><br><span class="line">		addrp = of_get_address(np, index, NULL, NULL);</span><br><span class="line">		pp-&gt;cfg0_mod_base = of_read_number(addrp, ns);</span><br><span class="line">		pp-&gt;cfg1_mod_base = pp-&gt;cfg0_mod_base + pp-&gt;cfg0_size;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		dev_err(pp-&gt;dev, &quot;missing *config* reg space\n&quot;);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (of_pci_range_parser_init(&amp;parser, np)) &#123;</span><br><span class="line">		/* na indicates pci address address cells, ns is related size */</span><br><span class="line">		--&gt; const int na = 3, ns = 2;</span><br><span class="line">		/*</span><br><span class="line">		 * so parser-&gt;pna is cpu address cell number which can be found</span><br><span class="line">		 * in parent dts node. take hip05.dtsi as an example, it should</span><br><span class="line">		 * be 2, as:</span><br><span class="line">		 *   	peripherals &#123;</span><br><span class="line">		 *		compatible = &quot;simple-bus&quot;;</span><br><span class="line">		 *		#address-cells = &lt;2&gt;;</span><br><span class="line">		 *		#size-cells = &lt;2&gt;;</span><br><span class="line">		 *		...</span><br><span class="line">		 * note: pcie dts nodes should under peripherals node.</span><br><span class="line">		 * above size-cells and address-cells should be both 2 for</span><br><span class="line">		 * 64bits cpu address. all are wrong in hulk-3.19/hulk-4.1</span><br><span class="line">		 * hip05.dtsi.</span><br><span class="line">		 *</span><br><span class="line">		 * for a 32bits Soc, above address-cells and size-cells both</span><br><span class="line">		 * are 1, so parser-&gt;pna is 1</span><br><span class="line">		 */</span><br><span class="line">		--&gt; parser-&gt;pna = of_n_addr_cells(node);</span><br><span class="line">			--&gt; if (np-&gt;parent)</span><br><span class="line">				np = np-&gt;parent;</span><br><span class="line">			--&gt; ip = of_get_property(np, &quot;#address-cells&quot;, NULL);</span><br><span class="line">		/*</span><br><span class="line">		 * ranges =</span><br><span class="line">		 * &lt;0x82000000 0 0xb5100000 0x240 0x00000000 0 0x00f00000&gt;;</span><br><span class="line">		 * |----&gt; pci address &lt;----|--&gt; cpu addr &lt;--|--&gt; size &lt;--|</span><br><span class="line">		 */</span><br><span class="line">		--&gt; parser-&gt;np = parser-&gt;pna + na + ns;</span><br><span class="line">		/*</span><br><span class="line">		 * ranges = &lt;0x00000800 0 0x01f00000 0x01f00000 0 0x00080000</span><br><span class="line"> 		 * 	     0x81000000 0 0          0x01f80000 0 0x00010000</span><br><span class="line"> 		 *	     0x82000000 0 0x01000000 0x01000000 0 0x00f00000&gt;;</span><br><span class="line">		 *</span><br><span class="line">		 * like above ranges, parser-&gt;range will point to 0x00000800,</span><br><span class="line">		 * parser-&gt;end will point to 0x00f00000.</span><br><span class="line">		 */</span><br><span class="line">		--&gt; parser-&gt;range = of_get_property(node, &quot;ranges&quot;, &amp;rlen);</span><br><span class="line">		--&gt; parser-&gt;end = parser-&gt;range + rlen / sizeof(__be32);</span><br><span class="line"></span><br><span class="line">		dev_err(pp-&gt;dev, &quot;missing ranges property\n&quot;);</span><br><span class="line">		return -EINVAL;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	/* Get the I/O and memory ranges from DT */</span><br><span class="line">	for_each_of_pci_range(&amp;parser, &amp;range) &#123;</span><br><span class="line">	  --&gt; of_pci_range_parser_one(parser, range)</span><br><span class="line">	    /* first byte in ranges in each line */</span><br><span class="line">	    --&gt; range-&gt;pci_space = parser-&gt;range[0];</span><br><span class="line">	    /*</span><br><span class="line">	     * from below function, we can see only mem/io/32bits/64bits/</span><br><span class="line">	     * prefetch bits are valid. so even we set other bits in dts,</span><br><span class="line">	     * code does not parse them.</span><br><span class="line">	     *</span><br><span class="line">	     * for each bit&#x27;s meaning, you can refer to:</span><br><span class="line">	     * http://www.devicetree.org/Device_Tree_Usage</span><br><span class="line">	     */</span><br><span class="line">	    --&gt; range-&gt;flags = of_bus_pci_get_flags(parser-&gt;range);</span><br><span class="line">	    /* point to pci address */</span><br><span class="line">	    --&gt; range-&gt;pci_addr = of_read_number(parser-&gt;range + 1, ns);</span><br><span class="line">	    /* point to cpu address, not very clear below function */</span><br><span class="line">	    --&gt; range-&gt;cpu_addr = of_translate_address(parser-&gt;node,</span><br><span class="line">				  parser-&gt;range + na);</span><br><span class="line">	    /* point to size section */</span><br><span class="line">	    --&gt; range-&gt;size = of_read_number(parser-&gt;range + parser-&gt;pna + na, ns);</span><br><span class="line">	    /*</span><br><span class="line">	     * after below, parser-&gt;range points to 0x81000000 for example,</span><br><span class="line">	     * next time, we start to parse the second line of ranges.</span><br><span class="line">	     */</span><br><span class="line">	    --&gt; parser-&gt;range += parser-&gt;np;</span><br><span class="line">	    /*</span><br><span class="line">	     * in below loop, it find the ranges sub-item which has same type </span><br><span class="line">	     * and are contiguous.</span><br><span class="line">	     */</span><br><span class="line">	    --&gt; while (parser-&gt;range + parser-&gt;np &lt;= parser-&gt;end) &#123;</span><br><span class="line">			...</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line">		...</span><br><span class="line">		if (restype == IORESOURCE_MEM) &#123;</span><br><span class="line">			of_pci_range_to_resource(&amp;range, np, &amp;pp-&gt;mem);</span><br><span class="line">			pp-&gt;mem.name = &quot;MEM&quot;;</span><br><span class="line">			pp-&gt;mem_size = resource_size(&amp;pp-&gt;mem);</span><br><span class="line">			pp-&gt;mem_bus_addr = range.pci_addr;</span><br><span class="line"></span><br><span class="line">			/*</span><br><span class="line">			 * take mem as example, ranges item as below:</span><br><span class="line">			 * ranges =</span><br><span class="line">			 * &lt;0x82000000 0 0xb5100000 0x240 0x00000000 0 0x00f00000&gt;;</span><br><span class="line">			 * now parser.range points to 0x00f00000,</span><br><span class="line">			 * parser.rang - parser.np shifts points to 0x82000000.</span><br><span class="line">			 * &quot;+ na&quot; will add pci address offset, so now point to</span><br><span class="line">			 * start of cpu address: 0x240</span><br><span class="line">			 *</span><br><span class="line">			 * the problem of v3 patch: of_n_addr_cells(np) - 5 + na</span><br><span class="line">			 * is ok, but there went wrong in parser_range_end.</span><br><span class="line">			 * if there is only one line in ranges item which is</span><br><span class="line">			 * my test case, it is ok. but if there are more than</span><br><span class="line">			 * one line in ranges item, parser_range_end points to</span><br><span class="line">			 * the end of ranges item which should points to each</span><br><span class="line">			 * line end.</span><br><span class="line">			 *</span><br><span class="line">			 * pp-&gt;mem_mod_base = of_read_number(parser_range_end -</span><br><span class="line">			 *		of_n_addr_cells(np) - 5 + na, ns);</span><br><span class="line">			 *</span><br><span class="line">			 * there is one problem: mem_mod_base is same with</span><br><span class="line">			 * mem_base, why do we use mem_base directly??</span><br><span class="line">			 */</span><br><span class="line">			pp-&gt;mem_mod_base = of_read_number(parser.range -</span><br><span class="line">							  parser.np + na, ns);</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line">		if (restype == 0) &#123;</span><br><span class="line">			of_pci_range_to_resource(&amp;range, np, &amp;pp-&gt;cfg);</span><br><span class="line">			pp-&gt;cfg0_size = resource_size(&amp;pp-&gt;cfg)/2;</span><br><span class="line">			pp-&gt;cfg1_size = resource_size(&amp;pp-&gt;cfg)/2;</span><br><span class="line">			pp-&gt;cfg0_base = pp-&gt;cfg.start;</span><br><span class="line">			pp-&gt;cfg1_base = pp-&gt;cfg.start + pp-&gt;cfg0_size;</span><br><span class="line"></span><br><span class="line">			/* Find the untranslated configuration space address */</span><br><span class="line">			pp-&gt;cfg0_mod_base = of_read_number(parser.range -</span><br><span class="line">							   parser.np + na, ns);</span><br><span class="line">			pp-&gt;cfg1_mod_base = pp-&gt;cfg0_mod_base +</span><br><span class="line">					    pp-&gt;cfg0_size;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>PCIe学习笔记(二)</title>
    <url>/2021/07/11/PCIe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C/</url>
    <content><![CDATA[<p> 调用关系：<br> pci_scan_root_bus<br>     –&gt; pci_create_root_bus</p>
<p>/*</p>
<ul>
<li><p>device说明见下文，bus是根总线号，ops是配置空间读写函数的接口，需要驱动作者</p>
</li>
<li><p>传入回调函数, 会在pci_scan_child_bus-&gt;pci_scan_slot-&gt;pci_scan_single_device-&gt;</p>
</li>
<li><p>pci_scan_device-&gt;pci_bus_read_dev_vendor_id调用到该ops中的read函数。sysdata</p>
</li>
<li><p>传入私有数据。resources链表的元素是struct pci_host_bridge_window, 是dts上</p>
</li>
<li><p>读上来的总线号，mem空间，I/O空间的信息, 一般一个pci_host_bridge_window对应</p>
</li>
<li><p>一个信息</p>
</li>
<li><p>/<br>struct pci_bus *pci_create_root_bus(struct device *parent, int bus,</p>
<pre><code>  struct pci_ops *ops, void *sysdata, struct list_head *resources)
</code></pre>
<p>{<br>  …<br>  /*</p>
<ul>
<li>分配 struct pci_host_bridge, 初始化其中的windows链表</li>
<li>windows链表上的存的结构是：struct pci_host_bridge_window</li>
<li>struct pci_host_bridge_window {</li>
<li><pre><code>struct list_head list;
</code></pre>
</li>
<li><pre><code>struct resource *res;    /* host bridge aperture (CPU address) */
</code></pre>
</li>
<li><pre><code>resource_size_t offset;    /* bus address + offset = CPU address */
</code></pre>
</li>
<li>};</li>
<li>/<br>bridge = pci_alloc_host_bridge();</li>
</ul>
<p>  /*</p>
<ul>
<li>输入参数parent来自pci host驱动中pci host核心结构的struct device *dev,</li>
<li>dev来自 platform_device 中的dev。可以以drivers/pci/host下的pci-mvebu.c</li>
<li>作为例子, 其中所谓的pci host核心结构是：struct mvebu_pcie</li>
<li>/<br>bridge-&gt;dev.parent = parent;</li>
</ul>
<p>  /* 分配 struct pci_bus */<br>  b = pci_alloc_bus(NULL);</p>
<p>  b-&gt;sysdata = sysdata;<br>  b-&gt;ops = ops;<br>  b-&gt;number = b-&gt;busn_res.start = bus;<br>  /* 在pcie dts节点中找见domain字段, 加入pci_bus的domain_nr <em>/<br>  pci_bus_assign_domain_nr(b, parent);<br>  /</em></p>
<ul>
<li>在pci_root_buses全局链表中找相应domain下的bus, 首次调用的时候返回NULL</li>
<li>上面分配的pci_root_buses是在当前函数的最后才加入pci_root_buses中的，现在该</li>
<li>全局链表为空</li>
<li>/<br>b2 = pci_find_bus(pci_domain_nr(b), bus);<br>/*</li>
<li>上面两行处理有关pci domain的信息，kernel pci子系统怎么处理pci domain</li>
<li>的呢？ 首先数据结构是全局的链表：pci_root_buses, 局部链表：pci_domain_busn_res_list</li>
<li>pci_root_buses中存放每个pci domain的根总线，根总线在pci_create_root_bus</li>
<li>函数的结尾被添加到pci_root_buses链表中。pci_domain_busn_res_list存放</li>
<li>各个domain的信息, 包括domain号、domain包含的bus号范围, 该链表上存放</li>
<li>存放的结构是：struct pci_domain_busn_res, 在函数get_pci_domain_busn_res</li>
<li>中查找相应domain号的pci_domain_busn_res, 如果没有就分配一个新的</li>
<li>pci_domain_busn_res, 然后加到pci_domain_busn_res_list上</li>
<li>/</li>
</ul>
<p>  bridge-&gt;bus = b;<br>  dev_set_name(&amp;bridge-&gt;dev, “pci%04x:%02x”, pci_domain_nr(b), bus);<br>  error = pcibios_root_bridge_prepare(bridge);</p>
<p>  error = device_register(&amp;bridge-&gt;dev);</p>
<p>  b-&gt;bridge = get_device(&amp;bridge-&gt;dev);<br>  device_enable_async_suspend(b-&gt;bridge);<br>  pci_set_bus_of_node(b);</p>
<p>  if (!parent)</p>
<pre><code>  set_dev_node(b-&gt;bridge, pcibus_to_node(b));
</code></pre>
<p>  b-&gt;dev.class = &amp;pcibus_class;<br>  /* b-&gt;bridge 为对应pci_host_bridge中struct device dev的指针 */<br>  b-&gt;dev.parent = b-&gt;bridge;<br>  dev_set_name(&amp;b-&gt;dev, “%04x:%02x”, pci_domain_nr(b), bus);<br>  error = device_register(&amp;b-&gt;dev);</p>
<p>  pcibios_add_bus(b);</p>
<p>  /* Create legacy_io and legacy_mem files for this bus */<br>  pci_create_legacy_files(b);</p>
<p>  …<br>  /*</p>
<ul>
<li>取出pci_create_root_bus函数传入的链表resources中的pci_host_bridge_window,</li>
<li>把每个pci_host_bridge_window加入pci_host_bridge中的window链表中</li>
<li>/<br>list_for_each_entry_safe(window, n, resources, list) {<br>  list_move_tail(&amp;window-&gt;list, &amp;bridge-&gt;windows);<br>  res = window-&gt;res;<br>  offset = window-&gt;offset;<br>  if (res-&gt;flags &amp; IORESOURCE_BUS)<pre><code>      /*
   * 一般的，resources链表上有bus number, MEM space, I/O
   * space的节点，如果是bus number节点则调用以下函数。该
   * 函数会找到当前pci_bus的父结构，生成pci_bus中的busn_res
   * 并和父结构中的struct resource busn_res建立联系。
   * 如果父子在bus号上存在冲突，则返回冲突的bus号[1]
   */
  pci_bus_insert_busn_res(b, bus, res-&gt;end);
</code></pre>
  else<pre><code>  /*
   * 向pci_bus中的链表resources中加入struct pci_bus_resource
   * 记录mem, io的资源
   */
  pci_bus_add_resource(b, res, 0);
</code></pre>
  if (offset) {<pre><code>  if (resource_type(res) == IORESOURCE_IO)
      fmt = &quot; (bus address [%#06llx-%#06llx])&quot;;
  else
      fmt = &quot; (bus address [%#010llx-%#010llx])&quot;;
  snprintf(bus_addr, sizeof(bus_addr), fmt,
       (unsigned long long) (res-&gt;start - offset),
       (unsigned long long) (res-&gt;end - offset));
</code></pre>
  } else<pre><code>  bus_addr[0] = &#39;\0&#39;;
</code></pre>
  dev_info(&amp;b-&gt;dev, “root bus resource %pR%s\n”, res, bus_addr);<br>}</li>
</ul>
<p>  down_write(&amp;pci_bus_sem);<br>  /* 把创建的pci_bus加入全局链表pci_root_buses中 */<br>  list_add_tail(&amp;b-&gt;node, &amp;pci_root_buses);<br>  up_write(&amp;pci_bus_sem);</p>
<p>  return b;<br>  …<br>}</p>
</li>
</ul>
<p>[1] 关于linux中resource结构对资源的管理可以参看:<br>    <span class="exturl" data-url="aHR0cDovL3d3dy5saW51eGlkYy5jb20vTGludXgvMjAxMS0wOS80MzcwOC5odG0=">http://www.linuxidc.com/Linux/2011-09/43708.htm<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>SMMU的BBML语意</title>
    <url>/2021/06/27/SMMU%E7%9A%84BBML%E8%AF%AD%E6%84%8F/</url>
    <content><![CDATA[<p>SMMUv3 spec 3.21里定义了BBML的基本逻辑。软件在修改页表的时候，硬件可能还在访问，<br>这个会带来同步问题。如果，我们的世界里只有CPU，这个问题比较容易解决，如果是单核，<br>没有同步问题，如果是多核，一个核改页表的时候，另外一个core可能在使用页表做翻译，<br>这就有同步问题。解决办法应该是，先加锁，再清页表，再做tlbi，这样同步的页表翻译在<br>tlbi之前都是可以的，在tlbi以及barrier之后触发fault，fault在之前的锁上排队。</p>
<p>但是，这个过程里，如果有IO设备也在访问相关的页表，这个同步应该怎么做? 其基本逻辑<br>也是一样的，只不过tlbi上增加了针对设备一层tlb的无效化。</p>
<p>但是，IO设备访问memory分为两种情况，一种是像在内核中一个设备的dma访问一段内存，<br>这个dma访问是不能stop的; 一种是像SVA中一个设备的dma访问一段内存，页表变动的时候，<br>可以进入fault流程，fault流程可以在锁上排队，也就是dma是可有stop的。对于可以stop<br>的dma，我们使用如上的办法可以解决页表切换时的同步问题。</p>
<p>但是对于内核dma的情况，就搞不定了。首先，一般情况下在做内核dma的时候，页表不会<br>变动，但是在做虚拟化热迁移的时候，需要把大页解开成小页，就可能遇到在做内核dma<br>的时候需要变动页表。</p>
<p>SMMU BBML定义了0、1、2三种level，0就是需要软件保证整个同步，如上我们已经描述的。<br>level1的基本逻辑是这样的, 我们只看他从大页拆分成小页的过程，首先改变block页表的<br>nTbit到1(这个flag的含义是不在tlb里缓存这个block的tlb)，然后做tlbi把tlb清掉，然后<br>在原子的把block页表换成散开的page页表，这个过程中如果有IO一侧访问页表都是没有同步<br>问题的，因为如上的过程通过nT和tlbi配合把可能的tlb先清掉，然后换页表又是原子的。</p>
<p>注意SMMU spec上说，这里的页表变动是只有大页拆小页的变动，也就是说，页的物理位置，<br>属性相关的都不能在这个过程中有变化。</p>
<p>Level2和level1的使用场景是一致的，只是level2不需要nTbit的帮助。他要求硬件设计的<br>时候在出现一个地址对应多个tlb的时候先选择一个使用，然后用一个tlbi把相关的tlb都<br>清掉，然后page table walk从新的页表里得到tlb。</p>
]]></content>
      <tags>
        <tag>SMMU</tag>
      </tags>
  </entry>
  <entry>
    <title>TPH analysis</title>
    <url>/2021/05/21/TPH-analysis/</url>
    <content><![CDATA[<h2 id="TPH-analysis"><a href="#TPH-analysis" class="headerlink" title="TPH analysis"></a>TPH analysis</h2><p>-v0.1 2017.6.1 Sherlock init</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">               +-----+</span><br><span class="line">               | CPU |</span><br><span class="line">               +--+--+</span><br><span class="line">                  |</span><br><span class="line">             +----+-----+</span><br><span class="line">             | L3 cache |</span><br><span class="line">             +----+-----+</span><br><span class="line">                  |                 +-----+</span><br><span class="line">----------+-------+-----------------+ DDR |</span><br><span class="line">          |                         +-----+</span><br><span class="line">          |</span><br><span class="line">        +-+--+ &lt;--- stash table</span><br><span class="line">        | RP |</span><br><span class="line">        +-+--+</span><br><span class="line">          |</span><br><span class="line">        +-+--+ &lt;--- TH, PH, ST</span><br><span class="line">        | EP |</span><br><span class="line">        +----+</span><br></pre></td></tr></table></figure>

<p>TPH is a feature by which we can controll if TLP from EP can read/write to DDR<br>or L3 cache directly. As the attribute of read/write flow is different, e.g.<br>some data will be read soon by CPU, after a DMA write to DDR/Cache, for this<br>kind of date, it is better to put them directly to cache.</p>
<p>Physically, RP will do the read/write to DDR/cach. TH, PH, ST in PCIe TLP<br>will be used to control above operation. TH bit(one bit) in TLP indicates if we<br>enable TPH feature. PH(2 bits) in TLP indicates which kind of flow it is. ST is<br>a hardware specific design, which can be stored in MSI-X table or TPH capability.</p>
<p>So the software interface about TPH is:</p>
<pre><code>    PR: Device Capability 2(offset 0x24): TPH Completer Supported(12,13bit RO)
    EP: TPH Requester Capability: Header, capability register(RO), controler register(RW),
        ST table.(ST table can be in MSI-X table)
    EP&#39;s DMA descriptor: must have a place to offer PH related information.
</code></pre>
<p>In our system, we have above registers, ST table is in EP’s MSI-X table. Further<br>more, we have a stash table in RP. We implement ST table with 8 bit ST entry in EP,<br>which indicates which core/cluster one flow is expected to go to. The stash table<br>is a self-defined table, which is used to translate PH/ST info to stash info.</p>
<p>So from software view, we can controll TPH from EP’s DMA descriptor, ST table,<br>stash table. Now ST table and stash table have relationship, it is hard to modify<br>a PCIe device driver according to our chip’s ST table definition :(</p>
<p>But currently there is no clear definition about ST in PCIe spec and ARM spec.</p>
<p>Above is the first consideration, which means we may need private patch to do<br>optimization in the future.</p>
<p>The second consideration is as stach table will be implemented in BIOS, which<br>is hard to modified, we only can use ST and DMA descriptor to controll TPH behavior.</p>
<p>Then third is that we should firstly configure cacheable attribute, then enable<br>TPH to put data to L3 cache directly, e.g. we should firstly enable CCA=1 for RP,<br>then enable TPH feature(e.g. we can enable SMMU and CCA=1 to enable cacheable attribute).</p>
<p>The last is integrated PCIe devices, e.g. networking, also have TPH like feature.</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
      </tags>
  </entry>
  <entry>
    <title>Some tips to help to boot an installed Linux distribution manually</title>
    <url>/2021/07/05/Some-tips-to-help-to-boot-an-installed-Linux-distribution-manually/</url>
    <content><![CDATA[<ol>
<li><p>enther UEFI shell: D06&gt;</p>
</li>
<li><p>enter “start fs1:\EFI\redhat\grubaa64.efi”</p>
<p>this step is to load grubaa64.efi. fs1 here is the partition<br>in which there is a grubaa64.efi. you maybe need to try other<br>partitions like: fs0, fs1, fs2…</p>
<p>for linux, the EFI partition will be mounted under /boot/efi/</p>
</li>
<li><p>then you will enter the shell of grub: grub&gt;<br>(or your grub will load grub.conf directly, then you can directly<br> go into grub menu)</p>
</li>
<li><p>find where is grub.cfg in grub shell, then enter:</p>
<p>configfile grub.cfg<br>(my case is: configfile (hd1,gpt1)/efi/redhat/grub.conf)</p>
<p>in this step, you may need go around to see where is the grub.conf,<br>you can use “ls” in grub shell, and “tab” works also in grub shell,<br>path can be completed by “tab”</p>
<p>then enter grub menu</p>
</li>
<li><p>choose which kernel to boot</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>UEFI</tag>
      </tags>
  </entry>
  <entry>
    <title>SMMU stalled transaction with device</title>
    <url>/2021/06/27/SMMU-stalled-transaction-with-device/</url>
    <content><![CDATA[<p>Currently when process dies, there is no way(software callback function) to<br>control device DMA stop. The software flow is as below:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     +-&gt; exit_mmap</span><br><span class="line">       +-&gt; mmu_notifier_release</span><br><span class="line">// iommu-sva.c: </span><br><span class="line">iommu_mmu_notifier_ops</span><br><span class="line">  +-&gt; io_mm_release</span><br><span class="line">    +-&gt; io_mm-&gt;ops-&gt;clear</span><br><span class="line">    // arm-smmu-v3.c:</span><br><span class="line">    arm_smmu_mm_ops-&gt;arm_smmu_mm_clear</span><br><span class="line">      +-&gt; arm_smmu_write_ctx_desc</span><br><span class="line">        +-&gt; __arm_smmu_write_ctx_desc</span><br><span class="line">	  +-&gt; if (cd == &amp;invalid_cd) </span><br><span class="line">	    +-&gt; CD.S = CD.R = 0; EPD0 = 1;</span><br><span class="line">	  +-&gt; arm_smmu_sync_cd</span><br><span class="line">	    +-&gt; CD CFGI</span><br></pre></td></tr></table></figure>
<p>This will stop smmu stall and stop recore events in SMMU event q, and prevent<br>page table walk. Note that: CD.S = CD.R = 0; EPD0 = 1 does not prevent translation.<br>So if there is a transaction from device can SMMU TLB hit, this transaction can<br>still be sent out SMMU to system bus. Another Note: CD.S = CD.R = 0; EPD0 = 1<br>works after CD CFGI.</p>
<p>Above flow may happen together with normal SMMU fault flow, consider this case:<br>We use “exit” as above flow, and “dma” as normal SMMU fault flow.</p>
<p>  SMMU page fault triggered by device DMA(dma) -&gt; software fault handling(dma) -&gt;<br>  SMMU CD modification(exit) -&gt; software sending CMD resume retry(dma).</p>
<p>In above case, the last CMD resume retry will fail, hardware should terminate<br>former stalled transaction, otherwise there will be transaction which is stalled<br>in SMMU hardware for ever, which will cause device sending the transaction die.</p>
<p>sva unbind flow:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iommu_sva_unbind_device</span><br><span class="line">  [...]</span><br><span class="line">  +-&gt; arm_smmu_write_ctx_desc</span><br><span class="line">    +-&gt; __arm_smmu_write_ctx_desc</span><br><span class="line">      +-&gt; if (!cd)</span><br><span class="line">        +-&gt; cd val = 0;</span><br><span class="line">      +-&gt; arm_smmu_sync_cd</span><br><span class="line">        +-&gt; CD CFGI</span><br></pre></td></tr></table></figure>
<p>This will invalid CD(CD.valid = 0), if device DMA arrive SMMU here, maybe<br>a SMMU bad CD event(0xa) will report. however, in uacce driver, the release<br>callback will firstly stop q, then do sva unbind. So if q is stopped, we will<br>not see SMMU bad CD event.</p>
]]></content>
      <tags>
        <tag>SMMU</tag>
      </tags>
  </entry>
  <entry>
    <title>SMMU TLBI分析</title>
    <url>/2021/06/19/SMMU-TLBI%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol>
<li>SMMU E2H支持</li>
</ol>
<hr>
<p> SMMU SVA支持的patchset里有一个补丁：iommu/arm-smmu-v3: Add support for VHE<br> 这个补丁的基本逻辑是, CPU支持VHE，且内核支持时，host kernel工作在EL2，在SVA<br> 下，因为需要做DVM tlbi广播，且tlbi广播在同一个EL里生效，所以需要SMMU这边的TLB<br> 也打上EL2的标记。这个是通过设置设备对应的STE里的STRW生效的，这个配置对SMMU tlb<br> 影响是全局的。</p>
<p> 这里面还有三个逻辑：</p>
<ol>
<li><p>要使得DVM tlbi的广播生效，还要设置下SMMU CR2.E2H寄存器(使得ASID标记生效)。<br>这个补丁里配置了E2H。</p>
</li>
<li><p>因为STRW的配置是STE全局的，SMMU管理的内核地址的TLB也打上了EL2的标记，这就<br>需要把原来对内核地址的tlbi命令从TLBI_NH改成TLBI_VA。</p>
</li>
<li><p>原来invalidate内核地址的tlbi命令又可以分为TLBI_VA和TLBI_ASID，所以，上面<br>一步还要把原来的TLBI_NH_VA/TLBI_NH_ASID都改成TLBI_EL2_VA/TLBI_EL2_ASID。</p>
</li>
</ol>
<p> 这个补丁做了上述的修改，使得SVA和内核地址的tlbi在VHE使能的时候都可以正常使用。</p>
<ol start="2">
<li>SMMU tlbi相关命令</li>
</ol>
<hr>
<p> SMMU spec 4.4章节介绍了tlbi相关的command, 上面提到的相关tlbi命令在这一章节中<br> 都有详细的描述。EL2和NH的标记在上面已经介绍。关于VA和ASID标记的区别在于做tlb<br> 无效化的时候作用的范围是不一样的。简单的讲带ASID的tlbi会无效化对应asid的tlb，<br> 理论上讲，SVA需要用这个，但是现在SVA用的是DVM，还没有用到带ASID的tlbi，在实际<br> 分析的时候，还看到no-strict时使用的是带ASID的tlbi，这个下面结合代码来分析。<br> 直接带VA的tlbi可以对一段地址做tlb无效化，相关的command域段里的值有: scale,<br> num, asid, address, tg, ttl, leaf。当SMMU硬件支持tlbi by range(IDR3_RIL = 1)时，<br> 还可以用带VA的tlbi实现tlbi by range.</p>
<ol start="3">
<li>linux SMMU驱动里的实现</li>
</ol>
<hr>
<p> 因为SVA没有用tlbi，所以我们主要分析内核dma内存中的tlbi就好了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/iommu/dma-iommu.c */</span><br><span class="line">iommu_dma_free</span><br><span class="line">  +-&gt; __iommu_dma_unmap</span><br><span class="line">    /* 用来init iotlb_gather，为以后做tlbi by range做准备 */</span><br><span class="line">    +-&gt; iommu_iotlb_gather_init</span><br><span class="line">    +-&gt; iommu_unmap_fast</span><br><span class="line">      ...</span><br><span class="line"> +-&gt; arm_smmu_unmap</span><br><span class="line">   /* drivers/iommu/io-pgtable-arm.c, 这一步跳的有点大:) */</span><br><span class="line">   +-&gt; arm_lpae_unmap </span><br><span class="line">     +-&gt; __arm_lpae_unmap</span><br><span class="line">       +-&gt; io_pgtable_tlb_add_page</span><br><span class="line">  /* drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c */</span><br><span class="line">  +-&gt; arm_smmu_tlb_inv_page_nosync </span><br><span class="line">    /*</span><br><span class="line">     * 不断的收集tlbi的range，遇到不相连的range就做tlbi</span><br><span class="line">     */</span><br><span class="line">    +-&gt; iommu_iotlb_gather_add_page</span><br><span class="line">      +-&gt; iommu_tlb_sync</span><br><span class="line">        /* arm-smmu-v3.c */</span><br><span class="line">        +-&gt; arm_smmu_iotlb_sync</span><br><span class="line">	  /* 使用VA tlbi，range invalidate的逻辑也在这里 */</span><br><span class="line">	  +-&gt; arm_smmu_tlb_inv_range</span><br><span class="line"></span><br><span class="line">    +-&gt; iommu_tlb_sync (如果没有fq_domain，即使用strict mode)</span><br><span class="line">      +-&gt; arm_smmu_iotlb_sync</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p> 使用no-strict模式会起一个定时器, 在队列满或者定时器到时做tlbi。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* drivers/iommu/iova.c */</span><br><span class="line">iova_domain_flush</span><br><span class="line">  /* drivers/iommu/dma-iommu.c */</span><br><span class="line">  +-&gt; iommu_dma_flush_iotlb_all</span><br><span class="line">    /* arm-smmu-v3.c */</span><br><span class="line">    +-&gt; arm_smmu_flush_iotlb_all</span><br><span class="line">      /* 这个函数里会下发带ASID的tlb */</span><br><span class="line">      +-&gt; arm_smmu_tlb_inv_context</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Use coccinelle to check patch in Ubuntu 14.04</title>
    <url>/2021/07/05/Use-coccinelle-to-check-patch-in-Ubuntu-14-04/</url>
    <content><![CDATA[<ol>
<li><p>install from its git repo: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NvY2NpbmVsbGUvY29jY2luZWxsZS5naXQ=">https://github.com/coccinelle/coccinelle.git<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>install the packedge mention in install.txt</p>
</li>
<li><p>./autogen<br>./configure<br>./make<br>./make install</p>
</li>
<li><p>cd in a kernel directory, run:</p>
<p>make coccicheck MODE=report M=arch/arm64/mm</p>
<p>It will check all the patch under, e.g. arch/arm64/mm</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>SMMU SSV的逻辑</title>
    <url>/2021/06/27/SMMU-SSV%E7%9A%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>SMMU的STE表里的S1DSS=0b10时(目前的Linux主线内核代码是这样配置)，对于外设报文的<br>处理逻辑是：</p>
<ul>
<li><p>a transction without a Substream ID is accepted and uses the CD of Substream 0.</p>
</li>
<li><p>a transction that arrive with Substream ID 0 are aborted and an event recorded.</p>
</li>
</ul>
<p>上面SMMU判断一个外设上来的请求是否带有Substream ID的方法是看SMMU和外设之间的一个<br>叫SSV的信号有没有使能。这个SSV信号一般是外设可以配置的，如果一个外设把这个SSV<br>配置成1, 发给SMMU的请求中的PASID有是0的话, 就会报SMMU event错误。</p>
<p>常见硬件设计错误是把这个SSV的配置搞成设备全局的。这样这个设备只能要么工作在内核<br>态，要么工作在用户态，不能一部分资源工作在内核态，一部分工作在用户态。</p>
<p>正确的硬件设计是，把SSV的配置粒度减小。对于在内核态使用的资源，软件把对应的SSV配置<br>成0，对于在用户态使用的资源，软件把对应的SSV配置成1。</p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>SMMU</tag>
      </tags>
  </entry>
  <entry>
    <title>autoconf/automake使用笔记</title>
    <url>/2021/07/17/autoconf-automake%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li><p>写hello.c程序</p>
</li>
<li><p>autoscan 生成：autoscan.log  configure.scan  hello.c<br>   cat configure.scan:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#                                               -*- Autoconf -*-</span><br><span class="line"># Process this file with autoconf to produce a configure script.</span><br><span class="line"></span><br><span class="line">AC_PREREQ([2.68])</span><br><span class="line">AC_INIT([FULL-PACKAGE-NAME], [VERSION], [BUG-REPORT-ADDRESS])</span><br><span class="line">AC_CONFIG_SRCDIR([hello.c])</span><br><span class="line">        AC_CONFIG_HEADERS([config.h])</span><br><span class="line"></span><br><span class="line"># Checks for programs.</span><br><span class="line">AC_PROG_CC</span><br><span class="line"></span><br><span class="line"># Checks for libraries.</span><br><span class="line"></span><br><span class="line"># Checks for header files.</span><br><span class="line"></span><br><span class="line"># Checks for typedefs, structures, and compiler characteristics.</span><br><span class="line"></span><br><span class="line"># Checks for library functions.</span><br><span class="line"></span><br><span class="line">AC_OUTPUT</span><br></pre></td></tr></table></figure>
<p>修改成：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   #                                               -*- Autoconf -*-</span><br><span class="line">   # Process this file with autoconf to produce a configure script.</span><br><span class="line"></span><br><span class="line">   AC_PREREQ([2.68])</span><br><span class="line">   AC_INIT([autoconf_test, [0.1]) # change this line!</span><br><span class="line">   AC_CONFIG_SRCDIR([hello.c])</span><br><span class="line">   AC_CONFIG_HEADERS([config.h])</span><br><span class="line">   AM_INIT_AUTOMAKE([autoconf], [0.1]) # add this line!</span><br><span class="line"></span><br><span class="line">   # Checks for programs.</span><br><span class="line">   AC_PROG_CC</span><br><span class="line"></span><br><span class="line">   # Checks for libraries.</span><br><span class="line"></span><br><span class="line">   # Checks for header files.</span><br><span class="line"></span><br><span class="line">   # Checks for typedefs, structures, and compiler characteristics.</span><br><span class="line"></span><br><span class="line">   # Checks for library functions.</span><br><span class="line"></span><br><span class="line">   AC_OUTPUT(Makefile) # change this line！</span><br></pre></td></tr></table></figure>
<p>   把文件名改成: configure.in</p>
</li>
<li><p>aclocal 生成：<br>   aclocal.m4  autom4te.cache  configure.in  hello.c <br>   (主要是生成aclocal.m4)</p>
</li>
<li><p>autoconf 生成：<br>   aclocal.m4  autom4te.cache  configure  configure.in  hello.c</p>
</li>
<li><p>autoheader 生成：<br>   aclocal.m4  autom4te.cache  config.h.in  configure  configure.in  hello.c</p>
</li>
<li><p>创建Makefile.am:<br>   AUTOMAKE_OPTIONS=foreign<br>   bin_PROGRAMS=hello<br>   hello_SOURCES=hello.c</p>
</li>
<li><p>automake –add-missing <br>   过程信息为：<br>   configure.in:8: installing <code>./install-sh&#39;    configure.in:8: installing </code>./missing’<br>   Makefile.am: installing `./depcomp’<br>   生成文件：<br>   aclocal.m4  autom4te.cache  config.h.in  configure  configure.in  depcomp  <br>   hello.c  install-sh  Makefile.am  Makefile.in  missing<br>   (makefile.in是这步生成的关键文件)</p>
</li>
<li><p>./configure 生成最终的Makefile文件(该步骤中可能需要指定编译器：export CC=gcc)</p>
</li>
<li><p>make 生成最终的可执行的程序：hello, ./hello运行输出：test autoconf!</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	printf(&quot;test autoconf!\n&quot;);</span><br><span class="line">	</span><br><span class="line">	#ifdef CONFIG_H_TEST</span><br><span class="line">	printf(&quot;test autoconf: test config.h\n&quot;);</span><br><span class="line">	#endif</span><br><span class="line">	</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <tags>
        <tag>软件开发</tag>
        <tag>autoconf</tag>
      </tags>
  </entry>
  <entry>
    <title>cpio compress and extract</title>
    <url>/2021/07/11/cpio-compress-and-extract/</url>
    <content><![CDATA[<p>we often see a file is a *.cpio.gz, normally it is a gzip compressed data.</p>
<p>“gunzip file.cpio.gz” to extract it to a file.cpio, whichi is a ASCII cpio archive.<br>“cpio -ivmd &lt; file.cpio” can extract it finally.</p>
<p>we can use “find . | cpio -o –format=newc &gt; ../file.cpio” to compress a directory<br>to file.cpio. this command should be run in the root of the directory.</p>
<p>then we use “gzip -c file.cpio &gt; file.cpio.gz” to get orignal gzip compressed file.</p>
<p>Above initrd file system should be cpio.gz type. for example, we can run a qemu<br>besed on memory initrd fiel system like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 -machine virt -cpu cortex-a57 \</span><br><span class="line">-m 300M \</span><br><span class="line">-kernel ~/repos/linux/arch/arm64/boot/Image \</span><br><span class="line">-initrd ~/rootfs/rootfs.cpio.gz \</span><br><span class="line">-append &quot;console=ttyAMA0 root=/dev/ram rdinit=/init&quot; \</span><br><span class="line">-nographic \</span><br></pre></td></tr></table></figure>
<p>Note: as we run rootfs in memory, -m parametre above should carefully choose.<br>      too small is a bad idea.</p>
]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Unix/Linux编程实践教程笔记(11-15)</title>
    <url>/2021/06/27/Unix-Linux%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-11-15/</url>
    <content><![CDATA[<p>从获取服务的服务的角度讲，一个进程可以直接调用函数，也可以通过和另个一个进程互动<br>获取相关的服务。从进程间通信的角度讲，socket是一种不同物理机器上进程间通信的方式。</p>
<p>本机间的进程间通信的方式有匿名管道，命名管道(mkfifo), Unix domain socket, 通过<br>相同的文件通信，共享内存。不同物理机器之间的进程见通信一般用socket。</p>
<h2 id="匿名管道"><a href="#匿名管道" class="headerlink" title="匿名管道"></a>匿名管道</h2><p>匿名管道的一般用法是，在父进程中创建管道，fork出子进程，这样子进程也继承了父进程<br>中的管道的读文件描述符和写文件描述符，如果是父进程往子进程写数据，那么父进程需要<br>需要关闭读读描述符，子进程需要关闭写描述符，反之亦然，通信的时候父进程向写文件<br>描述符写入信息，子进程读读文件描述符读出信息。</p>
<h2 id="命名管道"><a href="#命名管道" class="headerlink" title="命名管道"></a>命名管道</h2><p>命名管道的方式，需要首先使用mkfifo创建管道文件，然后写进程向管道里写入数据，读<br>进程从管道里读出数据。命名管道可以使用在独立的进程之间，而匿名管道必须在父子进程<br>之间使用。(如何查看系统里的管道?)</p>
<h2 id="文件通信"><a href="#文件通信" class="headerlink" title="文件通信"></a>文件通信</h2><p>文件通信需要使用文件锁保证一致性</p>
<h2 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h2><p>使用ipcs -m可以查看系统里现在的共享内存，共享内存是用户态的一块内存，两个进程可以<br>通过一块共享内存交换信息。使用共享内存会有同步问题，需要对共享内存加锁，做到各个<br>进程对共享内存访问的互斥，这个可以用信号量机制，信号量是一个系统机制。</p>
<p>  产生数据的进程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">seg_id = shmget(TIME_MEM_KEY, SEG_SIZE, IPC_CREAT | 0777);</span><br><span class="line"></span><br><span class="line">mem_ptr = shmat(seg_id, NULL, 0);</span><br><span class="line"></span><br><span class="line">for (n = 0; n &lt; 60; n++) &#123;</span><br><span class="line">	time(&amp;now);			/* get the time	*/</span><br><span class="line">	strcpy(mem_ptr, ctime(&amp;now));	/* write to mem */</span><br><span class="line">	sleep(1);			/* wait a sec   */</span><br><span class="line">&#125;</span><br><span class="line">	</span><br><span class="line">/* now remove it */</span><br><span class="line">shmctl(seg_id, IPC_RMID, NULL);</span><br></pre></td></tr></table></figure>
<p>  消耗数据的进程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">seg_id = shmget(TIME_MEM_KEY, SEG_SIZE, 0777);</span><br><span class="line"></span><br><span class="line">mem_ptr = shmat(seg_id, NULL, 0);</span><br><span class="line"></span><br><span class="line">printf(&quot;The time, direct from memory: ..%s&quot;, mem_ptr);</span><br><span class="line"></span><br><span class="line">shmdt( mem_ptr );</span><br></pre></td></tr></table></figure>

<h2 id="socket"><a href="#socket" class="headerlink" title="socket"></a>socket</h2><p>关于socket通信，这几章介绍了基于TCP的socket，数据报socket(基于UDP), 和Unix domain<br>socket。可以使用select和poll响应多个阻塞的文件。</p>
<ol>
<li><p>流式socket</p>
<p>client端连接建立的流程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct sockaddr_in  servadd;        /* the number to call */</span><br><span class="line">struct hostent      *hp;            /* used to get number */</span><br><span class="line">int    sock_id, sock_fd;            /* the socket and fd  */</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * AF_INET表示Internet域，SOCK_STREAM表示流式socket</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">sock_id = socket(AF_INET, SOCK_STREAM, 0);    /* get a line */</span><br><span class="line">if (sock_id == -1) </span><br><span class="line">	oops( &quot;socket&quot; );</span><br><span class="line"></span><br><span class="line">/* servadd用来描述server的地址, 地址由IP和port组成 */</span><br><span class="line">bzero(&amp;servadd, sizeof(servadd));   /* zero the address */</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * 这个函数返回host对应的IP地址，入参是host name或者域名。测试返现，</span><br><span class="line"> * 如果是host name，得到的是/etc/hosts下对应的IP; 如果是域名，会得到对应</span><br><span class="line"> × 的IP。</span><br><span class="line"> */</span><br><span class="line">hp = gethostbyname(av[1]);          /* lookup host&#x27;s ip # */</span><br><span class="line">if (hp == NULL) </span><br><span class="line">	oops(av[1]);                /* or die */</span><br><span class="line">bcopy(hp-&gt;h_addr, (struct sockaddr *)&amp;servadd.sin_addr, hp-&gt;h_length);</span><br><span class="line"></span><br><span class="line">servadd.sin_port = htons(atoi(av[2]));  /* fill in port number */</span><br><span class="line"></span><br><span class="line">servadd.sin_family = AF_INET ;          /* fill in socket type */</span><br><span class="line"></span><br><span class="line">/* now dial */</span><br><span class="line">if (connect(sock_id,(struct sockaddr *)&amp;servadd, sizeof(servadd)) != 0)</span><br><span class="line">       oops( &quot;connect&quot; );</span><br></pre></td></tr></table></figure>
<p>server端(省略了一些错误处理)：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct  sockaddr_in   saddr;   /* build our address here */</span><br><span class="line">struct	hostent		*hp;   /* this is part of our    */</span><br><span class="line">char	hostname[HOSTLEN];     /* address 	         */</span><br><span class="line">int	sock_id,sock_fd;       /* line id, file desc     */</span><br><span class="line">FILE	*sock_fp;              /* use socket as stream   */</span><br><span class="line"></span><br><span class="line">sock_id = socket(PF_INET, SOCK_STREAM, 0);    /* get a socket */</span><br><span class="line">bzero((void *)&amp;saddr, sizeof(saddr)); /* clear out struct     */</span><br><span class="line"></span><br><span class="line">gethostname(hostname, HOSTLEN);         /* where am I ?         */</span><br><span class="line">hp = gethostbyname(hostname);           /* get info about host  */</span><br><span class="line">                                        /* fill in host part    */</span><br><span class="line">bcopy((void *)hp-&gt;h_addr, (void *)&amp;saddr.sin_addr, hp-&gt;h_length);</span><br><span class="line">saddr.sin_port = htons(PORTNUM);        /* fill in socket port  */</span><br><span class="line">saddr.sin_family = AF_INET ;            /* fill in addr family  */</span><br><span class="line"></span><br><span class="line">if (bind(sock_id, (struct sockaddr *)&amp;saddr, sizeof(saddr)) != 0)</span><br><span class="line">       oops(&quot;bind&quot;);</span><br><span class="line"></span><br><span class="line">if (listen(sock_id, 1) != 0) </span><br><span class="line">	oops(&quot;listen&quot;);</span><br><span class="line"></span><br><span class="line">while (1) &#123;</span><br><span class="line">	sock_fd = accept(sock_id, NULL, NULL); /* wait for call */</span><br><span class="line">        if (sock_fd == -1)</span><br><span class="line">                oops(&quot;accept&quot;);         /* error getting calls  */</span><br><span class="line"></span><br><span class="line">	/* fdopen把一个文件fd和一个文件流建立联系 */</span><br><span class="line">        sock_fp = fdopen(sock_fd,&quot;w&quot;);  /* we&#x27;ll write to the   */</span><br><span class="line">        if (sock_fp == NULL)            /* socket as a stream   */</span><br><span class="line">                oops(&quot;fdopen&quot;);         /* unless we can&#x27;t      */</span><br><span class="line"></span><br><span class="line">        thetime = time(NULL);           /* get time             */</span><br><span class="line">					/* and convert to strng */</span><br><span class="line">        fprintf(sock_fp, &quot;The time here is ..&quot;);</span><br><span class="line">        fprintf(sock_fp, &quot;%s&quot;, ctime(&amp;thetime)); </span><br><span class="line">        fclose(sock_fp);              /* release connection   */</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>数据报socket</p>
<p>数据报socket不需要提前建立链路，所以流式socket里的connect，listen，accept都<br>没有。数据报socket用sendto和recvfrom来发送和接收数据, recvfrom可以得到发送<br>者的地址，这样就可以用sendto给发送者发送信息。</p>
<p>(数据报可以直接对socket fd做read操作么？)</p>
</li>
<li><p>Unix domain socket</p>
<p>和上面的使用IP加端口号的地址表示方式不同，Uninx domain socket用主机上的一个<br>文件表示地址。</p>
<p>client:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int	           sock;</span><br><span class="line">struct sockaddr_un addr;</span><br><span class="line"></span><br><span class="line">sock = socket(PF_UNIX, SOCK_DGRAM, 0);</span><br><span class="line"></span><br><span class="line">addr.sun_family = AF_UNIX;</span><br><span class="line">/* sockname是一个文件名字 */</span><br><span class="line">strcpy(addr.sun_path, sockname);</span><br><span class="line">addrlen = strlen(sockname) + sizeof(addr.sun_family);</span><br><span class="line"></span><br><span class="line">sendto(sock, msg, strlen(msg), 0, (struct sockaddr *)&amp;addr, addrlen);</span><br></pre></td></tr></table></figure>
<p>server:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int	sock;			/* read messages here	*/</span><br><span class="line">struct sockaddr_un addr;	/* this is its address	*/</span><br><span class="line"></span><br><span class="line">/* build an address */</span><br><span class="line">addr.sun_family = AF_UNIX;		/* note AF_UNIX */</span><br><span class="line">strcpy(addr.sun_path, sockname);	/* filename is address */</span><br><span class="line">addrlen = strlen(sockname) + sizeof(addr.sun_family);</span><br><span class="line"></span><br><span class="line">sock = socket(PF_UNIX, SOCK_DGRAM, 0);	/* note PF_UNIX  */</span><br><span class="line"></span><br><span class="line">/* bind the address */</span><br><span class="line">bind(sock, (struct sockaddr *) &amp;addr, addrlen);</span><br><span class="line"></span><br><span class="line">while (1) &#123;</span><br><span class="line">	/* 直接read socket fd */</span><br><span class="line">	l = read(sock, msg, MSGLEN);	/* read works for DGRAM	*/</span><br><span class="line">	msg[l] = &#x27;\0&#x27;;			/* make it a string 	*/</span><br><span class="line"></span><br><span class="line">	/* do your job... */</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>select poll</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>Linux用户态编程</tag>
      </tags>
  </entry>
  <entry>
    <title>UEFI/Linux系统加载过程</title>
    <url>/2021/07/11/UEFI-Linux%E7%B3%BB%E7%BB%9F%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>一般一个从硬盘启动的linux系统的启动顺序是：UEFI-&gt;GRUB-&gt;Linux。其中，我们一般称<br>UEFI是固件，GRUB是bootloader. </p>
<p>顾名思义，一般我们认为固件是固化在系统里的，启动会自动加载、执行的一段代码，这<br>里我们不关心固件的存储位置。一般，bootloader和linux kernel镜像都是放在磁盘上<br>(我们这里只看磁盘启动的情况，不关心网络启动，e.g. PXE)。</p>
<p>UEFI在加载bootloader(e.g. grub)的时候会从EFI system分区寻找grub程序(e.g. grub.efi).<br>这个程序是一个UEFI环境中的可执行程序。一般，UEFI里会在EFI system分区上的一组路径<br>上搜索grub.efi，这组路径是提前在UEFI静态配置好的。EFI system分区必须被格式化成<br>FAT文件系统，这样UEFI才可以读取其中的文件。grub.efi加载后，会找见它对应的配置<br>文件grub.cfg. 在grub.cfg中，我们可以配置grub到哪里去加载kernel镜像, 以及到哪里去<br>加载根文件系统. 一个grub.cfg的配置类似:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Sample GRUB configuration file</span><br><span class="line">#</span><br><span class="line"># Boot automatically after 5 secs.</span><br><span class="line">set timeout=5</span><br><span class="line"># By default, boot the Estuary with Centos filesystem</span><br><span class="line">set default=d05_centos_sata_console</span><br><span class="line"># For booting GNU/Linux</span><br><span class="line"></span><br><span class="line">menuentry &quot;D05 Centos SATA(CONSOLE)&quot; --id d05_centos_sata_console &#123;</span><br><span class="line">        search --no-floppy --fs-uuid --set=root &lt;UUID&gt;</span><br><span class="line">                linux /Image pcie_aspm=off pci=pcie_bus_perf rootwait root=PARTUUID=&lt;PARTUUID&gt; rw</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中上面的UUID是存放Image的分区的UUID, PARTUUID是存放根文件系统的分区的PARTUUID.<br>UUID, PARTUUID可以事先用blkid得到，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# blkid</span><br><span class="line">/dev/sdc4: UUID=&quot;87b76c0a-7c76-4d2a-9414-6b52e6a00b1c&quot; TYPE=&quot;xfs&quot; PARTUUID=&quot;511f3dc7-4b6b-4991-975e-1d60e5e3e616&quot; </span><br><span class="line">/dev/sdc2: UUID=&quot;2ee48246-43a2-4014-a176-5d723e6be5b4&quot; TYPE=&quot;xfs&quot; PARTUUID=&quot;c724693a-751a-40c7-a8bd-d3bdd56311e1&quot; </span><br><span class="line">/dev/sdc1: SEC_TYPE=&quot;msdos&quot; UUID=&quot;80A6-A4F2&quot; TYPE=&quot;vfat&quot; PARTLABEL=&quot;EFI System Partition&quot; PARTUUID=&quot;f768b945-a14d-47d0-a5c0-371d4a163316&quot; </span><br><span class="line">/dev/sdc3: UUID=&quot;417d3354-a5f8-4a2c-81e4-ef03b9d51e94&quot; TYPE=&quot;swap&quot; PARTUUID=&quot;d9ae07de-fdad-4fcc-8621-deeadace67c5&quot; </span><br><span class="line">/dev/sdc5: UUID=&quot;c03a5dee-f751-4ff5-bc7c-f6026388a676&quot; TYPE=&quot;xfs&quot; PARTUUID=&quot;29860a29-e084-4105-815b-1a94c296a473&quot; </span><br></pre></td></tr></table></figure>

<p>按照上面的配置做的系统只是一个临时可用的系统，在系统启动之后，如果不加入其他的<br>操作，启动分区(上面存放Image, grub.efi, grub.cfg)是没有办法挂在到根文件系统上的。<br>这样，如果我们想把内核编译成一个RPM包，然后用rpm -i kernel-package安装到系统，<br>使得下次系统启动的时候可以用新安装的内核，这样是不可能的。</p>
<p>rpm -i kernel-package的时候会把相关的配置加载grub.cfg里。to do…</p>
<p>实际上一个标准的ISO安装是这样的, 安装过成应该会自动的分区和在启动分区里放置相应<br>的文件，并且更改/etc/fstab里的内容，实现开机自动挂在启动分区到/boot. to do…</p>
<p>一个例子:</p>
<ul>
<li>磁盘分区: fdisk [4]</li>
<li>拷贝grub.efi, grub.cfg, kernel Image到对应的EFI system分区</li>
<li>更改grub.cfg里的参数, 主要是修改上面提到的UUID和PARTUUID</li>
<li>拷贝文件系统到PARTUUID对应的分区里</li>
</ul>
<p>reference:</p>
<p>[1] <span class="exturl" data-url="aHR0cDovL3d3dy5yb2RzYm9va3MuY29tL2xpbnV4LXVlZmkv">http://www.rodsbooks.com/linux-uefi/<i class="fa fa-external-link-alt"></i></span><br>[2] <span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRUZJX3N5c3RlbV9wYXJ0aXRpb24=">https://en.wikipedia.org/wiki/EFI_system_partition<i class="fa fa-external-link-alt"></i></span> (install bootloader)<br>[3] <span class="exturl" data-url="aHR0cDovL3d3dy5yb2RzYm9va3MuY29tL2VmaS1ib290bG9hZGVycy9pbnN0YWxsYXRpb24uaHRtbA==">http://www.rodsbooks.com/efi-bootloaders/installation.html<i class="fa fa-external-link-alt"></i></span><br>[4] <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29wZW4tZXN0dWFyeS9lc3R1YXJ5L2Jsb2IvbWFzdGVyL2RvYy9EZXBsb3lfTWFudWFsLjREMDUubWQjMy4z">https://github.com/open-estuary/estuary/blob/master/doc/Deploy_Manual.4D05.md#3.3<i class="fa fa-external-link-alt"></i></span><br>[4] <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL29wZW4tZXN0dWFyeS9lc3R1YXJ5L2Jsb2IvbWFzdGVyL2RvYy9EZXBsb3lfTWFudWFsLjREMDUubWQjMy4z">https://github.com/open-estuary/estuary/blob/master/doc/Deploy_Manual.4D05.md#3.3<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>UEFI</tag>
      </tags>
  </entry>
  <entry>
    <title>docker笔记</title>
    <url>/2021/07/11/docker%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>docker的三个最基本的概念是镜像(Image)，容器(Container)和仓库(Repository)。本文<br>简单介绍有关他们的基础操作。</p>
<ol>
<li>Image</li>
</ol>
<hr>
<p>Image可以看成是一个配置过的发行版(e.g. 带apach配置的ubuntu发行版)。我们可以自己<br>生成一个镜像，或者是直接下载已有的镜像使用。比如下载最新的ubuntu docker iamge:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wangzhou@EstBuildSvr1:~$ docker pull ubuntu</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/ubuntu</span><br><span class="line">8aec416115fd: Pull complete </span><br><span class="line">695f074e24e3: Pull complete </span><br><span class="line">946d6c48c2a7: Pull complete </span><br><span class="line">bc7277e579f0: Pull complete </span><br><span class="line">2508cbcde94b: Pull complete </span><br><span class="line">Digest: sha256:71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130950</span><br><span class="line">Status: Downloaded newer image for ubuntu:latest</span><br></pre></td></tr></table></figure>
<p>注意上面下载的Image和ubuntu官方发行版还是有一定的不同的, 希望一直用docker image<br>的同学要对这个问题有准备。</p>
<p>查看系统中已有的docker image可以使用docker images, 比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wangzhou@EstBuildSvr1:~$ docker images</span><br><span class="line">REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">ubuntu                  latest              f49eec89601e        3 weeks ago         129.5 MB</span><br><span class="line">ubuntu                  14.04.1             4bf30b29cec4        4 weeks ago         284.6 MB</span><br><span class="line">ubuntu                  14.04               3f755ca42730        8 weeks ago         188 MB</span><br><span class="line">compile/ubuntu/server   14.04               09637d6f7c6f        3 months ago        540.2 MB</span><br><span class="line">&lt;none&gt;                  &lt;none&gt;              bd3d4369aebc        5 months ago        126.6 MB</span><br><span class="line">build/ubuntu/server     14.04               ab01bc6b6a57        7 months ago        1.663 GB</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>container</li>
</ol>
<hr>
<p>假设我们现在已经有了ubuntu的image，可以运行以下的命令在容器中运行这个镜像。</p>
<p>sudo docker run -t -i ubuntu:latest /bin/bash</p>
<p>在容器中:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@518aa47b44c4:~#   /* here 518aa47b44c4 is the ID of this container */</span><br></pre></td></tr></table></figure>
<p>ubuntu的环境下可以用apt-get安装相应的软件：<br>apt-get update<br>then install basic programs: ping, vim, mutt, fetchmail, msmtp, procmail</p>
<p>但是，上面的改动只是在容器中的，容器退出我们的改动就不存在了，所以，容器退出后<br>要把相关的改动提交，从而形成一个新的docker image, 下次我们在容器里运行这个image,<br>就包含了这次提交的信息。具体如下节。</p>
<ol start="3">
<li>build a docker image</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wangzhou@EstBuildSvr1:~$ docker commit -m &quot;Add: basic tools: vim, mutt, fetchmail, procmail, msmtp&quot; -a &quot;Docker basic&quot; 518aa47b44c4 ubuntu/latest</span><br><span class="line">sha256:c8ea18e453cf756dcbb5acaa21a96e78baf18f7b7fbdd5cf8b44edd606702bf0</span><br><span class="line">wangzhou@EstBuildSvr1:~$ docker images</span><br><span class="line">REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">ubuntu/latest           latest              c8ea18e453cf        14 seconds ago      301.3 MB</span><br><span class="line">ubuntu                  latest              f49eec89601e        3 weeks ago         129.5 MB</span><br><span class="line">ubuntu                  14.04.1             4bf30b29cec4        4 weeks ago         284.6 MB</span><br><span class="line">ubuntu                  14.04               3f755ca42730        8 weeks ago         188 MB</span><br><span class="line">compile/ubuntu/server   14.04               09637d6f7c6f        3 months ago        540.2 MB</span><br><span class="line">&lt;none&gt;                  &lt;none&gt;              bd3d4369aebc        5 months ago        126.6 MB</span><br><span class="line">build/ubuntu/server     14.04               ab01bc6b6a57        7 months ago        1.663 GB</span><br></pre></td></tr></table></figure>
<p>下次再运行image的时候，可以指定image的ID运行，可以用repo:tag的方式运行, 比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker run -t -i c8ea18e453cf7</span><br><span class="line">sudo docker run -t -i ubuntu/latest</span><br><span class="line">sudo docker run -t -i ubuntu/latest:latest</span><br></pre></td></tr></table></figure>
<p>更改docker image的tag:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker tag 6b46dcca842d docker_test/with_net_tools:add_ping_change_tag</span><br><span class="line">sherlock@T440:~/notes$ sudo docker images</span><br><span class="line">REPOSITORY                   TAG                   IMAGE ID            CREATED             SIZE</span><br><span class="line">docker_test/with_net_tools   add_ping              6b46dcca842d        4 hours ago         170.9 MB</span><br><span class="line">docker_test/with_net_tools   add_ping_change_tag   6b46dcca842d        4 hours ago         170.9 MB</span><br><span class="line">docker_test/with_net_tools   latest                4b48ccb6135c        4 hours ago         167.2 MB</span><br><span class="line">ubuntu                       latest                f753707788c5        2 weeks ago         127.2 MB</span><br><span class="line">ubuntu                       12.04                 6e0ef8cc1b8a        12 months ago       136 MB</span><br></pre></td></tr></table></figure>

<p>从一个容器退出后，容器还没有彻底消亡。用sudo docker ps -a 可以查看现在系统里的容器,<br>STATUS一栏显示容器的状态，’Up XXX days’表示这个容器已经运行XXX天了，现在还在运行，<br>Exited (0) XXX days ago表示这个容器现在是退出状态。对于退出状态的容器，可以<br>docker start -i 容器id重新启动这个容器。对于运行状态的容器，可以docker attach 容器id<br>继续接入这个容器, 这时接入这个容器的所有终端将同步显示容器中运行的程序。</p>
<p>/* remove a contrainor <em>/<br>sudo docker rm<br>/</em></p>
<ul>
<li>remove a image, if a image was used by a stopped container, should firstly</li>
<li>remove related stopped container</li>
<li>/<br>sudo docker rmi</li>
</ul>
<ol start="4">
<li>upload a docker image</li>
</ol>
<hr>
<p>为了保存，传播image，我们可以把一个docker image上传到DockerHub的仓库。当然首先<br>要在DockerHub上注册有账户。在上传之前，先在本机上用docker login登录下DockerHub<br>账户, 用户名、密码用对答的方式输入。然后就是运行：docker push repo_name[:tag]<br>的方式上传了。值得注意的是，repo_name要用DockerHub_account/XXX的形式, 不然可能<br>无法上传。</p>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title>cache基础概念</title>
    <url>/2021/06/28/cache%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<p>cache的存在是为了弥补CPU和内存间性能的gap. 一般的，有数据cache，指令cache。按照<br>级别分，有L1 cache，L2 cache, L3 cache。下面讲的cache是一个广泛的cache概念，仅仅<br>用于梳理cache里的其他的基本概念。</p>
<p>Cache，比如L1 data cache, 可以被细分为cache set，中文叫cache组。cache中的每个<br>cache组大小相等, 每个cache组可以包含一定数目的cache line, 一个set里包含的cache<br>line的个数和cache way(中文翻译为“路”)的数值是一样的(后面会讲cache way这个概念)，<br>每个cache line 会包含一定byte, 比如一个cache line 32 byte, 64 byte 或者有可能是<br>128 byte. 下面是cache set，way, line的示意图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     ---   +---+-----+-----------------------------------+   </span><br><span class="line">      ^    | v | tag | cache line                        |  每个组的cache line</span><br><span class="line">      |    +---+-----+-----------------------------------+  的个数也是这个cache</span><br><span class="line">set   |    | v | tag | cache line                        |  way(路)的数目</span><br><span class="line">      |    +---+-----+-----------------------------------+</span><br><span class="line">      |    ...</span><br><span class="line">      v    | v | tag | cache line                        |</span><br><span class="line">     ---   +---+-----+-----------------------------------+</span><br><span class="line"></span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           ...</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line"></span><br><span class="line">           ...     </span><br><span class="line"></span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line">           ...</span><br><span class="line">           | v | tag | cache line                        |</span><br><span class="line">           +---+-----+-----------------------------------+</span><br><span class="line"></span><br><span class="line">           假设这里有S个set, 每个cache line包含B个bypt</span><br></pre></td></tr></table></figure>
<p>基础概念介绍完了，我们看看一个特定内存地址是怎么被映射到cache中的。假设一个内存<br>地址有m位, 那么中间的s位用来对应set的编号，指定去哪个set中找cache, 其中 2^s = S.<br>低b bit对应cache line中bypt的偏移, 其中 2^b = B。高 m-s-b bit 就是上面的tag域段。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  tag(m-s-b)       index(s bit)        offset(b bit)</span><br><span class="line">|            |                      |                |</span><br><span class="line">|&lt;----------&gt;|&lt;--------------------&gt;|&lt;--------------&gt;|</span><br><span class="line">|            |                      |                |</span><br></pre></td></tr></table></figure>
<p>去找一个内存地址对应的cache时，首先根据index域的值, 找到相应的cache set。然后<br>根据内存地址tag域的值和cache里tag的值匹配，找到相应的cache line。至于怎么找到<br>匹配的cache line, 以及找不到匹配的cache line的时候的cache line替换策略, 本文将<br>不去涉及, 这个主题涉及的范围远超过本文的内容。找到对应的cache line后, 再根据<br>offset的值找到对应的byte。</p>
<p>有了上述的铺垫，现在可以讲清全相连cache，组相连cache，直接映射cache这几个概念了。<br>直接映射cache就是只有一个set的cache; 全相连cache就是一个set里只有一个cache line<br>的cache; 组相连cache就是一个set里有多个cache line的cache。</p>
<p>在ARMv8的编程指导手册cache的章节有如下的示意图。我们现在把上面的概念和下面ARMv8<br>手册上的示意图联系到一起。下图里，每一行(连同它的背影里的行)组成一个cache set。<br>所以，下面的图里有几个立体的行，就是有多少个set。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">      --- +----------------------------------------------+</span><br><span class="line">       ^  |                                              |</span><br><span class="line">       |  |                                              |</span><br><span class="line">       |  |                                              |</span><br><span class="line">       |  |                                              |</span><br><span class="line">       |  |      +----------------------------------------------+</span><br><span class="line">       |  |      |                                              |</span><br><span class="line">       |  |      |   +----------------------------------------------+</span><br><span class="line">       |  |      |   |                                              |</span><br><span class="line">       |  |      |   |   +---+-----+-----------------------------------+  ---</span><br><span class="line">       |  |      |   |   | v | tag | cache line                        |   ^</span><br><span class="line">       |  |      |   |   +---+-----+-----------------------------------+   |</span><br><span class="line">       |  |      |   |   | v | tag | cache line                        |   |</span><br><span class="line">       |  |      |   |   +---+-----+-----------------------------------+   |</span><br><span class="line">       |  |      |   |   | v | tag | cache line                        |   |</span><br><span class="line">       v  |      |   |   +---+-----+-----------------------------------+   |  每一行是一个set，</span><br><span class="line">     ---&gt; +------|   |   | v | tag | cache line                        |   |  一共有S行</span><br><span class="line">        \  .     |   |   +---+-----+-----------------------------------+   |</span><br><span class="line">         \  .    |   |   | v | tag | cache line                        |   |</span><br><span class="line">          \  .   |   |   +---+-----+-----------------------------------+   |</span><br><span class="line">           \     |   |   ...                                               |</span><br><span class="line">=&gt; one      \    +---|   | v | tag | cache line                        |   |</span><br><span class="line">   set       \       |   +---+-----+-----------------------------------+   |</span><br><span class="line">              \     -+---| v | tag | cache line                        |   |</span><br><span class="line">               \         +---+-----+-----------------------------------+   |</span><br><span class="line">                \        | v | tag | cache line                        |   v</span><br><span class="line">               ----&gt;     +---+-----+-----------------------------------+  ---</span><br></pre></td></tr></table></figure>
<p>下面再对全相连cache，组相连cache，直接映射cache做下解释。直接映射的cache只有一个<br>cache set, 所以每次去cache里找一个地址对应的cache都要遍历整个cache。这样，cache<br>小还可以，cache大了，效率会很差。直接映射cache的最直接的应用是TLB，TLB是MMU页表<br>的cache(缓存)。一般的DDR的cache都是组相连cache，全相连cache有cache颠簸的潜在性能<br>影响, 画个具体的图比较容易明白:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   +------------+              +------------+</span><br><span class="line">A  | ddr        |  ---------&gt;  | cache line |  全相连cache，一个set一个cache line</span><br><span class="line">   +------------+         /    +------------+</span><br><span class="line">   | ddr        |  ------/--&gt;  | cache line |</span><br><span class="line">   +------------+       / /    +------------+</span><br><span class="line">   | ddr        |  ----/-/--&gt;  | cache line |</span><br><span class="line">   +------------+     / / /    +------------+</span><br><span class="line">   | ddr        |  --/-/-/--&gt;  | cache line |</span><br><span class="line">   +------------+   / / / /    +------------+</span><br><span class="line">B  | ddr        |  / / / /</span><br><span class="line">   +------------+   / / /</span><br><span class="line">   | ddr        |  / / /</span><br><span class="line">   +------------+   / /</span><br><span class="line">   | ddr        |  / /</span><br><span class="line">   +------------+   /</span><br><span class="line">   | ddr        |  /</span><br><span class="line">   +------------+</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure>
<p>上面的内存块都以是cache line粒度的。可以看到B地址对应的cache line和A地址对应的<br>cache line是相同的。当对A地址引用后，再对B地址引用，由于cache line中还是A地址<br>对应的内容，那么对B地址的引用必然会引起cache替换。所以如果有这样的程序，那么性能<br>将会很差:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">      for (i = 0; i &lt; NUM; i++) &#123;</span><br><span class="line">      	tmp = *(A + i) * *(B + i);</span><br><span class="line">value += tmp;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<p>组相连cache因为一个set里有多个cache line, 还可以在set这一维度引入很多处理算法，<br>减少cache颠簸的发生。</p>
<p>最后以一个具体的cache的示例结束这篇笔记，一个32K 一个cache line 32 byte 4路组相连<br>cache的示意图: 先计算地址里s, b的数值，2 ^ b = 32, b = 5; S = 32K / (32 * 4) =<br>256, 2 ^ s = S, s = 8。所以一个内存地址的划分是:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">|bit 31 ~ bit 13                        |bit 12 ~ bit 5 |bit 4 ~  bit 0|</span><br><span class="line">+---------------------------------------+---------------+--------------+</span><br><span class="line">|       tag                             |   index       |   offset     |</span><br></pre></td></tr></table></figure>
<p>这个具体的cache有256个组，各个组4路。</p>
]]></content>
      <tags>
        <tag>ARMv8</tag>
        <tag>计算机体系结构</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title>dpdk mempool的逻辑</title>
    <url>/2021/06/20/dpdk-mempool%E7%9A%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>dpdk里有块内存池的支持，用户可以调用相关接口创建固定block大小的内存池，然后从这<br>个内存池里申请和释放内存。</p>
<p>这个内存池叫做rte_mempool, 相关的实现代码在dpdk/lib/librte_mempool/*. dpdk里<br>提供了rte_mempool相关的测试app，在这个地方app/test/test_mempool.c, 顺着这个测试<br>app可以大概看出如何使用rte_mempool。</p>
<h2 id="相关的接口有："><a href="#相关的接口有：" class="headerlink" title="相关的接口有："></a>相关的接口有：</h2><ol>
<li><p>rte_mempool_create</p>
</li>
<li><p>rte_mempool_cache_create</p>
</li>
<li><p>rte_mempool_generic_get</p>
</li>
<li><p>rte_mempool_generic_put</p>
</li>
</ol>
<h2 id="基本的构架是："><a href="#基本的构架是：" class="headerlink" title="基本的构架是："></a>基本的构架是：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">          +------------+</span><br><span class="line">cpu0      |local_cache |----+</span><br><span class="line">          +------------+    |</span><br><span class="line">                            |</span><br><span class="line">          +------------+    +-&gt; +---------+           +-------------+</span><br><span class="line">cpu1      |local_cache |------&gt; |rte_ring |----------&gt;|memory block |</span><br><span class="line">          +------------+    +-&gt; +---------+           +-------------+</span><br><span class="line">                            |</span><br><span class="line">                            |</span><br><span class="line">                            |</span><br><span class="line">                            |</span><br><span class="line">                            |</span><br><span class="line">          +------------+    |</span><br><span class="line">cpuN      |local_cache |----+</span><br><span class="line">          +------------+</span><br></pre></td></tr></table></figure>
<p>支持mempool的基本数据结构式，local_cache, rte_ring, memzone。memzone是分配的<br>内存(包括管理需要的内存)，一般是大页内存。rte_ring是一个无锁队列，用来管理内存,<br>rte_ring的entry个数等于memory里block的个数。rte_ring用cmp and change的原子指令<br>实现无锁队列，但是，在多核的时候，这个指令的开销也很大，所以为每一个cpu建立了<br>block内存的cache，cpu先从自己的cache里取block内存，不够的时候再去rte_ring里批量取。</p>
<h2 id="具体实现的关键点："><a href="#具体实现的关键点：" class="headerlink" title="具体实现的关键点："></a>具体实现的关键点：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rte_mempool_create</span><br><span class="line">  /*</span><br><span class="line">   * flags指示具体挂哪个ops, ops的定义在dpdk/drivers/mempool/ring/rte_mempool_ring.c</span><br><span class="line">   * ring的初始化调用alloc，从ring里申请释放内存调用dequeue/enqueue</span><br><span class="line">   */</span><br><span class="line">  +-&gt; rte_mempool_set_ops_name</span><br><span class="line">  +-&gt; rte_mempool_populate_default</span><br><span class="line">    +-&gt; mempool_ops_alloc_onece</span><br><span class="line">      +-&gt; rte_mempool_ops_alloc</span><br><span class="line">        +-&gt; ops-&gt;alloc</span><br><span class="line">  /* 被管理的memory memzone的申请 */</span><br><span class="line">  +-&gt; rte_mempool_populate_default </span><br><span class="line"></span><br><span class="line">在dpdk/drivers/mempool/ring/rte_mempool_ring.c里有alloc的实现：</span><br><span class="line">ring_alloc</span><br><span class="line">  +-&gt; rte_ring_create</span><br><span class="line">    +-&gt; rte_ring_create_elem</span><br><span class="line">       /* ring memzone的申请在这个地方 */</span><br><span class="line">       +-&gt; rte_memzone_reserve_aligned</span><br><span class="line">       /* ring的初始化 */</span><br><span class="line">       +-&gt; rte_ring_init</span><br><span class="line"></span><br><span class="line">rte_mempool_generic_get</span><br><span class="line">  +-&gt; /* get from local cache */</span><br><span class="line">  ...</span><br><span class="line">  +-&gt; /* ring_dequeue: 这个地方是从ring里分配内存 */</span><br><span class="line">    +-&gt; rte_mempool_ops_dequeue_bluk</span><br><span class="line">      +-&gt; ops-&gt;dequeue</span><br><span class="line"></span><br><span class="line">在dpdk/drivers/mempool/ring/rte_mempool_ring.c里有dequeue的实现：</span><br><span class="line">e.g. rte_ring_sc_dequeue</span><br><span class="line">       +-&gt; rte_ring_sc_dequeue_bulk</span><br><span class="line">       ...</span><br><span class="line">         /* 具体申请bluck的逻辑 */</span><br><span class="line">         +-&gt; __rte_ring_do_dequeue_elem</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>dpdk</tag>
      </tags>
  </entry>
  <entry>
    <title>git am 冲突解决技巧</title>
    <url>/2021/07/05/git-am-%E5%86%B2%E7%AA%81%E8%A7%A3%E5%86%B3%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<p>使用git am合patch的时候可能有冲突出现，这个时候，手动解决的办法是看看冲突在哪里，<br>然后手动的把那个patch和入。手动合入需要的时间太长.</p>
<p>我们可以用git apply –reject patch的方式合入。这里需要注意几个问题。</p>
<p>git apply只会看到文件，它把patch里的一个个diff段拆出来, 然后合入相应的文件里,<br>而且git apply只会合入当前目录下的diff段，所以上面的命令要到所有diff段的最大的<br>一个目录里去执行，一般为了方便就在代码的根目录里执行。git apply后相当于修改了<br>原文见，所以要git add，git commit下。–reject的这个参数会把有冲突的段保存在一个<br>.rej的文件里。</p>
<p>所以，一般git am合patch的步骤可以是这样的：</p>
<ol>
<li><p>git am patch     –&gt; 没有conflict，over！</p>
</li>
<li><p>有冲突的时候： cd code_root/</p>
<pre><code>   git apply --reject patch
</code></pre>
</li>
<li><p>在.rej文件里找见冲突的diff段，手动修改对应的代码</p>
</li>
<li><p>git add related_files</p>
</li>
<li><p>git am –resolved</p>
</li>
</ol>
<p>注意最后一个操作, 我们现在已经把git am的冲突解决，用git am –resovled可以继续git<br>am的操作把commit log也自动的打上！</p>
]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>ftp服务器设置</title>
    <url>/2021/07/17/ftp%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<p>做嵌入式调试需要在主机上建立一个ftp服务器，然后通过网线将主机上的程序下载到嵌入<br>式开发板上，主机的ftp服务器需要有用户名和密码。以上是所知道的信息，怎么利用现有<br>的资源快速解决这个简单的问题？上网搜索是必要的，但是漫无目的的搜索效率并不是很<br>大；还有一种方法是系统的把ftp服务器的知识学一下，这样有太慢。这里提供几个工具和<br>思路，或许会比较有用。</p>
<ol>
<li><p>man -k ftp，man命令加-k选项可以列出所有ftp相关的帮助信息，我们可以从中选择<br>   运行这个命令后可以看到ftp，tftp，vsftpd等相关项目。可以以上面的信息作为基础<br>   再在网上搜索</p>
</li>
<li><p>可以看到vsftpd是一个FTP服务器，man 8 vsftpd可以查看和他相关的信息</p>
</li>
<li><p>在google中搜vsftpd的信息，输入ubuntu vsftpd，第一条就得到下面的信息：<br>   <span class="exturl" data-url="aHR0cHM6Ly9oZWxwLnVidW50dS5jb20vMTAuMDQvc2VydmVyZ3VpZGUvZnRwLXNlcnZlci5odG1s">https://help.ubuntu.com/10.04/serverguide/ftp-server.html<i class="fa fa-external-link-alt"></i></span></p>
</li>
</ol>
<p>   文件的头几行就得到这样的信息：<br>   …<br>   Access to an FTP server can be managed in two ways:<br>   Anonymous<br>   Authenticated<br>   …<br>   上面告诉我们FTP服务器分为两大类：匿名的(就是直接ftp <ip>就可以登陆的)，需要<br>                                    输入用户名、密码的<br>   结合我们的需要，我们的搜索词变成了ftp，authenticated，但是这篇文档已经介绍<br>   了需要怎么设置，我们就不需要去别的地方搜了。下面的”User Authenticated FTP <br>   Configuration”说明要设置/etc/vsftpd.conf中的：<br>          local_enable=YES<br>          write_enable=YES<br>   然后在重启vsftpd:<br>          sudo /etc/init.d/vsftpd restart<br>   <br>3. 说到这里我们的pc(ubuntu系统)上还没有vsftpd啊，试试ubuntu的软件下载管理工具<br>   sudo apt-get install vsftpd(可以自动补全)，果然可以。</p>
<ol start="4">
<li><p>vsftpd服务器有了，开始配置/etc/vsftpd.conf。我们打开对应的文件<br>   sudo vi /etc/vsftpd.conf<br>   仔细看，发现文档已经是充满注释了。找个和我们目的相关的：<br>   …<br>   # Allow anonymous FTP? (Beware - allowed by default if you comment this out).<br>   anonymous_enable=NO<br>   … <br>   后面的注释说，这个是默认带开的，我们用的是authenticate，所以这里选NO</p>
</li>
<li><p>按这样的设置，然后重启服务器，用ftp 127.0.0.1登陆自己的服务器，发现要输入的<br>   自己ubuntu系统的用户名和密码作为vsftpd的用户名和密码，但是到了哪个目录中了呢？<br>   用get &lt;自己home中的文件&gt;，发现可以把自己home中的文件拉到当前目录下，看来默认<br>   vsftpd的目录就是自己的/home/XXX</p>
</li>
<li><p>既然/etc/vsftpd.conf文件注释很好，那就到该文件中看看怎么设置，设置：<br>   local_root=/home/XXX/your_ftpboot<br>   chmod 777 /home/XXX/your_ftpboot<br>   重启服务器，发现vsftpd可以使用了，根目录就是上面设置的</p>
</li>
</ol>
<p>   至于想更好的用好ftp服务器，就是研究、尝试/etc/vsftpd.conf的事了</p>
]]></content>
      <tags>
        <tag>运维</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>glib option简易使用</title>
    <url>/2021/08/11/glib-option%E7%AE%80%E6%98%93%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>直接在代码里加注释说明基本的使用方式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static gboolean opt_hugepage = FALSE;</span><br><span class="line">static gint opt_size = 128;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * GOptionEntry定义一个输入参数项。每项的定义依次是：长参数名字，短参数名字，flag，</span><br><span class="line"> * 参数类型，参数存放地址，参数项描述，参数描述。</span><br><span class="line"> *</span><br><span class="line"> * 简单使用时，flag用G_OPTION_FLAG_NONE就好，最后一个参数描述会在长参数后输出，</span><br><span class="line"> * 可以参看最下面的--help输出。</span><br><span class="line"> */</span><br><span class="line">static GOptionEntry entries[] = &#123;</span><br><span class="line">	&#123; &quot;size&quot;, &#x27;s&#x27;, G_OPTION_FLAG_NONE, G_OPTION_ARG_INT, &amp;opt_size,</span><br><span class="line">	  &quot;size of dma copy in normal page case&quot;, &quot;size&quot; &#125;,</span><br><span class="line">	&#123; &quot;hugepage&quot;, &#x27;h&#x27;, G_OPTION_FLAG_NONE, G_OPTION_ARG_NONE, &amp;opt_hugepage,</span><br><span class="line">	  &quot;use hugepage(one 2M page for src, one for dts)&quot;, NULL &#125;,</span><br><span class="line">	&#123; NULL &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">static void handle_options(int argc, char *argv[])</span><br><span class="line">&#123;</span><br><span class="line">	GError *err = NULL;</span><br><span class="line">	/* 参数项描述的上下文 */</span><br><span class="line">	GOptionContext *context;</span><br><span class="line"></span><br><span class="line">	/* 创建参数项描述的上下文 */</span><br><span class="line">	context = g_option_context_new(&quot;- test devmmu pasid&quot;);</span><br><span class="line"></span><br><span class="line">	/* 把如上定义的各个参数项放到context中 */</span><br><span class="line">	g_option_context_add_main_entries(context, entries, NULL);</span><br><span class="line"></span><br><span class="line">	/* 调用这个函数解析输入值 */</span><br><span class="line">	g_option_context_parse(context, &amp;argc, &amp;argv, &amp;err);</span><br><span class="line"></span><br><span class="line">	/* 释放context */</span><br><span class="line">	g_option_context_free(context);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(int argc, char *argv[])</span><br><span class="line">&#123;</span><br><span class="line">	handle_options(argc, argv);</span><br><span class="line"></span><br><span class="line">	/* ... */</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如上的配置，–help的输出:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># ghms_test --help</span><br><span class="line">Usage:</span><br><span class="line">  ghms_test [OPTION*] - test devmmu pasid</span><br><span class="line"></span><br><span class="line">Help Options:</span><br><span class="line">  -?, --help          Show help options</span><br><span class="line"></span><br><span class="line">Application Options:</span><br><span class="line">  -s, --size=size     size of dma copy in normal page case</span><br><span class="line">  -h, --hugepage      use hugepage(one 2M page for src, one for dts)</span><br></pre></td></tr></table></figure>

<p>使用如上的函数可以简单的把glib option使用起来，更多使用方法需要去查glib的使用手册。</p>
]]></content>
      <tags>
        <tag>Linux用户态编程</tag>
        <tag>glib</tag>
      </tags>
  </entry>
  <entry>
    <title>how to compile perf tool statically</title>
    <url>/2021/07/05/how-to-compile-perf-tool-statically/</url>
    <content><![CDATA[<ol>
<li><p>D05 machine + CentOS7.4</p>
</li>
<li><p>kernel code</p>
</li>
<li><p>cd kernel/tools<br>make LDFLAGS=-static perf</p>
</li>
<li><p>you can find perf statically in kernel/tools/perf</p>
</li>
</ol>
<p>Note:</p>
<ol>
<li>you may need to: yum install glibc-static</li>
<li>you need install slang to enable perf report -tui. In ubuntu20.04, you need<br>sudo apt install libslang2-dev. you can use ldd perf to see if perf already<br>links slang</li>
<li>you may need to install a lot other libs to compile a full function perf.<br>(libelf-dev)</li>
</ol>
]]></content>
      <tags>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>ftrace学习笔记1</title>
    <url>/2021/07/17/ftrace%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ol>
<li><p>ftrace是一个内核调试工具，调试内核运行，是一个trace工具。它可以查看函数的<br>  调用关系、函数的执行时间、系统中断的最大关闭时间等等</p>
</li>
<li><p>需编译内核时进行配置，使用时挂载虚拟文件系统debugfs：<br>  mount -t debugfs nodev /sys/kernel/debug</p>
</li>
<li><p>通过对该文件系统中文件的读写完成所用功能</p>
</li>
</ol>
<h2 id="配置与编译"><a href="#配置与编译" class="headerlink" title="配置与编译"></a>配置与编译</h2><p>Kernel hacking  —&gt;<br>[<em>] Tracers —&gt;<br>-</em>- Tracers<br>…(配置相应的跟踪器)</p>
<h2 id="具体使用"><a href="#具体使用" class="headerlink" title="具体使用"></a>具体使用</h2><p>下面显示的就是ftrace使用时的目录了(pc ubuntu12.04)，所有的操作都可以通过读写这些<br>文件完成，读写的时候用到的命令多为echo, cat之类进入/sys/kernel/debug/tracing,<br>ubuntu12.04默认已经把ftrace编译入了内核, ls /sys/kernel/debug/tracing得到:</p>
<p>vailable_events            kprobe_profile      stack_trace<br>available_filter_functions  options             trace<br>available_tracers           per_cpu             trace_clock<br>buffer_size_kb              printk_formats      trace_marker<br>buffer_total_size_kb        README              trace_options<br>current_tracer              saved_cmdlines      trace_pipe<br>dyn_ftrace_total_info       set_event           trace_stat<br>enabled_functions           set_ftrace_filter   tracing_cpumask<br>events                      set_ftrace_notrace  tracing_enabled<br>free_buffer                 set_ftrace_pid      tracing_max_latency<br>function_profile_enabled    set_graph_function  tracing_on<br>kprobe_events               stack_max_size      tracing_thresh</p>
<p>可以 cat vailable_tracers 查看现在支持的tracer, 一般的tracer有：</p>
<p>function, 可以打印出内核函数的调用过程<br>function_graph, 以函数调用的格式打印函数调用过程，看起来要方便很多<br>irqsoff, 打印出禁止中断的时间，对于系统响应不及时的问题，可以用这个查看<br>wakeup，这个可以打印进程从ready到run的latency<br>sched_swich，显示的是关于调度的信息</p>
<p>cat current_tracer 查看当前的tracer是什么，所以要用一个tracer的时候，首先要<br>把它写到这个文件中，比如要用function，就写 echo function &gt; current_tracer</p>
<p>下面具体看一个一个tracer的用法和由此引出来的东西:</p>
<ul>
<li>function:</li>
</ul>
<p>因为测试的时候常常挂死（cat trace）, 这里就把function的使用写成了一个脚<br>本，我们一行一行看下，echo 0 &gt; tracing_on， 暂停跟踪器，参考网上的资料，<br>一般会有一个tracing_enabled的文件(但是有些系统上可能没有),关于tracing_on<br>和tracing_enabled的区别，现在的理解是，tracing_on是暂停跟踪器，此时跟踪<br>器还在跟踪内核的运行，只是不再向文件 trace 中写入跟踪信息，<br>echo 1 &gt; tracing_on 是可以继续当前的跟踪的，而tracing_enabled用来开关<br>ftrace。另外，ftrace提供了内核函数tracing_on()，这个东西可以直接写在内<br>核里，放在你想要停起的地方，当tracing_on()执行的时候，你在外面cat trace，<br>显示的就是附近的信息，如果你在用户态暂停，trace中的内容会离你想要定位的<br>地方远</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#! /system/bin/sh </span><br><span class="line"># change to /sys/kernel/debug/tracing </span><br><span class="line"></span><br><span class="line">dir=&quot;/sys/kernel/debug/tracing/&quot; </span><br><span class="line">echo 0 &gt; $&#123;dir&#125;tracing_on </span><br><span class="line">echo function &gt; $&#123;dir&#125;current_tracer          # 写入function</span><br><span class="line">echo 1 &gt; $&#123;dir&#125;tracing_on                     # 运行tracer</span><br><span class="line">sleep 5                                       # 叫tracer运行一段时间</span><br><span class="line">echo 0 &gt; $&#123;dir&#125;tracing_on                     # 暂停tracer</span><br><span class="line">cat $&#123;dir&#125;trace | head -10                    # 显示跟踪的函数内容</span><br></pre></td></tr></table></figure>
<p>截取了一段现实的结果放在这里</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tracer: function </span><br><span class="line">#                       </span><br><span class="line"># entries-in-buffer/entries-written: 222082/295723   #P:4 </span><br><span class="line">#                                               （online cpu number: 4）</span><br><span class="line">#                              _-----=&gt; irqs-off </span><br><span class="line">#                             / _----=&gt; need-resched </span><br><span class="line">#                            | / _---=&gt; hardirq/softirq </span><br><span class="line">#                            || / _--=&gt; preempt-depth </span><br><span class="line">#                            ||| /     delay </span><br><span class="line">#           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION </span><br><span class="line">#              | |       |   ||||       |         | </span><br><span class="line"> SurfaceFlinger-255   [001] d... 14810.259416: gic_handle_irq &lt;-__irq_usr </span><br><span class="line"> SurfaceFlinger-255   [001] d... 14810.259483: irq_find_mapping &lt;-gic_handle_irq </span><br></pre></td></tr></table></figure>
<ul>
<li> function_graph:</li>
</ul>
<p>同样的操作，看看function_graph的输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  # tracer: function_graph </span><br><span class="line">  # </span><br><span class="line">  # CPU  DURATION                  FUNCTION CALLS </span><br><span class="line">  # |     |   |                     |   |   |   | </span><br><span class="line">   3)               |  __do_fault() &#123; </span><br><span class="line">3)               |    i915_gem_fault() &#123; </span><br><span class="line">3)               |      i915_mutex_lock_interruptible() &#123; </span><br><span class="line">3)               |        mutex_lock_interruptible() &#123; </span><br><span class="line">3)   0.339 us    |          _cond_resched(); </span><br><span class="line">3)   1.212 us    |        &#125; </span><br><span class="line">3)   1.887 us    |      &#125; 3)               |      i915_gem_object_unbind() &#123; </span><br><span class="line">3)   0.114 us    |        i915_gem_object_finish_gpu(); </span><br><span class="line">3)   0.120 us    |        i915_gem_release_mmap();</span><br></pre></td></tr></table></figure>
<p>这里 echo __do_fault &gt; set_graph_function了一下，所以输出是针对<br>_do_fault()的检测</p>
<ul>
<li>irqsoff:</li>
</ul>
<p>下面截取一段irqsoff的输出信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tracer: irqsoff </span><br><span class="line"># </span><br><span class="line"># irqsoff latency trace v1.1.5 on 3.9.0-rc7-00004-g70df926-dirty </span><br><span class="line"># -------------------------------------------------------------------- </span><br><span class="line"># latency: 17790 us, #323/323, CPU#1 | (M:server VP:0, KP:0, SP:0 HP:0 #P:4) </span><br><span class="line">#    ----------------- </span><br><span class="line">#    | task: ProcessStats-502 (uid:1000 nice:0 policy:0 rt_prio:0) </span><br><span class="line">#    ----------------- </span><br><span class="line">#  =&gt; started at: __lock_task_sighand </span><br><span class="line">#  =&gt; ended at:   _raw_spin_unlock_irqrestore </span><br><span class="line"># </span><br><span class="line"># </span><br><span class="line">                  _------=&gt; CPU#            </span><br><span class="line"> / _-----=&gt; irqs-off        </span><br><span class="line"> | / _----=&gt; need-resched    </span><br><span class="line"> || / _---=&gt; hardirq/softirq </span><br><span class="line"> ||| / _--=&gt; preempt-depth   </span><br><span class="line"> |||| /     delay             </span><br><span class="line">        # cmd     pid      ||||| time  |   caller      </span><br><span class="line">      \   /          |||||  \       |   /           </span><br><span class="line">ProcessS-502     1d...   49us!: __lock_task_sighand </span><br><span class="line">ProcessS-502     1d...  154us!: _raw_spin_lock &lt;-__lock_task_sighand </span><br><span class="line">ProcessS-502     1d...  933us+: thread_group_cputime_adjusted &lt;-do_task_stat </span><br><span class="line">ProcessS-502     1d... 1016us+: thread_group_cputime &lt;-thread_group_cputime_adjusted </span><br><span class="line">ProcessS-502     1d... 1106us+: task_sched_runtime &lt;-thread_group_cputime </span><br><span class="line">ProcessS-502     1d... 1195us+: task_rq_lock &lt;-task_sched_runtime </span><br><span class="line">        ProcessS-502     1d... 1286us!: _raw_spin_lock_irqsave &lt;-task_rq_lock</span><br></pre></td></tr></table></figure>
<p>上面找到的是中断关闭时间最长的进程（ProcessStats-502），具体显示出关闭<br>和开中断的函数，CPU# 进程运行在哪个CPU上，irqs-off ‘d’表示中断关闭<br>(上半部)，need-resched设置时中断出来之后，执行一次调度，hardirq/softirq<br>表示硬中断/软中断正在运行。latency: 17790us表示的是关中断最长的时间</p>
<ul>
<li>Wakeup_rt:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  # tracer: wakeup_rt </span><br><span class="line">  # </span><br><span class="line">  # wakeup_rt latency trace v1.1.5 on 3.9.0-rc7-00004-g70df926-dirty </span><br><span class="line">  # -------------------------------------------------------------------- </span><br><span class="line">  # latency: 4155 us, #269/269, CPU#0 | (M:server VP:0, KP:0, SP:0 HP:0 #P:4) </span><br><span class="line">  #    ----------------- </span><br><span class="line">  #    | task: watchdog/0-11 (uid:0 nice:0 policy:1 rt_prio:99) </span><br><span class="line">  #    ----------------- </span><br><span class="line">  # </span><br><span class="line">  #                  _------=&gt; CPU#            </span><br><span class="line">  #                 / _-----=&gt; irqs-off        </span><br><span class="line">  #                | / _----=&gt; need-resched    </span><br><span class="line">  #                || / _---=&gt; hardirq/softirq </span><br><span class="line">  #                ||| / _--=&gt; preempt-depth   </span><br><span class="line">  #                |||| /     delay             </span><br><span class="line">  #  cmd     pid   ||||| time  |   caller      </span><br><span class="line">  #     \   /      |||||  \    |   /           </span><br><span class="line">    &lt;idle&gt;-0       0d.h.   32us+:      0:120:R   + [000]    11:  0:R watchdog/0 </span><br><span class="line">&lt;idle&gt;-0       0d.h.   99us+: 0 </span><br><span class="line">&lt;idle&gt;-0       0d.h.  114us+: check_preempt_curr &lt;-ttwu_do_wakeup </span><br><span class="line">&lt;idle&gt;-0       0d.h.  126us+: resched_task &lt;-check_preempt_curr </span><br><span class="line">&lt;idle&gt;-0       0dNh.  141us+: task_woken_rt &lt;-ttwu_do_wakeup </span><br><span class="line">&lt;idle&gt;-0       0dNh.  157us+: _raw_spin_unlock_irqrestore &lt;-try_to_wake_up </span><br><span class="line">&lt;idle&gt;-0       0dNh.  172us+: ktime_get &lt;-watchdog_timer_fn </span><br><span class="line">…</span><br><span class="line">&lt;idle&gt;-0       0dN.. 4064us+: put_prev_task_idle &lt;-__schedule &lt;idle&gt;-0       0dN.. 4077us+: pick_next_task_stop &lt;-__schedule </span><br><span class="line">&lt;idle&gt;-0       0dN.. 4090us+: pick_next_task_rt &lt;-__schedule </span><br><span class="line">&lt;idle&gt;-0       0dN.. 4103us+: dequeue_pushable_task &lt;-pick_next_task_rt </span><br><span class="line">&lt;idle&gt;-0       0d... 4126us+: __schedule </span><br><span class="line">&lt;idle&gt;-0       0d... 4138us :      0:120:R ==&gt; [000]    11:  0:R watchdog/0</span><br></pre></td></tr></table></figure>
可以看到最长实时进程的latency是4155us, 其中很多时间是消耗在过程函数的统<br>计上，echo 0 &gt; /sys/kernel/debug/tracing/options/function-trace <br>可以关闭对过程函数的统计, 有些系统上没有上面的目录或文件</li>
</ul>
<p>设定<br>sysctl  kernel.ftrace_enabled=1   或者 <br>echo 1 &gt; /proc/sys/kernel/ftrace_enabled<br>也可以关掉对过程函数的跟踪, 下面是使用wakeup tracer时，不记录过程函数时<br>的测试结果：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tracer: wakeup </span><br><span class="line"># </span><br><span class="line"># wakeup latency trace v1.1.5 on 3.2.0-41-generic </span><br><span class="line"># -------------------------------------------------------------------- </span><br><span class="line"># latency: 8307 us, #4/4, CPU#2 | (M:desktop VP:0, KP:0, SP:0 HP:0 #P:4) </span><br><span class="line">#    ----------------- </span><br><span class="line">#    | task: alsa-sink-22445 (uid:1002 nice:-11 policy:2 rt_prio:5) </span><br><span class="line">#    ----------------- </span><br><span class="line"># </span><br><span class="line">#                  _------=&gt; CPU#            </span><br><span class="line">#                 / _-----=&gt; irqs-off        </span><br><span class="line">#                | / _----=&gt; need-resched    </span><br><span class="line">#                || / _---=&gt; hardirq/softirq </span><br><span class="line">#                ||| / _--=&gt; preempt-depth   </span><br><span class="line">#                |||| /     delay             </span><br><span class="line">#  cmd     pid   ||||| time  |   caller      </span><br><span class="line">#     \   /      |||||  \    |   /           </span><br><span class="line">   Xorg-22173   2d.h1    0us :  22173:120:R   + [002] 22445: 94:R alsa-sink </span><br><span class="line">Xorg-22173   2d.h1    1us!: ttwu_do_activate.constprop.179 &lt;-sched_ttwu_pending </span><br><span class="line">Xorg-22173   2d... 8306us : probe_wakeup_sched_switch &lt;-__schedule </span><br><span class="line">Xorg-22173   2d... 8307us :  22173:120:R ==&gt; [002] 22445: 94:R alsa-sink </span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>ftrace</tag>
      </tags>
  </entry>
  <entry>
    <title>git使用笔记</title>
    <url>/2021/07/17/git%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="chaper-2"><a href="#chaper-2" class="headerlink" title="chaper 2"></a>chaper 2</h2><p>git add -u 加入缓冲区所有的改动文件<br>        -A 把所有增加/删除的文件加入缓冲区<br>    -i 提供一个添加文件的交互界面</p>
<p>git diff 显示改动，只显示没有提交到缓冲区的改动<br>         –cached 显示缓冲区中的改动<br>     old-ID new-ID 显示两次提交的改动</p>
<p>git stash/git stash pop 工作的时候往往要临时切换到一个分支，这时可以用git stash<br>    不需要git add, 当处理完另一个分支时，切换会原来的分支，使用git stash pop<br>    即可以回到原来的工作区</p>
<h2 id="chapter-3"><a href="#chapter-3" class="headerlink" title="chapter 3"></a>chapter 3</h2><p> 中文：UTF-8字符集(一般)</p>
<h2 id="chapter-4"><a href="#chapter-4" class="headerlink" title="chapter 4"></a>chapter 4</h2><p> git config -e           改当前库的配置(.git/config)<br> git config -e –global  改当前用户的配置(/home/***/.gitconfig)<br> git config -e –system  改整个系统的配置(/etc/gitconfig)</p>
<h2 id="chapter-5"><a href="#chapter-5" class="headerlink" title="chapter 5"></a>chapter 5</h2><p> git log –oneline 精简模式显示提交信息<br> git log –prety=fuller 显示AuthorData和Commit Date ?<br> git ls-tree -l HEAD 查看版本库中的最新提交<br> git ls-files 查看缓冲区的最新内容</p>
<h2 id="chapter-6"><a href="#chapter-6" class="headerlink" title="chapter 6"></a>chapter 6</h2><p> git cat-file -t <ID> 显示ID对应的类型(type)：commit, tree, blob…<br>              -p <ID> 显示ID对应的内容<br> git rev-parse HEAD/master… 显示对应的ID<br> git log –graph –all 显示commit的历史，加上–all可以显示所有分支</p>
<p> HEAD: 保存当前分支的路径，若当前分支test, HEAD内容是 ref: refs/heads/test<br> .git/refs/heads/master : heads目录下存放所有的分支，若此时git中还有一个分支test<br>                          在heads下会发现：master，test. master, test文件中放的<br>              是ID，是对应分支最新提交的ID<br> .git/refs/tags/*** : …</p>
<h2 id="chapter-7"><a href="#chapter-7" class="headerlink" title="chapter 7"></a>chapter 7</h2><p> git reset –hard HEAD^ 整个HEAD切换到他的父提交, 若现在有如下的提交:<br> A–&gt;B–&gt;C–&gt;D<br> 使用上述命令后，使用git log将只看到: A–&gt;B–&gt;C<br> git reset –hard 将版本库，缓存区，工作区全部切换到相应的版本(C), D版本相当于<br> 被丢掉了(没有显示出来)</p>
<p> git reset –soft HEAD^ 只把版本库切到了父提交，也就是回到了，上次git add ***<br> git commit *** 之前的状态，使用git status可以证明这点</p>
<p> git reset HEAD^ 把版本库，缓存区切到了父提交，也就是回到了上次编辑过工作区，<br> git add *** 之前的状态，使用git status可以看到这点</p>
<p> git reset/git reset HEAD 依照上面的分析，相当于缓冲区切到父提交，就是把git add<br> 加入缓冲区的东西去掉，是git add 的逆操作</p>
<p> git reset – filename 是git add filename的逆操作</p>
<p> git reset –hard HEAD^ 之后的挽救措施：（想恢复原来的提交）<br> 在.git/log/logs/HEAD中记录着每次HEAD的改动，找到想要的ID用来恢复<br> 更简单的方法：<br> git reflog show 找到要恢复的版本<br> git reset –hard master@{***} 即可</p>
<p> 注: git reset –hard HEAD^<br>     do some change…<br>     git add …<br>     git commit … (version E)<br>     实际是存储是：(其中version D是不可见的)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A--&gt;B--&gt;C--&gt;D</span><br><span class="line">         \--&gt;E</span><br></pre></td></tr></table></figure>
<pre><code> git reset 没有改变HEAD的内容，而是改变了.git/refs/head/... 的内容
</code></pre>
<h2 id="chapter-8"><a href="#chapter-8" class="headerlink" title="chapter 8"></a>chapter 8</h2><p> git checkout ID 检出ID所对应的提交. 比如；<br> A–&gt;B–&gt;C–&gt;D<br> git checkout ID(C) ID(C)表示C对应的ID<br> 这是用git branch察看所在的分支，会显示当前处于no branch的状态，实际上察看<br> .git/HEAD会发现其中的内容不是指向一个分支(如：ref:refs/heads/master), 而是一个<br> 具体提交的ID. 在这种no branch的状态可以查看代码，做验证，但是不能提交修改。<br> 其实也是可以在提交的，只是再从当前的状态切回某个分支(如：git checkout master),<br> 之前的提交不可见了:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">              /-- master</span><br><span class="line">A--&gt;B--&gt;C--&gt;D </span><br><span class="line">     \</span><br><span class="line">      E -- git checkout ID(B), git commit E</span><br></pre></td></tr></table></figure>
<p> 如上在no branch上提交了E，然后git checkout master切回了master这时候E不可见了.<br> 用git reflog show 查看提交的历史，然后git reset –hard HEAD@{…} 可以把HEAD<br> 指向E，这时 A–&gt;B–&gt;E 成了master分支，C、D不可见了</p>
<p> git checkout -b branch_name 创建新的分支，名字是branch_name<br> git checkout 改变HEAD的内容</p>
<h2 id="chapter-12"><a href="#chapter-12" class="headerlink" title="chapter 12"></a>chapter 12</h2><p> git cherry-pick id 把id对应的commit向当前的HEAD提交<br> A–&gt;B–&gt;C–&gt;D–&gt;E git checkout id(C) git cherry-pich id(E)会把E向C提交:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                 /-- master</span><br><span class="line">A--&gt;B--&gt;C--&gt;D--&gt;E </span><br><span class="line">         \</span><br><span class="line">          E &lt;--HEAD</span><br></pre></td></tr></table></figure>
<p> 这时HEAD分离的情况，HEAD不对应任何分支,可以建立新的branch, 也可以git reset可以<br> 把master的内容指向E, 这时D和其后的E将显示不出来</p>
<p> git cherry-pick id -e 可以修改commit的签名中的内容(邮箱)</p>
<p> git rebase</p>
<h2 id="chapter-15"><a href="#chapter-15" class="headerlink" title="chapter 15"></a>chapter 15</h2><p> git pull/push<br> git push 有时无法成功，可能是因为git push对应的git仓库不是bare的，直接推送会<br> 改变工作区。这可以配置对应的远程仓库：git config receive.denyCurrentBranch ignore<br> 这时可以成功push</p>
<h2 id="chapter-16"><a href="#chapter-16" class="headerlink" title=" chapter 16"></a> chapter 16</h2><p> -other<br> git commit –amend –author=’your name <email-box>‘ 可以修改commit中author一行的内容</p>
<p> patch的subject这一行有时不只是[PATCH], 比如在询问意见时可以是[PATCH RFC ***], 在第3版<br> patch时subject可以是[PATCH v3 <em><strong>]. 如何改变subject这一行的内容：可以在生成patch<br> 的时候加–subject-prefix=”</strong></em>“, 比如, git format-patch -s -2 –subject-prefix=”PATCH RFC”<br> 生成的patch subject为：[PATCH RFC 0/3], [PATCH RFC 1/3], [PATCH RFC 2/3], [PATCH RFC 3/3]</p>
<p> git send-email 使用git send-email发送patches, 组成的patches是一个系列的。<br> 用git format-patch生成patches, 然后一个个用普通邮箱发出，给出的patches是一个个分裂的。<br> git send-email *.patch 即可把当前目录里的patch都发送出去，而且git send-email提供一个<br> 对话是的发送过程，只要在过程中填入发送的邮箱即可。对于cc的邮箱可以在一开始的命令中给出：<br> git send-email *.patch --cc=<span class="exturl" data-url="bWFpbHRvOiYjeDc5OyYjMTExOyYjeDc1OyYjMTE0OyYjOTU7JiMxMDE7JiMxMDk7JiN4NjE7JiMxMDU7JiMxMDg7JiN4NWY7JiN4NjI7JiN4NmY7JiN4Nzg7JiN4NDA7JiM0OTsmI3gzMjsmIzU0OyYjNDY7JiN4NjM7JiMxMTE7JiN4NmQ7">&#x79;&#111;&#x75;&#114;&#95;&#101;&#109;&#x61;&#105;&#108;&#x5f;&#x62;&#x6f;&#x78;&#x40;&#49;&#x32;&#54;&#46;&#x63;&#111;&#x6d;<i class="fa fa-external-link-alt"></i></span></p>
<p> Message-ID to be used as In-Reply-To?</p>
<h2 id="git-pull-note"><a href="#git-pull-note" class="headerlink" title="git pull note"></a>git pull note</h2><ol>
<li><p>git repo A:<br>branch: master, test</p>
<p>git repo B;<br>branch: master, test(all pull from repo A)</p>
<p>若在repo A上test分支加一个提交, 在repo B的master分支上用git pull, reop<br>B的test分支将不会更新，repo B切换到test分支上，再使用git pull,<br>则可以更新test分支。</p>
</li>
<li><p>还是上面的场景，在repo A test分支上加一个commit new。在repo B中git fetch,<br>git checkout origin/test, git log, 会发现现在repo B的远程分支origin/test<br>有了repo A test分支上的commit new</p>
<p>在repo B的test分支上，git merge orgin/test，即可把git fetch得到的repo B<br>origin/test分支和test合并。这也就是常说的git pull = git fetch + git merge</p>
<p>可以看出repo B在本地是有origin/master, origin/test的远程分支的完整拷贝，也有<br>本地分支master, test。在git fetch操作时，只是把repo A上的新提交加到repo B的<br>origin/test“分支”上。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                    git clone</span><br><span class="line">repo A: A--&gt;B--&gt;C   ========&gt;   repo B: A--&gt;B--&gt;C </span><br><span class="line">                \                                    \</span><br><span class="line">               master                           master(也是origin/master)</span><br><span class="line"></span><br><span class="line">                     new commit</span><br><span class="line">                    /  </span><br><span class="line">repo A: A--&gt;B--&gt;C--&gt;D           repo B: A--&gt;B--&gt;C </span><br><span class="line">                    \                            \</span><br><span class="line">                     master                       master(也是origin/master)</span><br><span class="line"></span><br><span class="line">                     new commit        git fetch    origin/master</span><br><span class="line">                    /                               / </span><br><span class="line">repo A: A--&gt;B--&gt;C--&gt;D           repo B: A--&gt;B--&gt;C--&gt;D</span><br><span class="line">                    \                            \</span><br><span class="line">                     master                      master</span><br><span class="line"></span><br><span class="line">                     new commit       git merge     origin/master</span><br><span class="line">                    /                               / </span><br><span class="line">repo A: A--&gt;B--&gt;C--&gt;D           repo B: A--&gt;B--&gt;C--&gt;D</span><br><span class="line">                    \                                \</span><br><span class="line">                         master                                master</span><br><span class="line"></span><br><span class="line">如果在第三步中在repo B的master分支上又作了几次提交,比如:</span><br><span class="line">                     new commit                     origin/master</span><br><span class="line">                    /                               / </span><br><span class="line">repo A: A--&gt;B--&gt;C--&gt;D           repo B: A--&gt;B--&gt;C--&gt;D</span><br><span class="line">                    \                            \</span><br><span class="line">                    master                        --&gt;E--&gt;F  master</span><br><span class="line">                   </span><br><span class="line">那么在git merge会如下, master分支中会加入E, F两个提交</span><br><span class="line">                     new commit        git merge    origin/master</span><br><span class="line">                    /                               / </span><br><span class="line">repo A: A--&gt;B--&gt;C--&gt;D           repo B: A--&gt;B--&gt;C--&gt;D-------</span><br><span class="line">                    \                            \          \</span><br><span class="line">                    master                        --&gt;E--&gt;F--&gt;G  master</span><br></pre></td></tr></table></figure></li>
<li><p>git branch显示本地分支，git branch -r显示远程分支，<br>git checkout orgin/test -b local_branch 建立一个本地分支local_branch跟踪<br>远程分支</p>
</li>
<li><p>在repo B .git中的config文件中有这样的配置条目:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[remote &quot;origin&quot;]</span><br><span class="line"> url = /home/example/git_test_client/../git_test</span><br><span class="line"> fetch = +refs/heads/*:refs/remotes/origin/*</span><br><span class="line">[branch &quot;master&quot;]</span><br><span class="line"> remote = origin</span><br><span class="line"> merge = refs/heads/master</span><br><span class="line">[branch &quot;test_client&quot;]</span><br><span class="line"> remote = origin</span><br><span class="line"> merge = refs/heads/test</span><br></pre></td></tr></table></figure>
<p> 其中第一条[remote “origin”], url表示远程仓库的url, fetch表示做git fetch<br> 的时候远程仓库中的各个分支，对应本地仓库中的refs/remotes/orgin/下的各个“分支”。<br> 本地仓库的origin/master等严格的讲并不是一个分支，使用git checkout origin/master<br> 会显示处于头指针分离状态。<br> 后面的[branch “master”]条目表示，当时候git pull时，会把git fetch得到的orgin/master<br> merge到本地的master分支中。</p>
</li>
</ol>
<h2 id="给branch添加描述信息"><a href="#给branch添加描述信息" class="headerlink" title="给branch添加描述信息"></a>给branch添加描述信息</h2><p>   git branch –edit-description 可以添加当前git分支的说明，说明文字被添加到.git/config<br>   所以可以用git config -l 查看当前分支的说明，如果有说明的话。</p>
<h2 id="一些开发中有用的小技巧"><a href="#一些开发中有用的小技巧" class="headerlink" title="一些开发中有用的小技巧"></a>一些开发中有用的小技巧</h2><ol>
<li><p>已经知道了可以下载代码的git服务器的地址，比如：git://git.linaro.org/kernel.git<br>可以使用：</p>
<pre><code>git clone git://git.linaro.org/kernel.git
</code></pre>
<p>下在代码</p>
<p>要是你的本地电脑上已经有了之前clone的一个kernel的git仓库，可以使用：</p>
<pre><code>git clone git://git.linaro.org/kernel.git --reference /path/to/kernel.git
</code></pre>
<p>提高下载的速度，新的下载的git库将会重用以前已有的git库</p>
<p>下载好git库后，可以使用：</p>
<pre><code>git branch -r 
</code></pre>
<p>显示所用的远程分支, 然后用：</p>
<pre><code>git checkout branchname
</code></pre>
<p>提取出需要的分支，然后用；</p>
<pre><code>git branch
</code></pre>
<p>就可以看到上面提出来的分支的名字了</p>
</li>
<li><p>显示远程仓库的网址和名字：</p>
<pre><code>git remote -v
</code></pre>
<p>修改远程仓库，发现远程仓库的地址错了，导致一直下载不下来代码，需要添加正确<br>的地址：</p>
<pre><code>git remote add hilt ssh://git.linaro.org/kernel.git
</code></pre>
<p>其中hilt是这个远程仓库的名字</p>
<pre><code>git remote rm origin
</code></pre>
<p>其中orgin是原来错误远程仓库的名字，之后把远程仓库的名字从hilt改成origin：</p>
<pre><code>git remote rename hilt origin
</code></pre>
</li>
<li><p>在开发的时候会出现很多中间版本，这些版本做的改动对别人是无意义的, 比如有<br>version_1—&gt;version_2—&gt;version_3, 怎么把这三个版本合并成一个版本(一次提交):</p>
<pre><code>git rebase -i HEAD~3
</code></pre>
<p>其中3表示把最近的3次提交合并成一次提交</p>
<p>如果commit的log message写的不好，也可以用：</p>
<pre><code>git commit --amend
</code></pre>
<p>重写commit的log message</p>
</li>
<li><p>代码改好了，需要制作patch，可以使用：</p>
<pre><code>git format-patch -s -1
</code></pre>
<p>其中s表示patch中会加上签名项, 1表示对最近一次提交生成patch. 如果把1变成2，那么<br>会生成两个patch, 以version_1—&gt;version_2—&gt;version_3为例，这两个patch是<br>version_3对version_2的patch、version_2对version_1的patch</p>
</li>
</ol>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>软件开发</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>hisi perf uncore event</title>
    <url>/2021/06/19/hisi-perf-uncore-event/</url>
    <content><![CDATA[<p>你可以使用 perf list来列出系统支持的perf事件，有一类perf事件可以用来统计CPU的<br>L3 cache, HHA和DDRC的事件，他们统一叫uncore事件。他们对应的内核驱动是在<br>linux/drivers/perf/hisilicon/*。这些uncore event的命名是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hisi_sccl1_ddrc0/act_cmd/                          [Kernel PMU event]</span><br><span class="line">hisi_sccl1_ddrc0/flux_rcmd/                        [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_ddrc1/act_cmd/                          [Kernel PMU event]</span><br><span class="line">hisi_sccl1_ddrc1/flux_rcmd/                        [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_ddrc2/act_cmd/                          [Kernel PMU event]</span><br><span class="line">hisi_sccl1_ddrc2/flux_rcmd/                        [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_ddrc3/act_cmd/                          [Kernel PMU event]</span><br><span class="line">hisi_sccl1_ddrc3/flux_rcmd/                        [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_hha2/bi_num/                            [Kernel PMU event]</span><br><span class="line">hisi_sccl1_hha2/edir-hit/                          [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_hha3/bi_num/                            [Kernel PMU event]</span><br><span class="line">hisi_sccl1_hha3/edir-hit/                          [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_l3c10/back_invalid/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/prefetch_drop/                    [Kernel PMU event]</span><br><span class="line">[...]</span><br><span class="line">hisi_sccl1_l3c11/back_invalid/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c11/prefetch_drop/                    [Kernel PMU event]</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>

<p>我们在做性能分析的时候，首先要看懂这些统计，把这些项目和程序运行的CPU对应上。<br>现在依次介绍相关的概念。一个完整的服务器CPU系统(我们这里不看IO)，是由物理CPU，<br>物理CPU中的CPU die, CPU die上的一个个CPU core组成的。一般，物理CPU支持多个互联<br>在一起，我们下面用chip表示一个物理CPU, 一个物理CPU里可以有多个CPU die, 在uncore<br>event里CPU die我们叫做sccl<n>, 这里的n是sccl的编号，其中chip0(主片)上的CPU die<br>分别叫sccl1和sccl3, chip1(从片)上的叫sccl5、sccl7，注意我们这里举例的系统一个<br>物理CPU里有两个CPU die。</p>
<p>一个sccl中的CPU core是四个聚集在一起成一个cluster，一般一个sccl里有6个cluster,<br>那么一个sccl就有24个core，一个物理CPU有48个core, 一个2P系统就有96个core。这些core<br>和DDR的连接如下图, 他们通过HHA和DDRC连接，DDRC和DIM条连接。sccl之间通过HHA相连。<br>这些core和L3 cache的连接关系(这里先不考虑L1, L2 cache)是一个sccl和一大块L3相连，<br>在使用上，把这一大块L3 cache分成几个partition, 一般是有几个cluster就分几个partion,<br>一个cluster里的core优先使用自己cluster对应的L3 partition，当然也可以使用其他的<br>L3 partion。</p>
<p>这样，我们很好看懂上面的event，比如:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hisi_sccl1_l3c11/back_invalid/                     [Kernel PMU event]</span><br></pre></td></tr></table></figure>
<p>就表示，chip0上sccl1这个CPU die上L3 partition编号是11的back_invalid事件。一般，<br>一个sccl对应一个NUMA node节点，一个l3c后面的编号在一个sccl上是顺序增加的。<br>比如，sccl1上的的各个l3c的编号是，l3c10, l3cll, l3c12, l3c13, l3c14, l3c15，那么<br>l3c11对应的就是这个sccl1上的core4~core7。一般，sccl1对应的就是系统里的node0,<br>sccl3对应node1，sccl5对应node2，sccl7对应node3。</p>
<p>使用numactl -H可以确定各个NUMA node里的CPU编号，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">available: 4 nodes (0-3)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31</span><br><span class="line">node 0 size: 0 MB</span><br><span class="line">node 0 free: 0 MB</span><br><span class="line">node 1 cpus: 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63</span><br><span class="line">node 1 size: 31912 MB</span><br><span class="line">node 1 free: 30275 MB</span><br><span class="line">node 2 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95</span><br><span class="line">node 2 size: 0 MB</span><br><span class="line">node 2 free: 0 MB</span><br><span class="line">node 3 cpus: 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127</span><br><span class="line">node 3 size: 32097 MB</span><br><span class="line">node 3 free: 31514 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0   1   2   3 </span><br><span class="line">  0:  10  12  20  22 </span><br><span class="line">  1:  12  10  22  24 </span><br><span class="line">  2:  20  22  10  12 </span><br><span class="line">  3:  22  24  12  10 </span><br></pre></td></tr></table></figure>
<p>注意这个系统是128core的系统，无非是系统拓扑基本上不变，一个sccl里多了2个cluster。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                              sccl1      sccl3                 sccl5         sccl7</span><br><span class="line">    DIM0 DIM1  DIM2 DIM3        /         /</span><br><span class="line">      |  |       |  |          /  ...    /      chip0                               chip1</span><br><span class="line">+-----+--+-------+--+---------/---------/------------+   +-------------------------------+</span><br><span class="line">| +---+--+-------+--+--------/-+ +-----/-----------+ |   |                               |</span><br><span class="line">| | +-----+    +-----+      /  | |    /            | |   |                               |</span><br><span class="line">| | |DDRC0|    |DDRC1|     /   | |   /             | |   |                               |</span><br><span class="line">| | +---+-+    +-+---+    /    | |  /              | |   |                               |</span><br><span class="line">| |     |        |             | |                 | |   |                               |</span><br><span class="line">| |     +---+ +--+             | |                 | |   |                               |</span><br><span class="line">| |         | |                | |  ...            | |   |    ...                        |</span><br><span class="line">| |      +--+-+-+              | |                 | |   |                               |</span><br><span class="line">| |      | HHA0 +--------------+-+--               | |   |                               |</span><br><span class="line">| |      +--+---+              | |                 | |   |                               |</span><br><span class="line">| |         |         +------+ | |                 | |   |                               |</span><br><span class="line">| |   +-----+-----+   |      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |   |core0|core1|   |l3c&lt;n&gt;| | | |core0|core1|   | |   | |core0|core1|   |core0|core1| |</span><br><span class="line">| |   +-----+-----+---+      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |   |core2|core3|   +------+ | | |core2|core3|   | |   | |core2|core3|   |core2|core3| |</span><br><span class="line">| |   +-----+-----+   |      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |                   |...   | | |                 | |   |                               |</span><br><span class="line">| |   ...             |      | | | ...             | |   | ...             ...           |</span><br><span class="line">| |                   |      | | |                 | |   |                               |</span><br><span class="line">| |   +-----+-----+   |      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |   |core0|core1|   +------+ | | |core0|core1|   | |   | |core0|core1|   |core0|core1| |</span><br><span class="line">| |   +-----+-----+---+      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |   |core2|core3|   |l3c&lt;n&gt;| | | |core2|core3|   | |   | |core2|core3|   |core2|core3| |</span><br><span class="line">| |   +-----+-----+   |      | | | +-----+-----+   | |   | +-----+-----+   +-----+-----+ |</span><br><span class="line">| |         |         +------+ | |                 | |   |                               |</span><br><span class="line">| |      +--+---+              | |                 | |   |                               |</span><br><span class="line">| |      | HHA1 +--------------+-+--               | |   |                               |</span><br><span class="line">| |      +--+-+-+              | |  ...            | |   |    ...                        |</span><br><span class="line">| |         | |                | |                 | |   |                               |</span><br><span class="line">| |     +---+ +--+             | |                 | |   |                               |</span><br><span class="line">| |     |        |             | |                 | |   |                               |</span><br><span class="line">| | +---+-+   +--+--+          | |                 | |   |                               |</span><br><span class="line">| | |DDRC2|   |DDRC3|          | |                 | |   |                               |</span><br><span class="line">| | +-----+   +-----+          | |                 | |   |                               |</span><br><span class="line">| +--+--+-------+--+-----------+ +-----------------+ |   |                               |</span><br><span class="line">+----+--+-------+--+---------------------------------+   +-------------------------------+</span><br><span class="line">     |  |       |  |               ...                     ...</span><br><span class="line">   DIM0 DIM1  DIM2 DIM3</span><br></pre></td></tr></table></figure>
<p>下面我们先看下L3 cache的各个event的含义：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hisi_sccl1_l3c10/back_invalid/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/prefetch_drop/                    [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/rd_cpipe/                         [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/rd_hit_cpipe/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/rd_hit_spipe/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/rd_spipe/                         [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/retry_cpu/                        [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/retry_ring/                       [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/victim_num/                       [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/wr_cpipe/                         [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/wr_hit_cpipe/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/wr_hit_spipe/                     [Kernel PMU event]</span><br><span class="line">hisi_sccl1_l3c10/wr_spipe/                         [Kernel PMU event]</span><br></pre></td></tr></table></figure>
<p>  rd_cpipe, rd_spipe可以表示CPU发出的所有请求数。<br>  rd_hit_cpipe, rd_hit_spipe可以表示CPU发出的请求hit该L3 partition的数目。</p>
]]></content>
      <tags>
        <tag>软件性能</tag>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>how to test crypto accelerator engine</title>
    <url>/2021/07/05/how-to-test-crypto-accelerator-engine/</url>
    <content><![CDATA[<p>简单的讲，就是在你写的crypto驱动注册到crypto子系统的时候，注册函数会调用crypto_chain<br>注册链表上的回调函数。</p>
<p>如果使能了crypto/algboss.c这个驱动，这个驱动初始化的时候会向crypto_chain里注册相关<br>的测试代码, 这些测试代码会启动一个内核线程去测试刚刚注册的crypto算法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cryptomgr_schedule_test</span><br><span class="line">	--&gt; kthread_run(cryptomgr_test, param, &quot;cryptomgr_test&quot;)</span><br><span class="line"></span><br><span class="line">static int cryptomgr_test(void *data)</span><br><span class="line">&#123;</span><br><span class="line">	struct crypto_test_param *param = data;</span><br><span class="line">	u32 type = param-&gt;type;</span><br><span class="line">	int err = 0;</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_CRYPTO_MANAGER_DISABLE_TESTS</span><br><span class="line">	goto skiptest;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">	if (type &amp; CRYPTO_ALG_TESTED)</span><br><span class="line">		goto skiptest;</span><br><span class="line"></span><br><span class="line">	err = alg_test(param-&gt;driver, param-&gt;alg, type, CRYPTO_ALG_TESTED);</span><br><span class="line"></span><br><span class="line">skiptest:</span><br><span class="line">	crypto_alg_tested(param-&gt;driver, err);</span><br><span class="line"></span><br><span class="line">	kfree(param);</span><br><span class="line">	module_put_and_exit(0);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到要想要执行到测试程序alg_test, 需要把CONFIG_CRYPTO_MANAGER_DISABLE_TESTS<br>设置成n.</p>
<p>alg_test(crypto/testmgr.c)会根据算法的名字调用事先准备好的测试代码, 可以搜索<br>struct alg_test_desc alg_test_descs<a href="crypto/testmgr.c"></a>这个数组找到所有事先放好<br>的测试代码。</p>
<p>比如，我们可以找到deflate压缩解压缩相关的测试代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">	.alg = &quot;deflate&quot;,</span><br><span class="line">	.test = alg_test_comp,</span><br><span class="line">	.fips_allowed = 1,</span><br><span class="line">	.suite = &#123;</span><br><span class="line">		.comp = &#123;</span><br><span class="line">			.comp = __VECS(deflate_comp_tv_template),</span><br><span class="line">			.decomp = __VECS(deflate_decomp_tv_template)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>可以看到，alg_test_comp函数会调用crypto子系统提供的API做相应算法的测试，比如，<br>这里会用crypto_alloc_comp, crypto_comp_compress, crypto_free_comp(comp)<br>等crypto API对新注册的支持deflate算法的驱动做测试(假设你写的crypto算法的驱动支持<br>deflate算法), 当然测试使用的压缩解压缩的数据就是上面deflate_comp_tv_template,<br>deflate_decomp_tv_template中的数据。</p>
<p>但是，这里对于压缩解压缩算法似乎存在一个问题, 那就硬件加速器压缩解压缩得到的<br>数据和软件压缩解压缩可能是不一样的。也就是说，你为硬件压缩解压缩写一个crypto的<br>驱动，注册在crypto系统上，然后上面crypto自带的测试程序测试，由于硬件压缩，软件<br>压缩出来的结果是不一样的，可能你的硬件压缩的结果是对的，但是上面的测试是失败的。</p>
<p>对于压缩解压这个问题，可以通过用软件把硬件压缩完的数据解压一下，然后对比是否和<br>原来的压缩前的数据一致。具体实现的话，可以直接用crypto API申请一个基于软件的压缩<br>解压算法进行验证。也可以用内核lib/zlib_deflate, lib/zlib_inflate库来进行软件的<br>压缩和解压缩。对于第一种验证方法，我们可以修改内核的alg_test_comp函数来实现，<br>这个修改可以考虑upstream到内核主线。对于第二种验证方法，我们可以在硬件的crypto<br>驱动中直接调用zlib_deflate/zlib_inflate相关函数来做，基本的使用方法是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct z_stream_s stream;</span><br><span class="line">char decomp_result[COMP_BUF_SIZE];</span><br><span class="line"></span><br><span class="line">stream.workspace  = kmalloc(zlib_inflate_workspacesize(), GFP_KERNEL);</span><br><span class="line"></span><br><span class="line">ret = zlib_inflateInit2(&amp;stream, MAX_WINDOW_BIT);</span><br><span class="line"></span><br><span class="line">stream.next_in = dst;</span><br><span class="line">stream.avail_in = dlen;</span><br><span class="line">stream.total_in = 0;</span><br><span class="line">stream.next_out = decomp_result;</span><br><span class="line">stream.avail_out = COMP_BUF_SIZE;</span><br><span class="line">stream.total_out = 0;</span><br><span class="line"></span><br><span class="line">ret = zlib_inflate(&amp;stream, Z_FINISH);</span><br><span class="line"></span><br><span class="line">memcmp(decomp_result, src, slen);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件测试</tag>
        <tag>Linux内核</tag>
        <tag>crypto</tag>
      </tags>
  </entry>
  <entry>
    <title>linux O_CLOEXEC标志位笔记</title>
    <url>/2021/06/27/linux-O-CLOEXEC%E6%A0%87%E5%BF%97%E4%BD%8D%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>在linux系统中，open一个文件可以带上O_CLOEXEC标志位，这个表示位和用fcntl设置的<br>FD_CLOEXEC有同样的作用，都是在fork的子进程中用exec系列系统调用加载新的可执行<br>程序之前，关闭子进程中fork得到的fd。</p>
<p>fork之后，子进程得到父进程的完整拷贝，对于父进程已经open的文件，子进程也可以得<br>到一样的fd。内核里，子进程只是把fd对应的file指针指向父进程fd对应的struct file，<br>并且把file的引用加1。对于设置了这个标志位的fd来说，内核执行exec时，在加载二进制<br>可执行文件之前会调用filp_close关闭子进程的fd。可以看到filp_close的意图是关闭<br>子进程里的fd，但是因为父进程还持有fd的引用计数，所以这个关闭的动作只会执行诸如<br>文件fops对应的flush回调函数，并没有真正调用到fops的release回调函数把struct file<br>release掉。</p>
<p>这里有一个简单的测试程序：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL3RyZWUvbWFzdGVyL2ZvcmtfZXhlYw==">https://github.com/wangzhou/tests/tree/master/fork_exec<i class="fa fa-external-link-alt"></i></span><br>从test_log里可以看到，子进程执行execlp会触发设备驱动里的flush函数。</p>
<p>可以看到，如果这里的flush函数有对硬件的操作将有可能出错。比如这个设备正被<br>open在父进程里使用，子进程里调用到的flush函数也会操作到相同的struct file，可能<br>破坏这个正在打开的设备里的资源。</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title>how to test RoCE driver</title>
    <url>/2021/05/22/how-to-test-RoCE-driver/</url>
    <content><![CDATA[<h2 id="how-to-test-RoCE-driver"><a href="#how-to-test-RoCE-driver" class="headerlink" title="how to test RoCE driver"></a>how to test RoCE driver</h2><p>-v0.1 2018..27 Sherlock init</p>
<p>This document shares how to do a sanity test for a RoCE driver. If you are a<br>new guy for RoCE, it is for you.</p>
<p>Here is what I had done to test RoCE driver:</p>
<ol>
<li><p>I had a ARM64 based machine installed a Redhat system in it.</p>
</li>
<li><p>download perftest, this is a open source RoCE test cases:</p>
<p> git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xzZ3VudGgvcGVyZnRlc3QuZ2l0">https://github.com/lsgunth/perftest.git<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>download RoCE’s user space library, rdma-core:</p>
<p> git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xpbnV4LXJkbWEvcmRtYS1jb3JlLmdpdA==">https://github.com/linux-rdma/rdma-core.git<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>download kernel which includes the RoCE driver to test.</p>
</li>
<li><p>build perftest tools followed by the README.</p>
<p>Here in a Redhat(CentOS) system, when you did ./configure, it maybe show<br>that you lack ibverbs head files. you can install rdma-core-devel to solve it:</p>
<p> yum install rdma-core-devel.aarch64</p>
<p>if it is successful, you will get ib_send_bw …. tools in perftest.</p>
</li>
<li><p>build rdma-core followed by its README. In a Redhat(CentOS) system, maybe you<br>need:</p>
<p> yum install libnl-devel.aarch64</p>
<p>if it is successful, you will get user space library under rdma-core/build/lib</p>
</li>
<li><p>Copy the libraries under rdma-core/build/lib/* to /lib64 in your system.<br>(I also did the test in ubuntu, you should copy the libraries to /lib)</p>
</li>
<li><p>build your kernel together with your RoCE drivers.<br>In my case, RoCE driver will be built into some kernel modules.</p>
<p>install modules: make modules_install<br>install kernel image: make install<br>(before build kernel, you can add a localversion file to help to tell your kernel)</p>
</li>
<li><p>reboot system, then boot up using your built kernel.</p>
</li>
<li><p>in my case, RoCE kernel can not be loaded automatically, so I need to do:</p>
<p>modprobe hns-roce-hw-v2</p>
</li>
<li><p>then you can find your roce device in /sys/class/infiniband. Or use the tools<br>in rdma-core/build/bin: ibv_devices, ibv_devinfo to see your RoCE devices.</p>
</li>
<li><p>up your RoCE’s networking interface, and keep its link status as: yes<br>you can find RoCE’s networking interface by searching, e.g.:</p>
<p>/sys/class/infiniband/hns_0/ports/1/gid_attrs/ndevs</p>
</li>
<li><p>in a redhat system, you need do:</p>
<p>echo “driver hns” &gt; /etc/libibverbs.d/hns.driver</p>
<p>but in a ubuntu system, you need do:</p>
<p>echo “driver hns” &gt; /usr/local/etc/libibverbs.d/hns.driver</p>
</li>
<li><p>finally you can use ib_send_bw, ib_read_bw, ib_write_bw… to do the sanity<br>test. Every time you do the test, you should create a RoCE server and create<br>a RoCE client to access and send date to the server.</p>
<p>you can do as:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost perftest]# ./ib_write_bw -n 5 -d hns_0 &amp;</span><br><span class="line">[1] 10352</span><br><span class="line">[root@localhost perftest]# </span><br><span class="line">************************************</span><br><span class="line">* Waiting for client to connect... *</span><br><span class="line">************************************</span><br><span class="line"></span><br><span class="line">[root@localhost perftest]# ./ib_write_bw -n 5 -d hns_0 192.168.2.198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">                    RDMA_Write BW Test</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line">                    RDMA_Write BW Test</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> TX depth        : 5</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> Max inline data : 0[B]</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line"> Max inline data : 0[B]</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line"> local address: LID 0000 QPN 0x0022 PSN 0x16566e RKey 0x000300 VAddr 0x00ffff93c9b000</span><br><span class="line"> local address: LID 0000 QPN 0x0023 PSN 0x1fd9e RKey 0x000400 VAddr 0x00ffffbe24d000</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> remote address: LID 0000 QPN 0x0023 PSN 0x1fd9e RKey 0x000400 VAddr 0x00ffffbe24d000</span><br><span class="line"> remote address: LID 0000 QPN 0x0022 PSN 0x16566e RKey 0x000300 VAddr 0x00ffff93c9b000</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> 65536      5                8169.77            8161.23		   0.130580</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> 65536      5                8169.77            8161.23		   0.130580</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">[1]+  Done                    ./ib_write_bw -n 5 -d hns_0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@localhost perftest]# ./ib_read_bw -n 5 -d hns_0 &amp;</span><br><span class="line">[1] 10339</span><br><span class="line">[root@localhost perftest]# ---------------------------------------------------------------------------------------</span><br><span class="line">Device not recognized to implement inline feature. Disabling it</span><br><span class="line"></span><br><span class="line">************************************</span><br><span class="line">* Waiting for client to connect... *</span><br><span class="line">************************************</span><br><span class="line"></span><br><span class="line">[root@localhost perftest]# ./ib_read_bw -n 5 -d hns_0 192.168.2.198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">Device not recognized to implement inline feature. Disabling it</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">                    RDMA_Read BW Test</span><br><span class="line">                    RDMA_Read BW Test</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> TX depth        : 5</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> Outstand reads  : 128</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line"> Outstand reads  : 128</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line"> local address: LID 0000 QPN 0x0020 PSN 0xe6d890 OUT 0x80 RKey 0x000300 VAddr 0x00ffffb0250000</span><br><span class="line"> local address: LID 0000 QPN 0x0021 PSN 0x87bda4 OUT 0x80 RKey 0x000400 VAddr 0x00ffff9dbe8000</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> remote address: LID 0000 QPN 0x0020 PSN 0xe6d890 OUT 0x80 RKey 0x000300 VAddr 0x00ffffb0250000</span><br><span class="line"> remote address: LID 0000 QPN 0x0021 PSN 0x87bda4 OUT 0x80 RKey 0x000400 VAddr 0x00ffff9dbe8000</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> 65536      5                8002.35            8002.35		   0.128038</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> 65536      5                8002.35            8002.35		   0.128038</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">[1]+  Done                    ./ib_read_bw -n 5 -d hns_0</span><br><span class="line"></span><br><span class="line">[root@localhost perftest]# ./ib_send_bw -n 5 -d hns_0 &amp;</span><br><span class="line">[1] 10300</span><br><span class="line">[root@localhost perftest]# </span><br><span class="line">************************************</span><br><span class="line">* Waiting for client to connect... *</span><br><span class="line">************************************</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@localhost perftest]# ./ib_send_bw -n 5 -d hns_0 192.168.2.198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">                    Send BW Test</span><br><span class="line">                    Send BW Test</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line"> Dual-port       : OFF		Device         : hns_0</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> Number of qps   : 1		Transport type : IB</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> Connection type : RC		Using SRQ      : OFF</span><br><span class="line"> RX depth        : 6</span><br><span class="line"> TX depth        : 5</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> CQ Moderation   : 5</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> Mtu             : 1024[B]</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> Link type       : Ethernet</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> Gid index       : 0</span><br><span class="line"> Max inline data : 0[B]</span><br><span class="line"> Max inline data : 0[B]</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line"> rdma_cm QPs	 : OFF</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line"> Data ex. method : Ethernet</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line">hr_qp-&gt;port_num= 0x1</span><br><span class="line"> local address: LID 0000 QPN 0x001e PSN 0xab3cfa</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> local address: LID 0000 QPN 0x001f PSN 0xdefae7</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> remote address: LID 0000 QPN 0x001f PSN 0xdefae7</span><br><span class="line"> remote address: LID 0000 QPN 0x001e PSN 0xab3cfa</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:198</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]</span><br><span class="line"> 65536      5                8054.00            8051.92		   0.128831</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line"> 65536      5                0.00               15340.76		   0.245452</span><br><span class="line">---------------------------------------------------------------------------------------</span><br><span class="line">[1]+  Done                    ./ib_send_bw -n 5 -d hns_0</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <tags>
        <tag>RDMA</tag>
      </tags>
  </entry>
  <entry>
    <title>mtrace使用笔记</title>
    <url>/2021/07/17/mtrace%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="使用过程："><a href="#使用过程：" class="headerlink" title="使用过程："></a>使用过程：</h1><ol>
<li>程序中需要包含头文件mchech.h, 在程序开始处调用mtrace()</li>
<li>设定环境变量 export MALLOC_TRACE=”mtrace.out”</li>
<li>编译运行程序, 会生成mtrace.out文件</li>
<li>mtrace a.out mtrace.out得到内存泄露信息</li>
</ol>
<h2 id="Memory-not-freed"><a href="#Memory-not-freed" class="headerlink" title="Memory not freed:"></a>Memory not freed:</h2><p>Address     Size     Caller<br>0x0000000001650490     0x28  at /vm/<em><strong>/src/mtrace_test/mtrace_test.c:11<br>0x00000000016504f0     0x28  at /vm/</strong></em>/src/mtrace_test/mtrace_test.c:13<br>0x0000000001650550      0xa  at /vm/***/src/mtrace_test/mtrace_test.c:15</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* mtrace_test.c */</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;mcheck.h&gt;</span><br><span class="line">#</span><br><span class="line">int main(void)</span><br><span class="line">&#123;</span><br><span class="line">	mtrace(); </span><br><span class="line">	</span><br><span class="line">	int i;</span><br><span class="line">	int *p_0 = (int*)malloc(sizeof(int)*10);</span><br><span class="line">	int *p_1 = (int*)malloc(sizeof(int)*10);</span><br><span class="line">	int *p_2 = (int*)malloc(sizeof(int)*10);</span><br><span class="line">	int *p_3 = (int*)malloc(sizeof(int)*10);</span><br><span class="line">	int *p_4 = (int*)malloc(sizeof(int)*10);</span><br><span class="line">	char *p_char = (char*)malloc(sizeof(char)*10);</span><br><span class="line">	</span><br><span class="line">	for (i = 0; i &lt; 10; i++) &#123;</span><br><span class="line">		p_0[i] = i;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	for (i = 0; i &lt; 10; i++) &#123;</span><br><span class="line">		p_char[i] = &#x27;w&#x27;;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	free(p_0);</span><br><span class="line">	/* 制造人为的内存泄漏 */</span><br><span class="line">	//free(p_1)</span><br><span class="line">	free(p_2);</span><br><span class="line">	//free(p_3)</span><br><span class="line">	free(p_4);</span><br><span class="line">	//free(p_char);</span><br><span class="line">	</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><p>[1] <span class="exturl" data-url="aHR0cDovL3d3dy5nbnUub3JnL3NvZnR3YXJlL2xpYmMvbWFudWFsL2h0bWxfbm9kZS9BbGxvY2F0aW9uLURlYnVnZ2luZy5odG1s">http://www.gnu.org/software/libc/manual/html_node/Allocation-Debugging.html<i class="fa fa-external-link-alt"></i></span><br>[2] <span class="exturl" data-url="aHR0cDovL3d3dy5nbnUub3JnL3NvZnR3YXJlL2xpYmMvbWFudWFsL2h0bWxfbm9kZS9Ib29rcy1mb3ItTWFsbG9jLmh0bWw=">http://www.gnu.org/software/libc/manual/html_node/Hooks-for-Malloc.html<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>软件调试</tag>
      </tags>
  </entry>
  <entry>
    <title>pci设备直通qemu相关的RAS处理</title>
    <url>/2021/06/28/pci%E8%AE%BE%E5%A4%87%E7%9B%B4%E9%80%9Aqemu%E7%9B%B8%E5%85%B3%E7%9A%84RAS%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p>v5.0-rc6里pci aer的处理逻辑是，当pf有aer时会扫描pf父总线下的所有function, 并<br>调用对应function的pci_driver-&gt;err_handler-&gt;err_detected/slot_reset函数<br>(如果有这些回调，这里不展开描述)。</p>
<p>基于上面的逻辑，如果正好有vf通过vfio驱动直通到qemu, 那么vfio-pci驱动里的<br>pci_driver-&gt;err_handler-&gt;err_detected(具体函数是: vfio_pci_aer_err_detected)<br>将被调用到。vfio中的err_detected函数向qemu发一个eventfd消息。qemu收到该消息的<br>时候会终止qemu进程(abort)</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>QEMU</tag>
        <tag>RAS</tag>
      </tags>
  </entry>
  <entry>
    <title>linux kernel vfio mdev arch</title>
    <url>/2021/07/05/linux-kernel-vfio-mdev-arch/</url>
    <content><![CDATA[<p>Linux kernel adds a vfio mdev system in /driver/vfio/mdev. This sub-system can<br>create virtual devices and export their DMA outside to user space.</p>
<p>This subsystem creates one new bus called: struct bus_type mdev_bus_type.<br>And it uses a gobal list to store parent device, here we call orginal device as<br>parent device and virtual device created from orginal device as mdev device,<br>e.g. a NIC is a parent device and one of NIC’s queue can be a mdev device to<br>fullfil a seperated package sending/receiving work.</p>
<p>vfio mdev exports a API:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int mdev_register_device(struct device *dev, const struct mdev_parent_ops *ops)</span><br></pre></td></tr></table></figure>
<p>to let a device register to itself.</p>
<p>And vfio mdev exports a set of sysfs files to help user to create a mdev device,<br>here we call a mdev device as child device.</p>
<p>A child device will be added into mdev_bus_type, and a driver which belongs to<br>the mdev_bus_type in vfio_mdev.c will bind with related child device.</p>
<p>The probe of this vfio_mdev is very simple, it just call:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vfio_add_group_dev(dev, &amp;vfio_mdev_dev_ops, mdev)</span><br></pre></td></tr></table></figure>
<p>It just create a vfio_group and related vfio_device. So the vfio mdev will reuse<br>all the concept of vfio system.</p>
<p>The probe of mdev_bus_type will help to create a new iommu_group for child<br>device. Above vfio_add_group_dev creates iommu_group’s vfio_group and add this<br>vfio_group to vfio’s group list.</p>
<p>If user want to use above vfio_group together with a vfio container, it should<br>attach this vfio_group to related vfio container. When doing this attach<br>operation. vfio system will replay all mappings in vfio container to this<br>vfio group, which means this child’s DMA can see the address space managed by<br>vfio container.</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>vfio</tag>
      </tags>
  </entry>
  <entry>
    <title>pstack使用笔记</title>
    <url>/2021/07/17/pstack%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>使用方法：pstack pid, 就是说要先把程序跑起来，查见进程号，再使用pstack pid</p>
<ol>
<li><p>在ubunbu系统上，使用sudo apt-get install pstack可以直接安装pstack，注意这里<br>   安装上的是二进制的程序。之前对这样安装上的pstack作了测试，无法显示函数的调用<br>   关系。根据man pstack的说明，现在的pstack只支持32bit ELF binaries</p>
</li>
<li><p>网上的一些文章指出，pstack只是一个脚本程序，在相关网站上下载了pstack.sh脚本<br>   测试。使用时总是提示出错，但是表现看不出来错误，全部删去，就留下前几行，还是<br>   显示有错，新建一个脚本文件，照抄那几行过来（这时两个文件看起来一样），但是用<br>   diff ***.sh ***.sh测试一下，竟然不一样。用ghex查看二进制的文件，看出是字符行<br>   尾的时候的编码不一样。想到下载的脚本可以是在windows下的脚本，下载安装 dos2unix<br>编码转换工具，然后dos2unix pstack.sh, 工具可以使用。</p>
<p>pstack.sh脚本简单分析:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* pstack.sh */</span><br><span class="line">#! /bin/sh</span><br><span class="line"># 输入参数以及pid是否存在的判断</span><br><span class="line">if test $# -ne 1; then</span><br><span class="line">    echo &quot;Usage: `basename $0 .sh` &lt;process-id&gt;&quot; 1&gt;&amp;2</span><br><span class="line">    exit 1 </span><br><span class="line">fi</span><br><span class="line">if test ! -r /proc/$1; then</span><br><span class="line">    echo &quot;Process $1 not found.&quot; 1&gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># GDB doesn&#x27;t allow &quot;thread apply all bt&quot; when the process isn&#x27;t</span><br><span class="line"># threaded; need to peek at the process to determine if that or the</span><br><span class="line"># simpler &quot;bt&quot; should be used.</span><br><span class="line"># 先设置参数bt, 要是多线程的情况，那么设置为：thread apply all bt</span><br><span class="line">backtrace=&quot;bt&quot;</span><br><span class="line">if test -d /proc/$1/task ; then</span><br><span class="line">    # Newer kernel; has a task/ directory.</span><br><span class="line">    if test `/bin/ls /proc/$1/task | /usr/bin/wc -l` -gt 1 2&gt;/dev/null ; then</span><br><span class="line">        backtrace=&quot;thread apply all bt&quot;</span><br><span class="line">    fi</span><br><span class="line">elif test -f /proc/$1/maps ; then</span><br><span class="line">    # Older kernel; go by it loading libpthread.</span><br><span class="line">    if /bin/grep -e libpthread /proc/$1/maps &gt; /dev/null 2&gt;&amp;1 ; then</span><br><span class="line">        backtrace=&quot;thread apply all bt&quot;</span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line"> </span><br><span class="line">GDB=$&#123;GDB:-/usr/bin/gdb&#125;</span><br><span class="line"></span><br><span class="line"># echo $GDB -&gt; /usr/bin/gdb</span><br><span class="line"></span><br><span class="line"># -nx: Do not execute commands from any `.gdbinit&#x27; initialization files.  Normal</span><br><span class="line">   ly, the commands in these files are executed after all the command options  </span><br><span class="line">   and  argu‐ments have been processed.</span><br><span class="line"># --quiet: Do not print the introductory and copyright messages.  These message</span><br><span class="line">   s are also suppressed in batch mode</span><br><span class="line"># --batch: ...</span><br><span class="line"># --readnever: ...</span><br><span class="line">if $GDB -nx --quiet --batch --readnever &gt; /dev/null 2&gt;&amp;1; then</span><br><span class="line">    readnever=--readnever</span><br><span class="line">else</span><br><span class="line">    readnever=</span><br><span class="line">fi</span><br><span class="line"># 单步运行/usr/bin/gdb -nx --quiet --batch --readnever时,--readnever会出错</span><br><span class="line"> </span><br><span class="line"># Run GDB, strip out unwanted noise.</span><br><span class="line"># 原来用/proc/$1/exe找到pid对应的命令行，但是发现下面命令中的是/proc/$1/exe</span><br><span class="line"># 改成下面的&#x27;echo /proc/$1/exe&#x27;就可以了</span><br><span class="line">$GDB --quiet $readnever -nx &#x27;echo /proc/$1/exe&#x27; $1 &lt;&lt;EOF 2&gt;&amp;1 |</span><br><span class="line">$backtrace </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li>
</ol>
<p>这里，运行pstack [pid]一次会显示当时的函数调用，不能显示指定位置的堆栈情况。<br>可以先运行gdb –quiet -nx /proc/command/exe command, 待gdb起来后，在想要的地<br>方设立断点，程序停住之后，使用bt，显示出当时的堆栈函数调用情况<br> <br>整理显示输出格式<br>/bin/sed -n <br>    -e ‘s/^(gdb) //‘ <br>    -e ‘/^#/p’ <br>    -e ‘/^Thread/p’</p>
<p>附：测试程序和堆栈情况（单线程和多线程）</p>
<p>单线程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int func_called(int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">	int c;</span><br><span class="line">	c = a + b;</span><br><span class="line">	return c;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void func_calling(void)</span><br><span class="line">&#123;</span><br><span class="line">	int a = 1, b = 2;</span><br><span class="line">	int r;</span><br><span class="line">	r = func_called(a, b);</span><br><span class="line">	printf(&quot;r = %d\n&quot;, r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	while(1) &#123;</span><br><span class="line">		func_calling();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">29788 pts/5    00:00:34 test2</span><br><span class="line">29819 pts/4    00:00:00 ps</span><br><span class="line">***@A101107831:/vm/***/notes$ ./pstack.sh 29788</span><br><span class="line">[sudo] password for ***: </span><br><span class="line">#0  0x00007f6d7d366910 in __write_nocancel ()</span><br><span class="line">#1  0x00007f6d7d2f9883 in _IO_new_file_write (f=0x7f6d7d639260, </span><br><span class="line">#2  0x00007f6d7d2f974a in new_do_write (fp=0x7f6d7d639260, </span><br><span class="line">#3  0x00007f6d7d2faeb5 in _IO_new_do_write (fp=&lt;optimized out&gt;, </span><br><span class="line">#4  0x00007f6d7d2fa025 in _IO_new_file_xsputn (n=1, data=&lt;optimized out&gt;, </span><br><span class="line">#5  _IO_new_file_xsputn (f=0x7f6d7d639260, data=&lt;optimized out&gt;, n=1)</span><br><span class="line">#6  0x00007f6d7d2ca4a7 in _IO_vfprintf_internal (s=&lt;optimized out&gt;, </span><br><span class="line">#7  0x00007f6d7d2d38d9 in __printf (format=&lt;optimized out&gt;) at printf.c:35</span><br><span class="line">#8  0x000000000040054d in func_calling () at test2.c:15</span><br><span class="line">#9  0x0000000000400563 in main () at test2.c:27</span><br></pre></td></tr></table></figure>
<p>注：使用pc上已经在运行的应用程序调试时，无法显示具体的函数名和相关参数，原因是pc<br>    上的应用程序没有加入调试信息</p>
<p>多线程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># include&lt;stdio.h&gt;</span><br><span class="line"># include&lt;pthread.h&gt;</span><br><span class="line"># include&lt;string.h&gt;</span><br><span class="line"></span><br><span class="line">pthread_t tid1;</span><br><span class="line">pthread_t tid2;</span><br><span class="line"></span><br><span class="line">void* thread1(void* arg)</span><br><span class="line">&#123;</span><br><span class="line">	while (1) &#123;</span><br><span class="line">		printf(&quot;thread_id = %d\n&quot;, (int)tid1);</span><br><span class="line">		sleep(1);</span><br><span class="line">	&#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void* thread2(void* arg)</span><br><span class="line">&#123;</span><br><span class="line">	while (1) &#123;</span><br><span class="line">		printf(&quot;thread_id = %d\n&quot;, (int)tid2);</span><br><span class="line">		sleep(1);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	int err;</span><br><span class="line">	</span><br><span class="line">	err = pthread_create(&amp;tid1, NULL, thread1, NULL);</span><br><span class="line">	if (err != 0)&#123;</span><br><span class="line">		fprintf(stderr, &quot;can&#x27;t create thread: %s\n&quot;, strerror(err));</span><br><span class="line">	&#125;</span><br><span class="line">	err = pthread_create(&amp;tid2, NULL, thread2, NULL);</span><br><span class="line">	if (err != 0)&#123;</span><br><span class="line">		fprintf(stderr, &quot;can&#x27;t create thread: %s\n&quot;, strerror(err));</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	while(1); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>测试输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*** 30280 22661 30280 99    3 21:51 pts/5    00:00:12 ./multi_threads</span><br><span class="line">*** 30280 22661 30281  0    3 21:51 pts/5    00:00:00 ./multi_threads</span><br><span class="line">*** 30280 22661 30282  0    3 21:51 pts/5    00:00:00 ./multi_threads</span><br><span class="line">*** 30284  3758 30284  0    1 21:51 pts/4    00:00:00 ps -eLf</span><br><span class="line">***@A101107831:/vm/***/notes$ ./pstack.sh 30280</span><br><span class="line">[sudo] password for ***: </span><br><span class="line">Thread 3 (Thread 0x7f2a351ae700 (LWP 30281)):</span><br><span class="line">#0  0x00007f2a3526e84d in nanosleep () at ../sysdeps/unix/syscall-template.S:82</span><br><span class="line">#1  0x00007f2a3526e6ec in __sleep (seconds=0)</span><br><span class="line">#2  0x00000000004006fc in thread1 (arg=0x0) at multi_threads.c:14</span><br><span class="line">#3  0x00007f2a35575e9a in start_thread (arg=0x7f2a351ae700)</span><br><span class="line">#4  0x00007f2a352a2ccd in clone ()</span><br><span class="line">#5  0x0000000000000000 in ?? ()</span><br><span class="line">Thread 2 (Thread 0x7f2a349ad700 (LWP 30282)):</span><br><span class="line">#0  0x00007f2a3526e84d in nanosleep () at ../sysdeps/unix/syscall-template.S:82</span><br><span class="line">#1  0x00007f2a3526e6ec in __sleep (seconds=0)</span><br><span class="line">#2  0x0000000000400736 in thread2 (arg=0x0) at multi_threads.c:22</span><br><span class="line">#3  0x00007f2a35575e9a in start_thread (arg=0x7f2a349ad700)</span><br><span class="line">#4  0x00007f2a352a2ccd in clone ()</span><br><span class="line">#5  0x0000000000000000 in ?? ()</span><br><span class="line">Thread 1 (Thread 0x7f2a35988700 (LWP 30280)):</span><br><span class="line">#0  main () at multi_threads.c:39</span><br></pre></td></tr></table></figure>
<p>注：按照下面gdb multi-thread debug方法，在pc上无法显示有关多线程的信息(被调试进<br>    程使用pc已经在运行的应用程序进程)</p>
<p>gdb多线程debug:<br><span class="exturl" data-url="aHR0cHM6Ly9zb3VyY2V3YXJlLm9yZy9nZGIvb25saW5lZG9jcy9nZGIvVGhyZWFkcy5odG1sI1RocmVhZHM=">https://sourceware.org/gdb/onlinedocs/gdb/Threads.html#Threads<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>软件调试</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu iommu模拟思路分析</title>
    <url>/2021/08/21/qemu-iommu%E6%A8%A1%E6%8B%9F%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>smmuv3的父设备的代码里有：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">smmu_base_realize</span><br><span class="line">  +-&gt; pci_setup_iommu</span><br><span class="line">    +-&gt; bus-&gt;iommu_fn = smmu_find_add_as;</span><br><span class="line">    +-&gt; bus-&gt;iommu_opaque = SMMUState;</span><br></pre></td></tr></table></figure>
<p>把一个获取as的方法放到了PCIBus里，一般的这个方法是有IOMMU驱动定义，IOMMU驱动为<br>IOMMU管理的设备在IOMMU驱动里建立一个数据结构存储相关信息，这个信息里就有设备对应<br>的AS。通过这个方法，使用设备的bdf作为输入，后续设备可有拿到他自己对应的AS。<br>设备随后发dma的时候通过他自己的AS得到smmu里的translate函数然后做翻译。</p>
<p>我们可以看一个具体的intel e1000的虚拟网卡：qemu/hw/net/e1000.c<br>在这个设备注册的时候会调用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* qemu/hw/pci/pci.c */</span><br><span class="line">do_pci_register_device</span><br><span class="line">  +-&gt;pci_init_bus_master</span><br><span class="line">    +-&gt; AddressSpace *dma_as = pci_device_iommu_address_space</span><br><span class="line">          /* 这个iommu_fn就是上面的smmu_find_add_as */</span><br><span class="line">      +-&gt; iommu_bus-&gt;iommu_fn(PCIBus, SMMUState, devfn)</span><br><span class="line">      /* 在哪里把 dma_as存在dev-&gt;bus_master_as里的？*/</span><br></pre></td></tr></table></figure>

<p>设备模拟一个dma读写的实现：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pci_dma_read/write</span><br><span class="line">  ...</span><br><span class="line">    /* pci_get_address_space_space得到dev-&gt;bus_master_as */</span><br><span class="line">    +-&gt; dma_memory_rw(pci_get_address_space(dev), ...)</span><br></pre></td></tr></table></figure>
<p>可以看到这个函数后面的调用链里会最终调用到iommu里的translate函数，然后对翻译<br>都的地址读写。</p>
<p>对于普通的DMA，如上已经够了。但是，对于带PASID的翻译请求，现在还没有支持的逻辑。<br>我们查看具体的translate函数的入参：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IOMMUTLBEntry (*translate)(IOMMUMemoryRegion *iommu, hwaddr addr,</span><br><span class="line">                           IOMMUAccessFlags flag, int iommu_idx);</span><br></pre></td></tr></table></figure>
<p>我们可以用最后一个参数iommu_idx去传递pasid给具体的翻译函数，这个iommu_idx不能直接<br>使用，在使用前需要通过attrs_to_index这个函数把MemTxAttrs attrs翻译成iommu_idex。<br>所以，想要在当前的smmu驱动里支持pasid，就要在smmu驱动的imrc里实现attrs_to_index<br>的回调函数。如果我们的输入把attrs数值上相等映射成iommu_idx，attrs_to_index可以<br>是如下这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static int smmuv3_attrs_to_index(IOMMUMemoryRegion *iommu, MemTxAttrs attrs)</span><br><span class="line">&#123;</span><br><span class="line">    return attrs.pasid &amp; 0xffff;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应的我们给设备驱动可以提供这样的带PASID的DMA读写函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static inline int pci_dma_rw_pasid(PCIDevice *dev, dma_addr_t addr,</span><br><span class="line">                             void *buf, dma_addr_t len, DMADirection dir,</span><br><span class="line">                             uint32_t pasid)</span><br><span class="line">&#123;</span><br><span class="line">    MemTxAttrs attr = &#123; .pasid = 0xffff &amp; pasid &#125;;</span><br><span class="line"></span><br><span class="line">    return dma_memory_rw_attr(pci_get_address_space(dev), addr, buf, len,</span><br><span class="line">			      dir, attr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>qemu</tag>
        <tag>iommu</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu PCIe设备增加pasid capability</title>
    <url>/2021/07/26/qemu-PCIe%E8%AE%BE%E5%A4%87%E5%A2%9E%E5%8A%A0pasid-capability/</url>
    <content><![CDATA[<p>首先pasid cap是一个PCIe extended的cap，它的位置应该在PCIe配置空间0x100开始(包括)<br>往后的空间上。</p>
<p>在qemu的启动命令里直接加一个PCI设备，qemu把它看作的是一个PCI设备，用lspci看到的<br>配置空间只有0x0~0xff。qemu里对于PCI和PCIe设备是分开对待的，如果要接入一个PCIe设备，<br>需要先在根总线下接一个pcie_port，然后在pcie_port下在接入PCIe设备:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-device pcie-root-port,id=root_port,bus=pcie.0 \</span><br><span class="line">-device ghms_pci,bus=root_port</span><br></pre></td></tr></table></figure>

<p>为了使的接入的设备是一个PCIe设备，设备的TypeInfo中的接口应该定义成PCIe:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static InterfaceInfo ghms_pci_if[] = &#123;</span><br><span class="line">    &#123; INTERFACE_PCIE_DEVICE &#125;,</span><br><span class="line">    &#123; &#125;,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>下面就在设备class的realize函数里增加pasid cap的初始化代码，目前qemu代码(5.1.50)<br>里还没有直接可以调用的函数，我们仿照其他的cap，在hw/pci/pcie.c里给pasid加上一个<br>初始化函数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">void pcie_pasid_init(PCIDevice *dev, uint16_t offset)</span><br><span class="line">&#123;</span><br><span class="line">    pcie_add_capability(dev, PCI_EXT_CAP_ID_PASID, PCI_PASID_VER, offset,</span><br><span class="line">                        PCI_PASID_SIZEOF);</span><br><span class="line">    dev-&gt;exp.pasid_cap = offset;</span><br><span class="line"></span><br><span class="line">    /* 把pasid max bit配置成了2^4 - 1 = 15 */</span><br><span class="line">    pci_set_word(dev-&gt;config + offset + PCI_PASID_CAP, 4 &lt;&lt; 8 | 0x6);</span><br><span class="line">    pci_set_word(dev-&gt;wmask + offset + PCI_PASID_CTRL, 0);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在设备class的realize函数里调用如上函数加上pasid cap:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#define GHMS_PCI_EXPRESS_CAP_OFFSET         0xe0</span><br><span class="line">#define GHMS_PCI_PASID_CAP_OFFSET           0x100</span><br><span class="line">pcie_endpoint_cap_init(pdev, GHMS_PCI_EXPRESS_CAP_OFFSET);</span><br><span class="line">pcie_pasid_init(pdev, GHMS_PCI_PASID_CAP_OFFSET);</span><br></pre></td></tr></table></figure>
<p>pcie_add_capability里会用PCI_EXPRESS_CAP检测是不是PCIe设备，所以要先加上PCI_EXPRESS_CAP，<br>在0x40(0x34是capabilities pointer)到0xff这段地址选一个位置加上，如上选的是0xe0(避开<br>之前的MSI cap)。如上的函数内部会找到cap list尾，然后把新加的cap挂上去，所以我们这里<br>不需要做额外处理。</p>
]]></content>
      <tags>
        <tag>PCIe</tag>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>perf flame graph笔记</title>
    <url>/2021/06/27/perf-flame-graph%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li><p>git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2JyZW5kYW5ncmVnZy9GbGFtZUdyYXBo">https://github.com/brendangregg/FlameGraph<i class="fa fa-external-link-alt"></i></span></p>
</li>
<li><p>perf record -a -g -F 100<br>这里-g是打点的时候函数调用都算，比如a函数调用了b，b里打的点也同时算在a里。<br>-F是1秒中的打点次数，我们可以提高这个值来提高采样次数，比如这里的系统如果是<br>16核，那么1s的采样数量就是16 × 100。</p>
</li>
<li><p>perf script &gt; out.perf</p>
</li>
<li><p>cp out.perf /path_to/FlameGraph/</p>
</li>
<li><p>/path_to/FlameGraph/stackcollapse-perf.pl out.perf &gt; out.folded</p>
</li>
<li><p>/path_to/FlameGraph/flamegraph.pl out.folded &gt; out.svg</p>
</li>
<li><p>然后可以在浏览器里打开out.svg</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>python学习笔记</title>
    <url>/2021/07/17/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li>作用区域</li>
</ol>
<hr>
<blockquote>
<pre><code>#!/usr/bin/python

global y
y = 3

def func(x):
   x = 2
   print &quot;x is&quot;, x

def func_1():
   global y # if delete this line, &#39;y&#39; below is a local one
   y = 5
   print &quot;y is&quot;, y

x = 10
func(x)
print &quot;valuce of x is&quot;, x

func_1()
pRint &quot;valuce of y is&quot;, y
</code></pre>
</blockquote>
<p>函数内的x是local的，不改变x=10的值。在函数func_1内指示y是global的，函数内改变<br>y的值，函数外y=3变成y=5。</p>
<ol start="2">
<li>数据存储方式 </li>
</ol>
<hr>
<blockquote>
<pre><code>#!/usr/bin/python

x = 3
y = x

id_x = id(x)
id_y = id(y)
print &quot;x id is&quot;, id_x
print &quot;y id is&quot;, id_y

x = 5
id_x_new = id(x)
id_y_new = id(y)
print &quot;x_new id is&quot;, id_x_new
print &quot;y_new id is&quot;, id_y_new

# will appear error, as x has been deleted
#del(x)
#id_x_del = id(x)
#print &quot;x_del id is&quot;, id_x_del

# will not appear error
#del(x)
#id(y)

x_list = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]
y_list = x_list
id_x_list = id(x_list)
id_y_list = id(y_list)
print &quot;x_list id is&quot;, id_x_list
print &quot;y_list id is&quot;, id_y_list

x_list.append(&quot;d_added&quot;)
print &quot;new x_list is&quot;, x_list
id_new_x_list = id(x_list)
print &quot;new x_list id is&quot;, id_new_x_list

y_list.append(&quot;y&quot;)
print &quot;new y_list is&quot;, y_list
id_new_y_list = id(y_list)
print &quot;new y_list id is&quot;, id_new_y_list
</code></pre>
</blockquote>
<p>python中的数据都是类。python中的数据分为不可变变量和可变变量，其中数字，字符串<br>是不可变变量，其他的是可变变量。id(x)显示的是x变量的存储’地址’，根据id()可以了解<br>可变变量和不可变变量的性质。x的值不一样，id(x)的结果是不一样的，x是一个数字，是<br>不可变的，所以为x赋新值本质上是重新创建了一个变量，x_list是一个列表，是可变的，<br>所以改变x_list的值，就是改变它本身的值。y = x并没有新建了一个变量，而是为x变量<br>增加了一个叫y的索引，本质上是一个存储结构，所以id(x) = id(y); 所以改变x_list,<br>y_list也跟着变了。</p>
<ol start="3">
<li>典型数据结构</li>
</ol>
<hr>
<ul>
<li>列表</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

global shoplist
shoplist = [&#39;apple&#39;, &#39;mango&#39;, &#39;carrot&#39;, &#39;banana&#39;]

def print_shoplist():
   print shoplist


lenth = shoplist.__len__()
print &quot;len of shoplist is&quot;, lenth

shoplist.sort()
print_shoplist()

shoplist.append(&#39;pear&#39;)
print_shoplist()

shoplist.__delitem__(0)
print_shoplist()

print shoplist[0]
print shoplist[-1]

# use help() to check functions which list offered
#help(list)
</code></pre>
</blockquote>
<p>列表是python的内置数据结构, 存放一组数据，列表里面的值是可以改变的。用help(list)<br>可以查看列表类中所包含的方法。上面列出几个方法：<strong>len__返回列表的长度，sort对<br>列表的数据排序，append在列表的最后加入一个数据，__delitem</strong>(x)删去索引是x的数据.<br>列表中数据的索引和c语言中数组的下标一样，但是可以逆向索引，如shoplist[-1]得到最<br>后一个元素的值。</p>
<ul>
<li>元组</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

zoo = (&#39;wolf&#39;, &#39;elephant&#39;, &#39;penguin&#39;)
new_zoo = (&#39;monkey&#39;, &#39;dolphin&#39;, zoo)

print &quot;len of zoo is&quot;, len(zoo)
print &quot;len of new zoo is&quot;, len(new_zoo)
print &quot;2 of new zoo is&quot;, new_zoo[2]
print &quot;[2][2] of new zoo is&quot;, new_zoo[2][2]

print &quot;%s is %d years old&quot; %(&#39;John&#39;, 12)

def func_return_multi():
   return &#39;John&#39;, 12

print func_return_multi()
</code></pre>
</blockquote>
<p>元组也是python的内置数据结构，但是元组里的值是不可变的，当然如果元组里的元素是<br>一个列表，列表里的值是可以变的。元组的用途有很多，比如上面的格式话输出，当有多<br>个输出值时，它们的真值要用一个元组包含起来; python中的函数可以一次返回多个值，<br>返回的多个值被包含在一个元组中。</p>
<ul>
<li>字符串</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

string = &quot;0123456789&quot;
print &quot;string is&quot;, string

new_string = string[0:5]
print &quot;new_string is&quot;, new_string

print &#39;string 2 to end is&#39;, string[2:]
print &#39;string 1 to -1 is&#39;, string[1:-1]
print &#39;string start to end is&#39;, string[:]
print &#39;string start to end is&#39;, string[::4]

# list and tuple also have those kinds of operations
</code></pre>
</blockquote>
<p>上面是字符串的切片操作，列表和元组也有相同的操作。</p>
<ul>
<li>字典</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

b = &#123;
   &#39;Swaroop&#39;: &#39;swaroopch@byteofpython.info&#39;,
   &#39;Larry&#39;  : &#39;larry@wall.org&#39;,
   &#39;Matsumoto&#39; : &#39;matz@ruby-lang.org&#39;,
   &#39;Spammer&#39;   : &#39;spammer@hotmail.com&#39;
    &#125;

print b[&#39;Larry&#39;]

# add a key-&gt;value in dictionary
b[&#39;Sherlock&#39;] = &quot;Sherlock@gmail.com&quot;
print b

b.pop(&#39;Larry&#39;)
print b

if &#39;Larry&#39; in b:
   print &quot;Larry is in b&quot;
else:
   print &quot;Larry is not in b&quot;
</code></pre>
</blockquote>
<p>字典又一一对应的一组key-&gt;value组成，key需要是不可变变量。</p>
<ul>
<li>集合</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

s = set([1, 2, 3, 4])

print s

# add a key in set
s.add(10)
print s

s.remove(1)
print s

if 2 in s:
   print &quot;2 is in s&quot;
else:
   print &quot;2 is not in s&quot;
</code></pre>
</blockquote>
<p>集合是一组值的集合，用一个列表初始化。集合中的元素也需要是不可变变量。</p>
<ol start="4">
<li>函数</li>
</ol>
<hr>
<ul>
<li>函数基础</li>
</ul>
<blockquote>
<pre><code>#!/usr/bin/python

def test_add(a, b):
   return a + b

print &quot;1 + 2 =&quot;, test_add(1, 2)

add_test = test_add
print &quot;1 + 2 =&quot;, add_test(1, 2)

# need &quot;pass&quot; to fill this &quot;none content function&quot;
def nop():
   pass

# default input
def ball(r, color = &quot;red&quot;, vendor = &quot;A&quot;, llist = [1, 2, 3]):
   llist.append(4)
   print r
   print color
   print vendor
   print llist

print &quot;test_1&quot;
ball(5)

print &quot;test_2&quot;
ball(5, &quot;blue&quot;)

print &quot;test_3&quot;
ball(5, vendor = &quot;B&quot;)

print &quot;test_4&quot;
ball(5, llist = [4, 5, 6])
ball(5)

# variable input
print
print &quot;variable input test&quot;
def sum(*number):
   sum = 0
   for i in number:
       sum = sum + i
   return sum
print &quot;sum is&quot;, sum(1, 2, 3)

num = [1, 2, 3, 4]
print &quot;sum is&quot;, sum(*num)

# key word input
print
print &quot;key word input test&quot;
def key_test(a, b, **c):
   print a
   print b
   print c
print  &quot;key is&quot;, key_test(1, 2)
print
print  &quot;key is&quot;, key_test(1, 2, name = &quot;Sherlock&quot;)
print
dict_test = &#123;&quot;name&quot; : &quot;John&quot;, &quot;age&quot; : 12&#125;
print  &quot;key is&quot;, key_test(1, 2, **dict_test)
</code></pre>
</blockquote>
<p> **函数名是一个指向函数对象的引用，所以可以把一个函数名赋值给一个变量<br> **函数的参数可以是默认参数，可变参数，关键字参数等。<br>   当函数带默认参数时，默认参数需要是不可变参数。不然就像上面代码中显示的那样，<br>   如果函数中改变这个参数的值，以后这个参数的值相应的也都改变了。函数可以带可变<br>   参数, 可变参数可以直接传入函数，也可以先把所有参数组成一个列表，再通过*list<br>   传入。通过**dictionary的方式可以传入一个字典。</p>
<ol start="4">
<li>面向对象</li>
</ol>
<hr>
<blockquote>
<pre><code>#!/usr/bin/python

class ball(object):
   def __init__(self, r, color = &quot;green&quot;, vendor = &quot;A&quot;):
       # init r, but we need not to declare r
       self.r = r
       self.color = color
       # private element
       self.__vendor = vendor
       
   def run(self):
       print &quot;ball is running&quot;
   def show_color(self):
       print &quot;color of ball is&quot;, self.color

# init an instance
ball_test_1 = ball(5)
ball_test_1.show_color()

print
ball_test_2 = ball(5, &quot;red&quot;)
ball_test_2.show_color()

print
ball_test_1.name = &quot;John&quot;
print ball_test_1.name

# test private element
# will appear error when run below command
#print ball_test_1.__vendor

# ok when run below command, but could not write this code, we should write a
# function to show ball&#39;s vendor
print ball_test_1._ball__vendor

# inherit test
print
class football(ball):
   def run(self):
       print &quot;football is running&quot;

football_test_1 = football(10)
football_test_1.run()

# polymorphism test
print
def run(ball_t):
   ball_t.run()
   print &quot;**** ****&quot;
run(ball_test_1)
run(football_test_1)

print
print type(ball_test_1)
print type(football_test_1)
</code></pre>
</blockquote>
<ul>
<li>python中不需要在变量使用前先定义。</li>
<li>私有变量需要加__的前缀</li>
<li>note: raw_input()输入的变量是字符串的，要输入数字需要：int(raw_input())</li>
</ul>
<p> todo: 异常/标准库/I/O/进程/图形/网络/数据库/web…</p>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu qom分析</title>
    <url>/2021/07/26/qemu-qom%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="用c语言实现面向对象的模型"><a href="#用c语言实现面向对象的模型" class="headerlink" title="用c语言实现面向对象的模型"></a>用c语言实现面向对象的模型</h2><p> qemu里用c语言实现了面向对象的模型。我们先梳理用c实现面向对应的基本逻辑。面向对象<br> 的三个特征是：封装、继承和多态。</p>
<p> 封装可以用struct实现。</p>
<p> 继承可以用struct包含的方式实现，把父类的struct放到子类struct的最开始的位置，这样<br> 子类的指针可以直接强制转换成父类的指针，在子类的函数，比如子类的初始化函数里可以<br> 直接得到父类的指针，然后调用父类的初始化函数。但是继承层级大于两级的时候似乎是有<br> 问题的(fixme)</p>
<p> 多态可以用函数指针的方式实现。</p>
<p> qemu里的实现，多了TypeInfo这个概念，它是class的描述。</p>
<h2 id="类的定义"><a href="#类的定义" class="headerlink" title="类的定义"></a>类的定义</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">type_init(fn)</span><br><span class="line">      /* 用宏定义了一个动态库的初始化函数, qemu编译出的库有？*/</span><br><span class="line">  +-&gt; module_init</span><br><span class="line">   +-&gt; register_module_init</span><br><span class="line">         /* 如下都用edu设备举例(hw/misc/edu.c)，这里的fn就是pci_edu_register_types */</span><br><span class="line">     +-&gt; e-&gt;init = fn</span><br></pre></td></tr></table></figure>
<p> fn这个函数一般是TypeInfo的注册函数, 把TypeInfo挂到系统的链表里。class是随后解析<br> Typeinfo的内容动态生成的。</p>
<h2 id="对象的生成"><a href="#对象的生成" class="headerlink" title="对象的生成"></a>对象的生成</h2><p> 顺着qemu的main函数，看看class和对象是怎么生成的：qemu/softmmu/main.c</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">main</span><br><span class="line">  +-&gt; qemu_init</span><br><span class="line">    +-&gt; qemu_init_subsystems</span><br><span class="line">          /* 根据TypeInfo创建class */</span><br><span class="line">      +-&gt; module_call_init(MODULE_INIT_QOM)</span><br><span class="line">            /*</span><br><span class="line">             * init即为如上的fn, 这里的init只是把TypeInfo向qemu注册，类的</span><br><span class="line">             * 初始化还在后面。具体拿edu里的pci_edu_register_types函数看下。</span><br><span class="line">             */     </span><br><span class="line">        +-&gt; ModuleEntry-&gt;init</span><br></pre></td></tr></table></figure>
<p>  可以看见这个函数拿自己的初始化函数定义了TypeInfo数据结构，然后把他注册到系统<br>  TypeInfo的链表。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">static void pci_edu_register_types(void)</span><br><span class="line">&#123;</span><br><span class="line">    static InterfaceInfo interfaces[] = &#123;</span><br><span class="line">        &#123; INTERFACE_CONVENTIONAL_PCI_DEVICE &#125;,</span><br><span class="line">        &#123; &#125;,</span><br><span class="line">    &#125;;</span><br><span class="line">    static const TypeInfo edu_info = &#123;</span><br><span class="line">        .name          = TYPE_PCI_EDU_DEVICE,</span><br><span class="line">        .parent        = TYPE_PCI_DEVICE,</span><br><span class="line">        .instance_size = sizeof(EduState),</span><br><span class="line">        .instance_init = edu_instance_init,</span><br><span class="line">        .class_init    = edu_class_init,</span><br><span class="line">        .interfaces = interfaces,</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    type_register_static(&amp;edu_info);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  顺着qemu_init函数继续往下看，找下class以及具体的设备是在哪里创建的。还是以edu<br>  这个设备为例。这个设备使用-device edu的qemu命令行参数启动，所有它创建的位置应该<br>  在：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu_opts_foreach(qemu_find_opts(&quot;device&quot;),</span><br><span class="line">                  device_init_func, NULL, &amp;error_fatal);</span><br></pre></td></tr></table></figure>
<p>   下面具体分析其中的qdev_device_add:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> qdev_device_add</span><br><span class="line">       /* driver就是模块里的device name, edu的driver就是edu */</span><br><span class="line">   +-&gt; driver = qemu_opt_get(opts, &quot;driver&quot;)</span><br><span class="line">     +-&gt; module_object_class_by_name(*driver)</span><br><span class="line">       +-&gt; oc = object_class_by_name(typename)</span><br><span class="line">             /*</span><br><span class="line">              * type_initialize是根据注册的Type创建class的函数。创建class的具体</span><br><span class="line">              * 实例的时候，如果class没有创建，就会创建class，被创建的class的指针</span><br><span class="line">              * 会放到注册Type的class域段。</span><br><span class="line">              *</span><br><span class="line">              * 可以看到这个函数为class分配了空间，递归初始化了父类，把父类空间</span><br><span class="line">              * 中的内容copy到了当前类最开始的空间。初始化class的interface和</span><br><span class="line">              * property，并在最后调用了class的init函数，把class的数据和操作函数</span><br><span class="line">              * 都添加上。</span><br><span class="line">              *</span><br><span class="line">              * 创建的interface class会挂到对应device class的链表上。</span><br><span class="line">              */</span><br><span class="line">         +-&gt; type_initialize(type)</span><br><span class="line">       /* 注意，这里返回的是DeviceClass */</span><br><span class="line">   +-&gt; dc = qdev_get_device_class(&amp;driver, errp)</span><br><span class="line">       /* 找见设备对应的bus */</span><br><span class="line">   +-&gt; bus = qbus_find(path, errp)</span><br><span class="line">       /* 创建设备, 可以看到如果没有class的话，在如下函数里会先创建class */</span><br><span class="line">   +-&gt; dev = qdev_new(driver)</span><br><span class="line">     +-&gt; object_new(typename)</span><br><span class="line">           /*</span><br><span class="line">            * 为设备对象分配了内存空间, 把设备里的class指针指向class，为设备</span><br><span class="line">            * 初始化class里定义的各个property。调用instance_init初始化设备。</span><br><span class="line">            * 注意这个时候设备还不是在可用的状态。</span><br><span class="line">            */</span><br><span class="line">       +-&gt; object_new_with_type(ti)</span><br><span class="line">/* 解析输入的设备属性并且保存到设备的属性hash表里 */</span><br><span class="line">   +-&gt; qemu_opt_foreach(opts, set_property, dev, errp)</span><br><span class="line">   +-&gt; qdev_realize(DEVICE(dev))</span><br><span class="line">         /* 调用到class里的realize函数激活设备, 具体的分析在下面一节 */</span><br><span class="line">     +-&gt; object_property_set_bool(OBJECT(dev), &quot;realized&quot;, true, errp)</span><br></pre></td></tr></table></figure>

<h2 id="properties是什么"><a href="#properties是什么" class="headerlink" title="properties是什么"></a>properties是什么</h2><p> 所谓属性，就是在一个对象里定义的一些功能，这些功能有名字，有对应的执行函数，还有<br> 添加和删除函数。当添加一个属性的时候，就是把这个属性已经对应的执行函数保存到对象<br> 专门用来存各种属性的一个hash table。当执行属性的操作时，就是执行对应属性附带的<br> 执行函数。</p>
<p> 我们还是拿edu这个设备为例。edu在实例初始化的时候挂给PCIDeviceClass的realize一个<br> 回调函数pci_edu_realize，这个函数就是PCI设备里realize属性的执行函数。我们需要明确<br> 这个realize属性在哪里添加和在哪里调用。</p>
<p> device class的初始化函数里增加了realized属性：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* hw/core/qdev.c */</span><br><span class="line">device_class_init</span><br><span class="line">     /*</span><br><span class="line">      * 把realized属性加到ObjectClass里。device_set_realized里会调用DeviceClass里的</span><br><span class="line">      * realize回调函数。DeviceClass里的realize回调在pci_device_class_init里挂成</span><br><span class="line">      * pci_qdev_realize。pci_qdev_realize调用PCIDeviceClass里的realize函数，这个</span><br><span class="line">      * 函数又是由具体设备的class init函数添加，比如edu的edu_class_init。</span><br><span class="line">      */</span><br><span class="line">  -&gt; object_class_property_add_bool(ObjectClass, &quot;realized&quot;, device_get_realized, device_set_realized)</span><br></pre></td></tr></table></figure>

<p> 在如上的qdev_device_add里，在创建了设备的实例后，后调用qdev_realize把设备realize，<br> 这个函数会从Device这一层，层层的调用realize函数:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* hw/core/qdev.c */</span><br><span class="line">qdev_realize</span><br><span class="line">  -&gt; object_property_set_bool(OBJECT(dev), &quot;realized&quot;, true, errp)</span><br><span class="line">    [...]</span><br><span class="line">    -&gt; object_property_set</span><br><span class="line">         /* 可以看到realized相关的add和find都是发生在Object、ObjectClass这个层次 */</span><br><span class="line">      -&gt; ObjectProperty *prop = object_property_find(obj, name, errp)</span><br><span class="line">           /* 先用obj找到ObjectClass，再在ObjectClass找property */</span><br><span class="line">        -&gt; object_class_property_find(klass, name, NULL)</span><br><span class="line">           /* 在Object里找property */</span><br><span class="line">        -&gt; g_hash_table_lookup(obj-&gt;properties, name)</span><br><span class="line"></span><br><span class="line">      -&gt; prop-&gt;set(obj, v, name, prop-&gt;opaque, &amp;err)</span><br></pre></td></tr></table></figure>
<p> 如上，分析qemu的qom重点关注如下的文件: hw/misc/edu.c, hw/pci/pci.c, hw/core/qdev.c,<br> qom/object.c。各个层级的Type定义分别对应的文件里(这里用pci设备为例)</p>
<p> 一个典型的使用属性的地方是在qemu启动的时候通过命令行参数给一个设备传递一个属性值。<br> 我们分析这里的代码流程，还是以edu为例，edu_instance_init里的object_property_add_uint64_ptr<br> 为edu设备加了dma_mask这样一个设备属性。在qemu的启动命令行里可以如下配置使用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">--device edu,dma_mask=0xffffff</span><br></pre></td></tr></table></figure>
<p> 可以看到qemu_opt_foreach(opts, set_property, dev, errp)解析设备属性在instance_init<br> 之后，在realize函数调用之前。所以，edu驱动里在instance_init里把设备属性的定义加<br> 到设备对应的属性hash表里，如上的解析函数才能把命令行输入的属性和设备属性匹配。<br> edu需要在realize函数或者realize之后才能使用传入的设备属性。</p>
<p> link属性</p>
<p> 有了上面的分析，link属性的使用也可以想到，他同样可以使用qemu的启动命令行确定qemu<br> 部件之间的逻辑关系。</p>
<p> child属性</p>
<h2 id="interface"><a href="#interface" class="headerlink" title="interface"></a>interface</h2><p> 目前只看了PCI/PCIe设备里使用了interface这个东西，PCIe设备用INTERFACE_PCIE_DEVICE<br> PCI设备用INTERFACE_CONVENTIONAL_PCI_DEVICE。pci设备的realize函数里根据interface<br> 的情况决定是否要使能PCI_CAP_EXPRESS，这个只在PCIe的时候使能。</p>
<p> 注意，PCI设备只有0x0~0xff的配置空间。</p>
<h2 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h2><p> 这里写一个dma engine的qemu设备。</p>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu里pci设备的热插拔</title>
    <url>/2021/06/28/qemu%E9%87%8Cpci%E8%AE%BE%E5%A4%87%E7%9A%84%E7%83%AD%E6%8F%92%E6%8B%94/</url>
    <content><![CDATA[<p>场景是一个pcie设备的vf通过vfio直通给qemu使用，这时如果我们在host上通过sysfs把<br>对应的vf disable掉。</p>
<p>正常来讲，qemu里的vf pci设备会表现为一个pci设备热拔出的行为。与之相对应的设置为：</p>
<ol>
<li><p>guest kernel的配置里要打开pci hotplug: CONFIG_HOTPLUG_PCI_PCIE.</p>
</li>
<li><p>guest kernel的启动cmdline里要是能pci native hotplug, 加上pcie_port=native</p>
</li>
<li><p>启动qemu的时候，需要把直通上来的pci vf挂到一个支持pci热插拔的pci桥下面:<br>比如在qemu里挂接一个ioh3420的pci桥，然后再把直通的vf挂在这个桥下。</p>
</li>
<li><p>本文的测试是在主线linux v5.0-rc6上做的，这个版本有一个pci hotplug的bug，这个<br>bug会导致虚拟机里vf无法被热拔。相关的fix补丁已经被pci maintainer ack, 会合<br>入v5.1主线版本。如果是在v5.0, 以及之前的内核的版本上测试，需要确认这个补丁<br>是否合入:</p>
<pre><code>[PATCH RESEND] PCI: pciehp: Assign ctrl-&gt;slot_ctrl before writing it to hardware
</code></pre>
</li>
</ol>
<p> 综合以上，如下的qemu启动命令，配合正确的kernel，可以支持qemu里直通vf的pci热拔<br> 操作:(这里已ARM64平台为例)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-aarch64 -machine virt,gic_version=3 -enable-kvm -cpu host \</span><br><span class="line">-m 1024 -kernel ./Image -initrd ./minifs.cpio.gz -nographic -append \</span><br><span class="line">&quot;rdinit=init console=ttyAMA0 earlycon=pl011,0x9000000 pcie_ports=native&quot; \</span><br><span class="line">-device ioh3420,id=root_port \</span><br><span class="line">-device vfio-pci,host=0000:75:00.1,bus=root_port</span><br></pre></td></tr></table></figure>

<p>具体可以这样测试:(已HiSilicon D06 zip engine为例)</p>
<ol>
<li><p>在host上把vf和vfio驱动绑定:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo 1 &gt; /sys/devices/pci0000:74/0000:74:00.0/0000:75:00.0/sriov_numvfs</span><br><span class="line">echo 0000:75:00.1 &gt; /sys/bus/pci/drivers/hisi_zip/unbind</span><br><span class="line">echo vfio-pci &gt; /sys/devices/pci0000:74/0000:74:00.0/0000:75:00.1/driver_override</span><br><span class="line">echo 0000:75:00.1 &gt; /sys/bus/pci/drivers_probe</span><br></pre></td></tr></table></figure></li>
<li><p>启动qemu: 同上面的命令</p>
</li>
<li><p>在host上disable vf:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo 0 &gt; /sys/devices/pci0000:74/0000:74:00.0/0000:75:00.0/sriov_numvfs</span><br></pre></td></tr></table></figure></li>
</ol>
<p> 可以看到在qemu里，vf表现为一个pci热拔的动作:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(fix me: add log, 目前看上面的命令会在host上挂住)</span><br></pre></td></tr></table></figure>

<p>为了使得这篇介绍完整，对于qemu里pci设备的热插，可以这样来做:</p>
<ol>
<li><p>启动qemu后按ctrl a + c 进入qemu monitor(启动qemu的时候带ioh3420但是不带VF设备)</p>
</li>
<li><p>在qemu monitor里: device_add vfio-pci,host=0000:75:00.1,bus=root_port</p>
</li>
</ol>
<p> 这样可以把已经和vfio驱动绑定的VF PCI热插到qemu<br> (fix me: lspci看不到新设备，但是在qemu monitor里info pci可以看到新插入的设备)</p>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>PCIe</tag>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu里增加trace的方法</title>
    <url>/2021/07/26/qemu%E9%87%8C%E5%A2%9E%E5%8A%A0trace%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<ol>
<li><p>在要加trace的模块对应的文件中加：#include “trace.h”</p>
</li>
<li><p>在trace.h文件里写 #include “trace/trace-xxx_xxx.h”</p>
<p>注意这里的连接符必须按照上面，其中的xxx是模块代码的路径，比如你的模块在qemu代码<br>的hw/arm下，如上应该写成#include “trace/trace-hw_arm.h”, 每一级路径都是下划线<br>连接。</p>
</li>
<li><p>在相同的目录创建trace-events文件，并在其中定义trace。定义的格式大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">example(uint8_t level, uint32_t offset, uint64_t pte) &quot;level: %u, pte offset: %u, pte value: 0x%lx&quot;</span><br></pre></td></tr></table></figure>
<p>“example”是trace点的名字，括号里是各个参数的类型，引号里是输出的内容。</p>
</li>
<li><p>在模块文件需要使用该trace点的地方使用 trace_example(xxx, xxx, xxx); 的方式调用。</p>
</li>
</ol>
<p>详细的说明可以参考qemu的开发手册：<span class="exturl" data-url="aHR0cHM6Ly9xZW11LXByb2plY3QuZ2l0bGFiLmlvL3FlbXUvZGV2ZWwvdHJhY2luZy5odG1s">https://qemu-project.gitlab.io/qemu/devel/tracing.html<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu虚拟机通过tun/tap上网</title>
    <url>/2021/07/17/qemu%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%80%9A%E8%BF%87tun-tap%E4%B8%8A%E7%BD%91/</url>
    <content><![CDATA[<p>参考: <span class="exturl" data-url="aHR0cDovL3d3dy4zNjBkb2MuY29tL2NvbnRlbnQvMTIvMDYxMS8xNC83OTgyMzAyXzIxNzQzODg1Ny5zaHRtbA==">http://www.360doc.com/content/12/0611/14/7982302_217438857.shtml<i class="fa fa-external-link-alt"></i></span></p>
<p>考虑用tun/tap的方式，那么需要宿主机(本人的是ubuntu12.04)的内核支持tun/tap的功能，<br>宿主机内核是支持这样的功能的，如果您用的操作系统内核不支持tun/tap，需要下载源码<br>然后编译对应的模块，然后插入对应的模块。</p>
<ol>
<li><p>主机已经有了/dev/net/tun, 需要修改可执行权限:<br>  sudo chmod o+x /dev/net/tun<br>  可以通过主机/boot/config-XXX文件查看是否配置了CONFIG_TUN, 2.6.X以后的内核<br>  已默认将TUN直接编译进内核，所以这里是CONFIG_TUN=y</p>
</li>
<li><p>下载tunctl工具的源码：<br>  <span class="exturl" data-url="aHR0cDovL3NvdXJjZWZvcmdlLm5ldC9wcmplY3RzL3R1bmN0bC8=">http://sourceforge.net/prjects/tunctl/<i class="fa fa-external-link-alt"></i></span><br>  解压之后直接make就好了。过程中有可能会出现make: docbook2man: Command not <br>  found, 这个是在生成对应的帮助文档的时候缺少了docbook2man的工具，现在ls一<br>  下发现tunctl已经编译好了，这个错误可以不用去管。同样的道理make install也<br>  会出错，不用make install了，直接用tunctl就可以了</p>
</li>
<li><p>参考[1]进行设置，要注意确定主机内核对TUN/TAP设备和虚拟网桥的支持，这里采用<br>  TUN/TAP的方式搭建网络。第一步已经确认了TUN/TAP，现在只需要确认网桥：<br>  思路是要先找到网桥的内核配置项, 然后去/boot/config-XXX下查看：<br>  随便一个内核源码，查看linux-src/net/bridge/Kconfig，其中第一项就是参考[1]<br>  中的网桥的说明，可见配置项就是CONFIG_BRIDGE; 在/boot/config-XXX中查看，发<br>  现CONFIG_BRIDGE=m, 于是用sudo modprobe bridge将其插入内核中</p>
</li>
<li><p>将参考[1]中的配置在这里重复一下：<br>  ifconfig eth0 down                # 关闭网口<br>  brctl addbr br0                   # 添加虚拟的网桥<br>  brctl addif br0 eth0              # 在网桥上添加网口<br>  brctl stp br0 off                 <br>  brctl setfd br0 1                 <br>  brctl sethello br0 1              <br>  ifconfig br0 0.0.0.0 promisc up       # 释放br0的ip地址<br>  ifconfig eth0 0.0.0.0 promisc up      # 释放eth0的ip地址<br>  dhclient br0# 自动获得br0的ip地址<br>  brctl show br0<br>  brctl showstp br0</p>
<p>  tunctl -t tap0 -u root        # 设定虚拟网卡上的端口tap0<br>  brctl addif br0 tap0# 在网桥上添加虚拟网卡的端口tap0<br>  ifconfig tap0 0.0.0.0 promisc up# 释放tap0的ip地址<br>  brctl showstp br0</p>
</li>
<li><p>启动qemu虚拟机：<br>  sudo qemu-system-x86_64 -m 1024 -net nic -net tap,ifname=tap0,script=no,downscript=no ./ubuntu_qemu.img<br>  这里假设虚拟机已经可以运转起来，关于ubuntu_qemu.img的制作可以查看qemu-img的用法</p>
</li>
<li><p>测试虚拟机网络：<br>  ping <span class="exturl" data-url="aHR0cDovL3d3dy5iYWlkdS5jb20v">www.baidu.com<i class="fa fa-external-link-alt"></i></span><br>  可以了</p>
</li>
</ol>
<p>附录：<br>   虚拟机自己的ip是：192.168.201.108/25<br>   br0的ip是：       192.168.201.23/25<br>   tap0, eth0没有ip, 但是eth0, br0的MAC地址相同<br>   主从之间通过上面的ip可以相互通信，另一台独立的pc通过虚拟机ip可以和虚拟机通信</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+----------+     </span><br><span class="line">|bridge:   |---&gt; br0 (ip:192.168.201.23/25 78:ac:c0:a8:fc:fe)</span><br><span class="line">|br0       |---&gt; eth0(78:ac:c0:a8:fc:fe)</span><br><span class="line">|          |</span><br><span class="line">+----------+</span><br><span class="line">     |</span><br><span class="line">     |tap0(ca:fd:08:a3:65:9d)</span><br><span class="line">     |</span><br><span class="line">     |eth0(192.168.201.108/25 52:54:00:12:34:56)</span><br><span class="line">+-----------+</span><br><span class="line">| guest os  |</span><br><span class="line">|           |</span><br><span class="line">+-----------+</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>QEMU</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>rpm yum使用笔记</title>
    <url>/2021/07/05/rpm-yum%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="rpm"><a href="#rpm" class="headerlink" title="rpm"></a>rpm</h2><ol>
<li><p>install a rpm package: rpm -ivh *.rpm</p>
</li>
<li><p>query something:</p>
<p>  rpm -q kernel  –&gt; query a package name kernel.</p>
<p>  rpm -qf file   –&gt; query a file to find which package it belogs to</p>
<pre><code>                 so we can use this to find the package of one shell
                 command, like: rpm -qf /usr/sbin/lspci, it will get:
                 pciutils-3.5.1-2.el7.aarch64.
</code></pre>
<p>  rpm -ql package -&gt; list all files in this package.</p>
<p>  rpm -qa        –&gt; query all installed package in system.</p>
<p>  rpm -qi package -&gt; list related info of this package.</p>
<p>  rpm -qc package -&gt; find config file of this package, e.g.</p>
<pre><code>                 rpm -qc openssh-clients-xxxx, will get
                 /etc/ssh/ssh_config.
</code></pre>
<p>  rpm -qd package -&gt; find related document.</p>
<p>  rpm -qR package -&gt; find related required libs.</p>
</li>
</ol>
<h2 id="yum"><a href="#yum" class="headerlink" title="yum"></a>yum</h2><pre><code>   yum search kernel      --&gt; search in yum datebase.

   yum provides software  --&gt; find which package contains this software in
                              yum database. similar with rpm -qf file, but
                              rpm searchs packages locally.
                              yum provides fio, we will get:

Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
fio-3.1-1.el7.aarch64 : Multithreaded IO generation tool
Repo        : epel

   yum info package       --&gt; show information of one package
                     e.g. yum info kernel will show all kernel
                   packages with different versions

   yum list package       --&gt; list all version of this package
</code></pre>
<h2 id="rpmbuild"><a href="#rpmbuild" class="headerlink" title="rpmbuild"></a>rpmbuild</h2><pre><code>1. from source to build rpm package

    ...

P.S. For linux kernel, you can directly run: &quot;make rpm&quot; to do so,
     it will create kernel/kernel-dev/kernel-header rpm packages.

2. downlaod a rpm, modify it, and re-build a new rpm package

    ...
</code></pre>
<h2 id="download-rpm"><a href="#download-rpm" class="headerlink" title="download rpm"></a>download rpm</h2><pre><code>This document shares the way to download rpm package using yum tools:
https://www.ostechnix.com/download-rpm-package-dependencies-centos/

&quot;yum install --downloadonly&quot; will also install package.

&quot;yumdownloader&quot; just downloads the package.
</code></pre>
<h2 id="download-source"><a href="#download-source" class="headerlink" title="download source"></a>download source</h2><pre><code>   1. yum install yum-utils

      /* should be the name of package, just showed when yum search XXX */
   2. yumdownloader --source xorg-x11-drv-ati

   3. yum install mock
      useradd -s /sbin/nologin mockbuild

   4. rpm -hiv xorg-x11-drv-ati-7.7.1-3.20160928git3fc839ff.el7.src.rpm
      it will create ~/rpmbuild/SOURCE and install above package there.

   5. cd /root/rpmbuild/SOURCES

   6. xz -d *
      tar -xf *
</code></pre>
]]></content>
      <tags>
        <tag>包管理</tag>
        <tag>RPM/YUM</tag>
      </tags>
  </entry>
  <entry>
    <title>ssh笔记</title>
    <url>/2021/07/11/ssh%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>我们在日常工作中常常使用ssh连接远程服务器。</p>
<p>一般我们这样使用: ssh user_name@remote_host<br>其中remote_host可以是远程服务器的域名或者是IP. 之后输入密码就可以登录远程服务器</p>
<p>利用ssh通信的其他命令(e.g. scp)的格式也和上面的相似。</p>
<p>对于想不每次手动输入密码登录, 我们可以把ssh-keygen生成的公钥放到远程服务器对应<br>home目录下的.ssh/authorized_keys这个文件里。这样直接输入上面的命令就可以登录远程<br>服务器。</p>
<p>如何把公钥里的内容放到远程服务器的authorized_keys里, 我们可以使用命令:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh user_name@remote_host &quot;cat &gt;&gt; ~/.ssh/authorized_keys&quot; &lt; ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure>
<p>如果服务端的ssh server不在常用的22端口上。我们可以用-p指定所用的端口:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh -p port_number user_name@remote_host</span><br></pre></td></tr></table></figure>
<p>如果远程服务器没有图形化界面，可以使用 ssh -X user_name@remote_host “command”<br>(e.g. ssh -X user_name@remote_host “thunderbird”), 在远程服务器上运行命令command,<br>而在本地机器上显示command的图形化输出。</p>
]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>sed笔记</title>
    <url>/2021/07/11/sed%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>sed是一个面向数据流的编辑器。和vim这种交互式编辑器不用，sed按照定义的处理模式处理<br>每一行文本。比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* 把test_file文件中每行的第一个&#x27;haha&#x27;换成&#x27;taoge&#x27; */</span><br><span class="line">sed &#x27;s/haha/taoge/&#x27; test_file</span><br></pre></td></tr></table></figure>
<p>其中s/haha/taoge就是定义的处理模式，test_files是要处理的文件。sed把处理后的文本<br>向标准输出，不会更改原文件。</p>
<p>一个sed命令还可以在sed和处理模式之间加入命令选项。比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sed -f sed_script test_file</span><br></pre></td></tr></table></figure>
<p>表示用sed_script中的处理模式（每行一个处理模式）处理test_file中的所有行。<br>又比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sed -e &#x27;s/this/a/; s/that/b/&#x27; test</span><br></pre></td></tr></table></figure>
<p>-e后面是分号隔开的多个匹配模式。</p>
<p>sed编辑器的处理模式中常用的命令有s, d, i, a, c, w, p, r, 寻址…</p>
<p>s命令比较常见，即像上面展示的：s/old/new/. vim的修改命令也是这样的，一般的，<br>linux邮件列表里指示修改的评论有时候也写成sed命令的样子了。都是过来人，行业”黑话”<br>一看都懂。s命令的最后面还可以跟一些flag, 比如s/old/new/g, 表示整行所有的old都替换<br>成new; s/old/new/2, 表示替换第二个old成new; s/old/new/p, 表示输出发生替换的行，<br>一般常常是：sed -n s/old/new/p，-n表示结果不输出，于是最终结果是只输出了发生替换<br>的行; s/old/new/w file, 表示把输出的结果保存在file文件中。</p>
<p>想指定具体行的修改，可以寻址方式修改, 比如：2s/old/new/, 2,3s/old/new, 1,$s/old/new<br>分别是修改第二行，第二行和第三行，第一行到最后一行。vim里格式是一样的。</p>
<p>更加一般的，这里的寻址方式也可以换成匹配字符, 比如：/wang/s/old/new/<br>找见有字符串’wang’的一行, 在这一行用s命令把第一个old改成new. 这个匹配特定行，再<br>处理的方式也可以在除了s命令的其他命令上使用。</p>
<p>d是删除命令。上面熟悉的格式可以套用到d命令上：sed ‘1d’ test, sed ‘1,3d’ test,<br>sed ‘/wang/d’ test, sed -e “/wang/d; 1d” test, 分别是删除第一行，删除1~3行，删除<br>含有’wang’的一行，删除含有’wang’的一行和第一行。</p>
<p>i, a, c是插入, 附加操作和更改操作, 比如：sed ‘2i new_line’ test,<br>sed ‘2a new_line’ test, sed ‘2a new_line’ test. 以上各自是在test文件的第二行之前<br>插入new_line这行，在test文件第二行之后插入new_line这行，直接把test文件的第二行改成<br>new_line这行。</p>
<p>y是变换命令。可以完成类似文本加密的工作, 当然这里说的加密只是一对一的做字符替换,<br>比如：sed ‘y/123/579/‘ test, 可以把test中所有的字符1,2,3依次替换成5,7,9</p>
<p>之前提到flag p, p用于输出指定的值，一般都结合-n使用，-n用于禁止输出，这样就可以<br>控制输出想要输出的东西了，比如：sed -n  ‘/wang/p’ test, sed -n ‘2,3p’ test<br>输出有’wang’字符的行，输出第2行和第三行。</p>
<p>之前也提到了flag w, w用于把改动写入到文件, 比如：sed ‘1,3w new_file’ test,<br>sed ‘/wang/w new_file’ test, 把test文件的1~3行写入到new_file中, 把test文件中<br>含有’wang’的一行写入到new_file文件中。</p>
<p>和w相对的还有读命令，比如： sed ‘3r new’ file, 读出new文件中的内容然后加到file<br>文件的第三行之后。当然也可以 sed ‘/wang/r new’ file, 读出new文件中的内容，加到file<br>文件含有’wang’这一行之后。</p>
]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu包管理笔记</title>
    <url>/2021/07/17/ubuntu%E5%8C%85%E7%AE%A1%E7%90%86%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p> ubuntu是linux系统的一个发行版，其实在debian上修改而来。最开始debian的软<br> 件包管理命令是dpkg, 这个命令可以安装单个软件包，也可以对现有的软件包做<br> 一些查找之类的工作。但是dpkg命令只能处理单个deb包的情况，对于安装的软件<br> 要依赖其他包的情况debian开发了apt工具。ubuntu的包管理也是使用apt, ubuntu<br> 在全球范围内维护很多个软件仓库，这些仓库中的软件包都是一样的，使用apt命令<br> 的时候就是从这些软件仓库的其中之一去下载软件包并安装，这些软件仓库的URL保<br> 存在本地ubuntu的/etc/apt/source.list中</p>
<p>下面以例子的形式介绍具体的用法：</p>
<ol>
<li><p>下载自己ubuntu上的ls的源代码:<br>which ls 得到ls命令对应的二进制文件的路径: /bin/ls<br>dpkg -S /bin/ls 查找是什么deb包包含/bin/ls, 若只用ls会有很多无用的查找结果</p>
<pre><code>            该命令的到的结果: coreutils: /bin/ls
</code></pre>
<p>sudo apt-get search coreutils 下载coreutils包的源代码</p>
</li>
<li><p>sudo apt-get update 更新软件仓库</p>
</li>
<li><p>sudo apt-get upgrade 更新已经安装的软件到最新的版本</p>
</li>
<li><p>dpkg -i ***.deb 安装一个deb包到系统</p>
</li>
<li><p>sudo apt-get install *** 安装软件***，如果有依赖的包没有，其会自动下载需要依<br>赖的包，然后安装软件</p>
</li>
<li><p>sudo apt-get remove *** 卸载软件***</p>
</li>
<li><p>升级ubuntu内核：ubuntu的内核差最新的内核比较远，怎么更新到最新的内核?<br><span class="exturl" data-url="aHR0cDovL2tlcm5lbC51YnVudHUuY29tL35rZXJuZWwtcHBhL21haW5saW5lLw==">http://kernel.ubuntu.com/~kernel-ppa/mainline/<i class="fa fa-external-link-alt"></i></span> 有各个kernel版本的deb包, 选择<br>喜欢的一个进去，作为例子这里选v3.13-rc1-trusty<br>如果pc是64位的，下载linux-image-***amd64.deb, linux-headers-***amd64.deb<br>linux-header-***all.deb三个deb包<br>dpkg -i ***.deb分别安装这三个包，然后sudo update_grub更新grub<br>重启电脑后会发现kernel已经更新到3.13</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>包管理</tag>
        <tag>apt/dpkg</tag>
      </tags>
  </entry>
  <entry>
    <title>tmux tips</title>
    <url>/2021/07/05/tmux-tips/</url>
    <content><![CDATA[<p>There are three basic concepts in tmux: pane, window, session.<br>After you run tmux in shell, you open a pane in a window in a session.</p>
<p>Let’s first consider the pane operation in one window:</p>
<pre><code>ctl + B + %: splite current pane into two, left and right
ctl + B + &quot;: splite current pane into two, up and down
ctl + B + z: full screen for current pane, or return back.
</code></pre>
<p>If you want multiple window:</p>
<pre><code>ctl + B + c: create a window
</code></pre>
<p>If you login your linux system, you can:</p>
<pre><code>tmux ls: to see the sessions    
tmux a -t session_name(or session number): to attach to related session
</code></pre>
<p>If you want to copy log in tmux to a file:</p>
<pre><code>ctl + b + :
capture-pane -S -3000 + return  this copied 3000 lines into buffer
ctl + b + :
save-buffer /path/to/your_file  this copied content in buffer to file,
                                in my environment, path should be a full path.
</code></pre>
]]></content>
      <tags>
        <tag>开发工具</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>strace使用笔记</title>
    <url>/2021/07/17/strace%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="基本功能："><a href="#基本功能：" class="headerlink" title="基本功能："></a>基本功能：</h1><ol>
<li><p>strace ./a.out 依次显示各个系统调用</p>
</li>
<li><p>strace -c ./a.out 可以打印出a.out中各个系统调用的使用次数和时间等信息</p>
</li>
<li><p>strace -ff -o strace_log ./a.out 可以把输出的结果存在strace_log.<pid>中</p>
</li>
<li><p>strace -f 可以在fork之后跟踪各进程内的系统调用</p>
</li>
<li><p>strace -i ls 显示系统调用时的instruction pointer</p>
</li>
<li><p>strace -r|-t|-tt|-T 现实相对时间戳、绝对时间、微秒时间和系统调用消耗时间</p>
</li>
<li><p>strace -e 可以跟踪指定的事件(strace -e open ./a.out)</p>
</li>
<li><p>strace -p <pid> 可以对指定进程进行跟踪</p>
</li>
<li><p>strace -O &lt;<strong>ms&gt; 可以把</strong>ms的时间认为是strace的开销，strace在统计时间的时<br>  候会减去**ms时间，这个时间一般是由程序运行时间和strace跟踪情况下程序的运<br>  行时间相减得到, 是个经验值</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>软件调试</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu使用LDAP做用户管理</title>
    <url>/2021/07/11/ubuntu%E4%BD%BF%E7%94%A8LDAP%E5%81%9A%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<p>The basic view about centralized authentication using LDAP are as below:</p>
<ol>
<li>We need a server(A) to install LDAP server which has the accounts’<br>information and deals with authentication requests.</li>
<li>We need to install LDAP client in another server(B) which you want to login.</li>
<li>You can login server B after adding relative account in LDAP server. Or if<br>you already have some accounts information, you can login from another<br>server using LDAP server.</li>
</ol>
<h2 id="Configure-LDAP-server"><a href="#Configure-LDAP-server" class="headerlink" title="Configure LDAP server"></a>Configure LDAP server</h2><ol>
<li><p>sudo apt-get install slapd ldap-utils<br>During the install process, it needs to fill in some basic configurations.<br>You can also use command below to change them.</p>
</li>
<li><p>sudo dpkg-reconfigure slapd<br>Refer to [1] to see how to configure.</p>
</li>
<li><p>Add groups, users.<br>You can use ldif file to do this, another way is to use ldapscripts.<br>(1) ldif way</p>
<pre><code>a. Edit ldif file as below: vi add_content.ldif
...
b. Add users, groups, users&#39; directory and groups&#39; directory as below;
   ldapadd -x -D cn=admin, dc=company, dc=com -W -f add_content.ldif

it will appear:

Enter LDAP Password:
adding new entry &quot;ou=User,dc=company,dc=com&quot;
...

NOTE: if failed there, mostly you should check basic slapd configure.

c. ldapdelete -x -D &quot;cn=admin,dc=company,dc=com&quot; -W &quot;uid=test,ou=User,dc=company&quot;
   Using above command to delete one entry.
</code></pre>
<p>   ldapsearch -x -LLL -b dc=company,dc=com</p>
<pre><code>   Using above command to search information of all entries.
</code></pre>
<p>(2) ldapscripts way</p>
<pre><code>a. sudo apt-get install ldapscripts

b. Configure /etc/ldapscripts/ldapscripts.conf:
   SERVER=localhost
</code></pre>
<p>   SUFFIX=”dc=company,dc=com”<br>   GSUFFIX=”ou=Groups”<br>   USUFFIX=”ou=Users”<br>   BINDDN=”cn=admin,dc=company,dc=com”</p>
<pre><code>   BINDPWDFILE=&quot;/etc/ldapscripts/ldapscripts.passwd&quot;

c. echo -n &quot;your_root_passwd_for_ldap&quot; &gt; /etc/ldapscripts/ldapscripts.passwd

d. User relative command to add group, user: ldapadduser, ldapaddgroup

NOTE: We can add an account by ldapadduser account_name group_name.
      e.g. ldapadduser test User
  Use c step above to write password to ldapscripts.passwd. This
  will *NOT* write &quot;\n&quot; at the end of password line.
</code></pre>
</li>
</ol>
<h2 id="Configure-LDAP-client-2"><a href="#Configure-LDAP-client-2" class="headerlink" title="Configure LDAP client[2]"></a>Configure LDAP client[2]</h2><ol>
<li><p>sudo apt-get install libpam-ldap nscd<br>When installing libpam-ldap, it will ask you to configure LDAP client during<br>the install process. You can also change the configuration as below.</p>
</li>
<li><p>sudo apt-get install ldap-auth-config<br>sudo dpkg-reconfigure ldap-auth-config[2]</p>
</li>
<li><p>Modify /etc/nsswitch.conf to choose how to make authentication:</p>
<pre><code>passwd: files ldap
group: files ldap
shadow: files ldap
</code></pre>
<p>NOTE: You’d better put “filles” befort “ldap”</p>
</li>
<li><p>Build home directory automatically in LDAP client[3]<br>add line: session required pam_mkhomedir.so skel=/etc/skel/ umask=0022<br>to /etc/pam.d/common-account</p>
</li>
</ol>
<h2 id="Migrate-accounts-information"><a href="#Migrate-accounts-information" class="headerlink" title="Migrate accounts information"></a>Migrate accounts information</h2><p>If you already have users information in /etc/passwd, /etc/group, /etc/shadow,<br>and you want to use LDAP manage users information, you can do as below:</p>
<ol>
<li><p>sudo apt-get install migrationtools</p>
</li>
<li><p>Modify /etc/migrationtools/migrate_common.ph:<br>   $DEFAULT_BASE = “dc=company,dc=com”;</p>
</li>
<li><p>/usr/share/migrationtools/migrate_passwd.pl /etc/passwd add_people.ldif</p>
</li>
<li><p>Modify add_people.ldif:<br>Change the information about group for every user.</p>
<p>Copy the encrypted pass word in /etc/shadow to replace “x” in “userPassword: {crypt}x”<br>attribution in add_people.ldif</p>
<p>FIXME…</p>
</li>
<li><p>ldapadd -x -D cn=admin,dc=company,dc=com -W -f add_people.ldif</p>
</li>
</ol>
<p>NOTE: I just need the password information here. If you want login using LDAP,<br>      you should also consider to migrate group information in /etc/group</p>
<h2 id="Backup-existed-LDAP-date"><a href="#Backup-existed-LDAP-date" class="headerlink" title="Backup existed LDAP date"></a>Backup existed LDAP date</h2><p>…</p>
<p>Reference:<br>[1] <span class="exturl" data-url="aHR0cHM6Ly93d3cuZGlnaXRhbG9jZWFuLmNvbS9jb21tdW5pdHkvdHV0b3JpYWxzL2hvdy10by1pbnN0YWxsLWFuZC1jb25maWd1cmUtYS1iYXNpYy1sZGFwLXNlcnZlci1vbi1hbi11YnVudHUtMTItMDQtdnBz">https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-a-basic-ldap-server-on-an-ubuntu-12-04-vps<i class="fa fa-external-link-alt"></i></span><br>[2] <span class="exturl" data-url="aHR0cHM6Ly93d3cuZGlnaXRhbG9jZWFuLmNvbS9jb21tdW5pdHkvdHV0b3JpYWxzL2hvdy10by1hdXRoZW50aWNhdGUtY2xpZW50LWNvbXB1dGVycy11c2luZy1sZGFwLW9uLWFuLXVidW50dS0xMi0wNC12cHM=">https://www.digitalocean.com/community/tutorials/how-to-authenticate-client-computers-using-ldap-on-an-ubuntu-12-04-vps<i class="fa fa-external-link-alt"></i></span><br>[3] <span class="exturl" data-url="aHR0cDovL3d3dy5kZWJpYW4tYWRtaW5pc3RyYXRpb24ub3JnL2FydGljbGUvNDAzL0dpdmluZ191c2Vyc19hX2hvbWVfZGlyZWN0b3J5X2F1dG9tYXRpY2FsbHk=">http://www.debian-administration.org/article/403/Giving_users_a_home_directory_automatically<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>运维</tag>
        <tag>LDAP</tag>
      </tags>
  </entry>
  <entry>
    <title>uthash和glib hash</title>
    <url>/2021/07/23/uthash%E5%92%8Cglib-hash/</url>
    <content><![CDATA[<h2 id="hash"><a href="#hash" class="headerlink" title="hash"></a>hash</h2><p> 哈希表是一种这样的数据结构，它以key-value的形式把value存储到哈希表里。用户可以<br> 通过一组接口做增删改查的操作。</p>
<h2 id="uthash"><a href="#uthash" class="headerlink" title="uthash"></a>uthash</h2><p> uthash的介绍在这里: <span class="exturl" data-url="aHR0cHM6Ly90cm95ZGhhbnNvbi5naXRodWIuaW8vdXRoYXNoLw==">https://troydhanson.github.io/uthash/<i class="fa fa-external-link-alt"></i></span><br> 它是一个用宏写的哈希表，使用的时候只要include uthash.h就好，所有信息都在这个文件<br> 里了。uthash的代码里附带了一个example.c的使用示例，我们简单看下这个文件，主要是<br> 注意它使用时候的一些坑。目前还没有发现在哪里有使用uthash。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;string.h&gt;</span><br><span class="line">#include &lt;assert.h&gt;</span><br><span class="line">#include &quot;./uthash.h&quot;</span><br><span class="line"></span><br><span class="line">struct my_struct &#123;</span><br><span class="line">    int id;                    /* 这个id后面我们用来做my_struct的索引 */</span><br><span class="line">    char name[10];</span><br><span class="line">    UT_hash_handle hh;         /* 要hash的数据结构里必须放一个这个句柄，必须写成hh */</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">/* 定义hash表的表头，需要初始化成NULL */</span><br><span class="line">struct my_struct *users = NULL;</span><br><span class="line"></span><br><span class="line">/* 所有的uthash操作都是HASH_xx的定义，我们这里封装一层函数 */</span><br><span class="line">void add_user(int user_id, char *name)</span><br><span class="line">&#123;</span><br><span class="line">    struct my_struct *s;</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * 查看user_id为key的数据是否存在，返回数据的指针，s为NULL，数据不存在。</span><br><span class="line">     * 注意，user_id就是key的值，这个接口实现的比较特别，单通过这个接口内部的</span><br><span class="line">     * 实现根本不知道内部的那个数据是key。</span><br><span class="line">     *</span><br><span class="line">     * 这个接口要配合下面的HASH_ADD_INT来用。这个接口的语义是：</span><br><span class="line">     * 用s里的id为key插入s到users，这里这个id表示的是struct my_struct里的id这个</span><br><span class="line">     * 参数名字，所以一定要写的和struct my_struct里的id一样，本质上是一个字符。</span><br><span class="line">     *</span><br><span class="line">     * HASH_FIND_INT也是根据HASH_ADD_INT里的id知道key是s里的哪个元素。</span><br><span class="line">     */</span><br><span class="line">    HASH_FIND_INT(users, &amp;user_id, s);</span><br><span class="line">    if (s == NULL) &#123;</span><br><span class="line">        s = (struct my_struct*)malloc(sizeof(struct my_struct));</span><br><span class="line">        s-&gt;id = user_id;</span><br><span class="line">        HASH_ADD_INT(users, id, s);  /* 用s里的id为key插入s到users */</span><br><span class="line">    &#125;</span><br><span class="line">    strcpy(s-&gt;name, name);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">struct my_struct *find_user(int user_id)</span><br><span class="line">&#123;</span><br><span class="line">    struct my_struct *s;</span><br><span class="line"></span><br><span class="line">    HASH_FIND_INT(users, &amp;user_id, s);</span><br><span class="line">    return s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void delete_user(struct my_struct *user)</span><br><span class="line">&#123;</span><br><span class="line">    HASH_DEL(users, user);  /* 直接指向数据的指针, 用这个作为删除的标记 */</span><br><span class="line">    free(user);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void delete_all()</span><br><span class="line">&#123;</span><br><span class="line">    struct my_struct *current_user, *tmp;</span><br><span class="line"></span><br><span class="line">    /* 遍历哈希表中的每个元素 */</span><br><span class="line">    HASH_ITER(hh, users, current_user, tmp) &#123;</span><br><span class="line">        HASH_DEL(users, current_user);</span><br><span class="line">        free(current_user);  /* 删除操作并不影响数据内存，需要用户显示释放数据内存 */</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void print_users()</span><br><span class="line">&#123;</span><br><span class="line">    struct my_struct *s;</span><br><span class="line"></span><br><span class="line">    /* 也可以用这种直白的方式遍历哈希表，但是这个相当于知道了hh的内部数据，最好不要这样 */</span><br><span class="line">    for(s=users; s != NULL; s=(struct my_struct*)(s-&gt;hh.next)) &#123;</span><br><span class="line">        printf(&quot;user id %d: name %s\n&quot;, s-&gt;id, s-&gt;name);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int name_sort(struct my_struct *a, struct my_struct *b)</span><br><span class="line">&#123;</span><br><span class="line">    return strcmp(a-&gt;name, b-&gt;name);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int id_sort(struct my_struct *a, struct my_struct *b)</span><br><span class="line">&#123;</span><br><span class="line">    return (a-&gt;id - b-&gt;id);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void sort_by_name()</span><br><span class="line">&#123;</span><br><span class="line">    /* 还支持对哈希表里数据排序 */</span><br><span class="line">    HASH_SORT(users, name_sort);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void sort_by_id()</span><br><span class="line">&#123;</span><br><span class="line">    HASH_SORT(users, id_sort);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	struct my_struct *tmp;</span><br><span class="line">	int num;</span><br><span class="line"></span><br><span class="line">	add_user(5, &quot;wang&quot;);</span><br><span class="line">	add_user(1, &quot;zheng&quot;);</span><br><span class="line">	add_user(4, &quot;xu&quot;);</span><br><span class="line">	add_user(3, &quot;fang&quot;);</span><br><span class="line">	add_user(2, &quot;huang&quot;);</span><br><span class="line"></span><br><span class="line">	print_users();</span><br><span class="line"></span><br><span class="line">	printf(&quot;\n&quot;);</span><br><span class="line">	sort_by_id();</span><br><span class="line">	print_users();</span><br><span class="line">	/*</span><br><span class="line">	 * sort_by_id()打印出来的结果是这样的：</span><br><span class="line">	 *</span><br><span class="line">	 * user id 1: name zheng</span><br><span class="line">	 * user id 2: name huang</span><br><span class="line">	 * user id 3: name fang</span><br><span class="line">	 * user id 4: name xu</span><br><span class="line">	 * user id 5: name wang</span><br><span class="line">	 */</span><br><span class="line"></span><br><span class="line">	printf(&quot;\n&quot;);</span><br><span class="line">	sort_by_name();</span><br><span class="line">	print_users();</span><br><span class="line">	/*</span><br><span class="line">	 * sort_by_name()打印出来的结果是这样的：</span><br><span class="line">	 *</span><br><span class="line">	 * user id 3: name fang</span><br><span class="line">	 * user id 2: name huang</span><br><span class="line">	 * user id 5: name wang</span><br><span class="line">	 * user id 4: name xu</span><br><span class="line">	 * user id 1: name zheng</span><br><span class="line">	 */</span><br><span class="line"></span><br><span class="line">	printf(&quot;\n&quot;);</span><br><span class="line">	tmp = find_user(4);</span><br><span class="line">	/*</span><br><span class="line">	 * 非常别扭的删除接口，尽然不能用key作为索引直接调删除接口，还要先用key找见元素</span><br><span class="line">	 * 的指针，然后再删除。</span><br><span class="line">	 */</span><br><span class="line">	delete_user(tmp);</span><br><span class="line">	print_users();</span><br><span class="line"></span><br><span class="line">	printf(&quot;\n&quot;);</span><br><span class="line">	/* 统计哈希表里有多少个元素 */</span><br><span class="line">	num = HASH_COUNT(users);</span><br><span class="line">	printf(&quot;there is %d elements\n&quot;, num);</span><br><span class="line"></span><br><span class="line">	printf(&quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">	/* 释放哈希表 */</span><br><span class="line">	HASH_CLEAR(hh, users);</span><br><span class="line">	assert(!users);</span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="glib-hash"><a href="#glib-hash" class="headerlink" title="glib hash"></a>glib hash</h2><p>  glib是GNOME lib, 这个库提供了各种基本的数据结构，使用比较广泛。我们这里主要看<br>  glib中提供的哈希表相关接口的使用。QEMU的代码使用了glib，我们这里的介绍也截取<br>  了部分QEMU里和哈希表有关的东西。</p>
<p>  下面的测试在ubuntu20.04(aarch64)上，安装glib库和编译测试代码的命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libglib2.0-dev</span><br><span class="line">gcc test.c `pkg-config --cflags --libs glib-2.0`</span><br></pre></td></tr></table></figure>
<p>  下面把介绍直接写到代码的注释说明里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;glib.h&gt;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdint.h&gt;</span><br><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">#include &lt;malloc.h&gt;</span><br><span class="line"></span><br><span class="line">/* copy from qemu code */</span><br><span class="line">#define PRIME32_1   2654435761U</span><br><span class="line">#define PRIME32_2   2246822519U</span><br><span class="line">#define PRIME32_3   3266489917U</span><br><span class="line">#define PRIME32_4    668265263U</span><br><span class="line">#define PRIME32_5    374761393U</span><br><span class="line">#define QEMU_XXHASH_SEED 1</span><br><span class="line"></span><br><span class="line">static inline uint32_t rol32(uint32_t word, unsigned int shift)</span><br><span class="line">&#123;</span><br><span class="line">    return (word &lt;&lt; shift) | (word &gt;&gt; ((32 - shift) &amp; 31));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static inline uint32_t</span><br><span class="line">qemu_xxhash7(uint64_t ab, uint64_t cd, uint32_t e, uint32_t f, uint32_t g)</span><br><span class="line">&#123;</span><br><span class="line">    uint32_t v1 = QEMU_XXHASH_SEED + PRIME32_1 + PRIME32_2;</span><br><span class="line">    uint32_t v2 = QEMU_XXHASH_SEED + PRIME32_2;</span><br><span class="line">    uint32_t v3 = QEMU_XXHASH_SEED + 0;</span><br><span class="line">    uint32_t v4 = QEMU_XXHASH_SEED - PRIME32_1;</span><br><span class="line">    uint32_t a = ab;</span><br><span class="line">    uint32_t b = ab &gt;&gt; 32;</span><br><span class="line">    uint32_t c = cd;</span><br><span class="line">    uint32_t d = cd &gt;&gt; 32;</span><br><span class="line">    uint32_t h32;</span><br><span class="line"></span><br><span class="line">    v1 += a * PRIME32_2;</span><br><span class="line">    v1 = rol32(v1, 13);</span><br><span class="line">    v1 *= PRIME32_1;</span><br><span class="line"></span><br><span class="line">    v2 += b * PRIME32_2;</span><br><span class="line">    v2 = rol32(v2, 13);</span><br><span class="line">    v2 *= PRIME32_1;</span><br><span class="line"></span><br><span class="line">    v3 += c * PRIME32_2;</span><br><span class="line">    v3 = rol32(v3, 13);</span><br><span class="line">    v3 *= PRIME32_1;</span><br><span class="line"></span><br><span class="line">    v4 += d * PRIME32_2;</span><br><span class="line">    v4 = rol32(v4, 13);</span><br><span class="line">    v4 *= PRIME32_1;</span><br><span class="line"></span><br><span class="line">    h32 = rol32(v1, 1) + rol32(v2, 7) + rol32(v3, 12) + rol32(v4, 18);</span><br><span class="line">    h32 += 28;</span><br><span class="line"></span><br><span class="line">    h32 += e * PRIME32_3;</span><br><span class="line">    h32  = rol32(h32, 17) * PRIME32_4;</span><br><span class="line"></span><br><span class="line">    h32 += f * PRIME32_3;</span><br><span class="line">    h32  = rol32(h32, 17) * PRIME32_4;</span><br><span class="line"></span><br><span class="line">    h32 += g * PRIME32_3;</span><br><span class="line">    h32  = rol32(h32, 17) * PRIME32_4;</span><br><span class="line"></span><br><span class="line">    h32 ^= h32 &gt;&gt; 15;</span><br><span class="line">    h32 *= PRIME32_2;</span><br><span class="line">    h32 ^= h32 &gt;&gt; 13;</span><br><span class="line">    h32 *= PRIME32_3;</span><br><span class="line">    h32 ^= h32 &gt;&gt; 16;</span><br><span class="line"></span><br><span class="line">    return h32;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">typedef struct key &#123;</span><br><span class="line">    int bus;</span><br><span class="line">    int devfn;</span><br><span class="line">&#125; key;</span><br><span class="line"></span><br><span class="line">typedef struct value &#123;</span><br><span class="line">    int base;</span><br><span class="line">    int bar;</span><br><span class="line">&#125; value;</span><br><span class="line"></span><br><span class="line">static guint key_hash(gconstpointer v)</span><br><span class="line">&#123;</span><br><span class="line">    key *k = (key *)v;</span><br><span class="line"></span><br><span class="line">    return qemu_xxhash7((uint64_t)k-&gt;bus, k-&gt;devfn, 0, 0, 0);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static gboolean key_equal(gconstpointer v1, gconstpointer v2)</span><br><span class="line">&#123;</span><br><span class="line">    key *k1 = (key *)v1;</span><br><span class="line">    key *k2 = (key *)v2;</span><br><span class="line"></span><br><span class="line">    return (k1-&gt;bus == k2-&gt;bus) &amp;&amp; (k1-&gt;devfn == k2-&gt;devfn);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	/* 表示一个哈希表的具柄 */</span><br><span class="line">    	GHashTable *hash_table;</span><br><span class="line">	/* 哈希表的key是可以自定义的, 可以把用到的参数封装到一个struct里，把这个struct作为key */</span><br><span class="line">	value v, *p_v;</span><br><span class="line">	key k, *p_k;</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * 初始化哈希表具柄, 函数的定义在gnome的官网都可以查询：</span><br><span class="line">	 * https://developer.gnome.org/glib/stable/glib-Hash-Tables.html</span><br><span class="line">	 *</span><br><span class="line">	 * ghash是要建立一个key struct到一个value struct的映射，所以下面的key_hash,</span><br><span class="line">	 * key_equal这两个函数就比较容易理解。</span><br><span class="line">	 *</span><br><span class="line">	 * key_hash的输入是用户自定义的key struct，输出是一个hash值，ghash真正用</span><br><span class="line">	 * 计算得到的这个hash值作为key。可以看到这个计算是一个数学问题，QEMU里直接</span><br><span class="line">	 * 把Jenkins hash, xxhash的代码放到了QEMU的代码里，我们这里也照搬过来，</span><br><span class="line">	 * 上面直接copy的是xxhash的部分代码，他完成的功能比较直白，就是输入一组</span><br><span class="line">	 * 数，然后按照一定的算法输出一个哈希值。这里的key_hash就是直接调用xxhash</span><br><span class="line">	 * 的函数。</span><br><span class="line">	 *</span><br><span class="line">	 * key_equal是判断两个key相等的函数，一般就是key里面的每一个元素都相等就</span><br><span class="line">	 * 认为两个key相等。</span><br><span class="line">	 *</span><br><span class="line">	 * 后面的两个函数是key和value的销毁函数，一般是g_free。注意，如果不是动态</span><br><span class="line">	 * 创建的结构就不需要配置这里的销毁函数。</span><br><span class="line">	 */</span><br><span class="line">    	hash_table = g_hash_table_new_full(key_hash, key_equal, g_free, g_free);</span><br><span class="line"></span><br><span class="line">	p_v = malloc(sizeof(v)); p_v-&gt;base = 1; p_v-&gt;bar = 3;</span><br><span class="line">	p_k = malloc(sizeof(k)); p_k-&gt;bus = 0x10; p_k-&gt;devfn = 0x75;</span><br><span class="line"></span><br><span class="line">	/* 把一个key-value的map插入到哈希表里，如上，key, value这里是需要动态分配的结构 */</span><br><span class="line">	g_hash_table_insert(hash_table, p_k, p_v);</span><br><span class="line"></span><br><span class="line">	k.bus = 0x10; k.devfn = 0x75;</span><br><span class="line">	/* 找一个key对应的value, 这时key可以是静态的结构 */</span><br><span class="line">	p_v = g_hash_table_lookup(hash_table, &amp;k);</span><br><span class="line">	if (p_v)</span><br><span class="line">		printf(&quot;found!\n&quot;);</span><br><span class="line">	else</span><br><span class="line">		printf(&quot;not found!\n&quot;);</span><br><span class="line"></span><br><span class="line">	/* 销毁哈希表 */</span><br><span class="line">	g_hash_table_remove_all(hash_table);</span><br><span class="line">	</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  简单起见也可以用glib提供的key_hash和key_equal函数。比如，如果用一个int值作为key,<br>  就可以用g_direct_hash/g_direct_equal:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">h = g_hash_table_new(g_direct_hash, g_direct_equal);</span><br><span class="line">g_hash_table_lookup(h, GINT_TO_POINTER(int_key));</span><br><span class="line">g_hash_table_insert(h, GINT_TO_POINTER(int_key), value);</span><br></pre></td></tr></table></figure>
<p>  如上direct的方式是直接用key为形参做索引的。g_direct_hash的实现是把输入强转成int<br>  作为key。但是使用g_int_hash/g_int_equal时，q_int_hash把输入转成int指针然后取其中<br>  的内容，可见这个时候lookup/insert的输入应该是int型key的地址。</p>
]]></content>
      <tags>
        <tag>Linux用户态编程</tag>
        <tag>glib</tag>
        <tag>hash</tag>
      </tags>
  </entry>
  <entry>
    <title>vSVA逻辑分析</title>
    <url>/2021/06/21/vSVA%E9%80%BB%E8%BE%91%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol start="0">
<li>qemu基础认识</li>
</ol>
<hr>
<p> qemu里linux系统的用户态代码跑在cpu EL0, qemu里linux系统的内核态代码跑在cpu EL1。</p>
<p> qemu创建虚拟机的时候ioctl(CREATE_VM，VCPU，MEMORY)会到cpu EL2创建VM的记录信息。<br> ioctl(VM_RUN)会把PC指向虚拟机起始地址，然后退出到EL1。EL2只起到虚拟机管理的作用，<br> 虚拟机上的代码正常运行时，代码占据真实的CPU，并且如果是用户态代码，跑在物理CPU<br> 的EL0, 如果是内核态代码就直接跑到物理CPU的EL1。当CPU访问物理内存的时候，VA-&gt;IPA<br> 的转换由MMU S1直接支持, 当IPA的地址落在之前ioctl注册的虚拟机地址空间时，硬件自动<br> 完成MMU S2的转换。可见，虚拟机里的进程页表是直接放到虚拟机内核地址空间的。</p>
<p> 虚拟机里的代码运行在CPU EL0/EL1。当有IO访问的时候, 因为之前创建虚拟机的时候<br> 已经把IO地址空间配置给虚拟机，这里有IO访问的时候会触发CPU异常，虚拟机退出，CPU<br> 进入EL2, CPU在EL2处理后退出到虚拟机qemu里，qemu可以具体去处理这个IO，比如是一个<br> 网络IO，那qemu可以直接起socket，把报文发出去。注意，这里的虚拟机退出是指CPU不再<br> 运行虚拟机里执行的代码，因为CPU并不知道如果控制IO。</p>
<ol>
<li>vSVA</li>
</ol>
<hr>
<p> vSVA的目标是在虚拟机里(qemu)，使的IO设备可以直接使用进程VA。所以，我们这里的<br> 假设是物理IO设备已经通过host上vfio驱动直通给虚拟机。</p>
<p> 要实现vSVA的目标，我们需要同时使能SMMU的S1,S2地址翻译，S1进行VA-&gt;IPA翻译，S2<br> 进行IPA-&gt;PA翻译，如果是host vfio使能，我们认为S2的翻译已经通过vfio配置在SMMU里。</p>
<p> 所以，vSVA的方案需要把虚拟机系统里的进程页表同步到host SMMU上。因为是vSVA，就<br> 有可能出现设备发起内存访问的时候，host SMMU上虚拟机里的进程页表项不存在的情况，<br> 所以，host上的SMMU要可以支持S1缺页。因为，S2用vfio支持，vfio采用pin内存的方式，<br> 暂时我们不需要S2的缺页。这里说的host上SMMU支持S1缺页，并不是在host系统上做S1缺页，<br> 我们这里讨论的是nested SMMU, 所以在host SMMU硬件检测到S1缺页的时候，应该把这个<br> 信息上报给guest里的SMMU，guest里使用和host一样的SMMU驱动处理缺页，当guest处理完<br> 这个缺页后，应该把对应的页表信息同步到SMMU的物理硬件上(SMMU.CD.TT0里)。因为，<br> guest里的进程页表和SMMU CD上的页表物理上不是一个，很明显这里有一个设备和vcpu页<br> 表的同步问题，在host SVA上这个问题不存在，因为host SVA上cpu和SMMU是物理上共用<br> 相同页表。因此，在需要在vcpu无效化页表的时候，需要把信息同步到host的SMMU上，<br> 这个信息包括页表项和TLB。host SVA上也有这个问题，但是如果用SMMU stall mode, 可<br> 以配置DVM，把CPU侧TLB invalidate广播到SMMU，这样就不需要软件同步。</p>
<p> 在guest里多进程使用一个设备的资源，就需要支持PASID。这里的逻辑和上面的是一样的，<br> 只不过扩展到多进程。</p>
<ol start="2">
<li>软件框架</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  +----------------------+</span><br><span class="line">  | guest           user |</span><br><span class="line">  |                      |</span><br><span class="line">  |                      |</span><br><span class="line">  |                      |</span><br><span class="line">  |----------------------|   --------------------    VA</span><br><span class="line">  |               kernel |                                 +------------+</span><br><span class="line">  |                      |                                 | page table |</span><br><span class="line">  |                      |                                 +------------+</span><br><span class="line">  |                      |                                         ^</span><br><span class="line">  +----------------------+                                         |</span><br><span class="line">  +----------------------+         --------------------  IPA       |</span><br><span class="line">  | host                 |                                         |</span><br><span class="line">  |                      |                                         |</span><br><span class="line">  |                      |             +---------+                 |</span><br><span class="line">  |                      |             | DDR     |        PA       |</span><br><span class="line">  |                      |             +---------+                 |</span><br><span class="line">  |                      |                                         |</span><br><span class="line">  |                      |                                         |</span><br><span class="line">  |                      |                                         |</span><br><span class="line">  |                      |                                         |</span><br><span class="line">  +----------------------+                                         |</span><br><span class="line">          |                                                        |</span><br><span class="line">          |                   +-----+                              |</span><br><span class="line">          |          +------&gt; | S1  |  VA-&gt;IPA  &lt;------------------+</span><br><span class="line">       +--+---+ -----+        +-----+                  </span><br><span class="line">| SMMU |</span><br><span class="line">       +------+ -----+        +-----+</span><br><span class="line">          ^          +------&gt; | S2  |  IPA-&gt;PA</span><br><span class="line">          |                   +-----+</span><br><span class="line">       +-----+</span><br><span class="line">| dev |</span><br><span class="line">       +-----+</span><br></pre></td></tr></table></figure>
<p> 我们顺着具体的数据流看看需要的接口，在dev的控制寄存器被map到guest的用户态后，<br> 用户态可以直接给guest VA配置给dev，启动dev从VA处读写数据。dev发出的访问到达<br> SMMU后首先要进过S1的翻译，得到IPA，所以S1需要guest里的进程的页表。</p>
<p> 目前Redhat的Eric在做ARM nested SMMU的支持，他把相关的补丁集合到了他的分支里，<br> 你可以在这个地方看到完整的内核补丁：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2VhdWdlci9saW51eA==">https://github.com/eauger/linux<i class="fa fa-external-link-alt"></i></span> branch:<br> v5.6-2stage-v11_10.1。这组补丁里给vfio加了一个ioctl(VFIO_IOMMU_SET_PASID_TABLE)，<br> 用这个ioctl把虚拟机里的SMMU的CD地址(IPA)直接传给host，并且配置给物理SMMU的CD<br> 基地址。对于预先在vcpu一侧有缺页的情况，这里S1可以查页表翻译，SMMU硬件在nested<br> 模式下，会对CD基地址做S2翻译的到CD的真正物理地址，然后找见页表做翻译。可见qemu<br> 里的SMMU驱动使用和host SMMU相同的驱动，初始化qemu里SMMU的CD.TT0, 然后把CD直接<br> 通过系统调用配置到物理SMMU上。需要注意，这里CD里的页表基地址是IPA，SMMU硬件<br> 会先根据S2页表翻译IPA到PA得到页表物理基地址。</p>
<p> 对于dev传给SMMU的VA没有页表的情况, S1要做缺页处理。这里的缺页处理在逻辑上应该<br> 上报给guest，因为要做vSVA，是要给虚拟机里的进程的页表加页表项。Eric这组补丁里，<br> 在vfio里加了一个event queue的队列，mmap到host用户态，用来传递这个信息。逻辑上看，<br> qemu应该处理并上报这个缺页请求，qemu里的SMMU驱动做缺页处理。在qemu的SMMU驱动做<br> 缺页处理的时候，来自dev的请求是stall在SMMU里的，所以，SMMU缺页处理完毕后，应该<br> 有通知机制通知到host SMMU，使能stall的请求继续。</p>
<p> 可以看到当页表有变动的时候，在guest和物理SMMU上同步页表的开销是很大的。</p>
<p> 当guest里的进程有退出或者内存有释放时，需要更新guest里进程的页表，vcpu tlb，<br> host SMMU上相关进程页表和tlb。Eric补丁里vfio里提供了ioctl(VFIO_IOMMU_CACHE_INVALIDATE)<br> 用来更新host SMMU上的相关tlb。这里vcpu可以做带VMID/ASID的DVM, 直接无效化相关的tlb。</p>
<ol start="3">
<li>virtio iommu</li>
</ol>
<hr>
<p> 以上的分析都是基于nested IOMMU/SMMU的方案。目前Jean在做virtio iommu的方案。<br> 这个方案在qemu里实现一个virtio iommu的虚拟设备qemu/hw/virtio/virtio-iommu.c,<br> 虚拟机内核里的drivers/iommu/virtio-iommu.c驱动这个虚拟设备，现在看来这个是<br> 用纯软件实现VA-&gt;IPA的映射。</p>
<p> 基于以上的分析，可以基于vfio接口在virtio iommu里实现有物理SMMU支持的virtio-iommu。<br> 但是，这个需要virtio-iommu协议的支持。目前，Jean在搞virt-iommu的协议<br> jpbrucker.net/virtio-iommu/spec, 目前看virtio iommu spec中PASID/fault的支持<br> 还不完善。</p>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>QEMU</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>一个弹球小游戏</title>
    <url>/2021/07/17/%E4%B8%80%E4%B8%AA%E5%BC%B9%E7%90%83%E5%B0%8F%E6%B8%B8%E6%88%8F/</url>
    <content><![CDATA[<p>这个小游戏来自Unix/Linu编程时间教程(Understanding Unix/Linux Programming: A<br>Guide to Theory and Practice)。编译调试环境为ubuntu 14.04.2, 需要安装ncurses库。</p>
<p>下载代码后，运行：make生成可执行文件pong, ./pong 即可运行。</p>
<p>git库地址在：<br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3BvbmcuZ2l0">https://github.com/wangzhou/pong.git<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>Linux用户态编程</tag>
      </tags>
  </entry>
  <entry>
    <title>vim使用笔记</title>
    <url>/2021/07/17/vim%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li><p>水平创建一个terminal:</p>
<p>vert term</p>
</li>
<li><p>term</p>
</li>
<li><p>ctrl + w + N<br>打开的terminal进入vim的normal模式，可以使用vim的复制命令复制信息到原来的窗口里。<br>注意这里是大写的N。</p>
</li>
<li><p>在插入模式的时候，当前的一个单词输入错了，可以用ctrl + w删去当前的单词，<br>ctrl + u删除到首行。注意这个在shell里也是可以使用的。</p>
<p>这个可以在输入的时候不用退出插入模式来删除单词，当然是针对英文的。</p>
</li>
<li><p>切换到普通模式可以用ctrl + [, 用手指整体移开去按esc。</p>
</li>
<li><p>普通模式下，z + enter把当前行拉到最顶行。</p>
</li>
<li><p>普通模式下，按R进入替换模式，这时直接输入，输入的字符直接覆盖之前的内容。<br>这个功能和word下的替换是一样的。gR是虚拟替换模式，按实际的占位替换。</p>
</li>
<li><p>注意多用.命令，这个命令重复之前得到操作。我们要先定义一个操作的意思。<br>普通模式下，一个操作就是一个操作。进入和esc之间的整个插入模式算一个操作。</p>
</li>
<li><p>录制宏: q[a-z] 开始录制宏，q 停止录制宏，@[a-z] 使用宏。</p>
</li>
<li><p>daw 普通模式下删除当前的单词，delete a word </p>
</li>
<li><p>cw 普通模式下，删除当前位置到单词结尾，change word<br>caw 在光标处于一个单词中间的时候，可以删除这个单词。a是around的意思。</p>
</li>
<li><p>f + char，跳到本行第一个字符; t + char, 光标移动到char的前一个字符</p>
</li>
<li><p>全局替换：%s/xxx/yyy/g<br>s代表替换，%表示1,$，是全局的意思。</p>
</li>
<li><p>转换成大写: 可视状态选要转换的部分，gU</p>
</li>
<li><p>快速移动光标可以使用叫easymotion的vim插件。</p>
<p>在~/.vim/bundle下git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Vhc3ltb3Rpb24vdmltLWVhc3ltb3Rpb24uZ2l0">https://github.com/easymotion/vim-easymotion.git<i class="fa fa-external-link-alt"></i></span><br>在再~/.vim/bundle下git clone <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1Z1bmRsZVZpbS9WdW5kbGUudmltLmdpdA==">https://github.com/VundleVim/Vundle.vim.git<i class="fa fa-external-link-alt"></i></span><br>后者是一个vim插件的管理器，可以用来安装easymotion。之后，按照Vundle.vim里<br>README.md的提示, 把“3. Configure Plugins”的一段配置copy到.vimrc里，把其中<br>的Plugin xxx都删了，改成Plugin ‘easymotion/vim-easymotion’。代开vim run:<br>PluginInstall, 会有easymotion安装提示出来，并显示已经装好了。打开vim run:<br>help easymotion.txt, 可以看到easymotion的help文档。</p>
<p>具体使用的时候在.vimrc里加入一行快捷键的映射，可以是：<br>nmap ss &lt;Plug&gt;(easymotion-s2)  这样的效果是当你按ss的后会进入easymotion的<br>搜索输入，这个时候输入想要调到的地方的两个连续字符，之后整个屏幕凡是有这<br>两个连续字符的地方都会高亮，并且出现一个标记的字符，直接按这个字符就可以把<br>光标跳到对应的位置。</p>
<p>太太强大了！</p>
</li>
<li><p>使用vim画ASCII图</p>
<p>Linux kernel中的文档也有很多包含ASCII图。介绍两个在vim下画ASCII图的工具。<br>使用工具画图, 效率更高。</p>
<ol>
<li><p>boxe </p>
<p>可以插入一些这个软件中自带的图形，效果如下：<br>输入命令：echo “example” | boxes -d dog</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">          __   _,--=&quot;=--,_   __</span><br><span class="line">         /  \.&quot;    .-.    &quot;./  \</span><br><span class="line">        /  ,/  _   : :   _  \/` \</span><br><span class="line">        \  `| /o\  :_:  /o\ |\__/</span><br><span class="line">         `-&#x27;| :=&quot;~` _ `~&quot;=: |</span><br><span class="line">            \`     (_)     `/</span><br><span class="line">     .-&quot;-.   \      |      /   .-&quot;-.</span><br><span class="line">.---&#123;     &#125;--|  /,.-&#x27;-.,\  |--&#123;     &#125;---.</span><br><span class="line"> )  (_)_)_)  \_/`~-===-~`\_/  (_(_(_)  (</span><br><span class="line">(  example                              )</span><br><span class="line"> )                                     (</span><br><span class="line">&#x27;---------------------------------------&#x27;</span><br></pre></td></tr></table></figure>
<p>在ubuntu下直接 sudo apt-get install boxes 安装即可。<br>可以在/etc/boxes/boxes-config中查看其支持的图形。</p>
</li>
<li><p>vim插件drawIt</p>
<p>若手工去画ASCCI图，需要不断的调整。这个插件的功能简单的说，即他能使我们先把<br>字符摆好，然后运用该插件加上线条：</p>
<p>step 1: 输入表格中的字符</p>
<p>  AAAA     BBBB     BBBB      CCCC</p>
<p>  AAAA     BBBB     BBBB      CCCC</p>
<p>  AAAA     BBBB     BBBB      CCCC</p>
<p>step 2: 打开drawIt功能加上表格的框框</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+--------+--------+---------+--------+</span><br><span class="line">|  AAAA  |  BBBB  |  BBBB   |  CCCC  |</span><br><span class="line">+--------+--------+---------+--------+</span><br><span class="line">|  AAAA  |  BBBB  |  BBBB   |  CCCC  |</span><br><span class="line">+--------+--------+---------+--------+</span><br><span class="line">|  AAAA  |  BBBB  |  BBBB   |  CCCC  |</span><br><span class="line">+--------+--------+---------+--------+</span><br></pre></td></tr></table></figure>
<p>具体的安装和使用方法见：<br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY2hpbmF1bml4Lm5ldC91aWQtMjMxMDUyNjEtaWQtMTA5NTA4Lmh0bWw=">http://blog.chinaunix.net/uid-23105261-id-109508.html<i class="fa fa-external-link-alt"></i></span></p>
</li>
</ol>
</li>
<li><p>set cc=80 //第80列是一道红色的线，一般代码在红线以内（只占79列）<br>统一缩进：shift + &gt; or shift + &lt; </p>
</li>
<li><p>ts *** 查看定义, 在行末模式下输入:ts 待查类型/函数, 将得到他的定义</p>
</li>
</ol>
<ol start="19">
<li><p>XX, XXs/a/b/g 第XX行到第XX行中所有的a换成b。<br>**,**s/a/b/gc 可以选择要改变的“a”, 如果有些“a”不想变成“b”, 这个选项还是比较有用的</p>
<p>加注释的方式可以是：<strong>，</strong>s/^////g  或 <strong>，</strong>s/^/#/g</p>
<p>当然加注释的方法还可以，control+v, 向下选中要加的行，I 表示插入，光标会跳回<br>第一行，然后输入//作为注释最后按exit键，这时会发现之前选中的行之前都加上了//注释</p>
</li>
<li><p>代码补全 ctrl+p
 </p>
</li>
<li><p>多个tags文件在.vimrc中用逗号隔开即可, 搜索的时候可以在多个tags中:<br>set tags=/path_1/tags,/path_2/tags</p>
<p>set tags=tags;  注意要加“；”，这个配置可以逐级向上查找tag文件，找见后就set tag。<br>这个对于有多个代码库的情况比较方便，不用在.vimrc里把tag的路径写死，只要把tag<br>文件放在代码的根目录下，不管在哪里打开, 总可以找到tag,并且set tag</p>
</li>
<li><p>跳到函数的开始、结尾：<br>    [[, ]], [], ][<br>    1. 两个符号相同，则跳到函数的开头。[[跳到前一个的开头，]]跳到后一个的开头<br>    2. 两个符号不同，则跳到函数的结尾。[]跳到前一个的结尾，][跳到后一个的结尾</p>
</li>
<li><p>ctrl+d ctrl+u: 上下翻半页。ctrl+e ctrl+y: 光标不动，上下滚屏</p>
</li>
<li><p>ctrl+v块选择, shift+v行选择</p>
</li>
<li><p>set tabstop=8, tab按键一次缩进8个字符的宽度<br>    set softtabtop, linux下最好不要设置，若softtabtop=4, 第一次输入4个空格，第二次<br>    按输入4个空格，然后把8个空格转变成一个tab. kernel中的缩进是8个字符哦！</p>
</li>
<li><p>映射F4，F2，F3按键到对应的插件程序，第一个列出文件中所有的变量名、函数名和宏<br>    第二个列出目录结构，第三个查找光标所在处的字符串，可以改变该字符串的查找路径！</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">noremap &lt;F4&gt; :TlistToggle&lt;CR&gt;</span><br><span class="line">noremap &lt;F2&gt; :NERDTreeToggle&lt;CR&gt;</span><br><span class="line">nnoremap &lt;silent&gt; &lt;F3&gt; :Grep&lt;CR&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>set spell可以为你检查文件中有没有拼错的词，但貌似只支持英文. </p>
</li>
<li><p>jump between two files or two functions<br>    向前跳到前几次光标的位置：ctrl + i<br>    向后跳到后几次光标的位置：ctrl + o 这样可以在函数定义和调用处来回跳动</p>
</li>
<li><p>尽量在vim内完成所有操作，包括:make, vimgrep(缩写成vim)<br>vimgrep的格式是：vim /search_patten/ **/*.[ch] **是当前目录以及以下目录的通配符，<br>*是当前目录的通配符。vimgrep搜索得到的各个条目会保存到一个叫quickfix的表里，<br>这个是vim的一个基本的功能，copen可以开一个窗口，然后在新开窗口中打开quickfix表，<br>cclose关闭打开的quickfix表。</p>
</li>
<li><p>tj命名，tags jump 符号，可以搜索一个符号的位置，并且跳过去。stj split一个新<br>的窗口显示。有很多t开头的命令，最常用的要数ts了，tags select。</p>
</li>
<li><p>ab命令，用一个缩写来代替一组字符。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>wireshark and tcpdump</title>
    <url>/2021/07/11/wireshark-and-tcpdump/</url>
    <content><![CDATA[<p>tcpdump可以捕获某个网络端口的所有的网络报文。可以用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tcpdump -i eth0 -w dump_file</span><br></pre></td></tr></table></figure>
<p>捕获从eth0经过的网络报文，然后把他们的具体信息存在dump_file指定的文件中。</p>
<p>用wireshark可以打开dump_file文件解析其中的报文。wireshark是一个图形化的网络报文<br>分析工具。在windows和linux下都有wireshark的版本。</p>
<p>一般的，看报文的统计结果的时候，比如看有多少重传的TCP报文，可以在wireshark中<br>打开Analyze -&gt; Export Info -&gt; Notes 查看。</p>
]]></content>
      <tags>
        <tag>网络</tag>
        <tag>wireshark</tag>
      </tags>
  </entry>
  <entry>
    <title>一个服务器的配置过程</title>
    <url>/2021/07/17/%E4%B8%80%E4%B8%AA%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h2><p>服务器相关参数：HUAWEI RH2285 server, 300G x 12, memory:4G x 12,<br>LSI SAS 1068E, ip: ***, gateway: ***, netmask: ***,<br>DNS1: ***, DNS2: ***</p>
<p>配置一个开发用的服务器需要考虑的有：</p>
<ol>
<li>硬盘怎么分区, 2) RAID怎么设置，3) 网络怎么配置，4) 装什么软件, 5) 每个用<br>户的磁盘容量限制. 下面逐个说明</li>
</ol>
<h2 id="配置步骤"><a href="#配置步骤" class="headerlink" title="配置步骤"></a>配置步骤</h2><p>(1) 磁盘分区：</p>
<p>服务器原有12块硬盘，每个300G。一般安装系统的分区要做RAID1，故用硬盘0，1<br>做一个RAID1, 容量300G, 作系统分区等；硬盘2-11做一个RAID1E, 容量1.5T,<br>挂载在/home上。在安装操作系统的时候看到的是sda和sdb两个硬盘，这里把300G<br>的划分成和系统相关的分区，把1.5T独立作为一个分区挂载在/home上。具体分区如下：</p>
<p>|—&gt;/home 1.5T<br>|<br>|—&gt;/usr 40G<br>|<br>|—&gt;/var 40G<br>|<br>|—&gt;/ 10G<br>|<br>|—&gt;/tmp 5G<br>|<br>|—&gt;/boot 1G<br>|<br>|—&gt;none 200G</p>
<p>分区的思路为：/boot存放系统启动的内容，包括内核压缩文件和grub; /usr为系统<br>以后安装软件的目录，所以可分的大些； /var为系统存放日志文件<br>的目录且重启之后不会删除，所以也要大点；/tmp为系统存放临时<br>文件的地方并且要是挂载tmpfs文件系统后，/tmp的内容会存放在<br>内存中，所以不需要太大；根文件系统的其他部分以后不会有大的<br>变动，所以都放在/下</p>
<p>(2) RAID设置</p>
<p>本服务器支持了硬RAID，采用的是LSI的RAID卡1068E, 配置过程如下:</p>
<ol>
<li><p>开机, 一直等到initial界面，按contrl+c进入硬RAID配置界面。<br>华为RH2285服务器采用的是LSI 1068ＥRAID卡，可以配置1-2个RAID0，1-2个<br>RAID1，1-2个RAID1E</p>
</li>
<li><p>RAID设置是：硬盘0，1做一个RAID1，容量300G，用于除了home的分区；<br>硬盘2-11做一个RAID1E，容量1.5T，只用于home分区。</p>
</li>
<li><p>做RAID之前首先要删除之前的RAID:<br>C1068E—&gt;RAID Properties—&gt;Manage Array—&gt;Delete Array</p>
</li>
<li><p>做RAID1：<br>C1068E—&gt;RAID Properties—&gt;Create IM Volume—&gt;选择硬盘0、1，按c创<br>建RAID1</p>
</li>
<li><p>做RAID1E：<br>C1068E—&gt;RAID Properties—&gt;Create IME Volume—&gt;选择硬盘2-11，按c创<br>建RAID1E。系统的12块硬盘做成的一个RAID1和一个RAID1E，安装操作系统时将<br>只会看到sda和sdb两个硬盘</p>
</li>
</ol>
<p>(3) 网络配置</p>
<p>服务器的ip要配置成静态ip, 需要了解配置的有：ip, DNS, 网关, 一般在文件<br>/etc/network/interfaces, /etc/resolv.conf中配置</p>
<p>(4) 软件安装</p>
<p>配置好网络后，使用sudo apt-get install安装(这里安装的系统是ubuntu)需要的<br>软件。这里安装gcc, qemu, make, git, vim, gdb, ctags, cscope, openjdk, ftp,<br>tftp, scp, perf用户态程序等开发工具</p>
<p>windows和linux通信需要samba服务器, 其配置过程如下:</p>
<p>samba服务器要配置成为：<br>a. 用户名和大服务器用户名相同<br>b. 各用户分别共享自己的home目录，用户之间互不可见<br>c. /home/share向每个团队成员共享，对非团队成员不可见</p>
<ol>
<li><p>samba服务器的配置文件在/etc/samba/下，有smb.conf, smbpasswd等。<br>samba服务器可以设定不同的安全等级，如share级别无需密码就可以访问，这<br>里采用user级别，访问时需要用户名和密码。samba服务器的可用三种方式鉴<br>权：smbpasswd, tdbsam, ldapsam, 这里采用smbpasswd</p>
</li>
<li><p>sudo vi /etc/samba/smb.conf<br>设定安全级别：security = user<br>设定鉴权方式：passdb backend = smbpasswd<br>设定存放密码的文件：<br>smb passwd file = /etc/samba/smbpasswd<br>若是没有smbpasswd, 手动建一个, 以后创建的samba用户的密码会保存在其中<br>添加用户并设定初始密码：sudo smbpasswd -a username</p>
</li>
<li><p>建立共享的share目录：mkdir /home/share<br>在smb.conf中增加配置：<br>[share]<br>path = /home/share<br>writable = yes<br>broweable = yes<br>available = yes<br>valid users = @user<br>这里之前把需要有共享权限的用户加入了新建的用户组user, 也可以把用户名<br>用逗号隔开加入。加入组中的用户在windows—&gt;“运行”中输入//server-ip/<br>然后输入用户名和密码即可登陆共享目录</p>
</li>
<li><p>建立自己共享的目录：<br>在smb.conf的[global]段中加入：config file = /etc/samba/smb.conf.%U<br>这表示到对应的子配置文件去找配置项。<br>拷贝一份配置文件：cp smb.conf smb.conf.username1, 在其中加入：<br>[username1]<br>path = /home/username1<br>writable = yes<br>broweable = no<br>valid users = username1<br>其他用用户都这样配置。username1用户在windows—&gt;“运行”中输入//server-<br>ip/username1, 然后输入用户名和密码即可登陆/home/username1</p>
</li>
<li><p>重启samba服务器:<br>sudo service smbd restart<br>sudo service nmbd restart</p>
</li>
</ol>
<p>注：连接samba服务器，第一次输入密码后，再次连接不需要输入密码。这是因为<br>关闭窗口后连接并没有真正断开，可以在dos命令行下用net use查看相应的<br>连接，再用net usr *** /del 关闭该连接</p>
<p>参考：<br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY2hpbmF1bml4Lm5ldC91aWQtMjA1MzcwODQtaWQtMjk3Nzg1MC5odG1s">http://blog.chinaunix.net/uid-20537084-id-2977850.html<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jicy5jaGluYXVuaXgubmV0L3RocmVhZC05MjIwMjktMS0xLmh0bWw=">http://bbs.chinaunix.net/thread-922029-1-1.html<i class="fa fa-external-link-alt"></i></span></p>
<p>(5) quota配置</p>
<p>quota可以限制每个用户磁盘的使用量。这里总磁盘可用容量为1.3T，设计使用人<br>数为20，每个用户的home目录的限额为: 60G(soft), 65G(hard), 宽限时间为14天。<br>具体注意见:</p>
<p>配置参照<span class="exturl" data-url="aHR0cDovL2xpbnV4LnZiaXJkLm9yZy9saW51eF9iYXNpYy8wNDIwcXVvdGEucGhw">http://linux.vbird.org/linux_basic/0420quota.php<i class="fa fa-external-link-alt"></i></span> 即可。需注意：</p>
<ol>
<li><p>查看内核是否支持quota:<br>CONFIG_QUOTA = y<br>CONFIG_XFS_QUOTA = y</p>
</li>
<li><p>umount /home的时候可能出现device busy的情况。解决办法是:<br>以root进入系统，使用lsof | grep /home 找到相应的进程，杀死相应的进程<br>再执行 umoung /home</p>
</li>
<li><p>可能系统中没有quotacheck命令，sudo apt-get install quota 安装即可</p>
</li>
</ol>
<p>(6) 安装系统中需注意：</p>
<ol>
<li><p>安装系统前查一下服务器的系统兼容性，ubuntu-server-12.04在RH2285上会<br>安装出错，上面安装的是untuntu-server-10.04</p>
</li>
<li><p>安装grub的时候应该选择RAID1(300G)中对应的分区，这里选择/dev/sdb1，挂<br>载的目录是/boot</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>使用TAILQ</title>
    <url>/2021/06/20/%E4%BD%BF%E7%94%A8TAILQ/</url>
    <content><![CDATA[<p>TAILQ是BSD里实现的一套简单的链表，一般linux系统只要include sys/queue.h就可以使用。<br>在我使用的ARM版本的ubuntu系统，这个文件在/usr/include/aarch64-linux-gnu/sys/queue.h.</p>
<p>TAILQ作为链表使用的基本数据结构就是两个: TAILQ_ENTRY, TAILQ_HEAD。一个表示链表<br>的节点，一个表示链表头, 当使用的时候需要用TAILQ宏定义出相应的结构。具体使用的<br>方法和内核里链表的使用方式基本一致。在需要链表连起来的元素里每个埋一个TAILQ_ENTRY<br>的结构体。随后的各种操作宏里会大量的用到node这个变量。其实TAILQ_ENTRY就是定义了<br>一个包含指向本节点前后节点的指针结构。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct your_list_node &#123;</span><br><span class="line">	int data;</span><br><span class="line">	TAILQ_ENTRY(your_list_node) node;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">#define	_TAILQ_ENTRY(type, qual)					\</span><br><span class="line">struct &#123;								\</span><br><span class="line">	qual type *tqe_next;		/* next element */		\</span><br><span class="line">	qual type *qual *tqe_prev;	/* address of previous next element */\</span><br><span class="line">&#125;</span><br><span class="line">#define TAILQ_ENTRY(type)	_TAILQ_ENTRY(struct type,)</span><br></pre></td></tr></table></figure>
<p>使用TAILQ_HEAD定义一个类型是struct list_head, 所管理的链表元素类型是<br>truct your_list_node的链表头结构体。所以如下的宏其实只是定义了一个类型：<br>struct list_head。使用的时候还要有链表头实例的定义。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TAILQ_HEAD(list_head, your_list_node);	</span><br></pre></td></tr></table></figure>

<p>有了这两个基本数据结构，我们看几个基本的操作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct list_head head;</span><br><span class="line">TAILQ_INIT(&amp;head);</span><br></pre></td></tr></table></figure>
<p>初始化一个链表头。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct your_list_node e1;</span><br><span class="line">TAILQ_INSERT_TAIL(&amp;head, &amp;e1, node);</span><br></pre></td></tr></table></figure>
<p>把一个节点插入head链表头表示的链表的尾部。第二个参数是链表元素的指针。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TAILQ_FOREACH(tmp, &amp;head, node) &#123;</span><br><span class="line">	printf(&quot;---&gt; %d\n&quot;, tmp-&gt;data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>遍历head链表头表示的链表的所有元素，每次得到的元素指针放到tmp里。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct your_list_node e4;</span><br><span class="line">struct your_list_node e5;</span><br><span class="line">TAILQ_INSERT_BEFORE(&amp;e4, &amp;e5, node);</span><br></pre></td></tr></table></figure>
<p>把e5插到e4的前面，第一个, 第二个参数都是链表元素指针。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct your_list_node *tmp;</span><br><span class="line">tmp = TAILQ_LAST(&amp;head, list_head);</span><br></pre></td></tr></table></figure>
<p>得到head链表头表示的链表的最后一个元素的指针。head是链表头实例的名字，list_head<br>则是链表头的类型。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct your_list_node e2;</span><br><span class="line">TAILQ_REMOVE(&amp;head, &amp;e2, node);</span><br></pre></td></tr></table></figure>
<p>从head链表头表示的链表中删除e2节点，第二个参数是要删除节点的指针。</p>
<p>其他的宏基本逻辑和上面介绍的基本一样。<br>完整的测试代码可以参考: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dhbmd6aG91L3Rlc3RzL2Jsb2IvbWFzdGVyL3RhaWxxL3Rlc3QuYw==">https://github.com/wangzhou/tests/blob/master/tailq/test.c<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>一种硬件队列的驱动设计</title>
    <url>/2021/06/28/%E4%B8%80%E7%A7%8D%E7%A1%AC%E4%BB%B6%E9%98%9F%E5%88%97%E7%9A%84%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/</url>
    <content><![CDATA[<p>首先描述这个硬件的队列模型：</p>
<p>他有N个发送队列(sq)，N个完成队列(cq), 这N个sq和cq一一对应组成N个队列对(qp)。这<br>N个qp对应一个事件队列(eq)。</p>
<p>用户可以依次在sq里填入请求，然后发通知(doorbell)给硬件，要求硬件执行这个请求。<br>可以执行请求的部件我们把他独立开来(core)。core执行完请求后会依次在cq里填入请求<br>完成(描述符)。为了异步的通知给用户，当cq里有请求完成描述符的时候，并且没有屏蔽<br>cq写eq的通道的时候，cq会依次在eq里填入一个事件完成描述符(eqe)。对于eq，在eq里有事件<br>完成描述符，并且没有屏蔽中断的时候，向CPU报一个中断。</p>
<p>软件可以操作doorbell更新sq尾指针，更新cq头指针，更新eq头指针。其他硬件指针硬件<br>自己会更新(比如，core在取走一个sq请求的时候更新sq头指针，cq、eq在被写入的时候，<br>硬件更新cq tail和eq tail)。</p>
<p>cq在满足条件向eq填一个eqe的同时自动屏蔽cq写eq的通道。软件可以再下发一个特定的<br>doorbell打开这个通道。</p>
<p>eq满足条件上报中断的同时自动屏蔽中断上报。软件下发doorbell更新eq head时，除了<br>更新硬件eq head，还同时打开了中断屏蔽。</p>
<p>整体的硬件框图类似是这样的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">              cq/sq/eq doorbell</span><br><span class="line">          |       |                    |       ^ irq</span><br><span class="line">+---------+-------+--------------------+-------+----------------------+</span><br><span class="line">|         |       |                    |       |                      |</span><br><span class="line">|         |       v                    |       |            		|</span><br><span class="line">|   +---+-+-+---+---+----              |       |                      |</span><br><span class="line">|   |sqe|s|e|sqe|sqe|                  |       |                      |</span><br><span class="line">|   +---+-+-+---+---+----              |       |                      |</span><br><span class="line">|   +---+-v-+---+---+-\--              |       |                      |</span><br><span class="line">|   |cqe|cqe|cqe|cqe|  \  \            |       |                      |</span><br><span class="line">|   +---+---+---+---+---\  \           |       |                      |</span><br><span class="line">|         ^       ^ ^    \  \          |       |                      |</span><br><span class="line">|         |       |  \    \  \         |       |                      |</span><br><span class="line">|        head    tail \    \  \        |       |                      |</span><br><span class="line">|                      \    \  v       v       |                      |</span><br><span class="line">|                       \    \   +---+---+---+---+-----               |</span><br><span class="line">|   N个                  \    \  |eqe|eqe|eqe|eqe|...                 |</span><br><span class="line">|                         \    X +---+---+---+---+-----               |</span><br><span class="line">|                          \  / \      ^       ^                      |</span><br><span class="line">|   +---+---+---+---+----   \/   \     |       |                      |</span><br><span class="line">|   |sqe|sqe|sqe|sqe|       /\    \   head    tail                    |  </span><br><span class="line">|   +---+---+---+---+----  /  \    \                                  |</span><br><span class="line">|   +---+---+---+---+---- /    \    \                                 |</span><br><span class="line">|   |cqe|cqe|cqe|cqe|    /      \    \                                |</span><br><span class="line">|   +---+---+---+---+----        \    \                               |</span><br><span class="line">|                                 \    v                              |</span><br><span class="line">|                                  \+--------+                        |</span><br><span class="line">|                                   |        |                        |</span><br><span class="line">|                                   | core   |                        |</span><br><span class="line">|                                   |        |                        |</span><br><span class="line">|                                   +--------+                        |</span><br><span class="line">|  cq/sq depth = M   eq depth = K                                     |</span><br><span class="line">+---------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>对于sq，软件需要维护软件sq_head, sq_tail。保证发到sq的请求不会导致sq溢出。<br>对于cq，其input来自core, output来自中断处理程序中更新cq head。<br>对于eq，其input来自cq，output来自中断处理程序中更新eq head。</p>
<p>软件设计的重点在于，在保证cq，eq不发生溢出的情况下，做到高性能。我们先找到保证<br>cq、eq不发生溢出的条件。然后，提高中断的汇聚程度提高性能(同时不破坏之前的约束<br>条件)。</p>
<p>cq不溢出的条件是(考虑中断很久才到，这时候sq, cq已被填满, 注意因为上面我们已经<br>限制sq不溢出，所以这里即使没有中断更新cq head，cq也不会溢出)。中断中我们通过<br>eqe中的索引找到对应的cq进行处理，必须先跟新cq head，才能更新软件的sq head。否则，<br>如果先更新sq head，下发的新sq请求可能通过core产生cqe使得cq溢出。<br>(注意这里没有考虑cqe汇聚处理)</p>
<p>eq不溢出的条件是，考虑所有cq并发向eq写eqe，中断到来在写满eq时刻之后的情况，得出<br>eq深度至少要等于并发cq数N。假设eq已被写满，现在在中断处理函数中遍历各个cq处理，<br>并且处理完后打开cq写eq通道，并且N个cq上都有排队cqe(意味着会写N个新eqe给eq)。如果<br>eq的队列深度不增加(依然是N)，那么保证eq不溢出的条件是，irq中断中处理一个eq head<br>(相当于在eq里留出一个空位)，才能在处理一个cq的时候放开放cq写eq的通路(放开通路就<br>意味着可能有cq向eq写入一个eqe)。当然也可以处理n个eqe，放开&lt;=n个cq到eq的通路。</p>
<p>上面已经明确不溢出的条件，那么，下来就是看优化了。按照上面对eq的处理，需要在中断<br>中对每个eqe做eq head的更新，注意上面的硬件描述里提到，每次更新eq head都会打开<br>中断屏蔽，这样每个eq都会上报一个中断，性能会差。我们考虑把多个完成的eqe做一下汇聚，<br>然后更新eq head。注意, 在实际实现时，我们的处理大概是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">while (轮询已完成n个eqe) &#123;  /* 这里最大汇聚为n */</span><br><span class="line">	处理eqe对应的cqe;</span><br><span class="line">	打开cq到eq的通道；</span><br><span class="line">&#125;</span><br><span class="line">更新eq head到原来eq_head + n的位置;</span><br></pre></td></tr></table></figure>
<p>可以看到实际处理的时候为了把处理cq和打开cq到eq的通道放在一起，会在eq head处理<br>之前打开cq到eq的通道，这样和上面的eq不溢出的条件是冲突的。这样做eq会溢出。<br>注意到上面的分析是假设eq深度等于并发cq个数N。要使得eq不溢出，我们这里还可以增加<br>eq的深度来解决，可以看到我们需要至少把eq的深度加到N+n。</p>
<p>下面分析cqe汇聚处理的问题。一个eqe至少都对应着一个cqe，在cq写eq后，对应cq写eq的<br>通道被屏蔽，后续core处理完的请求将在cq上排队。在中断处理程序中，通过eq找到对应的<br>cq，然后处理cqe，如果只处理一个cqe后就打开cq到eq的通道，就会上报eq，eq可能会上报<br>中断(这个要看eq上的汇聚处理)。所以在处理cq的时候，我们可汇聚k个cq一起处理，然后<br>再打开cq到eq的通道。这个是一个和上面独立的逻辑。</p>
]]></content>
      <tags>
        <tag>软件架构</tag>
      </tags>
  </entry>
  <entry>
    <title>使用github做开源开发</title>
    <url>/2021/06/19/%E4%BD%BF%E7%94%A8github%E5%81%9A%E5%BC%80%E6%BA%90%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<p>一个项目的参与者有开发者和维护者。开发者要做的是，从主分支拉代码，开发，提交代码。<br>维护者要做的是，review开发者提交的代码，合入代码。</p>
<p>开发者在github上使用发git pull request的方法向维护者提交合入请求。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                    repo fork</span><br><span class="line">+-----------------+ -------&gt; +-------------+</span><br><span class="line">| 主线repo master |          | 开发者 repo |</span><br><span class="line">+-----------------+ &lt;------- +-------------+</span><br><span class="line">         \      git pull request    ^</span><br><span class="line">          \                          \</span><br><span class="line">           \                          \ git push dev-branch到开发者github repo</span><br><span class="line">            \                          \</span><br><span class="line">             \                          \</span><br><span class="line">              \                          \</span><br><span class="line">               \             +----------------+</span><br><span class="line">                ----------&gt;  | 开发者本地repo |</span><br><span class="line">           跟踪主线master    +----------------+</span><br><span class="line">                         依据最新master分支创建开发分支: dev-branch</span><br></pre></td></tr></table></figure>
<p>如上，开发者本地可以维护一个主线repo master的跟踪分支，开发一个新特性的时候，<br>开发者建立基于最新主线master的开发分支，开发完成后，把开发分支push到开发者github<br>repo上，然后再在github页面上发起git pull request。</p>
<p>维护者在收到git pull request的时候，可以在线review，也可以把需要review的patch<br>拉到本地。维护者可以用 git pull origin pull/<pull_request_id>/head把对应patch拉到<br>本地的当前分支上。维护者直接在github页面就可以点击合入补丁，合入补丁的时候有几种<br>方式，”Create a merge commit”的方式会在合入的时候自动创建一个merge的commit，记录<br>整个merge的信息; “Rebase and merge”的方式会只合入pull request里的patch。</p>
]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>使用SMMU PMU查看性能数据</title>
    <url>/2021/06/19/%E4%BD%BF%E7%94%A8SMMU-PMU%E6%9F%A5%E7%9C%8B%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<ul>
<li><p>首先要确定使用的系统里有arm_smmuv3_pmu这个模块，或者它已经被编译进内核。<br>这个模块的代码在内核目录kernel/drivers/perf/arm_smmuv3_pmu.c, 内核配置是:<br>CONFIG_ARM_SMMU_V3_PMU</p>
</li>
<li><p>确定使用的单板上的UEFI里有你要测试的模块对应的SMMU PMCG节点，没有这个节点的<br>的话即使加载上面的驱动也无法使用SMMU PMCG</p>
</li>
<li><p>正常使用的话，dmesg | grep pmcg可以看见类似信息:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Ubuntu:/ # dmesg | grep pmcg</span><br><span class="line">[ 1232.379951] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.8.auto: option mask 0x1</span><br><span class="line">[ 1232.380040] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.8.auto: Registered PMU @ 0x0000000148020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380094] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.9.auto: option mask 0x1</span><br><span class="line">[ 1232.380142] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.9.auto: Registered PMU @ 0x0000000201020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380190] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.10.auto: option mask 0x1</span><br><span class="line">[ 1232.380241] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.10.auto: Registered PMU @ 0x0000000100020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380286] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.11.auto: option mask 0x1</span><br><span class="line">[ 1232.380337] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.11.auto: Registered PMU @ 0x0000000140020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380397] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.12.auto: option mask 0x1</span><br><span class="line">[ 1232.380445] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.12.auto: Registered PMU @ 0x0000200148020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380491] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.13.auto: option mask 0x1</span><br><span class="line">[ 1232.380542] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.13.auto: Registered PMU @ 0x0000200201020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380601] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.14.auto: option mask 0x1</span><br><span class="line">[ 1232.380653] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.14.auto: Registered PMU @ 0x0000200100020000 using 8 counters with Individual filter settings</span><br><span class="line">[ 1232.380698] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.15.auto: option mask 0x1</span><br><span class="line">[ 1232.380770] arm-smmu-v3-pmcg arm-smmu-v3-pmcg.15.auto: Registered PMU @ 0x0000200140020000 using 8 counters with Individual filter settings</span><br></pre></td></tr></table></figure></li>
<li><p>使用perf list | grep pmcg可以查看系统支持的pmcg相关的时间类型:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Ubuntu:/ # perf list | grep pmcg</span><br><span class="line">  [...]</span><br><span class="line">  smmuv3_pmcg_140020/config_cache_miss/              [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/config_struct_access/           [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/cycles/                         [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/pcie_ats_trans_passed/          [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/pcie_ats_trans_rq/              [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/tlb_miss/                       [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/trans_table_walk_access/        [Kernel PMU event]</span><br><span class="line">  smmuv3_pmcg_140020/transaction/                    [Kernel PMU event]</span><br><span class="line">  [...]</span><br></pre></td></tr></table></figure></li>
<li><p>使用pmcg之前需要先明确需要测试的设备是在哪个pmcg之下，pmcg的命名方式是:<br>smmuv3_pmcg_<phys_addr_page>, 这里的phys_addr_page是对应SMMU PMCG基地址去掉<br>低12bit。这里的设计有点不好，使用者很难找到对应的关系 :(</p>
</li>
<li><p>当想观测一个程序对应的SMMU上统计信息时我们可以, 比如这样:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf stat -e smmuv3_pmcg_&lt;phys_addr_page&gt;/tlb_miss/ &lt;your_program&gt;</span><br></pre></td></tr></table></figure>
<p>的到程序执行过程的smmu tlb miss数目。把这里的tlb_miss换成上面perf list | grep pmcg<br>所示的其他事件，就可以得到其他事件的统计。</p>
</li>
<li><p>实际系统上可能一个smmu下接着多个外设，只想看一个外设在smmu上统计数据可以，比如<br>这样:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf stat -e smmuv3_pmcg_&lt;phys_addr_page&gt;/tlb_miss/,filter_enable=1,filter_span=0,filter_stream_id=0x75 &lt;your_program&gt;</span><br></pre></td></tr></table></figure>
<p>上面的0x75是设备对应的stream_id，PCIe设备的话，一般就是这个设备的BDF号。<br>(fix me: device function number怎么表示?)</p>
<p>如果有smmu自定义的event实现，可以指定具体的event编号, 比如:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf stat -e smmuv3_pmcg_&lt;phys_addr_page&gt;/event=0x80/ &lt;your_program&gt;</span><br></pre></td></tr></table></figure>
<p>对于自定义的event，定义的命令是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf stat -e smmuv4_pmcg_&lt;phys_addr_page&gt;/event=0x80,filter_enable=1,filter_span=0,filter_stream_id=0x75/ &lt;your_program&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>使用ftrace跟踪函数</title>
    <url>/2021/06/27/%E4%BD%BF%E7%94%A8ftrace%E8%B7%9F%E8%B8%AA%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>有些时候在调试内核代码时，我们想跟踪下内核代码的执行流程，以及函数执行时间。<br>这个时候我们可以用Linux内核自带的ftrace来跟踪。</p>
<ul>
<li><p>确定内核打开了ftrace的编译选项: CONFIG_FTRACE。这个Tracers目录下的子编译<br>选项可以都打开，我的经验是自测试的几个可以不开，不然开机过程加上自测会时间<br>很长，其他的都可以开打。</p>
</li>
<li><p>cd /sys/kernel/debug/tracing</p>
</li>
<li><p>echo 0 &gt; tracing_on //关闭trace</p>
</li>
<li><p>echo 0 &gt; trace //把之前跟踪buffer里的数据清空</p>
</li>
<li><p>echo function_graph &gt; current_tracer //设置当前的跟踪器是function_graph</p>
</li>
<li><p>echo funcgraph-proc &gt; trace_options</p>
</li>
<li><p>echo funcgraph-abstime &gt; trace_options //在输出结果里增加每个函数的时间戳</p>
</li>
<li><p>echo your_func &gt; set_ftrace_filter //把你要跟踪的函数配置进ftrace</p>
</li>
<li><p>grep -i your_func available_filter_functions //可以查看上个步骤的函数有没有设置好</p>
</li>
<li><p>echo 1 &gt; tracing_on //开始trace</p>
</li>
<li><p>启动你要跟踪的程序执行</p>
</li>
<li><p>echo 0 &gt; tracing_on //关闭trace</p>
</li>
<li><p>cat trace &gt; your_trace_result //输出得到的跟踪信息，可以将信息重定向到文件</p>
</li>
</ul>
<p>经过上面，你可以大概得到一个这样的输出信息:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tracer: function_graph</span><br><span class="line">#</span><br><span class="line">#     TIME        CPU  TASK/PID         DURATION                  FUNCTION CALLS</span><br><span class="line">#      |          |     |    |           |   |                     |   |   |   |</span><br><span class="line">[...]</span><br><span class="line"> 9520.457571 |     2)  dma0cha-1201  |   0.940 us    |  hisi_dma_issue_pending [hisi_dma]();</span><br><span class="line"> 9520.457583 |     0)    &lt;idle&gt;-0    |   0.680 us    |  hisi_dma_irq [hisi_dma]();</span><br><span class="line"> 9520.457584 |     0)    &lt;idle&gt;-0    |   0.490 us    |  hisi_dma_desc_free [hisi_dma]();</span><br><span class="line"> 9520.457589 |     2)  dma0cha-1201  |   0.420 us    |  hisi_dma_tx_status [hisi_dma]();</span><br><span class="line"> 9520.457666 |     2)  dma0cha-1201  |   0.930 us    |  hisi_dma_prep_dma_memcpy [hisi_dma]();</span><br><span class="line"> 9520.457668 |     2)  dma0cha-1201  |   0.900 us    |  hisi_dma_issue_pending [hisi_dma]();</span><br><span class="line"> 9520.462827 |     2)  dma0cha-1201  |   0.910 us    |  hisi_dma_issue_pending [hisi_dma]();</span><br><span class="line"> 9520.462833 |     0)    &lt;idle&gt;-0    |   0.620 us    |  hisi_dma_irq [hisi_dma]();</span><br><span class="line">[...] </span><br></pre></td></tr></table></figure>
<p>duration这一栏得到的是每个函数的执行时间。通过time一栏的时间戳，可以得到函数和<br>函数之间的时延, 比如可以得到从发起一个dma传输task(hisi_dma_issue_pending)到<br>中断函数执行(hisi_dma_irq)的大概时延是：9520.457583 -  9520.457571 = 12us。</p>
<p>如果要跟踪一个函数的调用链，可以使用set_graph_function。具体的使用设置是:</p>
<ul>
<li><p>echo function_graph &gt; current_tracer</p>
</li>
<li><p>echo your_func &gt; set_graph_function</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>Linux内核</tag>
        <tag>ftrace</tag>
      </tags>
  </entry>
  <entry>
    <title>使用linux zswap</title>
    <url>/2021/06/27/%E4%BD%BF%E7%94%A8linux-zswap/</url>
    <content><![CDATA[<p>zswap是Linux内核里压缩swap内存的一个特性，他可以把需要swap到swap设备上内存先压缩<br>下，直到一定的门限值后再向swap设备写入。这个特性可以优化系统内存被大量使用，系统<br>有swap的时候的系统性能。本文简单介绍怎么使用。</p>
<ol>
<li><p>内核需要打开zswap的配置：CONFIG_ZSWAP。还需要打开zswap可能用到的用于压缩<br>内存存储的内存分配器，比如: CONFIG_ZBUD</p>
</li>
<li><p>在内核的启动cmdline里加zswap.enable=1可以自动加载zswap模块，zswap.compressor=xxx<br>可以选择压缩算法。如果不配置zswap.compressor, 默认的压缩算法是LZO。</p>
</li>
<li><p>启动系统，dmesg | grep zswap可以看到zswap正常加载，并且选择后端压缩内存分配器<br>的打印log。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">zswap: loaded using pool lzo/zbud</span><br></pre></td></tr></table></figure></li>
<li><p>zswap的控制参数也可以在/sys/module/zswap/parameters/*里配置。</p>
<p>compressor 选择压缩算法，enabled使能zswap功能，max_pool_percent配置压缩内存<br>池占系统内存的百分比(zswap是先把压缩的内存放在一个zpool里), zpool是后端压缩<br>内存分配器，这里是zbud, same_filled_pages_enable使能对内存值相同的情况做优化<br>处理。</p>
<p>zswap的debug信息可以在/sys/kernel/debug/zswap/*里显示。</p>
</li>
<li><p>如果系统没有swap分区，测试之前需要给系统加上swap分区，可以对一个空闲的磁盘分<br>区做：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkswap /dev/sda</span><br><span class="line">swapon /dev/sda</span><br><span class="line">用swapon -s可以查看系统中已经有的swap分区。</span><br></pre></td></tr></table></figure></li>
<li><p>如果要看下zswap是否运行，需要构造系统内存被大量使用的场景。如果在服务器上，<br>这样的场景不好构造。可以使用cgroup构造。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cgcreate -g memory:bob</span><br><span class="line">echo 0x4000000 &gt; /sys/fs/cgroup/memory/bob/memory.limit_in_bytes</span><br></pre></td></tr></table></figure>
<p>创建一个名字是bob的memory cgroup，限定在其中的进程使用内存大小是0x40MB。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cgexec -g memory:bob memhog 128m</span><br></pre></td></tr></table></figure>
<p>运行memhog这个程序，这个程序是一个内存测试程序，其操作的虚拟内存大小是128MB.</p>
<p>运行上面的命令后，查看/sys/kernel/debug/zswap/stored_pages，可以发现其值变<br>为非0。</p>
</li>
<li><p>以上的环境可以建在虚拟机里。可以基于这里的设置：<br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzg2NDM4MDEx">https://blog.csdn.net/scarecrow_byr/article/details/86438011<i class="fa fa-external-link-alt"></i></span><br>在host上使用 qemu-img create disk.img 1G 生成一个1G大小的虚拟磁盘。再在qemu<br>启动命令行里加上 -hdb path_of_disk/disk.img 把这个磁盘加给虚拟机。<br>再在虚拟机里配置以上各个步骤即可。如果需要调试带硬件加速的zswap，可以把硬件<br>的VF直通到虚拟机里调试。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>zswap</tag>
      </tags>
  </entry>
  <entry>
    <title>使用eBPF得到内核执行过程的时间分布</title>
    <url>/2021/06/18/%E4%BD%BF%E7%94%A8eBPF%E5%BE%97%E5%88%B0%E5%86%85%E6%A0%B8%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%88%86%E5%B8%83/</url>
    <content><![CDATA[<ol start="0">
<li>具体例子介绍</li>
</ol>
<hr>
<p> 在SVA的使用场景中，IO缺页比较影响系统的性能。为此，我们需要观测在一个段程序执行<br> 的时候，系统中发生IO缺页的次数, 以及IO缺页的统计特征，比如IO缺页的平均时间、方差<br> 和分布情况。我们可以在内核代码中加tracepoint点，然后用perf或者是ftrace得到trace<br> 信息，然后用脚本处理得到上面的信息。但是，本文介绍的是用eBPF的方法得到这样的<br> 信息。</p>
<p> 使用eBPF的方法，可以在把一段用户态写的代码放到内核执行，这段用户态写的代码可以<br> 在已有的内核tracepoint点上附着。就是每次内核里跑到tracepoint点的时候，都可以跑<br> 一下用户态写的代码，用户可以自由编程统计自己想要的信息。</p>
<p> eBPF在内核中提供里各种help函数，方便统计代码调用。同时现在也有关于eBPF的用户态<br> 库(e.g. <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2lvdmlzb3IvYmNjLmdpdA==">https://github.com/iovisor/bcc.git<i class="fa fa-external-link-alt"></i></span>)，方便在用户态编写代码，记录和处理<br> 得到的信息。</p>
<ol>
<li>内核配置</li>
</ol>
<hr>
<p> 我们使用<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0xpbmFyby9saW51eC1rZXJuZWwtdWFkaw==">https://github.com/Linaro/linux-kernel-uadk<i class="fa fa-external-link-alt"></i></span> branch: uacce-devel-5.11<br> 的分支测试，需要打上如下第3节中的内核补丁。</p>
<p> 如下是需要打开的内核编译选项:</p>
<p> make ARCH=arm64 defconfig<br> CONFIG_UACCE=y<br> CONFIG_DEV_HISI_ZIP=y<br> CONFIG_DEV_HISI_QM=y<br> CONFIG_ARM_SMMU_V3=y (默认)<br> CONFIG_ARM_SMMU_V3_SVA=y (默认)<br> CONFIG_IOMMU_SVA_LIB=y (默认)<br> CONFIG_PCI_PASID=y (默认)<br> CONFIG_FTRACE=y<br> (以上是业务相关的)</p>
<p> CONFIG_BPF_SYSCALL=y<br> CONFIG_BPF_KPROBE_OVERRIDE=y<br> CONFIG_KPROBE=y</p>
<p> CONFIG_SQUASHFS_XZ=y<br> CONFIG_CGROUP_FREEZER=y<br> (以上是ebpf相关的)</p>
<ol start="2">
<li>用户态环境搭建</li>
</ol>
<hr>
<p> 本文测试所使用的环境是Ubuntu 16.04, 这个版本的ubuntu没有bcc相关的仓库，外部仓库<br> 现在也无法使用了(<span class="exturl" data-url="aHR0cDovL3d3dy5icmVuZGFuZ3JlZ2cuY29tL2Jsb2cvMjAxNi0wNi0xNC91YnVudHUteGVuaWFsLWJjYy1icGYuaHRtbA==">http://www.brendangregg.com/blog/2016-06-14/ubuntu-xenial-bcc-bpf.html<i class="fa fa-external-link-alt"></i></span>)。</p>
<p> 所以本文测试使用了snap来安装bcc相关的环境，snap是ubuntu apt之外的另一个包管理器。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install snap</span><br><span class="line">sudo snap install bcc</span><br></pre></td></tr></table></figure>
<p> 如上会把snap管理下的bcc包安装到/snap/*下。 ls -al /snap</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">total 28</span><br><span class="line">drwxr-xr-x  6 root root 4096 2月  20 02:34 .</span><br><span class="line">drwxr-xr-x 22 root root 4096 10月  1 15:48 ..</span><br><span class="line">drwxr-xr-x  3 root root 4096 2月  20 02:34 bcc</span><br><span class="line">drwxr-xr-x  2 root root 4096 2月  20 05:16 bin</span><br><span class="line">drwxr-xr-x  3 root root 4096 2月  20 02:33 core18</span><br><span class="line">-r--r--r--  1 root root  548 10月 17  2019 README</span><br><span class="line">drwxr-xr-x  3 root root 4096 2月  20 02:32 snapd</span><br></pre></td></tr></table></figure>

<p>为python增加搜索库的路径，可以看到snap管理的bcc python库都在snap自己的路劲下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PYTHONPATH=&quot;/snap/bcc/146/usr/lib/python3/dist-packages:&#123;$PYTHONPATH&#125;&quot;</span><br><span class="line">export PYTHONPATH</span><br><span class="line"></span><br><span class="line">LD_LIBRARY_PATH=&quot;/snap/bcc/146/usr/lib/aarch64-linux-gnu/:&#123;$LD_LIBRARY_PATH&#125;&quot;</span><br><span class="line">export LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure>

<p>注意，这里编译bcc代码的时候，会使用到内核头文件。本文尝试了各种内核头文件<br>导出的方式都还是一直有内核头文件找不到。索性把查找头文件的目录又重新软连接到<br>使用内核的源码路径上。为了简单起见，本文的测试使用了本地编译内核的方式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ln -s &lt;your kernel source&gt; /lib/modules/&lt;your_kernel_magic&gt;/build</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>代码示例</li>
</ol>
<hr>
<p>内核补丁:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff --git a/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c b/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c</span><br><span class="line">index 8d29aa1be282..637e95c237a1 100644</span><br><span class="line">--- a/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c</span><br><span class="line">+++ b/drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c</span><br><span class="line">@@ -32,6 +32,8 @@</span><br><span class="line"> </span><br><span class="line"> #include &lt;linux/amba/bus.h&gt;</span><br><span class="line"> </span><br><span class="line">+#include &lt;trace/events/smmu.h&gt;</span><br><span class="line">+</span><br><span class="line"> #include &quot;arm-smmu-v3.h&quot;</span><br><span class="line"> #include &quot;../../iommu-sva-lib.h&quot;</span><br><span class="line"> </span><br><span class="line">@@ -945,6 +947,8 @@ static int arm_smmu_page_response(struct device *dev,</span><br><span class="line"> 	 * forget.</span><br><span class="line"> 	 */</span><br><span class="line"> </span><br><span class="line">+	trace_io_fault_exit(dev, resp-&gt;pasid);</span><br><span class="line">+</span><br><span class="line"> 	return 0;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line">@@ -1474,6 +1478,8 @@ static int arm_smmu_handle_evt(struct arm_smmu_device *smmu, u64 *evt)</span><br><span class="line"> 	struct iommu_fault_event fault_evt = &#123; &#125;;</span><br><span class="line"> 	struct iommu_fault *flt = &amp;fault_evt.fault;</span><br><span class="line"> </span><br><span class="line">+	trace_io_fault_entry(smmu-&gt;dev, FIELD_GET(EVTQ_0_SSID, evt[0]));</span><br><span class="line">+</span><br><span class="line"> 	/* Stage-2 is always pinned at the moment */</span><br><span class="line"> 	if (evt[1] &amp; EVTQ_1_S2)</span><br><span class="line"> 		return -EFAULT;</span><br><span class="line">diff --git a/include/trace/events/smmu.h b/include/trace/events/smmu.h</span><br><span class="line">index e9b648407102..4d96bfd20726 100644</span><br><span class="line">--- a/include/trace/events/smmu.h</span><br><span class="line">+++ b/include/trace/events/smmu.h</span><br><span class="line">@@ -82,6 +82,28 @@ DEFINE_EVENT(smmu_mn, smmu_mn_free, TP_PROTO(unsigned int pasid), TP_ARGS(pasid)</span><br><span class="line"> DEFINE_EVENT(smmu_mn, smmu_mn_get, TP_PROTO(unsigned int pasid), TP_ARGS(pasid));</span><br><span class="line"> DEFINE_EVENT(smmu_mn, smmu_mn_put, TP_PROTO(unsigned int pasid), TP_ARGS(pasid));</span><br><span class="line"> </span><br><span class="line">+DECLARE_EVENT_CLASS(smmu_io_fault_class,</span><br><span class="line">+	TP_PROTO(struct device *dev, unsigned int pasid),</span><br><span class="line">+	TP_ARGS(dev, pasid),</span><br><span class="line">+</span><br><span class="line">+	TP_STRUCT__entry(</span><br><span class="line">+		__string(dev, dev_name(dev))</span><br><span class="line">+		__field(int, pasid)</span><br><span class="line">+	),</span><br><span class="line">+	TP_fast_assign(</span><br><span class="line">+		__assign_str(dev, dev_name(dev));</span><br><span class="line">+		__entry-&gt;pasid = pasid;</span><br><span class="line">+	),</span><br><span class="line">+	TP_printk(&quot;dev=%s pasid=%d&quot;, __get_str(dev), __entry-&gt;pasid)</span><br><span class="line">+);</span><br><span class="line">+</span><br><span class="line">+#define DEFINE_IO_FAULT_EVENT(name)       \</span><br><span class="line">+DEFINE_EVENT(smmu_io_fault_class, name,        \</span><br><span class="line">+	TP_PROTO(struct device *dev, unsigned int pasid), \</span><br><span class="line">+	TP_ARGS(dev, pasid))</span><br><span class="line">+</span><br><span class="line">+DEFINE_IO_FAULT_EVENT(io_fault_entry);</span><br><span class="line">+DEFINE_IO_FAULT_EVENT(io_fault_exit);</span><br><span class="line"> </span><br><span class="line"> #endif /* _TRACE_SMMU_H */</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如上，为了在eBPF的跟踪代码里跟踪io page fault的流程，我们在SMMU io page fault<br>的入口和出口出增加了新的tracepoint点。增加trancepoint的方法还可以参考这里：<br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzExMTI0MzQ1MA==">https://blog.csdn.net/scarecrow_byr/article/details/111243450<i class="fa fa-external-link-alt"></i></span></p>
<p>用户态python脚本, ebpf_smmu_iopf.py。可以参考bcc里自带的tools, 看看怎么写这些<br>脚本: <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2lvdmlzb3IvYmNjLmdpdA==">https://github.com/iovisor/bcc.git<i class="fa fa-external-link-alt"></i></span></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line">#</span><br><span class="line">from __future__ import print_function</span><br><span class="line">from bcc import BPF</span><br><span class="line">from ctypes import c_ushort, c_int, c_ulonglong</span><br><span class="line">from time import sleep</span><br><span class="line">from sys import argv</span><br><span class="line"></span><br><span class="line">def usage():</span><br><span class="line">        print(&quot;USAGE: %s [interval [count]]&quot; % argv[0])</span><br><span class="line">        exit()</span><br><span class="line"></span><br><span class="line"># arguments</span><br><span class="line">interval = 5</span><br><span class="line">count = -1</span><br><span class="line">if len(argv) &gt; 1:</span><br><span class="line">        try:</span><br><span class="line">                interval = int(argv[1])</span><br><span class="line">                if interval == 0:</span><br><span class="line">                        raise</span><br><span class="line">                if len(argv) &gt; 2:</span><br><span class="line">                        count = int(argv[2])</span><br><span class="line">        except: # also catches -h, --help</span><br><span class="line">                usage()</span><br><span class="line"></span><br><span class="line"># load BPF program</span><br><span class="line">b = BPF(text=&quot;&quot;&quot;</span><br><span class="line">#include &lt;uapi/linux/ptrace.h&gt;</span><br><span class="line"></span><br><span class="line">BPF_HASH(start, u64);</span><br><span class="line">BPF_HISTOGRAM(dist);</span><br><span class="line"></span><br><span class="line">TRACEPOINT_PROBE(smmu, io_fault_entry)</span><br><span class="line">&#123;</span><br><span class="line">        u64 ts;</span><br><span class="line"></span><br><span class="line">        ts = bpf_ktime_get_ns();</span><br><span class="line">        start.update((unsigned int *)args-&gt;pasid, &amp;ts);</span><br><span class="line"></span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TRACEPOINT_PROBE(smmu, io_fault_exit)</span><br><span class="line">&#123;</span><br><span class="line">        u64 *tsp, delta;</span><br><span class="line">        u64 pasid;</span><br><span class="line"></span><br><span class="line">        tsp = start.lookup((unsigned int *)args-&gt;pasid);</span><br><span class="line"></span><br><span class="line">        if (tsp != 0) &#123;</span><br><span class="line">                delta = bpf_ktime_get_ns() - *tsp;</span><br><span class="line">                dist.increment(bpf_log2l(delta));</span><br><span class="line">                start.delete((unsigned int *)args-&gt;pasid);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br><span class="line">&quot;&quot;&quot;)</span><br><span class="line"></span><br><span class="line"># header</span><br><span class="line">print(&quot;Tracing... Hit Ctrl-C to end.&quot;)</span><br><span class="line"></span><br><span class="line"># output</span><br><span class="line">loop = 0</span><br><span class="line">do_exit = 0</span><br><span class="line">while (1):</span><br><span class="line">        if count &gt; 0:</span><br><span class="line">                loop += 1</span><br><span class="line">                if loop &gt; count:</span><br><span class="line">                        exit()</span><br><span class="line">        try:</span><br><span class="line">                sleep(interval)</span><br><span class="line">        except KeyboardInterrupt:</span><br><span class="line">                pass; do_exit = 1</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line">        b[&quot;dist&quot;].print_log2_hist(&quot;nsecs&quot;)</span><br><span class="line">        b[&quot;dist&quot;].clear()</span><br><span class="line">        if do_exit:</span><br><span class="line">                exit()</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>运行效果</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ./ebpf_smmu_iopf.py</span><br><span class="line"></span><br><span class="line">另一个窗口运行：</span><br><span class="line">sudo zip_perf_test -b 8192 -s 819200000</span><br></pre></td></tr></table></figure>
<p>得到如下的iopf时延的分布图:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[...]</span><br></pre></td></tr></table></figure>

<p>在过程中测试eBPF的环境是否搭建ok，也可以跑下/snap/bin自带的程序，比如跑bcc.cpudist<br>我们得到了这样的分布图：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Tracing on-CPU time... Hit Ctrl-C to end.</span><br><span class="line"></span><br><span class="line">     usecs               : count     distribution</span><br><span class="line">         0 -&gt; 1          : 193      |*************************               |</span><br><span class="line">         2 -&gt; 3          : 49       |******                                  |</span><br><span class="line">         4 -&gt; 7          : 35       |****                                    |</span><br><span class="line">         8 -&gt; 15         : 41       |*****                                   |</span><br><span class="line">        16 -&gt; 31         : 37       |****                                    |</span><br><span class="line">        32 -&gt; 63         : 116      |***************                         |</span><br><span class="line">        64 -&gt; 127        : 14       |*                                       |</span><br><span class="line">       128 -&gt; 255        : 2        |                                        |</span><br><span class="line">       256 -&gt; 511        : 0        |                                        |</span><br><span class="line">       512 -&gt; 1023       : 3        |                                        |</span><br><span class="line">      1024 -&gt; 2047       : 12       |*                                       |</span><br><span class="line">      2048 -&gt; 4095       : 61       |********                                |</span><br><span class="line">      4096 -&gt; 8191       : 96       |************                            |</span><br><span class="line">      8192 -&gt; 16383      : 97       |************                            |</span><br><span class="line">     16384 -&gt; 32767      : 162      |*********************                   |</span><br><span class="line">     32768 -&gt; 65535      : 301      |****************************************|</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件性能</tag>
      </tags>
  </entry>
  <entry>
    <title>使用perf trace跟踪IO缺页</title>
    <url>/2021/06/19/%E4%BD%BF%E7%94%A8perf-trace%E8%B7%9F%E8%B8%AAIO%E7%BC%BA%E9%A1%B5/</url>
    <content><![CDATA[<p>使用perf list可以看到有trace point的软件定义的trace点。(fix me: 要开什么内核选项)<br>这些软件定义的trace point点要在代码里提前预埋，执行程序的时候可以用perf trace<br>把需要的信息统计出来。</p>
<p>我们拿Linux内核里IOMMU统计IO缺页的event做例子看看。这个event的定义在:<br>include/trace/events/iommu.h里：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TRACE_EVENT(dev_fault,</span><br><span class="line"></span><br><span class="line">	TP_PROTO(struct device *dev,  struct iommu_fault *evt),</span><br><span class="line"></span><br><span class="line">	TP_ARGS(dev, evt),</span><br><span class="line"></span><br><span class="line">	TP_STRUCT__entry(</span><br><span class="line">		__string(device, dev_name(dev))</span><br><span class="line">		__field(int, type)</span><br><span class="line">		__field(int, reason)</span><br><span class="line">		__field(u64, addr)</span><br><span class="line">		__field(u64, fetch_addr)</span><br><span class="line">		__field(u32, pasid)</span><br><span class="line">		__field(u32, grpid)</span><br><span class="line">		__field(u32, flags)</span><br><span class="line">		__field(u32, prot)</span><br><span class="line">	),</span><br><span class="line"></span><br><span class="line">	TP_fast_assign(</span><br><span class="line">		__assign_str(device, dev_name(dev));</span><br><span class="line">		__entry-&gt;type = evt-&gt;type;</span><br><span class="line">		if (evt-&gt;type == IOMMU_FAULT_DMA_UNRECOV) &#123;</span><br><span class="line">			__entry-&gt;reason		= evt-&gt;event.reason;</span><br><span class="line">			__entry-&gt;flags		= evt-&gt;event.flags;</span><br><span class="line">			__entry-&gt;pasid		= evt-&gt;event.pasid;</span><br><span class="line">			__entry-&gt;grpid		= 0;</span><br><span class="line">			__entry-&gt;prot		= evt-&gt;event.perm;</span><br><span class="line">			__entry-&gt;addr		= evt-&gt;event.addr;</span><br><span class="line">			__entry-&gt;fetch_addr	= evt-&gt;event.fetch_addr;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			__entry-&gt;reason		= 0;</span><br><span class="line">			__entry-&gt;flags		= evt-&gt;prm.flags;</span><br><span class="line">			__entry-&gt;pasid		= evt-&gt;prm.pasid;</span><br><span class="line">			__entry-&gt;grpid		= evt-&gt;prm.grpid;</span><br><span class="line">			__entry-&gt;prot		= evt-&gt;prm.perm;</span><br><span class="line">			__entry-&gt;addr		= evt-&gt;prm.addr;</span><br><span class="line">			__entry-&gt;fetch_addr	= 0;</span><br><span class="line">		&#125;</span><br><span class="line">	),</span><br><span class="line"></span><br><span class="line">	TP_printk(&quot;IOMMU:%s type=%d reason=%d addr=0x%016llx fetch=0x%016llx pasid=%d group=%d flags=%x prot=%d&quot;,</span><br><span class="line">		__get_str(device),</span><br><span class="line">		__entry-&gt;type,</span><br><span class="line">		__entry-&gt;reason,</span><br><span class="line">		__entry-&gt;addr,</span><br><span class="line">		__entry-&gt;fetch_addr,</span><br><span class="line">		__entry-&gt;pasid,</span><br><span class="line">		__entry-&gt;grpid,</span><br><span class="line">		__entry-&gt;flags,</span><br><span class="line">		__entry-&gt;prot</span><br><span class="line">	)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>在需要打点的地方插入一个trace_dev_fault(dev, evt)就好，其中dev是TP_PROTO里定义的<br>struct device *dev, evt是里面定义的struct iommu_fault *evt。</p>
<p>TP_STRUCT__entry定义记录结构里各个域段的定义。TP_fast_assign定义域段记录的值。<br>TP_printk定义打印的方式。</p>
<p>以UADK里一个测试用力为例，我们看看怎么用perf trace收集IO page fault的信息。具体<br>的运行命令如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ./perf trace -o log_sva -a -e iommu:* numactl --cpubind 1 --membind 1  \</span><br><span class="line">test_hisi_sec --perf --async --pktlen 1024 --block 8192 --blknum 100000 \</span><br><span class="line">--times 1000000 --multi 1 --ctxnum 1</span><br></pre></td></tr></table></figure>
<p>-o后面加需要存放log的文件。注意, 需要sudo权限，需要-a，不然无法看到<br>iommu:dev_fault的事件，另外这个用力要使用block 8192才会观察到iommu:dev_fault事件</p>
<p>观察到的log_sva里的记录可能是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   0.000 :0/0 iommu:unmap:IOMMU: iova=0x00000000fdb67000 size=4096 unmapped_size=4096</span><br><span class="line">   0.030 :0/0 iommu:unmap:IOMMU: iova=0x00000000fdabe000 size=4096 unmapped_size=4096</span><br><span class="line">   0.396 test_hisi_sec/115486 iommu:map:IOMMU: iova=0x00000000fbfe9000 paddr=0x000000217e0c8000 size=4096</span><br><span class="line">   0.432 test_hisi_sec/115486 iommu:unmap:IOMMU: iova=0x00000000fbfe9000 size=4096 unmapped_size=4096</span><br><span class="line">   0.444 test_hisi_sec/115486 iommu:map:IOMMU: iova=0x00000000fbfe9000 paddr=0x000000217e0c8000 size=4096</span><br><span class="line">   0.465 test_hisi_sec/115486 iommu:unmap:IOMMU: iova=0x00000000fbfe9000 size=4096 unmapped_size=4096</span><br><span class="line"> 671.920 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x000000002321c000 fetch=0x0000000000000000 pasid=1 group=138 flags=3 prot=1</span><br><span class="line"> 671.961 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000023220000 fetch=0x0000000000000000 pasid=1 group=119 flags=3 prot=1</span><br><span class="line"> 671.983 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000023230000 fetch=0x0000000000000000 pasid=1 group=158 flags=3 prot=1</span><br><span class="line"> 672.003 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000023234000 fetch=0x0000000000000000 pasid=1 group=132 flags=3 prot=1</span><br><span class="line"> 672.024 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x000000002323c000 fetch=0x0000000000000000 pasid=1 group=135 flags=3 prot=1</span><br><span class="line"> 672.041 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000023232000 fetch=0x0000000000000000 pasid=1 group=120 flags=3 prot=2</span><br><span class="line"></span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">1946.610 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000084d82000 fetch=0x0000000000000000 pasid=1 group=122 flags=3 prot=2</span><br><span class="line">1946.636 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000084da6300 fetch=0x0000000000000000 pasid=1 group=88 flags=3 prot=2</span><br><span class="line">1946.659 irq/33-arm-smm/873 iommu:dev_fault:IOMMU:0000:76:00.0 type=2 reason=0 addr=0x0000000084d8a180 fetch=0x0000000000000000 pasid=1 group=86 flags=3 prot=2</span><br><span class="line">3031.527 :0/0 iommu:unmap:IOMMU: iova=0x00000000fdbe2000 size=4096 unmapped_size=4096</span><br><span class="line">3031.550 :0/0 iommu:unmap:IOMMU: iova=0x00000000fdb68000 size=4096 unmapped_size=4096</span><br><span class="line">3031.499 test_hisi_sec/115486 iommu:map:IOMMU: iova=0x00000000fbfe9000 paddr=0x000000217e0c8000 size=4096</span><br><span class="line">3031.557 test_hisi_sec/115486 iommu:unmap:IOMMU: iova=0x00000000fbfe9000 size=4096 unmapped_size=4096</span><br></pre></td></tr></table></figure>

<p>除了用perf trace跟踪，也可以用ftrace跟踪。这需要ftrace的目录下(一般在/sys/kernel/debug/tracing)<br>的event里使能对应的trace point点，这样再去trace就可以看到输出的打印。</p>
<p>也可以在需要跟踪的地方简单的加一个trace_printk()的打印，把对应的模块写到<br>set_ftrace_filter: echo ‘:mod:xxx_module_name’ &gt; set_ftrace_filter。然后再去<br>trace，也可以看到输出的打印。</p>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>在qemu虚拟机上安装Linux发行版</title>
    <url>/2021/06/19/%E5%9C%A8qemu%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%8A%E5%AE%89%E8%A3%85Linux%E5%8F%91%E8%A1%8C%E7%89%88/</url>
    <content><![CDATA[<ol>
<li><p>qemu-img create -f qcow2 debian.img 10G</p>
</li>
<li><p>sudo kvm -hda debian.img -cdrom debian-10.2.0-amd64-netinst.iso -m 2048</p>
<p>这一步是把这个debian的iso安装到debian.img这个文件上。</p>
</li>
<li><p>qemu command:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">qemu-system-x86_64 -cpu host -enable-kvm -smp 4 \</span><br><span class="line">-m 1G \</span><br><span class="line">-kernel ~/repos/linux/arch/x86/boot/bzImage \</span><br><span class="line">-append &quot;console=ttyS0 root=/dev/sda1&quot; \</span><br><span class="line">-hda ./debian.img \</span><br><span class="line">-nographic \</span><br></pre></td></tr></table></figure>
<p>如上启动虚拟机，可以发现自己已经可以使用如上安装的debian系统。我们在第二步<br>安装系统的时候可以把需要的程序都装上，在这样的虚拟机里做测试，会方便很多。<br>而且你在虚拟机里创建的文件下次启动虚拟机的时候都还在。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>虚拟化</tag>
        <tag>QEMU</tag>
      </tags>
  </entry>
  <entry>
    <title>使用动态库实现私有驱动的加载</title>
    <url>/2021/06/21/%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81%E5%BA%93%E5%AE%9E%E7%8E%B0%E7%A7%81%E6%9C%89%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%8A%A0%E8%BD%BD/</url>
    <content><![CDATA[<ol>
<li>模型</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">          +----------------------+</span><br><span class="line">          |       用户app        |</span><br><span class="line">          +----------------------+</span><br><span class="line">                     |</span><br><span class="line">                     |</span><br><span class="line">                     v</span><br><span class="line">          +----------------------+</span><br><span class="line">          |        api库         |</span><br><span class="line">          +----------------------+</span><br><span class="line">          /          |           \</span><br><span class="line">         /           |            \</span><br><span class="line">        v            v             v</span><br><span class="line">+---------+      +---------+      +---------+</span><br><span class="line">| 驱动库1 |      | 驱动库2 |      | 驱动库3 |</span><br><span class="line">+---------+      +---------+      +---------+</span><br><span class="line">     |               |                 |</span><br><span class="line">     |               |                 |</span><br><span class="line">     v               v                 v</span><br><span class="line">+---------+      +---------+      +---------+</span><br><span class="line">|  硬件1  |      |  硬件2  |      |  硬件3  |</span><br><span class="line">+---------+      +---------+      +---------+</span><br></pre></td></tr></table></figure>
<p> 如上图，用户通过api库提供的接口使用api库提供的功能，由于底层的硬件不一样，对应<br> 不同的驱动。为了方便, api库和各个驱动库被独立的编译成动态库。可以想像各个驱动<br> 库里需要实现api库里函数的各个回调函数，然后通过某种方式挂到api库里。我们考虑<br> 怎么组织软件实现这样的自动挂接。实际上rdma-core的代码里已经我们提供了实现的<br> 样板，我们下面把相关的骨架抽出来，简单demo下。</p>
<ol start="2">
<li>设计demo代码</li>
</ol>
<hr>
<p>app.c:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;                                                              </span><br><span class="line">#include &quot;api_dl.h&quot;                                                             </span><br><span class="line">                                                                                </span><br><span class="line">int main()                                                                      </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        int a = 1, b = 5, c = 0xffff;                                           </span><br><span class="line">                                                                                </span><br><span class="line">        c = add(a, b);                                                          </span><br><span class="line">        printf(&quot;a + b = %d\n&quot;, c);                                              </span><br><span class="line">                                                                                </span><br><span class="line">        c = multi(a, b);                                                        </span><br><span class="line">        printf(&quot;a * b = %d\n&quot;, c);                                              </span><br><span class="line">                                                                                </span><br><span class="line">        return 0;                                                               </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>api_dl.h:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int add(int a, int b);                                                          </span><br><span class="line">int multi(int a, int b);                                                         </span><br></pre></td></tr></table></figure>
<p>api_dl.c:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;                                                              </span><br><span class="line">#include &lt;dlfcn.h&gt;                                                              </span><br><span class="line">#include &quot;api_internal.h&quot;                                                       </span><br><span class="line">                                                                                </span><br><span class="line">struct api_driver *global_driver;                                               </span><br><span class="line">                                                                                </span><br><span class="line">void __set_driver(struct api_driver *drv)                                       </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        global_driver = drv;                                                    </span><br><span class="line">&#125;                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">int add(int a, int b)                                                           </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        return global_driver-&gt;add(a, b);                                        </span><br><span class="line">&#125;                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">int multi(int a, int b)                                                         </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        return global_driver-&gt;multi(a, b);                                      </span><br><span class="line">&#125;                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">static void __attribute__((constructor)) open_driver_dl(void)                    </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        void *driver_dl;                                                        </span><br><span class="line">                                                                                </span><br><span class="line">        driver_dl = dlopen(&quot;./libdriver.so&quot;, RTLD_NOW);                         </span><br><span class="line">        if (!driver_dl)                                                         </span><br><span class="line">                printf(&quot;Fail to open libdriver\n&quot;);                             </span><br><span class="line">&#125;                                                                               </span><br></pre></td></tr></table></figure>

<p>api库中针对驱动库的头文件(api_internal.h):</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct api_driver &#123;                                                             </span><br><span class="line">        int (*add)(int a, int b);                                               </span><br><span class="line">        int (*multi)(int a, int b);                                             </span><br><span class="line">&#125;;                                                                              </span><br><span class="line">                                                                                </span><br><span class="line">void __set_driver(struct api_driver *drv);                                      </span><br><span class="line">                                                                                </span><br><span class="line">#define SET_DRIVER(drv)                                                 \       </span><br><span class="line">static void __attribute__((constructor)) set_driver(void)               \       </span><br><span class="line">&#123;                                                                       \       </span><br><span class="line">        __set_driver(drv);                                              \       </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>一个驱动库的实现:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &quot;api_internal.h&quot;                                                       </span><br><span class="line">                                                                                </span><br><span class="line">static int add(int a, int b)                                                    </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        return a + b;                                                           </span><br><span class="line">&#125;                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">static int multi(int a, int b)                                                  </span><br><span class="line">&#123;                                                                               </span><br><span class="line">        return a * b;                                                           </span><br><span class="line">&#125;                                                                               </span><br><span class="line">                                                                                </span><br><span class="line">struct api_driver one_driver = &#123;                                                </span><br><span class="line">        .add = add,                                                             </span><br><span class="line">        .multi = multi,                                                         </span><br><span class="line">&#125;;                                                                              </span><br><span class="line">                                                                                </span><br><span class="line">SET_DRIVER(&amp;one_driver);                                                         </span><br></pre></td></tr></table></figure>

<p>编译app, api库，驱动库的命令:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># build libdriver.so                                                            </span><br><span class="line">        gcc -shared -fPIC -o libdriver.so driver_dl.c                           </span><br><span class="line">                                                                                </span><br><span class="line"># build libapi.so                                                               </span><br><span class="line">        gcc -shared -fPIC -o libapi.so api_dl.c                                 </span><br><span class="line">                                                                                </span><br><span class="line"># build app                                                                     </span><br><span class="line">        gcc -o app app.c -L. -lapi -ldl                                         </span><br></pre></td></tr></table></figure>

<ol start="3">
<li>说明</li>
</ol>
<hr>
<p> 可以看到app在加载libapi.so的时候可以先打开libdriver.so，libdrver.so在打开的时候<br> 调用libapi.so中提供给下层驱动的接口把驱动结构体的指针传给libapi.so里的驱动指针。</p>
<p> 这里把libapi里打开的驱动库名字写死了，实际上，我们应该通过某种选择机制把这个<br> 搞成动态可以配置的, 这样就可以选择加载不同的驱动库。这个动态的机制是另一个独立<br> 的逻辑，可以通过配置文件的方式，用户提前把需要加载的驱动库写到一个配置文件里，<br> libapi库通过读这个配置文件知道自己要加载哪个驱动库。</p>
]]></content>
      <tags>
        <tag>软件架构</tag>
        <tag>动态库</tag>
      </tags>
  </entry>
  <entry>
    <title>使用qemu虚拟机学习Linux内核</title>
    <url>/2021/07/17/%E4%BD%BF%E7%94%A8qemu%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%A6%E4%B9%A0Linux%E5%86%85%E6%A0%B8/</url>
    <content><![CDATA[<p>当然可以在自己的PC机上装一个linux的发行版，然后写一些linux内核模块，插入到当前<br>的内核中。但是这样会使自己的PC机不稳定，内核模块有bug的话容易使得整个系统崩溃掉。<br>还有一点，现在做linux kernel开发的很多使用的不是X86的体系架构，如果做arm下的开发,<br>现在这样做也是不行的。本文以arm64为例子，介绍一个使用qemu虚拟机学习linux kernel<br>的环境。</p>
<ol start="0">
<li><p>PC机环境<br> 本人的PC机上的操作系统是ubuntu 14.04</p>
</li>
<li><p>qemu虚拟机</p>
<ol>
<li>下载qemu原码：<br> git clone git://git.qemu-project.org/qemu.git<br>(没有git的 sudo apt-get install git 安装一个)</li>
<li>编译arm64体系构架的qemu虚拟机：<br> cd qemu/<br>./configure -target-list=aarch64-softmmu<br>make<br> 在qemu/aarch64-softmmu下会有qemu-system-aarch64, 这就是我们将要用的虚拟机。</li>
</ol>
</li>
<li><p>linux kernel</p>
<ol>
<li>下载最新内核源码：<br> git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git</li>
<li>编译内核：<br> make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- defconfig<br>make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- Image<br> (需要提前下载好arm64的工具链，把工具链的路径加入PATH。这里CROSS_COMPILE示工具链而定)</li>
</ol>
</li>
<li><p>根文件系统<br> 根文件系统可以使用linaro网站上发布的openembedded的文件系统，如果遇到该文件系统里没有的<br> 工具，可以自己下载工具的源码，交叉编译后加入该文件系统中。</p>
<ol>
<li>下载openembedded文件系统：<br> <span class="exturl" data-url="aHR0cHM6Ly9yZWxlYXNlcy5saW5hcm8ub3JnL2xhdGVzdC9vcGVuZW1iZWRkZWQvYWFyY2g2NA==">https://releases.linaro.org/latest/openembedded/aarch64<i class="fa fa-external-link-alt"></i></span><br>中的vexpress64-openembedded_minimal-armv8-gcc-4.9_20141023-693.img.gz</li>
<li>解压：<br> gunzip vexpress64-openembedded_minimal-armv8-gcc-4.9_20140923-688.img.gz<br>mv vexpress64-openembedded_minimal-armv8-gcc-4.9_20140923-688.img fs.img</li>
<li>添加文件：<br> sudo kpartx -a fs.img<br>(没有kpartx的 sudo apt-get install kpartx 装一个)<br>sudo mount /dev/mapper/loopOp2 /mnt<br>这时可以访问文件系统，可以把自己编译的一些用户态程序放到文件系统中。初次尝试<br> 可以不做这一步骤。</li>
</ol>
</li>
<li><p>运行整个系统<br> ./qemu-system-aarch64 -machine virt -cpu cortex-a57 <br> -kernel ~/linux/arch/arm64/boot/Image <br> -drive if=none,file=/home/wangzhou/openembaded/fs.img,id=fs <br> -device virtio-blk-device,drive=fs <br> -append ‘console=ttyAMA0 root=/dev/vda2’ <br> -nographic</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>QEMU</tag>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo NexT搭建个人博客</title>
    <url>/2021/06/18/%E4%BD%BF%E7%94%A8hexo-NexT%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<ol>
<li>基础逻辑说明</li>
</ol>
<hr>
<p> github提供了静态网页展示的功能，我们可以在自己的github上创建username.github.io<br> 名字的仓库，然后在这个仓库的settings里开启Pages的功能。这里需要html文件才能以<br> 静态网页的形式把内容展示出来，而一般我们直接书写的是md或者是rst这样的文本文件。</p>
<p> 网上有各种各样的从文本文件生成html文件的框架，hexo就是其中的一个，它是基于JS的。<br> hexo支持主题可选，通过选不同的主题，博客的风格会不一样，我们选的是NexT主题，这个<br> 主题使用的人很多，相关的资料也容易找。</p>
<p> 我们把整个blog分成三个代码仓库来管理：</p>
<ul>
<li><p>hexo代码仓库，这个仓库是hexo代码，我们的博客原文也放在这个目录下。</p>
</li>
<li><p>静态网页仓库，这个仓库里的静态网页都是hexo生成的。我们把这个仓库推到如上的<br>username.github.io这个仓库，然后从浏览器就可有通过<span class="exturl" data-url="aHR0cHM6Ly91c2VybmFtZS5naXRodWIuaW8v">https://username.github.io<i class="fa fa-external-link-alt"></i></span><br>访问到。</p>
</li>
<li><p>NexT主题的仓库，我们把这个仓库fork一份到自己的github上，一般我们只需要改动<br>NexT的配置文件就好，但是我们这里还是把仓库本身也保存一份，日后可以直接修改NexT<br>的代码，然后提交到自己的仓库暂时保存。我们把自己的这个NexT仓库作为hexo仓库的<br>submodule保存。</p>
</li>
</ul>
<ol start="2">
<li>搭建过程</li>
</ol>
<hr>
<ul>
<li><p>安装npm、nodejs和hexo</p>
<p>sudo apt install npm nodejs<br>sudo npm install -g hexo-cli</p>
<p>Note: 上面安装的nodejs是10.x.x的，后面生成博客的时候会报错，我们这里需要安装</p>
<pre><code>  12.x.x的nodejs版本。nodejs的版本可以通过node -v 来看。
</code></pre>
</li>
<li><p>安装12.x.x nodejs</p>
<p>curl -sL <span class="exturl" data-url="aHR0cHM6Ly9kZWIubm9kZXNvdXJjZS5jb20vc2V0dXBfMTIueA==">https://deb.nodesource.com/setup_12.x<i class="fa fa-external-link-alt"></i></span> | sudo -E bash -<br>sudp apt update<br>sudo apt install -y nodejs</p>
</li>
<li><p>下载hexo和NexT</p>
<p>mkdir hexo; cd hexo<br>hexo init blog<br>cd blog<br>sudo npm install</p>
<p>把NexT fork到自己的github, 我们用url_priv_next表示自己NexT的github仓库地址。<br>创建一个空的github仓库用来存放hexo代码, 我们用url_priv_hexo表示自己的这个hexo<br>仓库的github地址。在hexo仓库把自己的NexT仓库设置成自己的hexo仓库的submodule:</p>
<p>git submodule add url_priv_next themes/next</p>
<p>到这里为止，基本的仓库以及他们之间的关系我们都搞定了。我们看看hexo里几个目录<br>里放什么：source里放博客相关的原文件，其中包括md文件和相关的图片；themes放各种<br>主题，所以NexT也放到这个目录下；scaffolds放各种模版，比如后面会讲到的生成文章<br>就会默认用到scaffolds/post.md这个模版，所以我们更改这个模版生成的文章也会带上<br>相关的改动，如下的description就是这样的一个例子, draft.md这个模版可以用来生成<br>草稿文档，比如我们可以用hexo new draft “draft_1”来生成名字是draft_1的草稿文档，<br>这个草稿文档以draft.md为模版，存放在source/_drafts下；public用来放后面生成<br>的html文档；node_modules是一些和Hexo nodejs相关的东西。</p>
</li>
<li><p>配置hexo以及NexT的配置文件</p>
<p>如上的配置已经可以生成静态网页。出于个人偏好，我自己的配置文件的改动如下，<br>具体的改动可以看相关地方的注释：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo _config.yml</span><br><span class="line"></span><br><span class="line">diff --git a/_config.yml b/_config.yml</span><br><span class="line">index 2dc35e6..97fc902 100644</span><br><span class="line">--- a/_config.yml</span><br><span class="line">+++ b/_config.yml</span><br><span class="line">@@ -97,7 +97,7 @@ ignore:</span><br><span class="line"> # Extensions</span><br><span class="line"> ## Plugins: https://hexo.io/plugins/</span><br><span class="line"> ## Themes: https://hexo.io/themes/</span><br><span class="line">-theme: landscape</span><br><span class="line">+theme: next  # 使用NexT主题</span><br><span class="line"></span><br><span class="line">diff --git a/scaffolds/post.md b/scaffolds/post.md</span><br><span class="line">index 1f9b9a4..91f86d0 100644</span><br><span class="line">--- a/scaffolds/post.md</span><br><span class="line">+++ b/scaffolds/post.md</span><br><span class="line">@@ -2,4 +2,5 @@</span><br><span class="line"> title: &#123;&#123; title &#125;&#125;</span><br><span class="line"> date: &#123;&#123; date &#125;&#125;</span><br><span class="line"> tags:</span><br><span class="line">+description:    # 如下使用hexo g 生成新md文档的时候会使用这个地方的模版，如果这里加上description</span><br><span class="line">                 # 主页就不会展示全部文件，而是只是显示这里的摘要。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NexT _config.yml</span><br><span class="line"></span><br><span class="line">diff --git a/_config.yml b/_config.yml</span><br><span class="line">index 61cc72d..957ae93 100644</span><br><span class="line">--- a/_config.yml</span><br><span class="line">+++ b/_config.yml</span><br><span class="line">@@ -67,7 +67,7 @@ footer:</span><br><span class="line">   copyright:</span><br><span class="line"> </span><br><span class="line">   # Powered by hexo &amp; NexT</span><br><span class="line">-  powered: true</span><br><span class="line">+  powered: false    # 个人喜欢极简的风格，所以去掉主页底部的&quot;Powered by hexo &amp; NexT&quot;</span><br><span class="line"> </span><br><span class="line">   # Beian ICP and gongan information for Chinese users. See: https://beian.miit.gov.cn, http://www.beian.gov.cn</span><br><span class="line">   beian:</span><br><span class="line">@@ -97,10 +97,10 @@ creative_commons:</span><br><span class="line"> # ---------------------------------------------------------------</span><br><span class="line"> </span><br><span class="line"> # Schemes</span><br><span class="line">-scheme: Muse</span><br><span class="line">+#scheme: Muse</span><br><span class="line"> #scheme: Mist</span><br><span class="line"> #scheme: Pisces</span><br><span class="line">-#scheme: Gemini</span><br><span class="line">+scheme: Gemini     # 换一个带边栏的风格，scheme是NexT内部的风格</span><br><span class="line"> </span><br><span class="line"> # Dark Mode</span><br><span class="line"> darkmode: false</span><br><span class="line">@@ -117,10 +117,10 @@ darkmode: false</span><br><span class="line"> # External url should start with http:// or https://</span><br><span class="line"> menu:</span><br><span class="line">   home: / || fa fa-home</span><br><span class="line">-  #about: /about/ || fa fa-user</span><br><span class="line">-  #tags: /tags/ || fa fa-tags</span><br><span class="line">+  about: /about || fa fa-user    # 打开about, tags, archives标签</span><br><span class="line">+  tags: /tags || fa fa-tags</span><br><span class="line">   #categories: /categories/ || fa fa-th</span><br><span class="line">-  archives: /archives/ || fa fa-archive</span><br><span class="line">+  archives: /archives || fa fa-archive</span><br><span class="line">   #schedule: /schedule/ || fa fa-calendar</span><br><span class="line">   #sitemap: /sitemap.xml || fa fa-sitemap</span><br><span class="line">   #commonweal: /404/ || fa fa-heartbeat</span><br><span class="line">@@ -138,8 +138,8 @@ menu_settings:</span><br><span class="line"> </span><br><span class="line"> sidebar:</span><br><span class="line">   # Sidebar Position.</span><br><span class="line">-  position: left</span><br><span class="line">-  #position: right</span><br><span class="line">+  #position: left</span><br><span class="line">+  position: right     # 边栏移动到右边</span><br><span class="line"> </span><br><span class="line">   # Manual define the sidebar width. If commented, will be default for:</span><br><span class="line">   # Muse | Mist: 320</span><br><span class="line">@@ -261,12 +261,12 @@ tag_icon: false</span><br><span class="line"> # Front-matter variable (unsupport animation).</span><br><span class="line"> reward_settings:</span><br><span class="line">   # If true, reward will be displayed in every article by default.</span><br><span class="line">-  enable: false</span><br><span class="line">+  enable: true        # 打开打赏的功能</span><br><span class="line">   animation: false</span><br><span class="line">   #comment: Donate comment here.</span><br><span class="line"> </span><br><span class="line"> reward:</span><br><span class="line">-  #wechatpay: /images/wechatpay.png</span><br><span class="line">+  wechatpay: /images/weixinpay.svg       # 把微信的付款二维码放到这个地方</span><br><span class="line">   #alipay: /images/alipay.png</span><br><span class="line">   #paypal: /images/paypal.png</span><br><span class="line">   #bitcoin: /images/bitcoin.png</span><br><span class="line">@@ -354,7 +354,7 @@ codeblock:</span><br><span class="line">   # Code Highlight theme</span><br><span class="line">   # Available values: normal | night | night eighties | night blue | night bright | solarized | solarized dark | galactic</span><br><span class="line">   # See: https://github.com/chriskempson/tomorrow-theme</span><br><span class="line">-  highlight_theme: normal</span><br><span class="line">+  highlight_theme: night blue            # 选择嵌入代码的显示风格，这里是深蓝色底</span><br><span class="line">   # Add copy button on codeblock</span><br><span class="line">   copy_button:</span><br><span class="line">     enable: false</span><br><span class="line">@@ -823,7 +823,7 @@ mermaid:</span><br><span class="line"> # Use velocity to animate everything.</span><br><span class="line"> # For more information: http://velocityjs.org</span><br><span class="line"> motion:</span><br><span class="line">-  enable: true</span><br><span class="line">+  enable: false          # 不禁止这个的话，打开一个文章会有动画，个人不喜欢这个</span><br><span class="line">   async: false</span><br><span class="line">   transition:</span><br><span class="line">     # Transition variants:</span><br></pre></td></tr></table></figure></li>
<li><p>添加文章和生成静态网页</p>
<p>hexo n “文章名字”<br>hexo clean &amp;&amp; hexo g</p>
<p>会在public目录里生成相关的静态网页，把public里的内容copy出来然后push到如上的<br>username.github.io仓库里。</p>
<p>本地调试的话可以先运行: hexo server, 然后在本地浏览器里通过<span class="exturl" data-url="aHR0cDovL2xvY2FsaG9zdDo0MDAwLw==">http://localhost:4000<i class="fa fa-external-link-alt"></i></span><br>观察生成的静态网页。</p>
</li>
<li><p>保存本地改动到GitHub</p>
<p>cd hexo/blog; git add &lt;改动文件&gt;; git commit -s -m &lt;改动描述&gt;;<br>cd hexo/blog/themes/next; git add &lt;改动文件&gt;; git commit -s -m &lt;改动描述&gt;;<br>git remote add url_priv_hexo<br>git push –recurse-submodules=on-demand origin master:master</p>
<p>在其他地方可以使用如下的方式迭代clone hexo和NexT仓库:<br>git clone –recursive url_priv_hexo</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>静态博客搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>如何尝试使用Linux SVA</title>
    <url>/2021/06/21/%E5%A6%82%E4%BD%95%E5%B0%9D%E8%AF%95%E4%BD%BF%E7%94%A8Linux-SVA/</url>
    <content><![CDATA[<ol>
<li>硬件确认</li>
</ol>
<hr>
<p> 首先你要有一台KunPeng920服务器，而且这台服务器上的压缩解压缩设备是可见的。你可以<br> lspci -s 75:00.0 -vv</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# lspci -s 75:00.0</span><br><span class="line">75:00.0 Processing accelerators: Device 19e5:a250 (rev 21)</span><br></pre></td></tr></table></figure>
<p> 如上，说明你的系统上有这个压缩解压缩的设备。</p>
<p> 系统的SMMU要在UEFI里打开。你可以看下系统启动日志，dmesg | grep iommu</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# dmesg | grep iommu</span><br><span class="line">[...]</span><br><span class="line">[   19.410490] hisi_zip 0000:75:00.0: Adding to iommu group 14</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p> 如上，可以认为SMMU的配置没有问题，当然group的编号可以是不同的。</p>
<ol start="2">
<li>内核配置和编译</li>
</ol>
<hr>
<p> 目前内核的相关补丁还没有完全上主线，我们在Linaro的github上维护了一个完整的可以<br> 跑的分支：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0xpbmFyby9saW51eC1rZXJuZWwtd2FycGRyaXZlL3RyZWUvdWFjY2UtZGV2ZWw=">https://github.com/Linaro/linux-kernel-warpdrive/tree/uacce-devel<i class="fa fa-external-link-alt"></i></span></p>
<p> make defconfig</p>
<p> make menuconfig</p>
<p> 这里defconfig的配置是不够的，你需要确保如下的内核配置是打开的:<br>    CONFIG_ARM_SMMU_V3=y<br>    CONFIG_PCI_PASID=y<br>    CONFIG_IOMMU_SVA=y<br>    CONFIG_CRYPTO_DEV_HISI_QM=y<br>    CONFIG_CRYPTO_DEV_HISI_ZIP=y<br>    CONFIG_UACCE=y                                                                  </p>
<p> 然后编译内核即可。</p>
<ol start="3">
<li>用户态代码配置和编译</li>
</ol>
<hr>
<p> 对应的用户态代码的仓库也在Linaro的github上：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0xpbmFyby93YXJwZHJpdmUvdHJlZS9tYXN0ZXI=">https://github.com/Linaro/warpdrive/tree/master<i class="fa fa-external-link-alt"></i></span></p>
<p> ./autogen.sh<br> ./conf.sh<br> make</p>
<p> 在.lib目录下会生成编译出的用户态库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Sherlock@EstBuildSvr1:~/repos/linaro_wd/warpdrive/.libs$ ls *.so</span><br><span class="line">libhisi_qm.so  libwd_ciper.so  libwd_comp.so  libwd_digest.so  libwd.so</span><br></pre></td></tr></table></figure>
<p> 在test目录下有编译好的测试app：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test_sva_bind test_sva_perf</span><br></pre></td></tr></table></figure>
<p> 如上的两个测试app基于压缩解压缩设备，所以依赖的库是：<br> libhisi_qm.so libwd_comp.so libwd.so</p>
<ol start="4">
<li>运行测试用例</li>
</ol>
<hr>
<p> 使用如上编译好的内核Image启动系统, 把libhisi_qm.so libwd_comp.so libwd.so<br> 拷贝到系统上，然后尝试运行下 test_sva_perf。如果运行OK的话会有性能数据打印出来：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@ubuntu:/home/sherlock/warpdrive/test# ./test_sva_perf </span><br><span class="line">Compress bz=512000 nb=1×10, speed=1433.5 MB/s (±0.0% N=1) overall=1334.3 MB/s (±0.0%)</span><br></pre></td></tr></table></figure>

<p> test_sva_bind test_sva_perf里各个命令参数的用法可以参考help说明。</p>
]]></content>
      <tags>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>对称加解密</title>
    <url>/2021/06/20/%E5%AF%B9%E7%A7%B0%E5%8A%A0%E8%A7%A3%E5%AF%86/</url>
    <content><![CDATA[<ol>
<li>常用算法</li>
</ol>
<hr>
<p>对称加解密常用的算法有DES，3DES，AES, SM4。</p>
<p>DES算法现在已经破解，明文64bit，密文64bit，秘钥64bit(实际56bit，每隔7bit有一个<br>错误检查bit)。</p>
<p>3DES是DES计算三次(加密，解密，加密), 有三个64bit秘钥。</p>
<p>AES(提交时的算法名字叫Rijndael, 比利时人设计), 分支固定长度是128bit，秘钥长度可以<br>是128,192或者256bit。</p>
<p>SM4是中国自己的一种分组密码标准。分组长度和秘钥都是128bit。</p>
<p>这里有一个密码算法的分类: 分组密码和流密码。一般，上面提到的密码都采用分组密码。<br>流密码的意思是，对一串数据流进行加密，加密过程存在中间状态。</p>
<ol start="2">
<li>分组密码的模式</li>
</ol>
<hr>
<p>使用分组密码加密的时候，要把数据切成一块一块的分组(block)。根据分组之间的不同关系，<br>我们有不同的分组密码工作模式。模式有：ECB，CBC, CFB，OFB, CTR, XTS等。</p>
<p>ECB(Electronic CodeBook), block之间完全没有关系的分组加密。不推荐使用。</p>
<p>CBC(Cipher Block Chaining), 加密的逻辑如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">            +---------+      +---------+      +---------+</span><br><span class="line">            |明文分组1|      |明文分组2|      |明文分组3|</span><br><span class="line">            +---------+      +---------+      +---------+</span><br><span class="line">                v                v                 v</span><br><span class="line">              +---+            +---+             +---+</span><br><span class="line">       +-----&gt;|XOR|       +---&gt;|XOR|       +----&gt;|XOR|</span><br><span class="line">       |      +---+       |    +---+       |     +---+</span><br><span class="line">       |        v         |      v         |       v</span><br><span class="line">       |      +----+      |    +----+      |     +----+</span><br><span class="line">       |      |加密|      |    |加密|      |     |加密|</span><br><span class="line">       |      +----+      |    +----+      |     +----+</span><br><span class="line">       |        v         |      v         |       v</span><br><span class="line">+--+   |    +---------+   |  +---------+   |  +---------+</span><br><span class="line">|IV|---+    |密文分组1|---+  |密文分组2|---+  |密文分组3|</span><br><span class="line">+--+        +---------+      +---------+      +---------+</span><br></pre></td></tr></table></figure>
<p>注意，IV(初始化向量)是一个分组长度的一个随机数。<br>CBC解密的逻辑如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">            +---------+      +---------+      +---------+</span><br><span class="line">            |明文分组1|      |明文分组2|      |明文分组3|</span><br><span class="line">            +---------+      +---------+      +---------+</span><br><span class="line">                ^                ^                 ^</span><br><span class="line">              +---+            +---+             +---+</span><br><span class="line">       +-----&gt;|XOR|       +---&gt;|XOR|       +----&gt;|XOR|</span><br><span class="line">       |      +---+       |    +---+       |     +---+</span><br><span class="line">       |        ^         |      ^         |       ^</span><br><span class="line">       |      +----+      |    +----+      |     +----+</span><br><span class="line">       |      |解密|      |    |解密|      |     |解密|</span><br><span class="line">       |      +----+      |    +----+      |     +----+</span><br><span class="line">       |        ^         |      ^         |       ^</span><br><span class="line">+--+   |    +---------+   |  +---------+   |  +---------+</span><br><span class="line">|IV|---+    |密文分组1|---+  |密文分组2|---+  |密文分组3|</span><br><span class="line">+--+        +---------+      +---------+      +---------+</span><br></pre></td></tr></table></figure>
<p>可以看到CBC加密只能一个block一个block的顺序进行。解密时，如果一个密文分组出了问题<br>会影响到当前和下一个明文分组。翻转密文分组中的一个bit，会使得下一个明文分组中的<br>对应bit翻转，会使得当前明文分组中的若干个bit变化，接收方无法识别这种攻击，需要<br>引入消息认证码解决。</p>
<p>CFB(Cipher FeedBack), 密文反馈模式。OFB(Output-Feedback), 输出反馈模式。<br>CTR(Counter模式)。这几种模式的模型差不多，区别在于秘钥流生成的方式不一样。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     +--------+   	         +--------+                     +--------+</span><br><span class="line">            |明文分组|                  |明文分组|                     |明文分组|</span><br><span class="line">            +--------+                  +--------+                     +--------+</span><br><span class="line">                |                           |       +--------+             |     </span><br><span class="line">                v                           v       |        |             v     </span><br><span class="line">   +----+     +---+          +----+       +---+     |  +---+ |  +----+   +---+   </span><br><span class="line">+-&gt;|加密|----&gt;|XOR|       +-&gt;|加密|--+---&gt;|XOR|     +-&gt;|CTR|-+-&gt;|加密|--&gt;|XOR|   </span><br><span class="line">|  +----+     +---+       |  +----+  |    +---+        +---+    +----+   +---+   </span><br><span class="line">|               |         |          |      |                              |     </span><br><span class="line">|               v         |          |      v                              v     </span><br><span class="line">|           +--------+    |          |  +--------+                     +--------+</span><br><span class="line">+-----------|密文分组|    +----------+  |密文分组|                     |密文分组|</span><br><span class="line">            +--------+                  +--------+                     +--------+</span><br><span class="line">   </span><br><span class="line">     CFB                             OFB                           CTR</span><br></pre></td></tr></table></figure>
<p>这几种模式的明文和密文之间都只差一个异或操作，唯一不同的是异或操作的另一个操作数。<br>CFB用的是上次生成的密文加密得到；OFB用IV反复加密得到；CTR用一个计数器反复加密得到。<br>可以看到CFB的每次操作必须要依赖上一次的加密结果，OFB可以提前准备每次要用的异或值，<br>CTR甚至可以直接得到某个分组需要用的异或值，前后依赖程度逐个变小。</p>
<p>XTS模式主要用于磁盘加解密，可以做到block之间独立的加解密。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        +------------+            +----+</span><br><span class="line">        |128bit tweak|            |明文|</span><br><span class="line">        +------------+            +----+</span><br><span class="line">             |                      v</span><br><span class="line">             |                    +---+</span><br><span class="line">             |                +---+XOR|</span><br><span class="line">             |                |   +---+</span><br><span class="line">             v                |     v</span><br><span class="line">+----+     +----+     +--+    |   +----+      +----+</span><br><span class="line">|key2|----&gt;|加密|----&gt;|GF|----+   |加密|&lt;-----|key1|</span><br><span class="line">+----+     +----+     +--+    |   +----+      +----+</span><br><span class="line">                              |     v</span><br><span class="line">                              |   +---+</span><br><span class="line">                              +--&gt;|XOR|</span><br><span class="line">                                  +---+</span><br><span class="line">                                    v</span><br><span class="line">                                  +----+</span><br><span class="line">                                  |密文|</span><br><span class="line">                                  +----+</span><br></pre></td></tr></table></figure>

<p>Note: 消息认证码中的GCM模式依赖上述的CTR模式;基于CBC的消息认证码是CCM;使用hash<br>      来生成消息认证码(带秘钥的hash)叫HMAC。</p>
]]></content>
      <tags>
        <tag>加解密</tag>
      </tags>
  </entry>
  <entry>
    <title>模块复位的基本逻辑</title>
    <url>/2021/06/28/%E6%A8%A1%E5%9D%97%E5%A4%8D%E4%BD%8D%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>设备复位的功能，需要软硬件一起联合起来考虑。本文试图梳理下要给一个linux设备驱动<br>加上复位功能需要考虑的基本逻辑。本文只从逻辑的层面给出我的认识，不涉及具体的<br>硬件和软件驱动。</p>
<p>首先我们对这个设备做出一些基本的假设。这个设备是一个PCIe设备，这个设备有多个PF，<br>一个PF又有多个附带的VF。每个function上又有很多可以独立工作的部件，比如很多队列。<br>这个设备可以工作在内核态，也可以工作在用户态, 也可以工作在虚拟机的内核态。</p>
<p>复位可以是整个设备的复位，我们称之为全局复位。全局复位发生的时候，这个设备上的<br>PF和VF都要被复位。复位也可以是function级别的，比如一个PF或者VF单独复位，其他<br>function的功能不受到影响, 我们把这种复位叫function复位，因为我们讨论的是PCIe<br>设备，这里也就是FLR。当然逻辑上，我们还有function里独立工作部件的复位，比如队列<br>的复位。</p>
<p>我们现在对复位做更清楚的定义。一般, 复位是对设备回归初始状态的操作，但它回归到的<br>状态不是刚刚上电的状态。用户在使用设备的时候，一般会先做一些初始的配置，比如，<br>enable几个VF, 配置一些队列的资源。这些配置在上下电或者驱动加卸载的时候要清除。<br>但是，在复位的时候，我们希望保留这些配置。一般需要复位的时候为系统发生了错误，无<br>法继续运行，复位只是叫系统回复之前可以运行的状态，清除用户之前的配置是不合适的。</p>
<p>所以，各种复位首先要考虑的问题是，是否需要保留用户配置，怎么保留，在复位完成后<br>怎么恢复这些配置。</p>
<p>我们还需要考虑带流量复位的问题，需要复位的时候，设备上可能还有部件在工作。冒然<br>做复位可能导致异常出现。一般的做法是要先把工作的部件停下来，再进行复位操作。</p>
<p>考虑到了以上两点，剩下的就是具体结合硬件提供的功能，写代码完成功能了。下面我们<br>再近一步说明以上两点。</p>
<p>一般，设备的全局配置保存在设备全局寄存器或者是PF里。所以，设备全局复位或者PF FLR<br>需要考虑配置保存，恢复相关的东西。全局复位中，如果硬件没有复位PCIe SRIOV相关的配置，<br>PCIe VF是一直保持的，设备驱动需要保留恢复设备业务相关的配置。但是PF的FLR，PCIe<br>协议规定VF要被disable，这也就意味着PF FLR会触发VF消失。不过，设备驱动并不需要在<br>复位完成后enable相应的VF，这是因为在PF FLR的流程里，PCIe总线驱动会保留VF数目的<br>配置，在PF FLR后enable相关VF(不过，全盘考虑这个问题，如果PF FLR的时候，VF在虚拟机<br>里正在使用，重新enable的VF在虚拟机里是否还可以继续使用，这里还不清楚)。</p>
<p>带流量复位的问题，可能是复位里最复杂的了。我们考虑全局复位的情况，一般，全局复位<br>的操作发生PF的驱动里, 我们可以在复位的时候先检查PF, 把PF的工作停下来再复位。但是，<br>我们怎么才能停下来正在虚拟机里工作的VF？这就需要PF和VF之间有硬件上的通知机制, PF<br>要进行全局复位的时候，先用相关的通知机制通知VF, VF收到通知后把它自己的工作停下<br>来，然后VF可以通知PF它已经停下工作，PF可以进行全局复位了。这个是一个合理的带流量<br>复位应该做的基本的软硬件配合的考虑。</p>
<p>如果，硬件没有PF/VF之间的通知机制，软件上可以做些什么来补救? 其实，补救的办法也<br>是要依赖硬件的行为。比如，如果全局复位的时候，VF不响应软件的请求(写入都丢弃，读<br>到的都是全1), 我们就可以在软件发送请求的时候加定时器，如果超时还没有完成，就猜测<br>发生了全局复位。但是，这样的补救一般运气的成分大，一不留神就有给你惊喜的地方。<br>比如，VF读到全1就有可能引起软件的误判, 超时之后VF怎么了解到是发生了全局复位，VF<br>又怎么判断全局复位完成了…</p>
]]></content>
      <tags>
        <tag>软件架构</tag>
      </tags>
  </entry>
  <entry>
    <title>测试openssl的性能</title>
    <url>/2021/06/19/%E6%B5%8B%E8%AF%95openssl%E7%9A%84%E6%80%A7%E8%83%BD/</url>
    <content><![CDATA[<ol>
<li>openssl基本命令</li>
</ol>
<hr>
<p>AES对称加解密：</p>
<p>openssl enc -aes-128-cbc -in data -out key_encrypt -K 12345678901234567890 -iv 12345678<br>openssl enc -aes-128-cbc -in key_encrypt -out key_decrypt -K 12345678901234567890 -iv 12345678 -d</p>
<p>公钥加密：<br>openssl rsautl -encrypt -in rsa_test -inkey test_pub.key -pubin -out rsa_test.en -engine uadk</p>
<p>公钥生成：<br>openssl rsa -in test.key -pubout -out test_pub.key -engine uadk</p>
<p>私钥生成:<br>openssl genrsa -out test.key -engine uadk 4096</p>
<p>私钥解密：<br>openssl rsautl -decrypt -in rsa_test.en -inkey test.key -out rsa_test.de -engine uadk</p>
<p>签名：<br>openssl rsautl -sign -in msg.txt -inkey test.key -out signed.txt -engine uadk</p>
<p>认证：<br>openssl rsautl -verify -in signed.txt -inkey test_pub.key -pubin -out verified.txt -engine uadk</p>
<p>哈希:<br>openssl md5/sha1/sha256/sm3 -engine uadk data<br>openssl md5/sha1/sha256/sm3 data</p>
<p>如上，有-engine xxx的表示用执行的硬件加解密engine做任务，没有指定就是用openssl<br>里提供的软件计算方法搞。</p>
<p>在非对称加解密的测试中，我们使用RSA算法。需要先生成私钥，然后生成公钥，然后用<br>秘钥进行加解密和签名、认证的测试。</p>
<ol start="2">
<li>openssl speed命令</li>
</ol>
<hr>
<p>openssl speed aes</p>
<p>openssl speed rsa</p>
<p>openssl speed -engine uadk -async_jobs 1 -evp md5</p>
<p>openssl speed -engine uadk -async_jobs 1 -evp aes-128-cbc</p>
<p>openssl speed -engine uadk -elapsed rsa2048  // 如下例子</p>
<p>openssl speed -engine uadk -elapsed -async_jobs 1 rsa2048</p>
<p>openssl speed -engine uadk -elapsed -async_jobs 36 rsa2048</p>
<p>如上，加了-async_jobs使用了openssl里的异步机制，如果engine里使用过了openssl里的<br>异步机制，这里就会触发engine里的异步机制生效。</p>
<p>openssl speed的代码在openssl/apps/speed.c，拿同步rsa2048为例。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">main</span><br><span class="line">  /* 分配input，output buffer */</span><br><span class="line">  +-&gt; app_malloc(buflen, &quot;input buffer&quot;);</span><br><span class="line">  /* 只看同步，即async_jobs = 0的情况 */</span><br><span class="line">  +-&gt; run_benchmark(async_jobs, RSA_sign_loop, loopargs);</span><br><span class="line">    /*</span><br><span class="line">     * 可以看到这里的内存使用模型是，反复的用一个buffer做sign。如果</span><br><span class="line">     * engine的实现没有另外申请内存，这个测试将反复用一块固定的buffer。</span><br><span class="line">     */</span><br><span class="line">    +-&gt; RSA_sign_loop</span><br><span class="line">      +-&gt; RSA_sign</span><br><span class="line">        /* 如果下面适配的是openssl engine, 可以实现如下的回调函数支持 */</span><br><span class="line">        +-&gt; rsa-&gt;meth-&gt;rsa_sign</span><br><span class="line">	+-&gt; RSA_private_encrypt</span><br><span class="line">	  +-&gt; rsa-&gt;meth-&gt;rsa_priv_enc</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>加解密</tag>
        <tag>openssl</tag>
      </tags>
  </entry>
  <entry>
    <title>图解密码技术笔记</title>
    <url>/2021/06/28/%E5%9B%BE%E8%A7%A3%E5%AF%86%E7%A0%81%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li>监听</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-----+               +-----+</span><br><span class="line">|  A  |------+------&gt; |  B  |</span><br><span class="line">+-----+      |        +-----+</span><br><span class="line">             v</span><br><span class="line">           +-----+</span><br><span class="line">           |  C  |</span><br><span class="line">           +-----+</span><br></pre></td></tr></table></figure>
<p>比如A发送信息给B，可能有C在链路上监听。应对的方法就是对要发送的信息加密, 比如我<br>们可以把发送的信息码字都加上1，B减去1就得到A原来发送的信息。这里就引入了密码学<br>上的几个基本概念, 这里对发送的信息加密就是原来的信息和额外的信息做运算，我们把<br>额外的信息叫做秘钥，把所做的运算叫做加解密算法。秘钥一般是不公开的，对于加解密<br>算法，有公开的算法，也有商业公司自己保密的算法，不过公开的算法的安全性要大大高于<br>私有的算法，现在一般的做法也是密码相关的行业组织会公开征集特定用途的算法，大家<br>通过竞争选出最好的算法。</p>
<p>可以看到上面的例子中，A用来加密的秘钥和B用来解密的秘钥是一样的。这种加解密的<br>方法叫对称加解密。实际使用中，秘钥和加解密算法是很复杂的，C即使听到了加密信息，<br>没有秘钥也解不出A发出的信息。</p>
<p>但是这样的加解密方法有一个要解决的问题: 怎么把一样的秘钥分发给A和B(前面我们已经<br>提到算法是公开的)。A直接传给B显然是不靠谱的，因为C完全可以听到。引入第三方，也不<br>靠谱，只要传输，C就可以听到秘钥。</p>
<p>为了应对是上面的问题，人们发明了非对称加解密。也就是说A用来加密的秘钥和B用来解密<br>的秘钥不是同一个。非对称加解密使用的方法就是: 把加密秘钥发给对方，请对方用这个加密<br>秘钥加密信息, 解密秘钥自己留着，用来解密信息。下面的图是基本流程:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> +-----+                      +-----+</span><br><span class="line"> |  A  |---------------------&gt;|  B  |</span><br><span class="line"> +-----+                      +-----+</span><br><span class="line"></span><br><span class="line">           +-------+</span><br><span class="line">           |pub key|  --&gt; </span><br><span class="line"> +-----+   +-------+          +-----+</span><br><span class="line"> |  A  |---------------------&gt;|  B  |</span><br><span class="line"> +-----+                      +-----+     +-------+   +---+</span><br><span class="line">                                          |message| + |key| --+</span><br><span class="line">             +--------------+             +-------+   +---+   |</span><br><span class="line">          &lt;--|crypto message| &lt;-------------------------------+</span><br><span class="line"> +-----+     +--------------+ +-----+</span><br><span class="line"> |  A  |---------------------&gt;|  B  |</span><br><span class="line"> +-----+                      +-----+</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">+--------+   +--------------+</span><br><span class="line">|priv key| + |crypto message|     </span><br><span class="line">+--------+ | +--------------+</span><br><span class="line">           v</span><br><span class="line">+-----------------+</span><br><span class="line">|message sent by B|</span><br><span class="line">+-----------------+</span><br></pre></td></tr></table></figure>
<p>可以看到C即使听到pub key也没有用，因为pub key是用来给发给A的信息加密的。上面又<br>有几个新的概念，公钥是可以公开的秘钥，私钥必须保密。</p>
<p>我们平时用的ssh用的就是非对称加密, 和上面的模型是可以对上的。非对称加密解决了<br>对称加密里对称秘钥分发的问题，我们可以使用非对称加解密代替对称加解密。当然，我们<br>也可以一开始采用非对称加解密解决对称加解密秘钥分发的问题，然后就可以使用对称<br>加解密了。</p>
<p>可以看到，非对称加解密的缺点是C可以拿到给A发信息用的加密秘钥。这样C可以把自己的<br>公钥发给B，诱导B用C的公钥加密信息，这样C监听B发给A的信息，就可以用C自己的私钥<br>解密信息。为了解决这个问题，就需要B可以确认他收到的是A的公钥。(to do: how to do)</p>
<p>非对称加解密还有一个缺点，就是秘钥生成和加解密都需要很大的算里。所以很适合做专门<br>的硬件加速器去offload cpu资源。</p>
<p>这里可以看下RSA算法步骤，RSA算法是经典的非对称加解密算法。我们这里只简单罗列RSA<br>算法的步骤，因为他表现的很简单有趣。这里E和N为加密秘钥，D和N为解密秘钥。加密和<br>解密做的运算就是:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">                  E                            D</span><br><span class="line">密文 = 明文  mod N           明文 = 密文  mod N</span><br></pre></td></tr></table></figure>
<p>是不是很简单? 上面的密文和明文都是数字，加解密完全是一个求幂然后取模运算。但是<br>E, D是一个很大的数，大到用二进制表示，需要512bit，1024bit，2048bit，4096bit…<br>不同秘钥长度，当然密码的强度是不一样的。</p>
<p>E, D, N生成的算法是：</p>
<ol>
<li><p>找两个很大的素数p, q</p>
</li>
<li><p>N = p × q</p>
</li>
<li><p>L = 最小公倍数(p - 1, q - 1)</p>
</li>
<li><p>E为和L互质的数 —&gt; (E, N)是加密秘钥，公钥</p>
</li>
<li><p>(E × D) mod L = 1, 求出D —&gt; (D, N)是解密秘钥，私钥</p>
</li>
</ol>
<p>第2,5步的逆运算在数学上很困难，这个是RSA算法保密的根本。</p>
<ol start="2">
<li>篡改</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-----+                      +-----+</span><br><span class="line">|  A  |------+  +-----------&gt;|  B  |</span><br><span class="line">+-----+      |  |            +-----+</span><br><span class="line">             v  |</span><br><span class="line">           +----++</span><br><span class="line">           |  C  |</span><br><span class="line">           +-----+</span><br></pre></td></tr></table></figure>
<p>相比较上面的被动攻击，C还可以篡改A发给B的数据，然后发给B(其实上面也提到了主动<br>攻击, 我们先从简单的说起)。为了应对这样的攻击，B需要有办法验证收到信息的完整性。<br>一般用单向散列得到A发出信息的hash码，然后A把这个hash码也发给B，B对收到的信息做<br>同样的单向散列，把得到的hash码和收到的hash作对比，从而验证信息的完整性。</p>
<p>这要的算法有SHA3, MD5等。其实，一般我们发送大文件的时候，用md5sum算文件hash码,<br>然后在对接收到的文件做校验，就是一样的道理。</p>
<p>单向散列算法需要保证的就是防止碰撞发生，简单说就是不同数据得到的hash码要不一样。</p>
<p>单向散列只是单纯的验证数据的完整性，并不能确认数据就是A发出的。C可以把A发给B的<br>数据和hash值都截获，修改数据，然后对修改后的数据做下单向散列，然后把修改的数据<br>和新生成的hash值发给B。</p>
<ol start="3">
<li>认证</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+-----+                      +-----+</span><br><span class="line">|  A  |--------------------&gt; |  B  |</span><br><span class="line">+-----+       +------------&gt; +-----+</span><br><span class="line">              |</span><br><span class="line">           +--+--+</span><br><span class="line">           |  C  |</span><br><span class="line">           +-----+</span><br></pre></td></tr></table></figure>
<p>如上，C也做下单向散列，然后把正确的hash码发送给B。B需要有办法确认他收到信息是A<br>发给他的，而不是别人伪造的。这里的问题和上面非对称加解密里C把自己的公钥发给B是<br>一样的问题。</p>
<p>当A，B有共享秘钥的时候，解决这个问题的方法是，信息和共享秘钥一起做单向散列。<br>但是非对称加密公钥配送的问题还是没有解决</p>
<p>对上面认证(也叫消息认证码)的攻击有: 重放攻击。有了消息认证码之后C无法篡改A发给B<br>的信息，但是C可以截获A发给B的信息，在A发给B信息后，再次发同样的信息给B。比如，A<br>给B的信息是请求B向A转一笔钱，那么B接收的C的信息后将会再次转钱给A。不过，应对这种<br>重放攻击，A只要在发送的信息加上编号或者是时间戳就好了。</p>
<p>消息认证码无法解决的问题: 对第三方证明，防止否认。对第三方证明比较好理解, 共享<br>秘钥存在于A和B，第三方的机构无法证明信息是A向B发的。防止否认的意思是, B也无法证<br>明A确实向B发了一条消息。其中的关键是消息认证码用的是共享秘钥, 比如A向B发了一个信<br>息说向B借了10万元钱(相当于A给B写了一个欠条)，这个信息用消息认证码加密, 当B拿着这<br>个消息向A去索要钱的时候，A完全可以否认自己发过这个消息，因为B也有A一样的秘钥，完<br>全是可以自己制造这个消息出来的。为了解决A否认消息是他发出的，就要引入数字签名，<br>就是给自己发出的信息签上自己的名字。</p>
<ol start="4">
<li>签名</li>
</ol>
<hr>
<p>如何给自己发出的消息上标记上自己无法否认的信息作为自己的签名？签名可以用非对称<br>加解密的相反运算实现: 用私钥加密信息给信息签名，用公钥解密加密的信息做验证。<br>因为私钥只有自己知道，而且用和私钥不对应的公钥算出来的数据不是被加密数据(算出<br>的数据接近随机值), 所以可以用这样的办法实现数字签名。</p>
<p>可以看出来，签名和签名验证的计算量是很大的，比如用RSA算法做签名，相当于对数据做<br>乘幂运算再做取模运算。当然，签名可以对数据的单向散列值做, 不过即使这样计算量也大。<br>可以看出签名和非对称加解密是可以用一个硬件加速器来做的。这里先做单向散列再做签名<br>是多个加解密算法级联使用的一个例子，在实际情况中这种情况很多，这对软件加解密框架<br>的设计提出了比较高的要求，关于这部分的设计实现可以参考Linux内核crypto子系统的<br>设计实现，同时该子系统也是c语言面向对象编程的一个很好的参考实例。</p>
<p>有很多针对签名的攻击, 其中利用签名解密信息的攻击很有意思。签名的本质是用私钥做<br>运算, 所做的运算如果数据是正好是加密数据，那么相当于做解密运算。攻击者如果手上<br>有一个别人发给签名者的加密信息(签名者对应的公钥加过密), 攻击者请求签名者给这段<br>信息签名, 如果签名，实际上签名者就被攻击者诱导做了解密运算。不过，一般情况下签名<br>者也不会对一段来历不明的信息做签名。</p>
<p>签名还有一个注意的地方是撤销签名，类比一下就是撕毁借条。但是数字签名无法撕毁，<br>办法就是再签一个说之前的签名无效了。</p>
<ol start="5">
<li>公钥证书</li>
</ol>
<hr>
<p>上面的各种办法其实还是没有办法解决非对称加解密里公钥配送的问题。</p>
<p>引入第三方机构办法公钥证书。</p>
<ol start="6">
<li>秘钥</li>
</ol>
<hr>
<p>上面各种加解密算法里多次提到秘钥。这里的秘钥其实就是一个巨大的数字。除了公钥<br>秘钥必须严格保密，因为加解密算法是公开的，秘钥的价值非常巨大，他的价值和明文是<br>相等的。</p>
<p>需要注意的是对称加解密里的秘钥配送还可以用秘钥配送去实现，比较经典的有DH算法。<br>直观的看，就是A和B预定几个数值(明文传送，C是可见的)，然后用公共的算法得到一个<br>相同的数值, C即使知道预定的值也无法计算出A和B的共享秘钥。</p>
<p>DH算法的一般步骤是:</p>
<p>秘钥管理里有秘钥作废比较有意思，秘钥作废之后要彻底销毁。这样做的原因是防止之前<br>的信息被解密，比如C长期截获并保存A和B之间的信息，如果C的到废弃的秘钥，他就可以<br>把之前的信息解密。</p>
<ol start="7">
<li>随机数</li>
</ol>
<hr>
<p>有些秘钥使用的是随机数，所以随机数是否是真随机这一点很关键。另外，并不是所有使用<br>随机数的地方都需要真随机数，但是加解密中使用的随机数一定要用真随机。单纯软件生<br>成的随机数一定是伪随机数。</p>
<p>和加解密类似，随机数的生成包括算法和种子，随机数生成算法类比加解密算法，是公开<br>的，种子类似秘钥，一定是私密的而且必须是真随机的。因为纯软件的种子一定不是随机<br>的，所以上面提到单纯软件生成的随机数一定是伪随机数。</p>
<p>Intel上硬件上已经提供了真随机数生成的指令。</p>
<ol start="8">
<li>PGP</li>
</ol>
<hr>
<ol start="9">
<li>SSL/TSL</li>
</ol>
<hr>
<ol start="10">
<li>区块链</li>
</ol>
<hr>
<ol start="11">
<li>硬件</li>
</ol>
<hr>
<p> SEC, HPRE, ZIP, RDE, TRNG, DMA</p>
]]></content>
      <categories>
        <category>read</category>
      </categories>
      <tags>
        <tag>加解密</tag>
      </tags>
  </entry>
  <entry>
    <title>搭建邮件客户端进行Linux kernel开发</title>
    <url>/2021/07/11/%E6%90%AD%E5%BB%BA%E9%82%AE%E4%BB%B6%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9B%E8%A1%8CLinux-kernel%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<p>进行linux kernel开发需要实时关注kernel相关邮件列表的动态，本文介绍怎么搭建一个<br>邮件客户端，以便更好的查看邮件列表中的邮件。根据本文内容可以搭建一个这样的邮件<br>阅读环境。</p>
<p>收到内核邮件列表中的邮件需要首先订阅相应的邮件列表，具体订阅的方法请自行google.<br>订阅的本质是向一个地址发送特定内容的邮件。</p>
<p>每天来自一个邮件列表中的邮件可能会有几十封，如果同时订阅几个邮件列表，每天邮箱<br>中的邮件会有几百封。怎么管理这些邮件，有一种方法是用雷鸟邮件客户端(thunderbird),<br>还有一种方法就是本文将要介绍的 fetchmail + procmail + msmtp + mutt。当然你也可以<br>直接从你注册的邮箱里看邮件，不过从成千上百的邮件中找到你想看的，再找到别人的回复<br>的邮寄真的非常麻烦。</p>
<p>除了邮件客户端，还有一个问题：用什么邮箱。一般的公司邮箱可以用来注册邮件列表，<br>但是业余时间也想学习下大牛们的讨论，用公司邮箱显然不太好。于是需要注册一个私人<br>邮箱，用gmail显然要显得专业，但是用gmail邮箱设置thunderbird的时候比较麻烦，要先<br>生成一个第三方app接入gmail邮箱的密码，在设置thunderbird密码的时候设置进去，本人<br>用 gmail + thunderbird 的方式搭的环境，收邮件的时候时而连上时而连不上，所以本文<br>对这个不做介绍。用163,126邮箱注册了邮件列表，发了邮件就没有了下文，估计是给屏蔽<br>了，大家也不用忙活用163，126邮箱注册邮件列表了。在网上发现有人用139的邮箱成功<br>注册内核邮件列表，试了一下，果然可以。所以本文使用139邮箱。(update: 139用了一段<br>时间也不行了)</p>
<p>有了上面的背景，我们可以开始介绍整个环境的搭建了。首先想到的肯定是 139邮箱 + thunderbird<br>的方法，thunderbird图形化操作似乎也更方便一点。thunderbird只是一个邮件客户端，你<br>要配置它，告诉它发送/接收邮件使用的发送和接收服务器分别是什么。139邮箱的发送接收<br>服务器可以在邮箱的设置里找到，至于怎么设置thunderbird, 具体方法请自行google.<br>用thunderbird的好处是可以设置不同的邮件目录，对应的目录设置不同的过滤器以归类<br>邮件，还有就是可以以一个个thread的形式管理邮件，回复关系一目了然。需要注意的是<br>thunderbird的默认格式不符合linux kernel patch的格式，需要简单配置一下，想用<br>thunderbird的人也要google下了。</p>
<p>但是像本人这样，有时需要远程调试，然后在服务器上把调试log发回本地的情况时。图形<br>界面的邮件客户端显然满足不了需求，于是有了本文要介绍的命令行邮件客户端配置。<br>本文使用139邮箱，139邮箱默认是开启pop, smtp服务的，下面的配置脚本中可以看到<br>pop, smtp服务器地址。</p>
<p>首先简要说明fetchmail, procmail, msmtp, mutt的作用。<br>fetchmail是从邮箱下在邮件, procmail提供对邮件的过滤功能, msmtp用来发送邮件，<br>mutt用来看邮件和写邮件。本人的工作环境是ubuntu, 所以下面配置都是在ubuntu的环境<br>下完成的。ubuntu下安装上面四个软件apt-get install即可搞定。</p>
<p>fetchmail的配置文件在自己的home目录下的.fetchmailrc文件，需要自己创建一个这样<br>的文件。文件内容大概如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">poll pop.139.com</span><br><span class="line">protocol POP3</span><br><span class="line">user &quot;your_email_address@139.com&quot;</span><br><span class="line">password &quot;your_password&quot;</span><br><span class="line">keep</span><br><span class="line">ssl</span><br><span class="line">mda &quot;procmail -d %T&quot;</span><br></pre></td></tr></table></figure>

<p>配置完后，运行fetchmail, 可以看到在/var/mail/下有以你账号名作为文件名的文件。<br>里面就是下载的邮件。本人一开始没有配置最后一行，下载邮件失败, 加上最后一行后可以<br>下载。</p>
<p>procmail的配置文件也在自己的home目录下，为.procmailrc， 内容大概如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PATH=/usr/local/bin:/usr/bin:/bin</span><br><span class="line">MAILDIR=$HOME/Mail</span><br><span class="line">DEFAULT=$MAILDIR/mbox</span><br><span class="line">LOGFILE=$MAILDIR/log</span><br></pre></td></tr></table></figure>

<p>msmtp的配置文件也在自己的home目录下，为.msmtprc， 内容大概如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">account default</span><br><span class="line">host smtp.139.com</span><br><span class="line">from your_email_address@139.com</span><br><span class="line">auth login</span><br><span class="line">tls on</span><br><span class="line">tls_certcheck off</span><br><span class="line">user your_email_address@139.com</span><br><span class="line">password &quot;your_password&quot;</span><br></pre></td></tr></table></figure>

<p>配置好后可以用 msmtp -S 检查自己的配置是否正确。</p>
<p>mutt的配置文件也在自己的home目录下，为.muttrc， 内容大概如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set from=&quot;your name &lt;your_email_address@139.com&gt;&quot;</span><br><span class="line">set use_from=yes</span><br><span class="line">set sendmail=&quot;/usr/bin/msmtp&quot;</span><br><span class="line">set editor=vim</span><br><span class="line">set folder=&quot;$HOME/Mail&quot;</span><br><span class="line">set record=&quot;+sent&quot;</span><br><span class="line">set mbox=&quot;+mbox&quot;</span><br><span class="line">set postponed=&quot;+postponed&quot;</span><br></pre></td></tr></table></figure>

<p>配置好后每次可以先fetchmail, 然后用mutt -f ~/Mail/mbox打开邮件查看。</p>
<p>到现在就基本可以命令行浏览，收发邮件了。但是如果你一下订阅了几个邮件列表，想<br>分类管理邮件，下面给一个简单的示范：</p>
<p>想单独管理下来自<span class="exturl" data-url="bWFpbHRvOiYjMTA4OyYjeDY5OyYjMTEwOyYjMTE3OyYjMTIwOyYjNDU7JiN4NzA7JiN4NjM7JiMxMDU7JiM2NDsmI3g3NjsmIzEwMzsmI3g2NTsmI3g3MjsmIzQ2OyYjMTA3OyYjeDY1OyYjMTE0OyYjMTEwOyYjMTAxOyYjMTA4OyYjeDJlOyYjeDZmOyYjMTE0OyYjeDY3Ow==">&#108;&#x69;&#110;&#117;&#120;&#45;&#x70;&#x63;&#105;&#64;&#x76;&#103;&#x65;&#x72;&#46;&#107;&#x65;&#114;&#110;&#101;&#108;&#x2e;&#x6f;&#114;&#x67;<i class="fa fa-external-link-alt"></i></span> 邮件列表里的邮件。先在家目录的Mail下<br>创建一个文件保存邮件，这里我们创建一个叫pci-mbox的文件。在.procmailrc文件中添加<br>一下几行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">:0</span><br><span class="line">* ^Cc:.*linux-pci@vger.kernel.org</span><br><span class="line">pci-mbox</span><br></pre></td></tr></table></figure>
<p>之后再fetchmail，来自pci邮件列表中的邮件就自动保存在~/Mail/pci-mbox中了。</p>
<p>注意：</p>
<ol>
<li>fetchmail失败: 在.procmailrc中配置不当（配置不当的过滤脚本）会导致fetchmail<br>下载不了邮件</li>
</ol>
]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>用mprotect定位踩内存问题</title>
    <url>/2021/06/27/%E7%94%A8mprotect%E5%AE%9A%E4%BD%8D%E8%B8%A9%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>Linux用户态程序踩内存时可以用mprotect定位，mprotect本身是linux系统上的一个系统<br>调用，这个系统调用可以改变一段内存的读写属性, 当有非法的访问访问对应的内存的时候<br>会给进程发一个SIGSEGV信号，进程可以在信号处理函数中加调试信息进行定位。</p>
<p>mprotect的参数为要保护的虚拟地址，保护地址的大小，和保护地址空间的属性。这里<br>地址size必须是已页对齐的，地址空间的属性有读、写、执行和不可接入。</p>
<p>显然，当被踩内存本身就是只读的时候，我们一开始就可以用mprotect把这段内存保护起来,<br>别的执行流踩了这段内存就会触发信号。如果，被踩的内存是一段可读可写的内存，我们<br>可以在正常执行的时候调用mprotect设置为读写，正常执行完后用mprotect设置为只读。</p>
<p>在信号处理函数中，可以调用backtrace, backtrace_symbols相关函数把调用栈打出来。<br>如下的测试代码，在X86上用gcc -rdynamic test.c编译运行是OK的，可以打出调用栈，<br>加-rdynamic是为了打出调用栈里的函数名。但是在ARM64的环境下，需要用<br>gcc -rdynamic -funwind-tables test.c来编译测试代码，否则只能打出模块的名字。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;execinfo.h&gt;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">#include &lt;signal.h&gt;</span><br><span class="line">#include &lt;malloc.h&gt;</span><br><span class="line">#include &lt;sys/mman.h&gt;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * test in x86 with gcc -rdynamic test.c is OK.</span><br><span class="line"> *</span><br><span class="line"> * however, in aarch64, return of backtrace is alway 1.</span><br><span class="line"> * use gcc -rdynamic -funwind-tables test.c to solve this problem.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">void fun_3(void);</span><br><span class="line"></span><br><span class="line">void handler(int sig, siginfo_t *si, void *unused)</span><br><span class="line">&#123;</span><br><span class="line">	fun_3();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void fun_3(void)</span><br><span class="line">&#123;</span><br><span class="line">#define SIZE 10</span><br><span class="line">	void *buffer[SIZE];</span><br><span class="line">	char **strings;</span><br><span class="line">	int n, i;</span><br><span class="line"></span><br><span class="line">	n = backtrace(buffer, SIZE);</span><br><span class="line"></span><br><span class="line">	strings = backtrace_symbols(buffer, n);</span><br><span class="line"></span><br><span class="line">	for (i = 0; i &lt; n; i++) &#123;</span><br><span class="line">		printf(&quot;%s\n&quot;, strings[i]);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	free(strings);</span><br><span class="line"></span><br><span class="line">	exit(EXIT_FAILURE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void fun_2(void)</span><br><span class="line">&#123;</span><br><span class="line">	fun_3();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void fun_1(void)</span><br><span class="line">&#123;</span><br><span class="line">	fun_2();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void fun_0(void)</span><br><span class="line">&#123;</span><br><span class="line">	fun_1();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	struct sigaction sa;</span><br><span class="line">	char *buffer;</span><br><span class="line">	int pagesize;</span><br><span class="line"></span><br><span class="line">	sa.sa_flags = SA_SIGINFO;</span><br><span class="line">	sigemptyset(&amp;sa.sa_mask);</span><br><span class="line">	sa.sa_sigaction = handler;</span><br><span class="line">	sigaction(SIGSEGV, &amp;sa, NULL);</span><br><span class="line"></span><br><span class="line">	pagesize = sysconf(_SC_PAGE_SIZE);</span><br><span class="line">	buffer = memalign(pagesize, 4 * pagesize);</span><br><span class="line"></span><br><span class="line">	if (mprotect(buffer, pagesize, PROT_READ | PROT_WRITE) == -1)</span><br><span class="line">		printf(&quot;fail to set mprotect\n&quot;);</span><br><span class="line"></span><br><span class="line">	printf(&quot;write a in buffer a\n&quot;);</span><br><span class="line">	*buffer = &#x27;a&#x27;;</span><br><span class="line">	printf(&quot;write a in buffer b\n&quot;);</span><br><span class="line">	sleep(2);</span><br><span class="line">	printf(&quot;write a in buffer c\n&quot;);</span><br><span class="line"></span><br><span class="line">	if (mprotect(buffer, pagesize, PROT_READ) == -1)</span><br><span class="line">		printf(&quot;fail to set mprotect\n&quot;);</span><br><span class="line"></span><br><span class="line">	*buffer = &#x27;b&#x27;;</span><br><span class="line"></span><br><span class="line">	//fun_0();	</span><br><span class="line"></span><br><span class="line">	exit(EXIT_SUCCESS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>软件调试</tag>
        <tag>mprotect</tag>
      </tags>
  </entry>
  <entry>
    <title>程序configure, compile, install的逻辑</title>
    <url>/2021/07/17/%E7%A8%8B%E5%BA%8Fconfigure-compile-install%E7%9A%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p> 拿到一个linux用户态程序的源码，到使用该程序分为一下几步：</p>
<ol>
<li><p>配置(configure)，在源码目录下一般会有configure脚本文件，执行该文件可以检测<br>源码需要的编译链接环境，然后相应的生成makefile文件。</p>
<p>使用./configure –help可以得到该脚本的使用帮助, 一般的有一下几个参数：<br>./configure –host=*** –prefix=*** CC=*** LDFLAGS=*** LIBS=***<br>其中–host指定编译生成的可执行文件的执行环境， –prefix指定make install<br>的安装路径，CC指定用到的编译器，LDFLAGS指定链接时标准的库搜索路径之外的<br>库搜索路径。</p>
</li>
<li><p>编译链接(make)<br>根据Makefile文件中的配置，编译链接成可执行程序。</p>
</li>
<li><p>安装(make install)<br>第一步中(configure)中–prefix会把程序的安装路径写入到makefile中。在这一步会<br>依照该路径把相应的文件拷贝到相应的目录。<br>一个程序可以就只有一个可执行文件。也可能除了可以执行文件外，还需要一些静态<br>库或者是动态库的支持, 这时安装程序就包括把可执行文件和动态库文件拷贝到相应<br>的目录。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title>用户态下使用gpio中断</title>
    <url>/2021/07/17/%E7%94%A8%E6%88%B7%E6%80%81%E4%B8%8B%E4%BD%BF%E7%94%A8gpio%E4%B8%AD%E6%96%AD/</url>
    <content><![CDATA[<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>确定你的gpio驱动是好用的，这时会在/sys/class/gpio下发现gpio对应的文件:gpio***,<br>export, unexport, gpio***是gpio控制器对应的文件，export, unexport是gpio框架<br>提供的用来向用户态导出gpio的文件</p>
<p>假设使用66号gpio口作为中断端口，即产生中断的器件的中断管脚连接的是soc的66号gpio<br>管脚。echo “66” &gt; export 向外导出管脚，会发现/sys/class/gpio下多了目录gpio66<br>gpio66目录中有文件value, direction, edge, power, device等等</p>
<p>echo “in” &gt; direction 设置gpio66脚为输入<br>echo “falling” &gt; edge 设置gpio66脚为下降沿触发中断, 也可以把falling改成rising<br>即为上升沿触发，这时当gpio66管脚上存在一个falling时就会接收到一个中断，怎么把<br>这个接收到的中断在用户态反应出来呢？</p>
<p>在接收到中断的时候value的值会从原来的1变成0，这里假设是下降沿触发, 所以可以<br>使用poll()函数阻塞在value文件对应的文件描述符上，当文件发生变化的时候poll返回<br>相应的中断，具体代码:<br><span class="exturl" data-url="aHR0cDovL2Jsb2cuc2tlcnBhLmNvbS9kc2NobmVsbC9ibG9nLzIwMTMvMTAvMjcvbGludXgtYW5kLWdwaW8tdXNlcnNwYWNlLWludGVycnVwdHMv">http://blog.skerpa.com/dschnell/blog/2013/10/27/linux-and-gpio-userspace-interrupts/<i class="fa fa-external-link-alt"></i></span><br>…</p>
<h2 id="gpiolib-c中的实现"><a href="#gpiolib-c中的实现" class="headerlink" title="gpiolib.c中的实现"></a>gpiolib.c中的实现</h2><p>对edge的读写，最后会调用到： gpio_edge_show()，gpio_edge_store(), 可以看看<br>echo “falling” &gt; edge 内核的函数调用</p>
<p>内核首先找到falling对应的编码，然后在gpio_setup_irq中注册中断处理函数:<br>gpio_sysfs_irq, 所以当gpio管脚上发生中断时，最后会调用中断处理函数中的<br>wake_up_interrupt()通知对应文件上的等待队列?</p>
<p>request_irq(), free_irq()函数在注册中断，释放中断的时候，会对相应的中断线<br>做一定的处理, 包括使能中断等</p>
<p>但是有一个东西不清楚：假设是一个下降沿触发的中断，在接收到中断的时候对应的<br>/sys/class/gpio/gpio***/value 中的值应该从1变成0, 但是使用上面博客中的代码<br>可以发现value中的值一直是1，手动cat value发现其中的值是0。依然没有找见value<br>被设置为0的对应代码?</p>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>gpio</tag>
      </tags>
  </entry>
  <entry>
    <title>软件之间的兼容性问题分析</title>
    <url>/2021/06/19/%E8%BD%AF%E4%BB%B6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<ol>
<li>问题</li>
</ol>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">       +-------+   +------+</span><br><span class="line">|  app  |   | app  |</span><br><span class="line">       +-------+   +------+</span><br><span class="line">                \ /</span><br><span class="line">                 X</span><br><span class="line">                / \</span><br><span class="line">               /   \</span><br><span class="line">       +-------+   +------+</span><br><span class="line">|  lib  |   | lib  |</span><br><span class="line">       +-------+   +------+</span><br><span class="line">                \  /</span><br><span class="line">                 \/</span><br><span class="line">                 /\</span><br><span class="line">                /  \</span><br><span class="line">       +--------+  +--------+</span><br><span class="line">| kernel |  | kernel |</span><br><span class="line">       +--------+  +--------+</span><br><span class="line"></span><br><span class="line">  old ------------------------&gt; new</span><br></pre></td></tr></table></figure>
<p> 如上图，我们需要解决的是新老软件版本之间可以兼容使用的问题。</p>
<ol start="2">
<li>kernel和user space</li>
</ol>
<hr>
<p> 内核和用户态接口包括，系统调用、设备文件、sysfs/proc等。这些接口可以看成是独立<br> 的功能。所以，在用户态使用一个如上的接口时，可以先检测是否有这样的接口。对于<br> 设备文件和sysfs/proc很容易判断一个文件是否存在，对于系统调用如果一个接口底层<br> 驱动没有支持，用户态应该得到不支持的错误码，这需要内核驱动做必要的异常处理并<br> 返回错误码。</p>
<p> 我们考虑lib和kernel之间的新老兼容问题。对于老的内核，新的lib，lib中的代码依赖<br> 老的kernel接口编程，并先要检测kernel的接口是否可以使用，之所有要检测kernel接口<br> 是否可以使用，是为了防止随后kernel版本里删除之前的接口，造成lib的break; 内核<br> 升级但是lib还是老的情况，kernel里新增的接口lib里使用不到是正常现象，kernel里<br> 删除的接口(一般不会发生)，lib在之前使用的时候已经先判断是否支持，考虑的逻辑已经<br> 存在。</p>
<ol start="3">
<li>user space lib和app</li>
</ol>
<hr>
<p> lib和APP之间的接口一般是函数接口，比较难做成特性独立定义。那么在lib发展的过程<br> 中，删除一个特性，必然造成接口的不兼容。所以，要实现新旧库和APP相互兼容，我们<br> 可以lib库里的特性持续增加，并在增加特性的时候做好库版本的定义升级。APP在编程的<br> 时候根据lib版本决定是否可以使用某一个特性。</p>
<p> 持续增加lib中的接口必然造成库的膨胀，可以给将来不计划使用的接口加上deprecation<br> 的标记，提示用户相关接口将会在未来弃用。</p>
<p>[1] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzEwNjQzMTk4Mw==">https://blog.csdn.net/scarecrow_byr/article/details/106431983<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>软件架构</tag>
      </tags>
  </entry>
  <entry>
    <title>软件实现状态机要点</title>
    <url>/2021/06/28/%E8%BD%AF%E4%BB%B6%E5%AE%9E%E7%8E%B0%E7%8A%B6%E6%80%81%E6%9C%BA%E8%A6%81%E7%82%B9/</url>
    <content><![CDATA[<p>在比较复杂一点的软件系统建模的时候，有时需要理清楚系统的状态。这个时候画个系统<br>的状态机出来就很有帮助。注意对于一个系统，可能同时又好几个状态机，之所以这样，是<br>因为每个状态机关注的主题是不一样，每个独立的状态机只能保证关注的主题逻辑上是自恰<br>的。</p>
<p>我们以一个例子说明下。假设我们要为一个设备写一个Linux内核驱动，在驱动里这个设备<br>被抽象为:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct test_dev &#123;</span><br><span class="line">	int state;</span><br><span class="line">	int a;</span><br><span class="line">	int b;</span><br><span class="line">	int c;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这个设备需要初始化，然后配置下才能正常工作，这个设备出错的情况下需要复位这个设备。<br>所以我们给出的test_dev这个设备的状态有：初始化状态，正常工作状态，复位状态。在<br>判断可能的状态的时候，如果没有必要就不要新增加状态进来，一个状态机里每新增加一个<br>状态以后都会成为负担。</p>
<p>有了第一个状态，就可以找可以进入这个状态的激励(event), 比如，这里可能是:设备初始化。<br>基于第一个状态，分析在这个状态可能接收到的所有激励, 看看在这个状态的test_dev接收<br>到某个激励后对test_dev采取的动作(action)应该是怎么样的。比如这里对于初始化状态，<br>可能接收到的event可能有，1. 对test_dev配置，这个动作会使其进入正常工作状态；2.<br>用户还可以做初始化的逆操作，把这个设备释放掉。</p>
<p>重复对每一个状态做上面的分析。我们可以大概得到一个这样的状态机:<br><img src="test_dev_state.svg" alt="一个状态机示例"></p>
<p>接下来我们要针对每一个状态分析是否还有其他的event会发生，比如，我们有可能发现<br>这个设备驱动对应的fd文件上有ioctl，那就需要分析在每一种状态上，这个ioctl进来的<br>时候，test_dev是怎么变化的。当然，在某种状态的时候，我们是可以拒绝执行某种event<br>的。注意这里分析的中心还是test_dev的变化，因为我们这里要建立的是test_dev的状态机，<br>我们不能脱离test_dev而去分析比如fd的变化。再比如，我们这个设备驱动还mmap了一段<br>mmio空间到用户态去，用户态程序可以直接读写硬件寄存器，在test_dev的状态机中也不应<br>该去考虑用户态程序读写硬件寄存器这样的event，因为test_dev的状态机根本处理不了<br>这样的情况。针对用户态程序读写硬件寄存器这样的event，我们要单独分析。test_dev<br>状态机需要考虑的是test_dev结构里各个成员的变化。</p>
<p>完成对所有状态在全部event下的分析，我们的状态机就基本上建完了。剩下就是具体实现<br>的问题。实现中，要求所有对test_dev的action是原子的，也就是说状态变迁的过程是不能<br>被打断的，不然，可以看到我们会陷入各种各样同步带来的问题中。</p>
<p>还是看上面的图，从初始状态到正常工作状态的切换如果不是原子的，那么在这个过程中，<br>如果reset event来了，整个系统的状态将如何切换？如果，configure action里包括很多<br>步骤，而reset event可以在任何步骤到来，那么这个切换的分析工作将变得非常复杂。<br>状态切换变成原子操作将避免这样的情况发生，使得整个系统的状态变得可控。状态切换<br>是原子行为时，如果test_dev正从初始状态切换到工作状态，那么中间到来的reset event<br>将不得不排队，之后再在一个明确的状态响应reset event。</p>
<p>保证原子操作，一个简单的办法就是执行action的时候加锁。</p>
]]></content>
      <tags>
        <tag>软件架构</tag>
      </tags>
  </entry>
  <entry>
    <title>锁使用的一些笔记</title>
    <url>/2021/06/27/%E9%94%81%E4%BD%BF%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li>使用的场景</li>
</ol>
<hr>
<p>   当一个数据结构有多个并发的流程去访问的时候，可以加锁去做互斥，这样数据的<br>   一致性的到保护。并发流程有多种表现形式，比如在linux内核里，内核线程，中断，<br>   来自用户态的系统调用等都可以并发起来; 用户态的话, 各个线程，信号(信号处理<br>   函数里不能使用锁)可以并发。</p>
<p>   加锁其实就是多个执行流在临界区外排队(这里不考虑try锁，就是那种试一下可以加上<br>   就加锁，不可以加上当场就返回的锁)。我们也可以给临界区配置一个原子变量标记，<br>   每个执行流在先抢到这个标记才可以操作保护的数据结构, 操作完保护的数据结构，<br>   退出临界区的时候释放这个标记。原子变量标记的方式，其实是用try锁去保护相应的<br>   数据结构。但是try锁没有了排队等待，需要在加锁失败时做必要的处理。</p>
<ol start="2">
<li>使用时注意的事项</li>
</ol>
<hr>
<ul>
<li><p>锁使用时最需要注意的就是死锁，死锁的最典型方式是两把锁交叉加锁:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  	thread1        thread2</span><br><span class="line"></span><br><span class="line">lock1          lock2</span><br><span class="line">[...]          [...]</span><br><span class="line">lock2          lock1</span><br></pre></td></tr></table></figure>
<p>如上，thread1加了lock1，执行下面的代码，thead2加了lock2。这时, thread1想<br>要加lock2，但是加不上，thread2想要加lock1的，但是也加不上。</p>
<p>同一个执行流重复加一把锁也会带来死锁:</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    	thread1</span><br><span class="line"></span><br><span class="line">lock1</span><br><span class="line">lock1</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<pre><code> 使用锁的时候要注意锁的不同种类，一般有spinlock和mutex，他们都有对应的读写
 锁的版本。一般情况我们可以先不用读写锁，直到确实是性能瓶颈, 我们才去做优化。
 spinlock是死循环等待获取锁的，mutex在获取锁失败后会sleep, 直到可以得到锁。
 所以在不可以sleep的场景里，我们要用spinlock锁, 和mutex比较，spinlock不sleep，
 所以，spinlock也适用于临界区很短的加锁保护。
</code></pre>
<ol start="3">
<li>调试锁相关代码</li>
</ol>
<hr>
<p>   一般我们写好Linux内核里所相关的代码，可以打开内核里死锁检测:<br>   <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vYXJub2xkbHUvcC84NTgwMzg3Lmh0bWw=">https://www.cnblogs.com/arnoldlu/p/8580387.html<i class="fa fa-external-link-alt"></i></span><br>   hange检测: Kernel hacking —&gt; Debug Lockups and Hangs</p>
<p>   死锁检测会检测出潜在的死锁位置，然后输出报告。<br>   hange检测在超出配置的超时时间后会把调用栈打出来。</p>
<p>   不过这些检测打开后会对系统性能有较大的影响，这些配置只能在调试版本里打开。</p>
<p>   (to do: 用户态死锁检测的工具)</p>
<ol start="4">
<li>加锁后相关的性能问题</li>
</ol>
<hr>
<p>   为了维护并发访问的资源，所以需要加锁。所以，加锁之后有可能带来的性能下降，<br>   本质上是各个执行流相互等待带来的开销。所以，要提高性能，还是要把各个执行流的<br>   相互依赖解开。</p>
<ol start="5">
<li>加锁对构架演进的影响</li>
</ol>
<hr>
<p>   锁临界区太大，会对后续添加新的锁进来产生影响，比较容易造成各种死锁问题。</p>
<ol start="6">
<li>锁的实现</li>
</ol>
<hr>
<p>   (to do: spinlock, q spinlock, 原子变量简单实现spinlock, mutex)</p>
]]></content>
      <tags>
        <tag>Linux用户态编程</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title>ARM SMMUv3 architecture</title>
    <url>/2021/06/21/ARM-SMMUv3-architecture/</url>
    <content><![CDATA[<p>IOMMU是外设的MMU。原来的外设主动发起的DMA的操作使用的都是系统的物理地址，直接<br>使用物理地址有很多不方便的地方，在外设和内存之间引入一个新的IOMMU硬件，完成一些<br>诸如地址翻译的功能，这样又有很多新的玩法可以加进来。</p>
<p>加入IOMMU上的这个地址翻译，就可以引入翻译时候的权限管理，这样保证了外设发出的<br>访问系统内存的地址是安全的。加上地址翻译，还可以把一片连续的虚拟地址空间映射到<br>诸多离散的物理地址上，这样满足一部分设备要访问连续大地址的需要。IOMMU把外设<br>变得更像CPU，CPU和外设在使用内存方面都只看到虚拟地址, 如果虚拟地址到物理地址的<br>映射在CPU MMU和IOMMU上是一样的，CPU和外设将可以看到相同的虚拟地址空间。CPU MMU<br>上的七七八八的功能上都可以在IOMMU上都加上。</p>
<p>CPU core和MMU是绑在一起的，而IOMMU和外设是独立的两个设备, IOMMU一般是不做在<br>外设里的，不然带IOMMU的外设有了地址管理的功能，对系统不安全。MMU上地址翻译的<br>页表，不同进程切换的时候都要换一套, 外设并不能被进程独占，不同的进程可以同时<br>给外设发请求, 广义上作为外设状态一部分的IOMMU自然也用有别MMU的方法描述不同外设<br>在IOMMU的地址翻译配置。</p>
<p>为了说明白整个IOMMU/SMMU的大体架构, 大概要说清楚一下几个方面：</p>
<ol>
<li><p>IOMMU(以下都用SMMU)是一个什么设备，它在系统中的位置和作用是什么。</p>
</li>
<li><p>固件(UEFI)里如何描述SMMU和系统其他部件的关系(PCI, GIC), 如何描述SMMU和它<br>管理的外设的关系。系统软件(一下都用Linux内核)如何解析，进而构建这种关系。</p>
</li>
<li><p>SMMU硬件都提供怎么样的功能, SMMU使用怎么样的软硬件结构来支持这样的功能。<br>Linux内核如何构建SMMU硬件需要的执行环境。SMMU驱动运行时如何工作。</p>
</li>
<li><p>SMMU驱动怎么对外提供功能，外界访问IOMMU/SMMU的接口有哪些。IOMMU这一层如何支持<br>IOMMU的对外接口。</p>
</li>
<li><p>SMMU的虚拟化(S1 + S2)是怎么用起来的。</p>
</li>
</ol>
<p>下面来一一介绍下上面的内容。</p>
<ol>
<li>SMMU的固件描述和Linux解析</li>
</ol>
<hr>
<p>  参考[2][3], ACPI(先不关注DT里的描述方法)在IORT表格里描述SMMU和系统里其他部件<br>  的关系。</p>
<ol start="2">
<li>SMMU功能简介</li>
</ol>
<hr>
<p>  SMMU为外设提供和地址翻译相关的诸多功能。硬件上, SMMU用三个队列和两个表格支持<br>  其基本功能。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">               +----------+</span><br><span class="line">               | CPU core |</span><br><span class="line">               | +-----+  |</span><br><span class="line">               | | MMU |  |</span><br><span class="line">               +-+--+--+--+       +------------------------------------------+</span><br><span class="line">   system bus       v             |                                          |</span><br><span class="line">          -----------------------&gt;|  DDR     +-------+                       |</span><br><span class="line">                   ^              |          |pa     |                       |</span><br><span class="line">             pa    |              |          +-------+                       |</span><br><span class="line">                   |              |                                          |</span><br><span class="line">               +---+-----+        | +---+    +---+    +----------+           |</span><br><span class="line">               |  SMMU   |-------&gt;| |STE|---&gt;|CD |---&gt;|Page table|           |</span><br><span class="line">               |         |        | |   |    +---+    | va -&gt; ipa| (s1)      |</span><br><span class="line">               |         |        | |   |    |.. |    +----------+           |</span><br><span class="line">               |         |        | |   |    +---+                           |</span><br><span class="line">               |  TLB    |        | |   |    |CD |                           |</span><br><span class="line">               |         |        | |   |    +---+                           |</span><br><span class="line">               |STE cache|&lt;-------| |   |    +-----------+                   |</span><br><span class="line">               |         |        | |   |---&gt;|Page table | (s2)              |</span><br><span class="line">               |CD cache |        | |   |    | ipa -&gt; pa |                   |</span><br><span class="line">               |         |        | +---+    +-----------+                   |</span><br><span class="line">               |         |        | |.. |                                    |</span><br><span class="line">               |         |        | +---+                                    |</span><br><span class="line">               |         |        | |STE|                                    |</span><br><span class="line">               |         |        | +---+                                    |</span><br><span class="line">               |         |        |                                          |</span><br><span class="line">               |         |        | +------------------+                     |</span><br><span class="line">               |         |&lt;-------| |command queue     |                     |</span><br><span class="line">               |         |        | +------------------+                     |</span><br><span class="line">               |         |        | +------------------+                     |</span><br><span class="line">               |         |-------&gt;| |event queue       |                     |</span><br><span class="line">               |         |        | +------------------+                     |</span><br><span class="line">               |         |        | +------------------+                     |</span><br><span class="line">               |         |-------&gt;| |pri queue         |                     |</span><br><span class="line">               +---------+        | +------------------+                     |</span><br><span class="line">                   ^              +------------------------------------------+</span><br><span class="line">trasaction with va |              </span><br><span class="line">           +---+       +---+      </span><br><span class="line">           |dev|  ...  |dev|      </span><br><span class="line">           +---+       +---+      </span><br></pre></td></tr></table></figure>
<p>  如上是一个SMMU硬件的简单示意图, 为了把SMMU相关的一些内存里的数据结构也画下，<br>  上图把SMMU画的很大。一般的，一个SMMU物理硬件会同时服务几个外设，而每个外设又<br>  有可能可以独立的发出多个内存访问，这些内存访问需要靠SMMU相互区分，又要靠SMMU<br>  做地址翻译。SMMU硬件靠STE(stream table entry)和CD(context descriptor), 去区分<br>  不同硬件以及相同硬件上的不用内存访问流。如上STE和CD内存里的表格，SMMU硬件可以<br>  认知这些表格，一个外设相关的内存访问信息放在一个STE里，一个外设上的一部分资源<br>  的内存访问信息放在一个CD里。外设和STE的对应关系需要SID(stream id)建立联系，<br>  对于PCI设备，他的SID一般就是BDF，外设硬件在发出的内存访问请求中带上BDF信息，<br>  内存访问请求被SMMU解析，SMMU通过其中的SID找见对应的STE。外设的可以独立发内存<br>  访问请求的单位(比如外设的一个队列)和CD的关系需要SSID(substream id)建立联系，<br>  在PCI设备上，SSID对应的就是PCI协议里说的PASID，这个PASID一般从系统软件中申请<br>  得到，然后分别配置到外设的内存访问单元，和STE一样，设备发出内存访问的时候会<br>  带上这个PASID，SMMU根据PASID找见对应的CD。可以SMMU驱动需要先为对应的设备或者<br>  设备的独立内存访问单元建立STE或者CD，以及填充STE和CD中的域段以支持随后设备的<br>  内存访问。</p>
<p>  STE和CD里包含页表，和MMU一样，为了加快翻译速度，SMMU也做了TLB。为了加快STE和<br>  CD查找的速度，SMMU里也可能放STE和CD的cache。</p>
<p>  SMMU的软硬件控制接口，包括SMMU的基本的MMIO寄存器，三个硬件队列。如上，这是三个<br>  硬件队列中command queue用于软件向SMMU发送命令，event queue用于SMMU向软件报异常<br>  事件(包括缺页)，pri queue是和PCI设备配合一起用的，用于硬件向软件上报外设的<br>  page request请求。软件可以通过command queue向硬件发命令，SMMU的命令基本上可以<br>  分为，配置无效化命令，比如无效掉SMMU cache的STE和CD；TLB无效化命令，缺页相关<br>  的命令，比如用于继续stall请求的RESUME命令；prefetch命令；SYNC命令。</p>
<ol start="3">
<li>SMMU驱动分析</li>
</ol>
<hr>
<p>  SMMUv3相关的驱动的文件包括arm-smmu-v3.c, io-pgtable-arm.c,<br>  drivers/perf/arm_smmuv3_pmu.c。第一个文件是smmu驱动的主体，第二个文件是和<br>  页表相关的操作，第三个文件是和SMMU PMU(PMCG)相关的东西。第一二个文件编译出来<br>  SMMU驱动，第三个文件编译出SMMU PMCG驱动。</p>
<p>  SMMU驱动是一个普通的平台设备驱动。这驱动的probe函数里初始化SMMU硬件，包括：<br>  ACPI/DTS解析，中断初始化，硬件特性解析，SMMU STE初始化，probe里还把SMMU向<br>  iommu子系统注册，以及把iommu_ops回调函数注册给SMMU结构和总线。</p>
<p>  这其中涉及的SMMU驱动相关的一些数据结构包括：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct arm_smmu_device 描述一个物理的SMMU设备。</span><br><span class="line">struct fwnode_handle 描述struct iommu_device的固件描述。</span><br><span class="line">struct arm_smmu_master 描述SMMU物理设备所管理的一个外设, 这个外设可以对应一组</span><br><span class="line">                       stream id, 但是一般是一个外设一个stream id。</span><br><span class="line">struct device</span><br><span class="line">  +-&gt; struct dev_iommu 一个外设device里和iommu相关的东西</span><br><span class="line">    +-&gt; struct iommu_fwspec 一个外设device和iommu硬件相关的东西</span><br><span class="line">      +-&gt; struct fwnode_handle iommu_device的固件描述</span><br></pre></td></tr></table></figure>
<p>  probe里的流程比较直白：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">arm_smmu_device_probe</span><br><span class="line">  ...</span><br><span class="line">  +-&gt; arm_smmu_device_hw_probe 探测各种硬件配置信息</span><br><span class="line">  +-&gt; arm_smmu_init_structures 初始化cmd, event, pri队列的内存, 初始化STE表,</span><br><span class="line">                               如果是两级STE表，只初始化L1 STE</span><br><span class="line">  +-&gt; iommu_device_register 向iommu子系统注册SMMU</span><br><span class="line">  +-&gt; arm_smmu_set_bus_ops 向pci，amba或者platform总线注册iommu_ops</span><br></pre></td></tr></table></figure>

<p>  struct iommu_ops和具体iommu设备相关的回调函数需要通过上层的接口使用。<br>    +-&gt; arm_smmu_add_device:<br>        创建外设对应的arm_smmu_master结构, 创建外设对应的iommu_group。<br>    外设和SMMU的关系怎么传给这个函数的？很明显在这个函数调用之前外设device<br>    之中的iommu_fwspece已经被赋值。这个赋值的地方在[1]中已经提到，就是<br>    pci_device_add(这里只看PCI设备的情况)，基本逻辑是PCI设备在被加入系统中<br>    的时候调用acpi_dma_configure找见它自己的RC，从而找见对应的SMMU。</p>
<pre><code>但是，内核代码后来修改了这部分，现在的内核代码(v5.7-rc1)，把xxx_dma_configure
移到了really_probe里。已PCI总线为例，这里调用的是pci_dma_configure:

pci_dma_configure
  +-&gt; acpi_dma_configure
    +-&gt; iort_iommu_configure 这个函数里创建device的iommu_fwspec
    +-&gt; arch_setup_dma_ops 调到比如arm64的回调中
      +-&gt; iommu_setup_dma_ops 在有iommu的情况下给device-&gt;dma_ops挂上iommu_dma_ops
      (诸如dma_alloc_coherent的dma接口在做dma相关的操作时，在有iommu的
       情况下，调用的就是这里挂在device-&gt;dma_ops里的各种回调函数)

上面说的是为啥arm_smmu_add_device这个函数调用到的时候，device入参里已经
有device-&gt;iommu_fwspec的域段，下面说arm_smmu_add_device这个函数怎么调用到：

arm_smmu_add_device这个函数在iommu层里被封装, 然后注册成总线的一个notifier:
smmu probe -&gt; arm_smmu_set_bus_ops -&gt; bus_set_iommu
    +-&gt; iommu_bus_init
      +-&gt; iommu_bus_notifier  BUS_NOTIFY_ADD_DEVICE会触发
        +-&gt; iommu_probe_device
      +-&gt; ops-&gt;add_device

如上pci_device_add的最后会调用devcie_add向bus添加外设device, 这个过程
会触发以上notifier回调函数，最终调用到arm_smmu_add_device。注意，这里
really_probe在后，触发notifier执行在前。触发notifier在设备加载的时候，
而really_probe在驱动加载的时候。

下面看add_device都做了什么:
add_device
  +-&gt; blocking_notifier_call_chain
  +-&gt; bus_probe_device
        +-&gt; device_initial_probe
      +-&gt; __device_attach
        +-&gt; __device_attch_driver 注意：如果驱动没有加载不会再继续下面的调用
    ...
      +-&gt; really_probe

也就是说，一般先枚举设备，再insmod驱动的场景，只有在驱动加载的时候才会
生成device结构里的iommu_fwspec。

但是，实际跟踪执行流程，你会发现对于一个外设的arm_smmu_add_device调用是
发生在对应的设备驱动加载的时候。这是因为smmu驱动加载比pci驱动晚，add_device
想要触发notifier时，因为smmu驱动没有加载，notifier都还没有注册，自然也
不会在add_device这个流程中触发arm_smmu_add_device这个函数。那么really_probe
为啥会最终调用到arm_smmu_add_device? 这是因为在iort_iommu_configure里
调用了iort_add_device_replay-&gt;iommu_probe_device。综上struct device里的
和iommu有关的struct dev_iommu，以及iommu_fwspec都是在iort_iommu_configure
里创建的。

如最开始所述，这个arm_smmu_add_device创建特定外设在smmu处的各种描述信息。
其中包括，创建arm_smmu_master, 建立设备对应的STE entry，初始化设备的
pasid，pri功能，建立这个设备相关的iommu_group和iommu_domain, 在这个过程
中会调用smmu ops里的device_group, domain_alloc以及attach_dev。

    架构设计上，iommu_group描述共享一个地址空间的设备的集合, iommu_domain
打包地址翻译的类型以及为具体的iomm_domain实现提供桥梁，比如arm_smmu_domain。
iommu_ops里的有些操作，比如，map会最终调用到arm_smmu_domain-&gt;io_pagetable_ops-&gt;map

arm_smmu_add_device
  +-&gt; iommu_group_get_for_dev
    +-&gt; iommu_group_add_device
      +-&gt; __iommu_attach_device
        +-&gt; arm_smmu_attach_dev
里创建iommu_group, iommu_domain, 并最终调用到arm_smmu_attach_dev。可以看到
先按全局变量iommu_def_domain_type创建domain, 再用iommu_domain_set_attr
补充设置domain的attr。一般domain的type有IDENTITY, UNMANAGED, DMA。IDENTITY
    是IOMMU不做处理，DMA是在内核里使用IOMMU，UNMANAGED是把IOMMU的能力暴露出去
给其他模块使用，现在一般是VFIO和SVA再用UNMANAGED domain。attr用来区分
domain里更细一步的配置，比如，arm_smmu_domain_set_attr里，对于DMA domain,
用attr区分non_strict mode; 对于UNMANAGED domain, 用attr区分Nested的使用
方式。可以看到UNMANAGED domain时, 如果只用一个stage，用的是stage1; 对于
内核的DMA流程，一般顺着这个调用关系下来，但是，对于UNMANAGED domain和
Nested，iommu还有其他对外接口，提供配置的方法。

下面看arm_smmu_attach_dev里具体干了什么。

+-&gt; arm_smmu_attach_dev:
    这个函数的入参是，struct iommu_domain, struct device。如上，这里的domain
表示的是一种使用方式。也就是说这里是把一个device和一种使用方式建立起联系。
注意，上面的add_device是把一个外设加入到smmu这个系统里来。

如上，我们可以看下iommu_domain的各种不同类型，以及内核里的不同子系统是
怎么调用这个函数的，以及这个函数到底做了什么。

这个函数对于特定的设备，根据传进来的domain类型, 初始化具体domain的相关
操作函数，然后配置STE生效:
arm_smmu_attach_dev
  +-&gt; arm_smmu_domain_finalise
    +-&gt; arm_smmu_domain_finalise_xxx[cd, s1, s2]
    +-&gt; smmu_domain-&gt;pgtbl_ops = alloc_io_pgtable_ops
  +-&gt; arm_smmu_install_ste_for_dev
可见arm_smmu_domain_finalise里首先根绝smmu_domain里stage的配置，去配置
smmu的CD,或者STE和s2相关的内容。然后，把对应的页表操作函数赋值给smmu_domain
里的回调函数: smmu_domain-&gt;pgtbl_ops。以后，iommu_ops里的mmap，unmap函数
直接会调用到这里。

iommu_attach_group, iommu_attach_device, iommu_group_add_device,
iommu_request_dm_for_dev, iommu_request_dma_domain_for_dev都会最终调用到
attach_dev这个回调。在vfio里会使用到iommu_attach_group, iommu_attach_device,
vfio会用UNMANAGED domain。(具体逻辑： todo)
</code></pre>
<ol start="4">
<li>IOMMU接口分析</li>
</ol>
<hr>
<p>  DMA接口和IOMMU的关系在上面已给出。</p>
<p>  目前社区正在上传SVA的相关补丁，SVA的接口也是IOMMU子系统对外接口的一部分, 这<br>  一部分的分析可以参见[5]。目前的SVA接口使用的是DMA domain, 通过SVA的接口触发<br>  在STE表里建立SSID非零的CD表，从而可以实现一个外设，一部分在内核态使用(使用CD0),<br>  一部风在用户态使用(使用其他CD)。</p>
<p>  VFIO重度使用IOMMU接口, 从这个角度看IOMMU子系统的对外接口参见[6]</p>
<ol start="5">
<li>SMMU虚拟化分析</li>
</ol>
<hr>
<p>  Nested SMMU的分析参见[4]。</p>
<p>Reference</p>
<p>[1] <span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2NhcmVjcm93X2J5ci9hcnRpY2xlL2RldGFpbHMvNTM4NDQxNjI=">http://blog.csdn.net/scarecrow_byr/article/details/53844162<i class="fa fa-external-link-alt"></i></span><br>[2] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzUyMzQ4MjM0">https://blog.csdn.net/scarecrow_byr/article/details/52348234<i class="fa fa-external-link-alt"></i></span><br>[3] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzUzODQ0MTYy">https://blog.csdn.net/scarecrow_byr/article/details/53844162<i class="fa fa-external-link-alt"></i></span><br>[4] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzEwNDYwNjU3MQ==">https://blog.csdn.net/scarecrow_byr/article/details/104606571<i class="fa fa-external-link-alt"></i></span><br>[5] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzEwMDk4MzYxOQ==">https://blog.csdn.net/scarecrow_byr/article/details/100983619<i class="fa fa-external-link-alt"></i></span><br>[6] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzUxNDk0NDAx">https://blog.csdn.net/scarecrow_byr/article/details/51494401<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>计算机体系结构</tag>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
      </tags>
  </entry>
  <entry>
    <title>Dump Linux内核和用户进程页表</title>
    <url>/2021/06/19/Dump-Linux%E5%86%85%E6%A0%B8%E5%92%8C%E7%94%A8%E6%88%B7%E8%BF%9B%E7%A8%8B%E9%A1%B5%E8%A1%A8/</url>
    <content><![CDATA[<ol>
<li><p>dump 内核页表</p>
<ol>
<li><p>打开内核编译选项：CONFIG_PTDUMP_CORE, CONFIG_PTDUMP_DEBUGFS, 编译内核。</p>
</li>
<li><p>在/sys/kernel/debug/kernel_page_tables下可以dump kernel page table。dump<br>出的数据大概如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0x0000000000000000-0xffff000000000000    16776960T PGD</span><br><span class="line">0xffff000000000000-0xffff0000002dd000        2932K PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff0000002dd000-0xffff0000002de000           4K PTE       ro NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff0000002de000-0xffff00002d800000      742536K PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff00002d800000-0xffff00002f200000          26M PMD       ro NX SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xffff00002f200000-0xffff00002f3a0000        1664K PTE       ro NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff00002f3a0000-0xffff0000385ec000      149808K PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff0000385ec000-0xffff0000386c0000         848K PTE</span><br><span class="line">0xffff0000386c0000-0xffff0000386e0000         128K PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff0000386e0000-0xffff000038750000         448K PTE</span><br><span class="line">0xffff000038750000-0xffff00003bc20000       54080K PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff00003bc20000-0xffff00003be00000        1920K PTE</span><br><span class="line">0xffff00003be00000-0xffff00003c000000           2M PMD</span><br><span class="line">0xffff00003c000000-0xffff000040000000          64M PTE       RW NX SHD AF            UXN    MEM/NORMAL-TAGGED</span><br><span class="line">0xffff000040000000-0xffff008000000000         511G PUD</span><br><span class="line">0xffff008000000000-0xffff800000000000      130560G PGD</span><br><span class="line">---[ Linear Mapping end ]---</span><br><span class="line">---[ BPF start ]---</span><br><span class="line">0xffff800000000000-0xffff800000001000           4K PTE       ro x  SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800000001000-0xffff800000200000        2044K PTE</span><br><span class="line">0xffff800000200000-0xffff800008000000         126M PMD</span><br><span class="line">---[ BPF end ]---</span><br><span class="line">---[ Modules start ]---</span><br><span class="line">0xffff800008000000-0xffff800010000000         128M PMD</span><br><span class="line">---[ Modules end ]---</span><br><span class="line">---[ vmalloc() area ]---</span><br><span class="line">0xffff800010000000-0xffff800011200000          18M PMD       ro x  SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xffff800011200000-0xffff800011240000         256K PTE       ro x  SHD AF    CON     UXN    MEM/NORMAL</span><br><span class="line">0xffff800011240000-0xffff800011400000        1792K PTE       ro NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800011400000-0xffff800011a00000           6M PMD       ro NX SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xffff800011a00000-0xffff800011ba0000        1664K PTE       ro NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800011ba0000-0xffff800011e00000        2432K PTE</span><br><span class="line">0xffff800011e00000-0xffff800012400000           6M PMD</span><br><span class="line">0xffff800012400000-0xffff8000125d0000        1856K PTE</span><br><span class="line">0xffff8000125d0000-0xffff800012600000         192K PTE       RW NX SHD AF    CON     UXN    MEM/NORMAL</span><br><span class="line">0xffff800012600000-0xffff800012a00000           4M PMD       RW NX SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xffff800012a00000-0xffff800012b90000        1600K PTE       RW NX SHD AF    CON     UXN    MEM/NORMAL</span><br><span class="line">0xffff800012b90000-0xffff800012b91000           4K PTE</span><br><span class="line">0xffff800012b91000-0xffff800012b92000           4K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012b92000-0xffff800012b93000           4K PTE</span><br><span class="line">0xffff800012b93000-0xffff800012b94000           4K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012b94000-0xffff800012b95000           4K PTE</span><br><span class="line">0xffff800012b95000-0xffff800012b96000           4K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012b96000-0xffff800012b98000           8K PTE</span><br><span class="line">0xffff800012b98000-0xffff800012b9c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012b9c000-0xffff800012b9d000           4K PTE</span><br><span class="line">0xffff800012b9d000-0xffff800012b9e000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800012b9e000-0xffff800012ba0000           8K PTE</span><br><span class="line">0xffff800012ba0000-0xffff800012bb0000          64K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800012bb0000-0xffff800012bb1000           4K PTE</span><br><span class="line">0xffff800012bb1000-0xffff800012bb2000           4K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bb2000-0xffff800012bb3000           4K PTE</span><br><span class="line">0xffff800012bb3000-0xffff800012bb4000           4K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bb4000-0xffff800012bb8000          16K PTE</span><br><span class="line">0xffff800012bb8000-0xffff800012bbc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bbc000-0xffff800012bc0000          16K PTE</span><br><span class="line">0xffff800012bc0000-0xffff800012bd0000          64K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800012bd0000-0xffff800012bd8000          32K PTE</span><br><span class="line">0xffff800012bd8000-0xffff800012bdc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bdc000-0xffff800012be0000          16K PTE</span><br><span class="line">0xffff800012be0000-0xffff800012be4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012be4000-0xffff800012be8000          16K PTE</span><br><span class="line">0xffff800012be8000-0xffff800012bec000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bec000-0xffff800012bf0000          16K PTE</span><br><span class="line">0xffff800012bf0000-0xffff800012bf4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bf4000-0xffff800012bf8000          16K PTE</span><br><span class="line">0xffff800012bf8000-0xffff800012bfc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012bfc000-0xffff800012c00000          16K PTE</span><br><span class="line">0xffff800012c00000-0xffff800012c04000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c04000-0xffff800012c08000          16K PTE</span><br><span class="line">0xffff800012c08000-0xffff800012c0c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c0c000-0xffff800012c10000          16K PTE</span><br><span class="line">0xffff800012c10000-0xffff800012c14000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c14000-0xffff800012c18000          16K PTE</span><br><span class="line">0xffff800012c18000-0xffff800012c1c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c1c000-0xffff800012c20000          16K PTE</span><br><span class="line">0xffff800012c20000-0xffff800012c24000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c24000-0xffff800012c28000          16K PTE</span><br><span class="line">0xffff800012c28000-0xffff800012c2c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c2c000-0xffff800012c30000          16K PTE</span><br><span class="line">0xffff800012c30000-0xffff800012c34000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c34000-0xffff800012c38000          16K PTE</span><br><span class="line">0xffff800012c38000-0xffff800012c3c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c3c000-0xffff800012c40000          16K PTE</span><br><span class="line">0xffff800012c40000-0xffff800012c44000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c44000-0xffff800012c48000          16K PTE</span><br><span class="line">0xffff800012c48000-0xffff800012c4c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c4c000-0xffff800012c50000          16K PTE</span><br><span class="line">0xffff800012c50000-0xffff800012c54000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012c54000-0xffff800012c55000           4K PTE</span><br><span class="line">0xffff800012c55000-0xffff800012c75000         128K PTE       RW NX SHD AF            UXN    MEM/NORMAL-NC</span><br><span class="line">0xffff800012c75000-0xffff800012c76000           4K PTE</span><br><span class="line">0xffff800012c76000-0xffff800012c96000         128K PTE       RW NX SHD AF            UXN    MEM/NORMAL-NC</span><br><span class="line">0xffff800012c96000-0xffff800012c97000           4K PTE</span><br><span class="line">0xffff800012c97000-0xffff800012cb7000         128K PTE       RW NX SHD AF            UXN    MEM/NORMAL-NC</span><br><span class="line">0xffff800012cb7000-0xffff800012cb8000           4K PTE</span><br><span class="line">0xffff800012cb8000-0xffff800012cbc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cbc000-0xffff800012cc0000          16K PTE</span><br><span class="line">0xffff800012cc0000-0xffff800012cc4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cc4000-0xffff800012cc8000          16K PTE</span><br><span class="line">0xffff800012cc8000-0xffff800012ccc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012ccc000-0xffff800012cd0000          16K PTE</span><br><span class="line">0xffff800012cd0000-0xffff800012cd4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cd4000-0xffff800012cd8000          16K PTE</span><br><span class="line">0xffff800012cd8000-0xffff800012cdc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cdc000-0xffff800012ce0000          16K PTE</span><br><span class="line">0xffff800012ce0000-0xffff800012ce4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012ce4000-0xffff800012ce8000          16K PTE</span><br><span class="line">0xffff800012ce8000-0xffff800012cec000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cec000-0xffff800012cf0000          16K PTE</span><br><span class="line">0xffff800012cf0000-0xffff800012cf4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cf4000-0xffff800012cf8000          16K PTE</span><br><span class="line">0xffff800012cf8000-0xffff800012cfc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800012cfc000-0xffff800013000000        3088K PTE</span><br><span class="line">0xffff800013000000-0xffff800013f60000       15744K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013f60000-0xffff800013fb0000         320K PTE</span><br><span class="line">0xffff800013fb0000-0xffff800013fb4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fb4000-0xffff800013fbd000          36K PTE</span><br><span class="line">0xffff800013fbd000-0xffff800013fbe000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fbe000-0xffff800013fc0000           8K PTE</span><br><span class="line">0xffff800013fc0000-0xffff800013fc4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fc4000-0xffff800013fc5000           4K PTE</span><br><span class="line">0xffff800013fc5000-0xffff800013fc6000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fc6000-0xffff800013fc8000           8K PTE</span><br><span class="line">0xffff800013fc8000-0xffff800013fcc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fcc000-0xffff800013fcd000           4K PTE</span><br><span class="line">0xffff800013fcd000-0xffff800013fce000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fce000-0xffff800013fd0000           8K PTE</span><br><span class="line">0xffff800013fd0000-0xffff800013fd4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fd4000-0xffff800013fd5000           4K PTE</span><br><span class="line">0xffff800013fd5000-0xffff800013fd6000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fd6000-0xffff800013fd8000           8K PTE</span><br><span class="line">0xffff800013fd8000-0xffff800013fdc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fdc000-0xffff800013fdd000           4K PTE</span><br><span class="line">0xffff800013fdd000-0xffff800013fde000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fde000-0xffff800013fe0000           8K PTE</span><br><span class="line">0xffff800013fe0000-0xffff800013fe4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fe4000-0xffff800013fe5000           4K PTE</span><br><span class="line">0xffff800013fe5000-0xffff800013fe6000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fe6000-0xffff800013fe8000           8K PTE</span><br><span class="line">0xffff800013fe8000-0xffff800013fec000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013fec000-0xffff800013fed000           4K PTE</span><br><span class="line">0xffff800013fed000-0xffff800013fee000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013fee000-0xffff800013ff0000           8K PTE</span><br><span class="line">0xffff800013ff0000-0xffff800013ff4000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013ff4000-0xffff800013ff5000           4K PTE</span><br><span class="line">0xffff800013ff5000-0xffff800013ff6000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013ff6000-0xffff800013ff8000           8K PTE</span><br><span class="line">0xffff800013ff8000-0xffff800013ffc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800013ffc000-0xffff800013ffd000           4K PTE</span><br><span class="line">0xffff800013ffd000-0xffff800013ffe000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800013ffe000-0xffff800014000000           8K PTE</span><br><span class="line">0xffff800014000000-0xffff800014004000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014004000-0xffff800014005000           4K PTE</span><br><span class="line">0xffff800014005000-0xffff800014006000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800014006000-0xffff80001400d000          28K PTE</span><br><span class="line">0xffff80001400d000-0xffff80001400e000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff80001400e000-0xffff800014015000          28K PTE</span><br><span class="line">0xffff800014015000-0xffff800014016000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xffff800014016000-0xffff800014038000         136K PTE</span><br><span class="line">0xffff800014038000-0xffff80001403c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff80001403c000-0xffff800014040000          16K PTE</span><br><span class="line">0xffff800014040000-0xffff800014044000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014044000-0xffff800014048000          16K PTE</span><br><span class="line">0xffff800014048000-0xffff80001404c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff80001404c000-0xffff800014050000          16K PTE</span><br><span class="line">0xffff800014050000-0xffff800014054000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014054000-0xffff800014058000          16K PTE</span><br><span class="line">0xffff800014058000-0xffff80001405c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff80001405c000-0xffff800014060000          16K PTE</span><br><span class="line">0xffff800014060000-0xffff800014064000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014064000-0xffff800014065000           4K PTE</span><br><span class="line">0xffff800014065000-0xffff8000140a7000         264K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff8000140a7000-0xffff8000140a8000           4K PTE</span><br><span class="line">0xffff8000140a8000-0xffff8000140b3000          44K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff8000140b3000-0xffff8000140b8000          20K PTE</span><br><span class="line">0xffff8000140b8000-0xffff8000140bc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff8000140bc000-0xffff8000140c8000          48K PTE</span><br><span class="line">0xffff8000140c8000-0xffff8000140cc000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff8000140cc000-0xffff8000140e8000         112K PTE</span><br><span class="line">0xffff8000140e8000-0xffff8000140ec000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff8000140ec000-0xffff800014128000         240K PTE</span><br><span class="line">0xffff800014128000-0xffff80001412c000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff80001412c000-0xffff800014130000          16K PTE</span><br><span class="line">0xffff800014130000-0xffff800014134000          16K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014134000-0xffff800014135000           4K PTE</span><br><span class="line">0xffff800014135000-0xffff800014138000          12K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xffff800014138000-0xffff800014200000         800K PTE</span><br><span class="line">0xffff800014200000-0xffff800020000000         190M PMD</span><br><span class="line">0xffff800020000000-0xffff800030000000         256M PTE       RW NX SHD AF            UXN    DEVICE/nGnRnE</span><br><span class="line">0xffff800030000000-0xffff800040000000         256M PMD</span><br><span class="line">0xffff800040000000-0xffff808000000000         511G PUD</span><br><span class="line">0xffff808000000000-0xfffffd8000000000         125T PGD</span><br><span class="line">0xfffffd8000000000-0xfffffdff80000000         510G PUD</span><br><span class="line">0xfffffdff80000000-0xfffffdffbfe00000        1022M PMD</span><br><span class="line">0xfffffdffbfe00000-0xfffffdffbffd1000        1860K PTE</span><br><span class="line">0xfffffdffbffd1000-0xfffffdffbffd4000          12K PTE       RW NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xfffffdffbffd4000-0xfffffdffbfff0000         112K PTE</span><br><span class="line">---[ vmalloc() end ]---</span><br><span class="line">0xfffffdffbfff0000-0xfffffdffc0000000          64K PTE</span><br><span class="line">0xfffffdffc0000000-0xfffffdfffe400000         996M PMD</span><br><span class="line">0xfffffdfffe400000-0xfffffdfffe5f9000        2020K PTE</span><br><span class="line">---[ Fixmap start ]---</span><br><span class="line">0xfffffdfffe5f9000-0xfffffdfffe5fa000           4K PTE</span><br><span class="line">0xfffffdfffe5fa000-0xfffffdfffe5fb000           4K PTE       ro x  SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xfffffdfffe5fb000-0xfffffdfffe5fc000           4K PTE       ro NX SHD AF            UXN    MEM/NORMAL</span><br><span class="line">0xfffffdfffe5fc000-0xfffffdfffe5ff000          12K PTE</span><br><span class="line">0xfffffdfffe5ff000-0xfffffdfffe600000           4K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xfffffdfffe600000-0xfffffdfffe800000           2M PMD       ro NX SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xfffffdfffe800000-0xfffffdfffea00000           2M PMD</span><br><span class="line">---[ Fixmap end ]---</span><br><span class="line">0xfffffdfffea00000-0xfffffdfffec00000           2M PMD</span><br><span class="line">---[ PCI I/O start ]---</span><br><span class="line">0xfffffdfffec00000-0xfffffdfffec10000          64K PTE       RW NX SHD AF            UXN    DEVICE/nGnRE</span><br><span class="line">0xfffffdfffec10000-0xfffffdfffee00000        1984K PTE</span><br><span class="line">0xfffffdfffee00000-0xfffffdffffc00000          14M PMD</span><br><span class="line">---[ PCI I/O end ]---</span><br><span class="line">0xfffffdffffc00000-0xfffffdffffe00000           2M PMD</span><br><span class="line">---[ vmemmap start ]---</span><br><span class="line">0xfffffdffffe00000-0xfffffe0000e00000          16M PMD       RW NX SHD AF        BLK UXN    MEM/NORMAL</span><br><span class="line">0xfffffe0000e00000-0xfffffe0040000000        1010M PMD</span><br><span class="line">0xfffffe0040000000-0xfffffe8000000000         511G PUD</span><br><span class="line">0xfffffe8000000000-0x0000000000000000        1536G PGD</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>dump 进程页表</p>
<p>linux系统上，通过对/proc/&lt;pid&gt;/pagemap的操作可以dump进程的页表信息。<br>这个文件的介绍可以参考linux/Documentation/vm/pagemap.txt。直接cat<br>这个文件的无法得到信息，我们需要按照上面文件中描述的格式写代码解析。<br>内核代码在tools/vm/page-types.c提供了解析的工具。可以使用:<br>make -C tools/vm 编译得到page-types这个工具。使用的效果类似:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ./page-types --pid 91228</span><br><span class="line">             flags	page-count       MB  symbolic-flags			long-symbolic-flags</span><br><span class="line">0x0000000000000800	         1        0  ___________M________________________________	mmap</span><br><span class="line">0x000000000000086c	       706        2  __RU_lA____M________________________________	referenced,uptodate,lru,active,mmap</span><br><span class="line">0x0000000000005828	       520        2  ___U_l_____Ma_b_____________________________	uptodate,lru,mmap,anonymous,swapbacked</span><br><span class="line">0x000000000000586c	         1        0  __RU_lA____Ma_b_____________________________	referenced,uptodate,lru,active,mmap,anonymous,swapbacked</span><br><span class="line">             total	      1228        4</span><br></pre></td></tr></table></figure>
<p>使用 ./page-types –pid <pid> –list-each 可以看到具体va到pa的映射：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">voffset	offset	flags</span><br><span class="line">aaaab6b57	6e938	__RU_lA____M________________________________</span><br><span class="line">aaaab6b58	aa3cb	__RU_lA____M________________________________</span><br><span class="line">aaaab6b59	55172	__RU_lA____M________________________________</span><br><span class="line">[...]</span><br><span class="line">ffffc744c	a17e7	___U_lA____Ma_b_____________________________</span><br><span class="line">ffffc744d	117a3a	___U_lA____Ma_b_____________________________</span><br><span class="line">ffffc744e	109913	___U_lA____Ma_b_____________________________</span><br><span class="line">ffffc744f	136574	__RU_lA____Ma_b_____________________________</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">             flags	page-count       MB  symbolic-flags			long-symbolic-flags</span><br><span class="line">0x0000000000000800	         1        0  ___________M________________________________	mmap</span><br><span class="line">0x0000000000000804	         1        0  __R________M________________________________	referenced,mmap</span><br><span class="line">0x000000000004082c	       429        1  __RU_l_____M______u_________________________	referenced,uptodate,lru,mmap,unevictable</span><br><span class="line">0x0000000000000868	         3        0  ___U_lA____M________________________________	uptodate,lru,active,mmap</span><br><span class="line">0x000000000000086c	      2009        7  __RU_lA____M________________________________	referenced,uptodate,lru,active,mmap</span><br><span class="line">0x0000000000005868	      1073        4  ___U_lA____Ma_b_____________________________	uptodate,lru,active,mmap,anonymous,swapbacked</span><br><span class="line">0x000000000000586c	         1        0  __RU_lA____Ma_b_____________________________	referenced,uptodate,lru,active,mmap,anonymous,swapbacked</span><br><span class="line">             total	      3517       13</span><br></pre></td></tr></table></figure>
<p>这里显示的是页之间的映射，所以还要乘上页大小，如果是4K页的话，上面的va到pa的<br>映射就是：(左边是va, 右边是pa)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aaaab6b57000	6e938000</span><br><span class="line">aaaab6b58000	aa3cb000</span><br><span class="line">aaaab6b59000	55172000</span><br><span class="line">[...]</span><br><span class="line">ffffc744c000	a17e7000</span><br><span class="line">ffffc744d000	117a3a000</span><br><span class="line">ffffc744e000	109913000</span><br><span class="line">ffffc744f000	136574000</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <tags>
        <tag>Linux内核</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux SVA特性分析</title>
    <url>/2021/06/27/Linux-SVA%E7%89%B9%E6%80%A7%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h2 id="使用场景介绍"><a href="#使用场景介绍" class="headerlink" title="使用场景介绍"></a>使用场景介绍</h2><hr>
<p>  如上，SVA特性可以做到进程虚拟地址在进程和设备之间共享。最直观的使用场景就是在<br>  用户态做DMA。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">            </span><br><span class="line">   +------------------+     +----------------+</span><br><span class="line">   |   进程           |     |  进程          |       ...</span><br><span class="line">   |                  |     |                |</span><br><span class="line">   | va = malloc();   |     |                |</span><br><span class="line">   | set dma dst: va  |     |                |</span><br><span class="line">   |      +---------+ |     |                |          用户态</span><br><span class="line">   +------+ mmap io +-+     +----------------+</span><br><span class="line">&lt;---------+         +------------------------------------------&gt; </span><br><span class="line">          +---------+</span><br><span class="line">                       内核</span><br><span class="line"></span><br><span class="line">&lt;--------------------------------------------------------------&gt; </span><br><span class="line">               +--------------+</span><br><span class="line">               |  DMA dst: va |</span><br><span class="line">               |              |</span><br><span class="line">               |  设备        |</span><br><span class="line">               +--------------+</span><br></pre></td></tr></table></figure>
<p>  如上图所示，在SVA的支持下，我们可以在用户态进程malloc一段内存，然后把得到va<br>  直接配置给设备的DMA，然后启动DMA向va对应的虚拟地址写数据，当然也可以从va对应<br>  的虚拟地址上往设备读数据。这里我们把设备DMA相关的寄存器先mmap到用户态，这样<br>  DMA操作在用户态就可以完成。</p>
<p>  可以注意到，SVA可以支持功能很大一部分取决于设备的功能，SVA就是提供一个进程和<br>  设备一致的虚拟地址空间，其他的设备想怎么用都可以。如上，如果设备做的足够强，<br>  设备完全可以执行va上对应的代码。</p>
<p>  可以看到，设备完全可以把自身的资源分配给不同的进程同时使用。</p>
<p>  为了满足上面的使用场景，SVA特性需要硬件支持IOMMU以及设备发起缺页。在下一节<br>  先介绍硬件，再基于此分析SVA的软件实现。</p>
<h2 id="硬件基础介绍"><a href="#硬件基础介绍" class="headerlink" title="硬件基础介绍"></a>硬件基础介绍</h2><hr>
<p>  本文以ARM64体系结构为基础分析，在ARM64下，IOMMU指的就是SMMU。对于设备，ARM64<br>  下有平台设备和PCI设备。整体的硬件示意图如下，图中也画出了硬件工作时相关的内存<br>  里的数据结构。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">             +-----+</span><br><span class="line">             | CPU |</span><br><span class="line">             +--+--+</span><br><span class="line">                |             </span><br><span class="line">                v             </span><br><span class="line">             +-----+           +---------------------------------------------+</span><br><span class="line">             | MMU |-----------+------------------------------------+        |</span><br><span class="line">             +--+--+           | DDR                                |        |</span><br><span class="line">                |              |                                    |        |</span><br><span class="line">                v              |                                    |        |</span><br><span class="line">system bus ------------------&gt; |                                    |        |</span><br><span class="line">                ^              |                                    v        |</span><br><span class="line">                |   SID/SSID   |      +-----+     +----+      +------------+ |</span><br><span class="line">                |       +------+----&gt; | STE |----&gt;| CD |-----&gt;| page table | |</span><br><span class="line">                |       |      |      +-----+     +----+      +------------+ |</span><br><span class="line">                |       |      |       ...        | CD |-----&gt;| page table | |</span><br><span class="line">                |       |      |                  +----+      +------------+ |</span><br><span class="line">        IRQs    |       |      |                  | .. |      | ..         | |</span><br><span class="line">              ^ |^  ^   |      |                  +----+      +------------+ |</span><br><span class="line">              | ||  |   |      |                  | CD |-----&gt;| page table | |</span><br><span class="line">    +---------+-++--+---+---+  |                  +----+      +------------+ |</span><br><span class="line">    | SMMU    |  |  |   |   |  |                                             |</span><br><span class="line">    |         |  |  |       |  |     +-------------+                         |</span><br><span class="line">    |  +------+--+-CMD Q ---+--+----&gt;| CMD queue   |                         |</span><br><span class="line">    +--v--+   |  |          |  |     +-------------+                         |</span><br><span class="line">    | PRI |---&gt; PRI Q ------+--+----&gt;| PRI queue   |                         |</span><br><span class="line">    +-----+   |             |  |     +-------------+                         |</span><br><span class="line">    | ATS |  EVENT Q -------+--+----&gt;| EVENT queue |                         |</span><br><span class="line">    +-----+-----------------+  |     +-------------+                         |</span><br><span class="line">      ^ |     ^            ^   +---------------------------------------------+</span><br><span class="line">      | |     |            |</span><br><span class="line">      | v     | BDF/PASID  +--------------+</span><br><span class="line">    +---------+-------------+             | </span><br><span class="line">    |         RP            |             | </span><br><span class="line">    +-----------------------+             | </span><br><span class="line">      ^ |       ^                         | </span><br><span class="line">      | v       |  BDF/PASID              | </span><br><span class="line">    +-+----+----+-+---------+  +-----+----+----------+</span><br><span class="line">    | ATC  |      |         |  |     |               |                       </span><br><span class="line">    +------+      |         |  | DMA |               |                       </span><br><span class="line">    | PRI  |      |  EP     |  |     |               |                       </span><br><span class="line">    +------+ DMA  |         |  +-----+  platform dev |                       </span><br><span class="line">    +-------------+         |  |                     |                       </span><br><span class="line">    +-----------------------+  +---------------------+</span><br></pre></td></tr></table></figure>
<p>   基于上一节中提到的使用场景, 我们梳理硬件中的逻辑关系。调用malloc后，其实只是<br>   拿到了一个虚拟地址，内核并没有为申请的地址空间分配实际的物理内存，直到访问这<br>   块地址空间时引发缺页，内核在缺页流程里分配实际的物理内存，然后建立虚拟地址到<br>   物理内存的映射。这个过程需要MMU的参与。设想SVA的场景中，先malloc得到va, 然后<br>   把这个va传给设备，配置设备DMA去访问该地址空间，这时内核并没有为va分配实际的<br>   物理内存，所以设备一侧的访问流程必然需要进行类似的缺页请求。支持设备侧缺页<br>   请求的硬件设备就是上面所示的SMMU，其中对于PCI设备，还需要ATS、PRI硬件特性支持。<br>   平台设备需要SMMU stall mode支持(使用event queue)。PCI设备和平台设备都需要<br>   PASID特性的支持。</p>
<p>   如上图所示，引入SVA后，MMU和SMMU使用相同的进程页表, SMMU使用STE表和CD表管理<br>   当前SMMU下设备的页表，其中STE表用来区分设备，CD表用来区分同一个设备上分配给<br>   不同进程使用的硬件资源所对应的进程页表。STE表和CD表都需要SMMU驱动预先分配好。</p>
<p>   SMMU内部使用command queue，event queue，pri queue做基本的事件管理。当有相应<br>   硬件事件发生时，硬件把相应的描述符写入event queue或者pri queue, 然后上报中断。<br>   软件使用command queue下发相应的命令操作硬件。</p>
<p>   PCI设备和平台设备的硬件缺页流程有一些差异，下面分别介绍。对于PCI设备，ATS,<br>   PRI和PASID的概念同时存在于PCIe和SMMU规范中。对于ATS的介绍可以参考这里:<br>   <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzc0Mjc2OTQwJUUzJTgwJTgyJUU3JUFFJTgwJUU1JThEJTk1JUU4JUFFJUIyJUVGJUJDJThDQVRTJUU3JTg5JUI5JUU2JTgwJUE3">https://blog.csdn.net/scarecrow_byr/article/details/74276940。简单讲，ATS特性<i class="fa fa-external-link-alt"></i></span><br>   由设备侧的ATC和SMMU侧的ATS同时支持，其目的是在设备中缓存va对应的pa，设备随后<br>   使用pa做内存访问时无需经过SMMU页表转换，可以提高性能。PRI(page request<br>   interface)也是需要设备和SMMU一起工作，PCIe设备可以发出缺页请求，SMMU硬件在解<br>   析到缺页请求后可以直接将缺页请求写入PRI queueu, 软件在建立好页表后，可以通过<br>   CMD queue发送PRI response给PCIe设备。具体的ATS和PRI的实现是硬件相关的，目前<br>   市面上还没有实现这两个硬件特性的PCIe设备，但是我们可以设想一下ATS和PRI的硬件<br>   实现，最好的实现应该是软件透明的，也就是软件配置给设备DMA的访问地址是va, 软件<br>   控制DMA发起后，硬件先发起ATC请求，从SMMU请求该va对应的pa，如果SMMU里已经有va<br>   到pa的映射，那么设备可以得到pa，然后设备再用pa发起一次内存访问，该访问将直接<br>   访问对应pa地址，不在SMMU做地址翻译，如果SMMU没有va到pa的映射, 那么设备得到<br>   这个消息后会继续向SMMU发PRI请求，设备得到从SMMU来的PRI response后发送内存访问<br>   请求，该请求就可以在SMMU中翻译得到pa, 最终访问到物理内存。</p>
<p>   PRI请求是基于PCIe协议的, 平台设备无法用PRI发起缺页请求。实际上，平台设备是无法<br>   靠自身发起缺页请求的，SMMU用stall模式支持平台设备的缺页，当一个平台设备的内存<br>   访问请求到达SMMU后，如果SMMU里没有为va做到pa的映射，硬件会给SMMU的event queue<br>   里写一个信息，SMMU的event queue中断处理里可以做缺页处理，然后SMMU可以回信息给<br>   设备(fix me: 请求设备重发，还是smmu缺页处理后已经把该访问翻译后送到上游总线)。<br>   实际上, SMMU使用event queue来处理各种错误异常，这里的stall模式是借用了event<br>   queue来处理缺页。</p>
<p>   可以注意到PRI和stall模式完成缺页的区别是，PRI缺页的时候并不是在IO实际发生的<br>   时候，因为如果PRI response表示PRI请求失败，硬件完全可以不发起后续的IO操作。<br>   而stall模式，完全发生在IO请求的途中。所以，他被叫做stall模式。</p>
<h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct device</span><br><span class="line">        +-&gt; struct iommu_param</span><br><span class="line">                +-&gt; struct iommu_fault_param</span><br><span class="line">                        +-&gt; handler(struct iommu_fault, void *)</span><br><span class="line">                        +-&gt; faults list</span><br><span class="line">                +-&gt; struct iopf_device_param</span><br><span class="line">                     +-&gt; struct iopf_queue</span><br><span class="line">                             +-&gt; work queue</span><br><span class="line">                             +-&gt; devices list</span><br><span class="line">                     +-&gt; wait queue</span><br><span class="line">                +-&gt; struct iommu_sva_param</span><br></pre></td></tr></table></figure>
<p>  在引起缺页的外设的device里，需要添加缺页相关的数据结构。handler是缺页要执行<br>  的函数，具体见下面动态流程分析。iopf_queue在smmu驱动初始化时添加，这里iopf_queue<br>  可能是eventq对应的，也可能是priq对应的，iopf_queue是smmu里的概念，所以同一个<br>  iopf_queue会服务同一个smmu下的所有外设, 上面的devices list链表就是用来收集这个<br>  smmu下的外设。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">struct iommu_bond</span><br><span class="line">      +-&gt; struct iommu_sva</span><br><span class="line">              +-&gt; device</span><br><span class="line">              +-&gt; struct iommu_sva_ops</span><br><span class="line">                      +-&gt; iommu_mmu_exit_handle ?</span><br><span class="line">      +-&gt; struct io_mm</span><br><span class="line">              +-&gt; pasid</span><br><span class="line">              +-&gt; device list</span><br><span class="line">              +-&gt; mm</span><br><span class="line">              +-&gt; mmu_notifier</span><br><span class="line">              +-&gt; iommu_ops</span><br><span class="line">                      +-&gt; attach</span><br><span class="line">                      +-&gt; dettach</span><br><span class="line">                      +-&gt; invalidat</span><br><span class="line">                      +-&gt; release</span><br><span class="line">      +-&gt; mm list</span><br><span class="line">      +-&gt; device list</span><br><span class="line">      +-&gt; wait queue</span><br></pre></td></tr></table></figure>
<p>  下面的图引用自JPB的补丁，该图描述的是建立好的静态数据结构之间的关系，以及IOMMU<br>  (e.g. SMMU的STE和CD表在这种数据结构下的具体配置情况)表格的配置。用这个图可以<br>  很好说明iommu_bond中的各个数据结构的意义以及之间的关系。</p>
<p>  iommu_bond这个结构并不对外, 用下面的iommu_sva_bind_device/unbind接口时，函数<br>  参数都是iommu_sva。使用SVA的设备可以把一个设备的一些资源和一个进程地址空间绑定，<br>  这种绑定关系是灵活的，比如可以一个设备上的不同资源和不用的进程地址空间绑定<br>  (bond 1, bond 2), 还可以同一个设备上的资源都绑定在一个进程的地址空间上(bond 3,<br>  bond 4)。从进程地址空间的角度看，一个进程地址空间可能和多个设备资源绑定。</p>
<p>  iommu_bond指的就是一个绑定，io_mm指的是绑定了外设资源的一个进程地址空间。<br>  io_pgtables是指内核dma接口申请内存的页表。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        ___________________________</span><br><span class="line">       |  IOMMU domain A           |</span><br><span class="line">       |  ________________         |</span><br><span class="line">       | |  IOMMU group   |        +------- io_pgtables</span><br><span class="line">       | |                |        |</span><br><span class="line">       | |   dev 00:00.0 ----+------- bond 1 --- io_mm X</span><br><span class="line">       | |________________|   \    |</span><br><span class="line">       |                       &#x27;----- bond 2 ---.</span><br><span class="line">       |___________________________|             \</span><br><span class="line">        ___________________________               \</span><br><span class="line">       |  IOMMU domain B           |             io_mm Y</span><br><span class="line">       |  ________________         |             / /</span><br><span class="line">       | |  IOMMU group   |        |            / /</span><br><span class="line">       | |                |        |           / /</span><br><span class="line">       | |   dev 00:01.0 ------------ bond 3 -&#x27; /</span><br><span class="line">       | |   dev 00:01.1 ------------ bond 4 --&#x27;</span><br><span class="line">       | |________________|        |</span><br><span class="line">       |                           +------- io_pgtables</span><br><span class="line">       |___________________________|</span><br><span class="line"></span><br><span class="line">                          PASID tables</span><br><span class="line">                           of domain A</span><br><span class="line">                        .-&gt;+--------+</span><br><span class="line">                       / 0 |        |-------&gt; io_pgtable</span><br><span class="line">                      /    +--------+</span><br><span class="line">      Device tables  /   1 |        |-------&gt; pgd X</span><br><span class="line">        +--------+  /      +--------+</span><br><span class="line">00:00.0 |      A |-&#x27;     2 |        |--.</span><br><span class="line">        +--------+         +--------+   \</span><br><span class="line">        :        :       3 |        |    \</span><br><span class="line">        +--------+         +--------+     --&gt; pgd Y</span><br><span class="line">00:01.0 |      B |--.                    /</span><br><span class="line">        +--------+   \                  |</span><br><span class="line">00:01.1 |      B |----+   PASID tables  |</span><br><span class="line">        +--------+     \   of domain B  |</span><br><span class="line">                        &#x27;-&gt;+--------+   |</span><br><span class="line">                         0 |        |-- | --&gt; io_pgtable</span><br><span class="line">                           +--------+   |</span><br><span class="line">                         1 |        |   |</span><br><span class="line">                           +--------+   |</span><br><span class="line">                         2 |        |---&#x27;</span><br><span class="line">                           +--------+</span><br><span class="line">                         3 |        |</span><br><span class="line">                           +--------+</span><br></pre></td></tr></table></figure>

<p>  相关接口:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">- iommu_dev_enable_feature: 准备和sva相关的smmu中的管理结构, 该接口可以在</span><br><span class="line">                                设备驱动里调用，用来使能sva的功能</span><br><span class="line">  +-&gt; arm_smmu_dev_enable_feature</span><br><span class="line">      +-&gt; arm_smmu_dev_enable_sva</span><br><span class="line">        +-&gt; iommu_sva_enable</span><br><span class="line">          +-&gt; iommu_register_device_fault_handler(dev, iommu_queue_iopf, dev)</span><br><span class="line">              /* 动态部分将执行iommu_queue_iopf */</span><br><span class="line">              把iommu_queue_iopf赋值给iommu_fault_param里的handler                  </span><br><span class="line">        +-&gt; iopf_queue_add_device(struct iopf_queue, dev)</span><br><span class="line">            把相应的iopf_queue赋值给iopf_device_param里的iopf_queue, 这里有</span><br><span class="line">            pri对应的iopf_queue或者是stall mode对应的iopf_queue。初始化</span><br><span class="line">            iopf_device_param里的wait queue</span><br><span class="line"></span><br><span class="line">            对应iopf_queue的初始化在在smmu驱动probe流程中: e.g.</span><br><span class="line">            arm_smmu_init_queues</span><br><span class="line">              +-&gt; smmu-&gt;prq.iopf = iopf_queue_alloc</span><br><span class="line">                                       +-&gt; alloc_workqueue</span><br><span class="line">                                         分配以及初始化iopf_queue里的工作队列</span><br><span class="line">        +-&gt; arm_smmu_enable_pri</span><br><span class="line">            调用PCI函数是能EP设备的PRI功能</span><br><span class="line"></span><br><span class="line">- iommu_sva_bind_device: 将设备和mm绑定, 该接口可以在设备驱动里调用，把一个</span><br><span class="line">                         设备和mm绑定在一起。返回struct iommu_sva *</span><br><span class="line">  +-&gt; iommu_sva_bind_group</span><br><span class="line">    +-&gt; iommu_group_do_bind_dev</span><br><span class="line">      +-&gt; arm_smmu_sva_bind</span><br><span class="line">        +-&gt; arm_smmu_alloc_shared_cd(mm)</span><br><span class="line">            分配相应的CD表项，并且把CD表项里的页表地址指向mm里保存的进程页表</span><br><span class="line">            地址。这个函数主要配置SMMU的硬件</span><br><span class="line">        +-&gt; iommu_sva_bind_generic(dev, mm, cd, &amp;arm_smmu_mm_ops, drvdata)</span><br><span class="line">          +-&gt; io_mm_alloc</span><br><span class="line">              分配io_mm以及初始化其中的数据域段, 向mm注册io_mm的notifier</span><br><span class="line">              /* to do: mm发生变化的时候通知io_mm */</span><br><span class="line">              +-&gt; mmu_notifier_register</span><br><span class="line">          +-&gt; io_mm_attach</span><br><span class="line">            +-&gt; init_waitqueue_head</span><br><span class="line">                初始化iommu_bond里的等待队列mm_exit_wq /* to do: 作用 */</span><br><span class="line">            +-&gt; io_mm-&gt;ops-&gt;attach(bond-&gt;sva.dev, io_mm-&gt;pasid, io_mm-&gt;ctx)</span><br><span class="line">                调用e.g.SMMU arm_smmu_mm_ops里的attach函数</span><br><span class="line">                +-&gt; arm_smmu_mm_attach</span><br><span class="line">                  +-&gt; __arm_smmu_write_ctx_desc</span><br><span class="line">                    下发SMMU command使能CD配置。可以看到arm_smmu_mm_ops里的</span><br><span class="line">                    这一组回调函数基本都是下发SMMU命令控制CD/ATC/TLB相关的</span><br><span class="line">                    配置</span><br><span class="line"></span><br><span class="line">- iommu_sva_unbind_device</span><br><span class="line">    +-&gt; iopf_queue_flush_dev</span><br><span class="line">    +-&gt; iommu_unbind_locked(to_iommu_bond(handle))</span><br><span class="line">    这里的handle是一个struct iommu_sva</span><br></pre></td></tr></table></figure>
<pre><code>- iommu_sva_set_ops(iommu_sva, iommu_sva_ops)

  这个接口把iommu_sva_ops传递给iommu_sva, iommu_sva_ops包含mm_exit回调。
  在上面的iommu_sva_bind_generic里会调用mmu_notifier_register给当前的mm里
  注册一个notifier，在mm发生改变的时候，mm子系统可以触发notifier里的回调
  函数。当前的代码里，是在notifier的release回调里去调用iommu_sva里保存的
  mm_exit回调函数。

  SVA特性使得设备可以看到进程虚拟地址空间，这样在进程虚拟地址空间销毁的时候
  应该调用设备驱动提供的函数停止设备继续访问进程虚拟地址空间。这里iommu_sva_set_ops
  就是把设备驱动的回调函数注册给进程mm。

  注意上面的mmu_notifier_register注册的iommu_mmu_notifier_ops回调里。release
  只在进程异常时调用到，用户态进程正常退出时并不会调用。在进程正常退出时，
  如何保证设备停止访问将要释放的进程地址空间，这里还有疑问。

  进程退出的调用链是:
  kernel/exit.c:
  do_exit
    +-&gt; exit_mm
  +-&gt; mmput
    +-&gt; exit_mmap
      +-&gt; mmu_notifier_release

- iommu_sva_get_pasid

  这个接口返回返回smmu_sva对应的pasid数值，设备驱动需要把pasid配置给与这个
  smmu_sva相关的硬件资源。
</code></pre>
<p>  需要使用SVA特性的社区驱动在调用上面的接口后，可以建立起静态的数据结构。</p>
<h2 id="动态分析"><a href="#动态分析" class="headerlink" title="动态分析"></a>动态分析</h2><hr>
<ul>
<li><p>缺页流程</p>
<p>当一个PRI或者是一个stall event上报后, 软件会在缺页流程里建立页表，然后控制<br>SMMU给设备返送reponse信息。我们可以从SMMU PRI queue或者是event queue的中断<br>处理流程入手跟踪: e.g.PRI中断流程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">devm_request_threaded_irq(..., arm_smmu_priq_thread, ...)</span><br><span class="line">arm_smmu_priq_thread</span><br><span class="line">  +-&gt; arm_smmu_handle_ppr</span><br><span class="line">    +-&gt; iommu_report_device_fault</span><br><span class="line">      +-&gt; iommu_fault_param-&gt;handler</span><br><span class="line">        +-&gt; iommu_queue_iopf /* 初始化参见上面第2部分 */</span><br><span class="line">          +-&gt; iopf_group = kzalloc</span><br><span class="line">          +-&gt; list_add(faults list in group, fault)</span><br><span class="line">          +-&gt; INIT_WORK(&amp;group-&gt;work, iopf_handle_group)</span><br><span class="line">          +-&gt; queue_work(iopf_param-&gt;queue-&gt;wq, &amp;group-&gt;work)</span><br><span class="line">          这段代码创建缺页的group，并把当前的缺页请求挂入group里的链表，然后</span><br><span class="line">          创建一个任务，并调度这个任务运行</span><br><span class="line"></span><br><span class="line">          在工作队列线程中:</span><br><span class="line">          +-&gt; iopf_handle_group</span><br><span class="line">            +-&gt; iopf_handle_single</span><br><span class="line">              +-&gt; handle_mm_fault</span><br><span class="line">                  这里会最终申请内存并建立页表</span><br><span class="line"></span><br><span class="line">    +-&gt; arm_smmu_page_response</span><br><span class="line">        软件执行完缺页流程后，软件控制SMMU向设备回响应。</span><br></pre></td></tr></table></figure></li>
<li><p>Invalid流程</p>
<p>当软件释放申请的内存时，SMMU中关于这些内存的tlb以及设备ATC里都要Invalid。<br>进程mm变动的时候，调用注册的io_mm notifier完成相关的tlb、atc的invalid。</p>
<p>[…]</p>
</li>
</ul>
<h2 id="性能分析"><a href="#性能分析" class="headerlink" title="性能分析"></a>性能分析</h2><hr>
<pre><code>[...]
</code></pre>
<h2 id="虚拟化"><a href="#虚拟化" class="headerlink" title="虚拟化"></a>虚拟化</h2><hr>
<p> <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NjYXJlY3Jvd19ieXIvYXJ0aWNsZS9kZXRhaWxzLzEwNDYwNjU3MQ==">https://blog.csdn.net/scarecrow_byr/article/details/104606571<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <tags>
        <tag>SMMU</tag>
        <tag>Linux内核</tag>
        <tag>iommu</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
</search>
